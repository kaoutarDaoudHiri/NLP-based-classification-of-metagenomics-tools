{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYtlnI5MNFAV",
    "outputId": "18f52638-74d2-4b68-fc04-5859e181d6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 12.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 65.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 83.7 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 62.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Collecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx7L4i8T1Qj4"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqzVfe1W1hFu",
    "outputId": "aa9ee6e8-549c-4567-89e3-6388eda81520"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#preprocessing:entry to tokens, map each token to integers\n",
    "from transformers import AutoTokenizer\n",
    "#checkpoint = \"giacomomiolo/electramed_base_scivocab_1M\"\n",
    "#checkpoint = \"google/electra-base-discriminator\"\n",
    "checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(checkpoint, output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1Z3DV_H1lsY"
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "pe1ILMke1Vkr",
    "outputId": "7e131f11-b7fd-45a3-f4ae-7a7a0382ede4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>text</th>\n",
       "      <th>Category_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gapped Blast</td>\n",
       "      <td>Gapped BLAST and PSI-BLAST: a new generation o...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>gap blast  psiblast  new generation  protein d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RapSearch</td>\n",
       "      <td>RAPSearch: a fast protein similarity search to...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>rapsearch  fast protein similarity search tool...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PhenoMeter</td>\n",
       "      <td>PhenoMeter: A Metabolome Database Search Tool ...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>phenometer  metabolome database search tool us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cuBlASTp</td>\n",
       "      <td>cuBLASTP: Fine-Grained Parallelization of Prot...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>cublastp finegrained parallelization  protein ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muBLASTP</td>\n",
       "      <td>muBLASTP: database-indexed protein sequence se...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>mublastp databaseindexed protein sequence sear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File_Name  ... Category_Code\n",
       "0  Gapped Blast  ...             0\n",
       "1     RapSearch  ...             0\n",
       "2    PhenoMeter  ...             0\n",
       "3      cuBlASTp  ...             0\n",
       "4      muBLASTP  ...             0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "df=pd.read_csv(\"./Abstracts-Parsed.csv\", sep=',')\n",
    "#df=pd.read_csv(\"./Methods-Parsed.csv\", sep=',')\n",
    "df=df.rename(columns={'Content_Parsed': 'text'})\n",
    "df=df.drop(columns={'Unnamed: 0'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaAggKRWIXjD"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "for i in range (len(df['text'])):\n",
    "  df['text'][i] = unicodedata.normalize('NFKD', df['text'][i]).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "  df['text'][i] = re.sub(r'[^\\w]', ' ', df['text'][i])\n",
    "  df['text'][i] = df['text'][i].encode(\"ascii\", \"ignore\")\n",
    "  df['text'][i] = df['text'][i].decode()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR17omcw1luk",
    "outputId": "55129e09-52de-4834-c9cc-73ae5b666490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************item  0\n",
      "*****************item  1\n",
      "*****************item  2\n",
      "*****************item  3\n",
      "*****************item  4\n",
      "*****************item  5\n",
      "*****************item  6\n",
      "*****************item  7\n",
      "*****************item  8\n",
      "*****************item  9\n",
      "*****************item  10\n",
      "*****************item  11\n",
      "*****************item  12\n",
      "*****************item  13\n",
      "*****************item  14\n",
      "*****************item  15\n",
      "*****************item  16\n",
      "*****************item  17\n",
      "*****************item  18\n",
      "*****************item  19\n",
      "*****************item  20\n",
      "*****************item  21\n",
      "*****************item  22\n",
      "*****************item  23\n",
      "*****************item  24\n",
      "*****************item  25\n",
      "*****************item  26\n",
      "*****************item  27\n",
      "*****************item  28\n",
      "*****************item  29\n",
      "*****************item  30\n",
      "*****************item  31\n",
      "*****************item  32\n",
      "*****************item  33\n",
      "*****************item  34\n",
      "*****************item  35\n",
      "*****************item  36\n",
      "*****************item  37\n",
      "*****************item  38\n",
      "*****************item  39\n",
      "*****************item  40\n",
      "*****************item  41\n",
      "*****************item  42\n",
      "*****************item  43\n",
      "*****************item  44\n",
      "*****************item  45\n",
      "*****************item  46\n",
      "*****************item  47\n",
      "*****************item  48\n",
      "*****************item  49\n",
      "*****************item  50\n",
      "*****************item  51\n",
      "*****************item  52\n",
      "*****************item  53\n",
      "*****************item  54\n",
      "*****************item  55\n",
      "*****************item  56\n",
      "*****************item  57\n",
      "*****************item  58\n",
      "*****************item  59\n",
      "*****************item  60\n",
      "*****************item  61\n",
      "*****************item  62\n",
      "*****************item  63\n",
      "*****************item  64\n",
      "*****************item  65\n",
      "*****************item  66\n",
      "*****************item  67\n",
      "*****************item  68\n",
      "*****************item  69\n",
      "*****************item  70\n",
      "*****************item  71\n",
      "*****************item  72\n",
      "*****************item  73\n",
      "*****************item  74\n",
      "*****************item  75\n",
      "*****************item  76\n",
      "*****************item  77\n",
      "*****************item  78\n",
      "*****************item  79\n",
      "*****************item  80\n",
      "*****************item  81\n",
      "*****************item  82\n",
      "*****************item  83\n",
      "*****************item  84\n",
      "*****************item  85\n",
      "*****************item  86\n",
      "*****************item  87\n",
      "*****************item  88\n",
      "*****************item  89\n",
      "*****************item  90\n",
      "*****************item  91\n",
      "*****************item  92\n",
      "*****************item  93\n",
      "*****************item  94\n",
      "*****************item  95\n",
      "*****************item  96\n",
      "*****************item  97\n",
      "*****************item  98\n",
      "*****************item  99\n",
      "*****************item  100\n",
      "*****************item  101\n",
      "*****************item  102\n",
      "*****************item  103\n",
      "*****************item  104\n",
      "*****************item  105\n",
      "*****************item  106\n",
      "*****************item  107\n",
      "*****************item  108\n",
      "*****************item  109\n",
      "*****************item  110\n",
      "*****************item  111\n",
      "*****************item  112\n",
      "*****************item  113\n",
      "*****************item  114\n",
      "*****************item  115\n",
      "*****************item  116\n",
      "*****************item  117\n",
      "*****************item  118\n",
      "*****************item  119\n",
      "*****************item  120\n",
      "*****************item  121\n",
      "*****************item  122\n",
      "*****************item  123\n",
      "*****************item  124\n",
      "*****************item  125\n",
      "*****************item  126\n",
      "*****************item  127\n",
      "*****************item  128\n",
      "*****************item  129\n",
      "*****************item  130\n",
      "*****************item  131\n",
      "*****************item  132\n",
      "*****************item  133\n",
      "*****************item  134\n",
      "*****************item  135\n",
      "*****************item  136\n",
      "*****************item  137\n",
      "*****************item  138\n",
      "*****************item  139\n",
      "*****************item  140\n",
      "*****************item  141\n",
      "*****************item  142\n",
      "*****************item  143\n",
      "*****************item  144\n",
      "*****************item  145\n",
      "*****************item  146\n",
      "*****************item  147\n",
      "*****************item  148\n",
      "*****************item  149\n",
      "*****************item  150\n",
      "*****************item  151\n",
      "*****************item  152\n",
      "*****************item  153\n",
      "*****************item  154\n",
      "*****************item  155\n",
      "*****************item  156\n",
      "*****************item  157\n",
      "*****************item  158\n",
      "*****************item  159\n",
      "*****************item  160\n",
      "*****************item  161\n",
      "*****************item  162\n",
      "*****************item  163\n",
      "*****************item  164\n",
      "*****************item  165\n",
      "*****************item  166\n",
      "*****************item  167\n",
      "*****************item  168\n",
      "*****************item  169\n",
      "*****************item  170\n",
      "*****************item  171\n",
      "*****************item  172\n",
      "*****************item  173\n",
      "*****************item  174\n",
      "*****************item  175\n",
      "*****************item  176\n",
      "*****************item  177\n",
      "*****************item  178\n",
      "*****************item  179\n",
      "*****************item  180\n",
      "*****************item  181\n",
      "*****************item  182\n",
      "*****************item  183\n",
      "*****************item  184\n",
      "*****************item  185\n",
      "*****************item  186\n",
      "*****************item  187\n",
      "*****************item  188\n",
      "*****************item  189\n",
      "*****************item  190\n",
      "*****************item  191\n",
      "*****************item  192\n",
      "*****************item  193\n",
      "*****************item  194\n",
      "*****************item  195\n",
      "*****************item  196\n",
      "*****************item  197\n",
      "*****************item  198\n",
      "*****************item  199\n",
      "*****************item  200\n",
      "*****************item  201\n",
      "*****************item  202\n",
      "*****************item  203\n",
      "*****************item  204\n",
      "*****************item  205\n",
      "*****************item  206\n",
      "*****************item  207\n",
      "*****************item  208\n",
      "*****************item  209\n",
      "*****************item  210\n",
      "*****************item  211\n",
      "*****************item  212\n",
      "*****************item  213\n",
      "*****************item  214\n",
      "*****************item  215\n",
      "*****************item  216\n",
      "*****************item  217\n",
      "*****************item  218\n",
      "*****************item  219\n",
      "*****************item  220\n",
      "*****************item  221\n",
      "*****************item  222\n",
      "*****************item  223\n"
     ]
    }
   ],
   "source": [
    "# Getting embeddings for the target\n",
    "\n",
    "embeddings = []\n",
    "all_embeddings = []\n",
    "sentence_embedding = np.empty(768, dtype=object)\n",
    "c=0\n",
    "for text in df[\"text\"]:\n",
    "    print(\"*****************item \",c)\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    embeddings.append(list_token_embeddings)\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuDKh5AV_w52"
   },
   "outputs": [],
   "source": [
    "for l in range(len(embeddings)):\n",
    "  for v in embeddings[l]:\n",
    "    sentence_embedding=np.vstack((sentence_embedding, v))\n",
    "  sentence_embedding = np.delete(sentence_embedding, obj=0, axis=0)\n",
    "  sentence_embedding = (np.mean(sentence_embedding, axis=0)).tolist()\n",
    "  all_embeddings.append(sentence_embedding)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_embeddings = pd.DataFrame(all_embeddings)\n",
    "\n",
    "all_embeddings.insert(loc=0, column='text', value=df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObMQAh3flXQ6"
   },
   "outputs": [],
   "source": [
    "#all_embeddings.to_csv('AbstractsMethodsROBERTAstsb.csv') \n",
    "#files.download(\"AbstractsMethodsROBERTAstsb.csv\")\n",
    "all_embeddings.to_csv('Abstracts-BioBERT-NoSL.csv') \n",
    "files.download(\"Abstracts-BioBERT-NoSL.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERTbased-NoSL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
