{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Trained_Models/abstracts only/BIOBERTabs-nsl/best_BIOBERT-nsl_lrc.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAVISUNK: Genome assembly validation via inter-SUNK distances in Oxford Nanopore reads\n",
      " Highly contiguous de novo genome assemblies are now feasible for large numbers of species and individuals. Methods are needed to validate assembly accuracy and detect misassemblies with orthologous sequencing data to allow for confident downstream analyses. We developed GAVISUNK, an open-source pipeline that detects misassemblies and produces a set of reliable regions genome-wide by assessing concordance of distances between unique k-mers in Pacific Biosciences high-fidelity (HiFi) assemblies and raw Oxford Nanopore Technologies reads.\n"
     ]
    }
   ],
   "source": [
    "with open('examples/GAVISUNK.txt') as f:\n",
    "    test_abstract = f.read()\n",
    "    print(test_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "test_abstract = test_abstract.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing links\n",
    "regex_link = r\"\\bhttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b\"\n",
    "test_abstract = test_abstract.replace(regex_link, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing numbers\n",
    "regex_nums = r\"\\b[0-9][0-9]*\\b\"\n",
    "test_abstract = test_abstract.replace(regex_nums, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing special characters\n",
    "special_character = list(\"←=()[]/‘’|><\\\\∼+%$&×–−-·\")\n",
    "for spec_char in special_character:\n",
    "    test_abstract = test_abstract.replace(spec_char, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "for punct_sign in punctuation_signs:\n",
    "    test_abstract = test_abstract.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing strings with length 1-2\n",
    "regex_short = r\"\\b\\w{0,2}\\b\"\n",
    "test_abstract = test_abstract.replace(regex_short, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing strings starting with numbers\n",
    "regex_short = r\"\\b[0-9][0-9]*\\w\\b\"\n",
    "test_abstract = test_abstract.replace(regex_short, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gavisunk genome assembly validation via intersunk distances in oxford nanopore reads\n",
      " highly contiguous de novo genome assemblies are now feasible for large numbers of species and individuals methods are needed to validate assembly accuracy and detect misassemblies with orthologous sequencing data to allow for confident downstream analyses we developed gavisunk an opensource pipeline that detects misassemblies and produces a set of reliable regions genomewide by assessing concordance of distances between unique kmers in pacific biosciences highfidelity hifi assemblies and raw oxford nanopore technologies reads\n"
     ]
    }
   ],
   "source": [
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Iterating through every word to lemmatize\n",
    "lemmatized_text_list = []\n",
    "lemmatized_list = []\n",
    "text_words = test_abstract.split(\" \")\n",
    "# Iterate through every word to lemmatize\n",
    "for word in text_words:\n",
    "    lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "# Join the list\n",
    "lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "# Append to the list containing the texts\n",
    "lemmatized_text_list.append(lemmatized_text)\n",
    "df=pd.DataFrame(lemmatized_text_list,columns=[\"text\"])\n",
    "df[\"text\"] = df[\"text\"].replace(\"'s\", \"\")\n",
    "\n",
    "# removing possessive pronoun terminations\n",
    "#lemmatized_text_list = lemmatized_text_list.replace(\"'s\", \"\")\n",
    "# removing english stop words\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# looping through all stop words\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df[\"text\"] = df[\"text\"].replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#preprocessing:entry to tokens, map each token to integers\n",
    "checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "\n",
    "model = BertModel.from_pretrained(checkpoint, output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile('<title>(.*)</title>')\n",
    "df['text'][0] = unicodedata.normalize('NFKD', df['text'][0]).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "df['text'][0] = re.sub(r'[^\\w]', ' ', df['text'][0])\n",
    "df['text'][0] = df['text'][0].encode(\"ascii\", \"ignore\")\n",
    "df['text'][0] = df['text'][0].decode()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings for the target word in all given contexts\n",
    "embeddings = []\n",
    "all_embeddings = []\n",
    "sentence_embedding = np.empty(768, dtype=object)\n",
    "c=0\n",
    "\n",
    "tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(df['text'][0], tokenizer)\n",
    "list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "embeddings.append(list_token_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(embeddings)):\n",
    "    for v in embeddings[l]:\n",
    "        sentence_embedding=np.vstack((sentence_embedding, v))\n",
    "    sentence_embedding = np.delete(sentence_embedding, obj=0, axis=0)\n",
    "    sentence_embedding = (np.mean(sentence_embedding, axis=0)).tolist()\n",
    "    all_embeddings.append(sentence_embedding)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_embeddings = pd.DataFrame(all_embeddings)\n",
    "\n",
    "all_embeddings.insert(loc=0, column='text', value=df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gavisunk genome assembly validation via inters...</td>\n",
       "      <td>0.421444</td>\n",
       "      <td>-0.090017</td>\n",
       "      <td>-0.219932</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>0.164829</td>\n",
       "      <td>-0.303141</td>\n",
       "      <td>0.080285</td>\n",
       "      <td>0.183378</td>\n",
       "      <td>-0.027494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077531</td>\n",
       "      <td>0.12741</td>\n",
       "      <td>-0.212417</td>\n",
       "      <td>-0.26115</td>\n",
       "      <td>0.108368</td>\n",
       "      <td>-0.106858</td>\n",
       "      <td>-0.093772</td>\n",
       "      <td>0.254505</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.120043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         0         1  \\\n",
       "0  gavisunk genome assembly validation via inters...  0.421444 -0.090017   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -0.219932  0.168668  0.164829 -0.303141  0.080285  0.183378 -0.027494  ...   \n",
       "\n",
       "        758      759       760      761       762       763       764  \\\n",
       "0 -0.077531  0.12741 -0.212417 -0.26115  0.108368 -0.106858 -0.093772   \n",
       "\n",
       "        765       766       767  \n",
       "0  0.254505  0.060753  0.120043  \n",
       "\n",
       "[1 rows x 769 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'Sequence alignment': 0,\n",
    "    'Taxonomic classification': 1,\n",
    "    'Virus detection': 2,\n",
    "    'Virus identification': 3,\n",
    "    'Mapping': 4,\n",
    "    'Sequence assembly': 5,\n",
    "    'RNA-seq quantification for abundance estimation': 6,\n",
    "    'Sequence trimming': 7,\n",
    "    'Sequencing quality control': 8,\n",
    "    'Sequence annotation' : 9,\n",
    "    'SNP-Discovery' : 10,\n",
    "    'Visualization' : 11,\n",
    "    'Sequence assembly validation' : 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main task of this tool is: {'Sequence assembly validation'}\n"
     ]
    }
   ],
   "source": [
    "value = {i for i in category_codes if category_codes[i]==res}\n",
    "print(\"The main task of this tool is:\",value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
