,File_Name,Content,Category,Content_Parsed,Category_Code
0,Gapped Blast,"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",Alignment,"gap blast  psiblast  new generation  protein database search programs
 blast program  widely use tool  search protein  dna databases  sequence similarities  protein comparisons  variety  definitional algorithmic  statistical refinements describe  permit  execution time   blast program   decrease substantially  enhance  sensitivity  weak similarities  new criterion  trigger  extension  word hit combine   new heuristic  generate gap alignments yield  gap blast program  run  approximately three time  speed   original  addition  method  introduce  automatically combine statistically significant alignments produce  blast   positionspecific score matrix  search  database use  matrix  result positionspecific iterate blast psiblast program run  approximately   speed per iteration  gap blast   many case  much  sensitive  weak  biologically relevant sequence similarities psiblast  use  uncover several new  interest members   brct superfamily",0
1,RapSearch,"RAPSearch: a fast protein similarity search tool for short reads
Next Generation Sequencing (NGS) is producing enormous corpuses of short DNA reads, affecting emerging fields like metagenomics. Protein similarity search--a key step to achieve annotation of protein-coding genes in these short reads, and identification of their biological functions--faces daunting challenges because of the very sizes of the short read datasets. We developed a fast protein similarity search tool RAPSearch that utilizes a reduced amino acid alphabet and suffix array to detect seeds of flexible length. For short reads (translated in 6 frames) we tested, RAPSearch achieved ~20-90 times speedup as compared to BLASTX. RAPSearch missed only a small fraction (~1.3-3.2%) of BLASTX similarity hits, but it also discovered additional homologous proteins (~0.3-2.1%) that BLASTX missed. By contrast, BLAT, a tool that is even slightly faster than RAPSearch, had significant loss of sensitivity as compared to RAPSearch and BLAST. It enables faster protein similarity search. The application of RAPSearch in metageomics has also been demonstrated.",Alignment,"rapsearch  fast protein similarity search tool  short reads
next generation sequence ngs  produce enormous corpuses  short dna read affect emerge field like metagenomics protein similarity searcha key step  achieve annotation  proteincoding genes   short read  identification   biological functionsfaces daunt challenge     size   short read datasets  develop  fast protein similarity search tool rapsearch  utilize  reduce amino acid alphabet  suffix array  detect seed  flexible length  short read translate   frame  test rapsearch achieve ~ time speedup  compare  blastx rapsearch miss   small fraction ~  blastx similarity hit   also discover additional homologous proteins ~  blastx miss  contrast blat  tool   even slightly faster  rapsearch  significant loss  sensitivity  compare  rapsearch  blast  enable faster protein similarity search  application  rapsearch  metageomics  also  demonstrate",0
2,PhenoMeter,"PhenoMeter: A Metabolome Database Search Tool Using Statistical Similarity Matching of Metabolic Phenotypes for High-Confidence Detection of Functional Links
This article describes PhenoMeter (PM), a new type of metabolomics database search that accepts metabolite response patterns as queries and searches the MetaPhen database of reference patterns for responses that are statistically significantly similar or inverse for the purposes of detecting functional links. To identify a similarity measure that would detect functional links as reliably as possible, we compared the performance of four statistics in correctly top-matching metabolic phenotypes of Arabidopsis thaliana metabolism mutants affected in different steps of the photorespiration metabolic pathway to reference phenotypes of mutants affected in the same enzymes by independent mutations. The best performing statistic, the PM score, was a function of both Pearson correlation and Fisher’s Exact Test of directional overlap. This statistic outperformed Pearson correlation, biweight midcorrelation and Fisher’s Exact Test used alone. To demonstrate general applicability, we show that the PM reliably retrieved the most closely functionally linked response in the database when queried with responses to a wide variety of environmental and genetic perturbations. Attempts to match metabolic phenotypes between independent studies were met with varying success and possible reasons for this are discussed. Overall, our results suggest that integration of pattern-based search tools into metabolomics databases will aid functional annotation of newly recorded metabolic phenotypes analogously to the way sequence similarity search algorithms have aided the functional annotation of genes and proteins.",Alignment,"phenometer  metabolome database search tool use statistical similarity match  metabolic phenotypes  highconfidence detection  functional links
 article describe phenometer   new type  metabolomics database search  accept metabolite response pattern  query  search  metaphen database  reference pattern  responses   statistically significantly similar  inverse   purpose  detect functional link  identify  similarity measure  would detect functional link  reliably  possible  compare  performance  four statistics  correctly topmatching metabolic phenotypes  arabidopsis thaliana metabolism mutants affect  different step   photorespiration metabolic pathway  reference phenotypes  mutants affect    enzymes  independent mutations  best perform statistic   score   function   pearson correlation  fishers exact test  directional overlap  statistic outperform pearson correlation biweight midcorrelation  fishers exact test use alone  demonstrate general applicability  show    reliably retrieve   closely functionally link response   database  query  responses   wide variety  environmental  genetic perturbations attempt  match metabolic phenotypes  independent study  meet  vary success  possible reason    discuss overall  result suggest  integration  patternbased search tool  metabolomics databases  aid functional annotation  newly record metabolic phenotypes analogously   way sequence similarity search algorithms  aid  functional annotation  genes  proteins",0
3,cuBlASTp,"cuBLASTP: Fine-Grained Parallelization of Protein Sequence Search on a GPU
BLAST, short for Basic Local Alignment Search Tool, is a fundamental algorithm in the life sciences that compares biological sequences. However, with the advent of next-generation sequencing (NGS) and increase in sequence read-lengths, whether at the outset or downstream from NGS, the exponential growth of sequence databases is arguably outstripping our ability to analyze the data. Though several recent studies have utilized the graphics processing unit (GPU) to speedup the BLAST algorithm for searching protein sequences (i.e., BLASTP), these studies used coarse-grained parallel approaches, where one sequence alignment is mapped to only one thread. Moreover, due to the irregular memory access patterns in BLASTP, there remain significant challenges to map the most time-consuming phases (i.e., hit detection and ungapped extension) to the GPU using a fine-grained multithreaded approach. To address the above issues, we propose cuBLASTP, an efficient fine-grained BLASTP implementation for the GPU using CUDA. Our cuBLASTP realization encompasses many research contributions, including (1) memory-access reordering to reorder hits from column-major order to diagonal-major order, (2) position-based indexing to map a hit with a packed data structure to a bin, (3) aggressive hit filtering to eliminate hits beyond the threshold distance along the diagonal, (4) diagonal-based parallelism and hit-based parallelism for ungapped extension to extend sequences with different lengths in databases, and (5) hierarchical buffering to reduce memory-access overhead for the core data structures. The experimental results show that on a NVIDIA Kepler GPU, cuBLASTP delivers up to a 5.0-fold speedup over sequential FSA-BLAST and a 3.7-fold speedup over multithreaded NCBI-BLAST for the overall program execution. In addition, compared with GPU-BLASTP (the fastest GPU implementation of BLASTP to date), cuBLASTP achieves up to a 2.8-fold speedup for the kernel execution on the GPU and a 1.8-fold speedup for the overall program execution.",Alignment,"cublastp finegrained parallelization  protein sequence search   gpu
blast short  basic local alignment search tool   fundamental algorithm   life sciences  compare biological sequence however   advent  nextgeneration sequence ngs  increase  sequence readlengths whether   outset  downstream  ngs  exponential growth  sequence databases  arguably outstrip  ability  analyze  data though several recent study  utilize  graphics process unit gpu  speedup  blast algorithm  search protein sequence  blastp  study use coarsegrained parallel approach  one sequence alignment  map   one thread moreover due   irregular memory access pattern  blastp  remain significant challenge  map   timeconsuming phase  hit detection  ungapped extension   gpu use  finegrained multithreaded approach  address   issue  propose cublastp  efficient finegrained blastp implementation   gpu use cuda  cublastp realization encompass many research contributions include  memoryaccess reorder  reorder hit  columnmajor order  diagonalmajor order  positionbased index  map  hit   pack data structure   bin  aggressive hit filter  eliminate hit beyond  threshold distance along  diagonal  diagonalbased parallelism  hitbased parallelism  ungapped extension  extend sequence  different lengths  databases   hierarchical buffer  reduce memoryaccess overhead   core data structure  experimental result show    nvidia kepler gpu cublastp deliver    fold speedup  sequential fsablast   fold speedup  multithreaded ncbiblast   overall program execution  addition compare  gpublastp  fastest gpu implementation  blastp  date cublastp achieve    fold speedup   kernel execution   gpu   fold speedup   overall program execution",0
4,muBLASTP,"muBLASTP: database-indexed protein sequence search on multicore CPUs
The Basic Local Alignment Search Tool (BLAST) is a fundamental program in the life sciences that searches databases for sequences that are most similar to a query sequence. Currently, the BLAST algorithm utilizes a query-indexed approach. Although many approaches suggest that sequence search with a database index can achieve much higher throughput (e.g., BLAT, SSAHA, and CAFE), they cannot deliver the same level of sensitivity as the query-indexed BLAST, i.e., NCBI BLAST, or they can only support nucleotide sequence search, e.g., MegaBLAST. Due to different challenges and characteristics between query indexing and database indexing, the existing techniques for query-indexed search cannot be used into database indexed search. muBLASTP, a novel database-indexed BLAST for protein sequence search, delivers identical hits returned to NCBI BLAST. On Intel Haswell multicore CPUs, for a single query, the single-threaded muBLASTP achieves up to a 4.41-fold speedup for alignment stages, and up to a 1.75-fold end-to-end speedup over single-threaded NCBI BLAST. For a batch of queries, the multithreaded muBLASTP achieves up to a 5.7-fold speedups for alignment stages, and up to a 4.56-fold end-to-end speedup over multithreaded NCBI BLAST. With a newly designed index structure for protein database and associated optimizations in BLASTP algorithm, we re-factored BLASTP algorithm for modern multicore processors that achieves much higher throughput with acceptable memory footprint for the database index.",Alignment,"mublastp databaseindexed protein sequence search  multicore cpus
 basic local alignment search tool blast   fundamental program   life sciences  search databases  sequence    similar   query sequence currently  blast algorithm utilize  queryindexed approach although many approach suggest  sequence search   database index  achieve much higher throughput  blat ssaha  cafe  cannot deliver   level  sensitivity   queryindexed blast  ncbi blast     support nucleotide sequence search  megablast due  different challenge  characteristics  query index  database index  exist techniques  queryindexed search cannot  use  database index search mublastp  novel databaseindexed blast  protein sequence search deliver identical hit return  ncbi blast  intel haswell multicore cpus   single query  singlethreaded mublastp achieve    fold speedup  alignment stag     fold endtoend speedup  singlethreaded ncbi blast   batch  query  multithreaded mublastp achieve    fold speedups  alignment stag     fold endtoend speedup  multithreaded ncbi blast   newly design index structure  protein database  associate optimizations  blastp algorithm  refactored blastp algorithm  modern multicore processors  achieve much higher throughput  acceptable memory footprint   database index",0
5,Pauda,"A poor man’s BLASTX—high-throughput metagenomic protein database search using PAUDA
In the context of metagenomics, we introduce a new approach to protein database search called PAUDA, which runs ∼10 000 times faster than BLASTX, while achieving about one-third of the assignment rate of reads to KEGG orthology groups, and producing gene and taxon abundance profiles that are highly correlated to those obtained with BLASTX. PAUDA requires <80 CPU hours to analyze a dataset of 246 million Illumina DNA reads from permafrost soil for which a previous BLASTX analysis (on a subset of 176 million reads) reportedly required 800 000 CPU hours, leading to the same clustering of samples by functional profiles.",Alignment," poor man blastx—highthroughput metagenomic protein database search use pauda
  context  metagenomics  introduce  new approach  protein database search call pauda  run   time faster  blastx  achieve  onethird   assignment rate  read  kegg orthology group  produce gene  taxon abundance profile   highly correlate   obtain  blastx pauda require  cpu hours  analyze  dataset   million illumina dna read  permafrost soil    previous blastx analysis   subset   million read reportedly require   cpu hours lead    cluster  sample  functional profile",0
6,Diamond,"Fast and sensitive protein alignment using DIAMOND
The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20,000 times faster than BLASTX on short reads and has a similar degree of sensitivity.",Alignment,"fast  sensitive protein alignment use diamond
 alignment  sequence read   protein reference database   major computational bottleneck  metagenomics  dataintensive evolutionary project although recent tool offer improve performance   gold standard blastx  exhibit   modest speedup  low sensitivity  introduce diamond  opensource algorithm base  double index    time faster  blastx  short read    similar degree  sensitivity",0
7,Minimap2,"Minimap2: pairwise alignment for nucleotide sequences
Recent advances in sequencing technologies promise ultra-long reads of 100 kb in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms. Minimap2 is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of 100 bp in length, 1 kb genomic reads at error rate 15%, full-length noisy Direct RNA or cDNA reads and assembly contigs or closely related full chromosomes of hundreds of megabases in length. Minimap2 does split-read alignment, employs concave gap cost for long insertions and deletions and introduces new heuristics to reduce spurious alignments. It is 3–4 times as fast as mainstream short-read mappers at comparable accuracy, and is 30 times faster than longread genomic or cDNA mappers at higher accuracy, surpassing most aligners specialized in one type of alignment.",Alignment,"minimap2 pairwise alignment  nucleotide sequences
recent advance  sequence technologies promise ultralong read     average fulllength mrna  cdna read  high throughput  genomic contigs     length exist alignment program  unable  inefficient  process  data  scale  press   development  new alignment algorithms minimap2   generalpurpose alignment program  map dna  long mrna sequence   large reference database  work  accurate short read     length   genomic read  error rate  fulllength noisy direct rna  cdna read  assembly contigs  closely relate full chromosomes  hundreds  megabases  length minimap2  splitread alignment employ concave gap cost  long insertions  deletions  introduce new heuristics  reduce spurious alignments    time  fast  mainstream shortread mappers  comparable accuracy    time faster  longread genomic  cdna mappers  higher accuracy surpass  aligners specialize  one type  alignment",0
8,Bowtie,"Ultrafast and memory-efficient alignment of short DNA sequences to the human genome
Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds.",Alignment,"ultrafast  memoryefficient alignment  short dna sequence   human genome
bowtie   ultrafast memoryefficient alignment program  align short dna sequence read  large genomes   human genome burrowswheeler index allow bowtie  align    million read per cpu hour   memory footprint  approximately  gigabytes bowtie extend previous burrowswheeler techniques   novel qualityaware backtrack algorithm  permit mismatch multiple processor core   use simultaneously  achieve even greater alignment speed",0
9,HISAT2,"Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype
The human reference genome represents only a small number of individuals, which limits its usefulness for genotyping. We present a method named HISAT2 (hierarchical indexing for spliced alignment of transcripts 2) that can align both DNA and RNA sequences using a graph Ferragina Manzini index. We use HISAT2 to represent and search an expanded model of the human reference genome in which over 14.5 million genomic variants in combination with haplotypes are incorporated into the data structure used for searching and alignment. We benchmark HISAT2 using simulated and real datasets to demonstrate that our strategy of representing a population of genomes, together with a fast, memory-efficient search algorithm, provides more detailed and accurate variant analyses than other methods. We apply HISAT2 for HLA typing and DNA fingerprinting; both applications form part of the HISAT-genotype software that enables analysis of haplotype-resolved genes or genomic regions. HISAT-genotype outperforms other computational methods and matches or exceeds the performance of laboratory-based assays.",Alignment,"graphbased genome alignment  genotyping  hisat2  hisatgenotype
 human reference genome represent   small number  individuals  limit  usefulness  genotyping  present  method name hisat2 hierarchical index  splice alignment  transcripts    align  dna  rna sequence use  graph ferragina manzini index  use hisat2  represent  search  expand model   human reference genome     million genomic variants  combination  haplotypes  incorporate   data structure use  search  alignment  benchmark hisat2 use simulate  real datasets  demonstrate   strategy  represent  population  genomes together   fast memoryefficient search algorithm provide  detail  accurate variant analyse   methods  apply hisat2  hla type  dna fingerprint  applications form part   hisatgenotype software  enable analysis  haplotyperesolved genes  genomic regions hisatgenotype outperform  computational methods  match  exceed  performance  laboratorybased assay",0
10,STAR,"STAR: ultrafast universal RNA-seq aligner.
Accurate alignment of high-throughput RNA-seq data is a challenging and yet unsolved problem because of the non-contiguous transcript structure, relatively short read lengths and constantly increasing throughput of the sequencing technologies. Currently available RNA-seq aligners suffer from high mapping error rates, low mapping speed, read length limitation and mapping biases. To align our large (480 billon reads) ENCODE Transcriptome RNA-seq dataset, we developed the Spliced Transcripts Alignment to a Reference (STAR) software based on a previously undescribed RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. STAR outperforms other aligners by a factor of450 in mapping speed, aligning to the human genome 550 million 2 76 bp paired-end reads per hour on a modest 12-core server, while at the same time improving alignment sensitivity and precision. In addition to unbiased de novo detection of canonical junctions, STAR can discover non-canonical splices and chimeric (fusion) transcripts, and is also capable of mapping full-length RNA sequences. Using Roche 454 sequencing of reverse transcription polymerase chain reaction amplicons, we experimentally validated 1960 novel intergenic splice junctions with an 80–90% success rate, corroborating the high precision of the STAR mapping strategy",Alignment,"star ultrafast universal rnaseq aligner
accurate alignment  highthroughput rnaseq data   challenge  yet unsolved problem    noncontiguous transcript structure relatively short read lengths  constantly increase throughput   sequence technologies currently available rnaseq aligners suffer  high map error rat low map speed read length limitation  map bias  align  large  billon read encode transcriptome rnaseq dataset  develop  splice transcripts alignment   reference star software base   previously undescribed rnaseq alignment algorithm  use sequential maximum mappable seed search  uncompress suffix array follow  seed cluster  stitch procedure star outperform  aligners   factor of450  map speed align   human genome  million    pairedend read per hour   modest core server     time improve alignment sensitivity  precision  addition  unbiased  novo detection  canonical junctions star  discover noncanonical splice  chimeric fusion transcripts   also capable  map fulllength rna sequence use roche  sequence  reverse transcription polymerase chain reaction amplicons  experimentally validate  novel intergenic splice junctions    success rate corroborate  high precision   star map strategy",0
11,TopHat2,"TopHat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions
TopHat is a popular spliced aligner for RNA-sequence (RNA-seq) experiments. In this paper, we describe TopHat2, which incorporates many significant enhancements to TopHat. TopHat2 can align reads of various lengths produced by the latest sequencing technologies, while allowing for variable-length indels with respect to the reference genome. In addition to de novo spliced alignment, TopHat2 can align reads across fusion breaks, which can occur after genomic translocations. TopHat2 combines the ability to identify novel splice sites with direct mapping to known transcripts, producing sensitive and accurate alignments, even for highly repetitive genomes or in the presence of pseudogenes.",Alignment,"tophat2 accurate alignment  transcriptomes   presence  insertions deletions  gene fusions
tophat   popular splice aligner  rnasequence rnaseq experiment   paper  describe tophat2  incorporate many significant enhancements  tophat tophat2  align read  various lengths produce   latest sequence technologies  allow  variablelength indels  respect   reference genome  addition   novo splice alignment tophat2  align read across fusion break   occur  genomic translocations tophat2 combine  ability  identify novel splice sit  direct map  know transcripts produce sensitive  accurate alignments even  highly repetitive genomes    presence  pseudogenes",0
12,LordFast,"lordFAST: sensitive and Fast Alignment Search Tool for LOng noisy Read sequencing Data
Recent advances in genomics and precision medicine have been made possible through the application of high throughput sequencing (HTS) to large collections of human genomes. Although HTS technologies have proven their use in cataloging human genome variation, computational analysis of the data they generate is still far from being perfect. The main limitation of Illumina and other popular sequencing technologies is their short read length relative to the lengths of (common) genomic repeats. Newer (single molecule sequencing – SMS) technologies such as Pacific Biosciences and Oxford Nanopore are producing longer reads, making it theoretically possible to overcome the difficulties imposed by repeat regions. Unfortunately, because of their high sequencing error rate, reads generated by these technologies are very difficult to work with and cannot be used in many of the standard downstream analysis pipelines. Note that it is not only difficult to find the correct mapping locations of such reads in a reference genome, but also to establish their correct alignment so as to differentiate sequencing errors from real genomic variants. Furthermore, especially since newer SMS instruments provide higher throughput, mapping and alignment need to be performed much faster than before, maintaining high sensitivity. We introduce lordFAST, a novel 
long-read mapper that is specifically designed to align reads generated by PacBio and potentially other SMS technologies to a reference. lordFAST not only has higher sensitivity than the available alternatives, it is also among the fastest and has a very low memory footprint.",Alignment,"lordfast sensitive  fast alignment search tool  long noisy read sequence data
recent advance  genomics  precision medicine   make possible   application  high throughput sequence hts  large collections  human genomes although hts technologies  prove  use  catalog human genome variation computational analysis   data  generate  still far   perfect  main limitation  illumina   popular sequence technologies   short read length relative   lengths  common genomic repeat newer single molecule sequence  sms technologies   pacific biosciences  oxford nanopore  produce longer read make  theoretically possible  overcome  difficulties impose  repeat regions unfortunately    high sequence error rate read generate   technologies   difficult  work   cannot  use  many   standard downstream analysis pipelines note      difficult  find  correct map locations   read   reference genome  also  establish  correct alignment    differentiate sequence errors  real genomic variants furthermore especially since newer sms instrument provide higher throughput map  alignment need   perform much faster   maintain high sensitivity  introduce lordfast  novel 
longread mapper   specifically design  align read generate  pacbio  potentially  sms technologies   reference lordfast    higher sensitivity   available alternatives   also among  fastest     low memory footprint",0
13,BWAMEM,"Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM
BWA-MEM is a new alignment algorithm for aligning sequence reads or assembly contigs against a large reference genome such as human. It automatically chooses between local and end-to-end alignments, supports paired-end reads and performs chimeric alignment. The algorithm is robust to sequencing errors and applicable to a wide range of sequence lengths from 70bp to a few megabases. For mapping 100bp sequences, BWA-MEM shows better performance than several state-of-art read aligners to date.",Alignment,"align sequence read clone sequence  assembly contigs  bwamem
bwamem   new alignment algorithm  align sequence read  assembly contigs   large reference genome   human  automatically choose  local  endtoend alignments support pairedend read  perform chimeric alignment  algorithm  robust  sequence errors  applicable   wide range  sequence lengths  70bp    megabases  map 100bp sequence bwamem show better performance  several stateofart read aligners  date",0
14,Kart,"Kart: a divide-and-conquer algorithm for NGS read alignment 
Next-generation sequencing (NGS) provides a great opportunity to investigate genome-wide variation at nucleotide resolution. Due to the huge amount of data, NGS applications require very fast and accurate alignment algorithms. Most existing algorithms for read mapping basically adopt seed-and-extend strategy, which is sequential in nature and takes much longer time on longer reads. We develop a divide-and-conquer algorithm, called Kart, which can process long reads as fast as short reads by dividing a read into small fragments that can be aligned independently. Our experiment result indicates that the average size of fragments requiring the more time-consuming gapped alignment is around 20 bp regardless of the original read length. Furthermore, it can tolerate much higher error rates. The experiments show that Kart spends much less time on longer reads than other aligners and still produce reliable alignments even when the error rate is as high as 15%.",Alignment,"kart  divideandconquer algorithm  ngs read alignment 
nextgeneration sequence ngs provide  great opportunity  investigate genomewide variation  nucleotide resolution due   huge amount  data ngs applications require  fast  accurate alignment algorithms  exist algorithms  read map basically adopt seedandextend strategy   sequential  nature  take much longer time  longer read  develop  divideandconquer algorithm call kart   process long read  fast  short read  divide  read  small fragment    align independently  experiment result indicate   average size  fragment require   timeconsuming gap alignment  around   regardless   original read length furthermore   tolerate much higher error rat  experiment show  kart spend much less time  longer read   aligners  still produce reliable alignments even   error rate   high  ",0
15,NGMLR,"Accurate detection of complex structural variations using single molecule sequencing
Structural variations (SVs) are the largest source of genetic variation, but remain poorly understood because of limited genomics technology. Single molecule long-read sequencing from Pacific Biosciences and Oxford Nanopore has the potential to dramatically advance the field, although their high error rates challenge existing methods. Addressing this need, we introduce open-source methods for long-read alignment (NGMLR, https://github.com/philres/ngmlr) and SV identification (Sniffles, https://github.com/fritzsedlazeck/Sniffles) that enable unprecedented SV sensitivity and precision, including within repeat-rich regions and of complex nested events that can have significant impact on human disorders. Examining several datasets, including healthy and cancerous human genomes, we discover thousands of novel variants using long-reads and categorize systematic errors in short-read approaches. NGMLR and Sniffles are further able to automatically filter false events and operate on low amounts of coverage to address the cost factor that has hindered the application of long-reads in clinical and research settings.",Alignment,"accurate detection  complex structural variations use single molecule sequencing
structural variations svs   largest source  genetic variation  remain poorly understand   limit genomics technology single molecule longread sequence  pacific biosciences  oxford nanopore   potential  dramatically advance  field although  high error rat challenge exist methods address  need  introduce opensource methods  longread alignment ngmlr    identification sniffle   enable unprecedented  sensitivity  precision include within repeatrich regions   complex nest events    significant impact  human disorder examine several datasets include healthy  cancerous human genomes  discover thousands  novel variants use longreads  categorize systematic errors  shortread approach ngmlr  sniffle   able  automatically filter false events  operate  low amount  coverage  address  cost factor   hinder  application  longreads  clinical  research settings",0
16,BFAST,"BFAST: An Alignment Tool for Large Scale Genome Resequencing
The new generation of massively parallel DNA sequencers, combined with the challenge of whole human genome resequencing, result in the need for rapid and accurate alignment of billions of short DNA sequence reads to a large reference genome. Speed is obviously of great importance, but equally important is maintaining alignment accuracy of short reads, in the 25–100 base range, in the presence of errors and true biological variation. We introduce a new algorithm specifically optimized for this task, as well as a freely available implementation, BFAST, which can align data produced by any of current sequencing platforms, allows for user-customizable levels of speed and accuracy, supports paired end data, and provides for efficient parallel and multi-threaded computation on a computer cluster. The new method is based on creating flexible, efficient whole genome indexes to rapidly map reads to candidate alignment locations, with arbitrary multiple independent indexes allowed to achieve robustness against read errors and sequence variants. The final local alignment uses a Smith-Waterman method, with gaps to support the detection of small indels. We compare BFAST to a selection of large-scale alignment tools - BLAT, MAQ, SHRiMP, and SOAP - in terms of both speed and accuracy, using simulated and real-world datasets. We show BFAST can achieve substantially greater sensitivity of alignment in the context of errors and true variants, especially insertions and deletions, and minimize false mappings, while maintaining adequate speed compared to other current methods. We show BFAST can align the amount of data needed to fully resequence a human genome, one billion reads, with high sensitivity and accuracy, on a modest computer cluster in less than 24 hours.",Alignment,"bfast  alignment tool  large scale genome resequencing
 new generation  massively parallel dna sequencers combine   challenge  whole human genome resequencing result   need  rapid  accurate alignment  billions  short dna sequence read   large reference genome speed  obviously  great importance  equally important  maintain alignment accuracy  short read    base range   presence  errors  true biological variation  introduce  new algorithm specifically optimize   task  well   freely available implementation bfast   align data produce    current sequence platforms allow  usercustomizable level  speed  accuracy support pair end data  provide  efficient parallel  multithreaded computation   computer cluster  new method  base  create flexible efficient whole genome index  rapidly map read  candidate alignment locations  arbitrary multiple independent index allow  achieve robustness  read errors  sequence variants  final local alignment use  smithwaterman method  gap  support  detection  small indels  compare bfast   selection  largescale alignment tool  blat maq shrimp  soap   term   speed  accuracy use simulate  realworld datasets  show bfast  achieve substantially greater sensitivity  alignment   context  errors  true variants especially insertions  deletions  minimize false mappings  maintain adequate speed compare   current methods  show bfast  align  amount  data need  fully resequence  human genome one billion read  high sensitivity  accuracy   modest computer cluster  less   hours",0
17,SeqAlto,"Fast and accurate read alignment for resequencing
Next-generation sequence analysis has become an important task both in laboratory and clinical settings. A key stage in the majority sequence analysis workflows, such as resequencing, is the alignment of genomic reads to a reference genome. The accurate alignment of reads with large indels is a computationally challenging task for researchers. We introduce SeqAlto as a new algorithm for read alignment. For reads longer than or equal to 100 bp, SeqAlto is up to 10 × faster than existing algorithms, while retaining high accuracy and the ability to align reads with large (up to 50 bp) indels. This improvement in efficiency is particularly important in the analysis of future sequencing data where the number of reads approaches many billions. Furthermore, SeqAlto uses less than 8 GB of memory to align against the human genome. SeqAlto is benchmarked against several existing tools with both real and simulated data.",Alignment,"fast  accurate read alignment  resequencing
nextgeneration sequence analysis  become  important task   laboratory  clinical settings  key stage   majority sequence analysis workflows   resequencing   alignment  genomic read   reference genome  accurate alignment  read  large indels   computationally challenge task  researchers  introduce seqalto   new algorithm  read alignment  read longer   equal    seqalto      faster  exist algorithms  retain high accuracy   ability  align read  large     indels  improvement  efficiency  particularly important   analysis  future sequence data   number  read approach many billions furthermore seqalto use less     memory  align   human genome seqalto  benchmarked  several exist tool   real  simulate data",0
18,Blast,"Basic local alignment search tool
A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.",Alignment,"basic local alignment search tool
 new approach  rapid sequence comparison basic local alignment search tool blast directly approximate alignments  optimize  measure  local similarity  maximal segment pair msp score recent mathematical result   stochastic properties  msp score allow  analysis   performance   method  well   statistical significance  alignments  generate  basic algorithm  simple  robust    implement   number  ways  apply   variety  contexts include straightforward dna  protein sequence database search motif search gene identification search    analysis  multiple regions  similarity  long dna sequence  addition   flexibility  tractability  mathematical analysis blast   order  magnitude faster  exist sequence comparison tool  comparable sensitivity",0
19,H-BLAST,"H-BLAST: a fast protein sequence alignment toolkit on heterogeneous computers with GPUs
The sequence alignment is a fundamental problem in bioinformatics. BLAST is a routinely used tool for this purpose with over 118 000 citations in the past two decades. As the size of bio-sequence databases grows exponentially, the computational speed of alignment softwares must be improved. We develop the heterogeneous BLAST (H-BLAST), a fast parallel search tool for a heterogeneous computer that couples CPUs and GPUs, to accelerate BLASTX and BLASTP—basic tools of NCBI-BLAST. H-BLAST employs a locally decoupled seed-extension algorithm for better performance on GPUs, and offers a performance tuning mechanism for better efficiency among various CPUs and GPUs combinations. H-BLAST produces identical alignment results as NCBI-BLAST and its computational speed is much faster than that of NCBI-BLAST. Speedups achieved by H-BLAST over sequential NCBI-BLASTP (resp. NCBI-BLASTX) range mostly from 4 to 10 (resp. 5 to 7.2). With 2 CPU threads and 2 GPUs, H-BLAST can be faster than 16-threaded NCBI-BLASTX. Furthermore, H-BLAST is 1.5–4 times faster than GPU-BLAST.",Alignment,"hblast  fast protein sequence alignment toolkit  heterogeneous computers  gpus
 sequence alignment   fundamental problem  bioinformatics blast   routinely use tool   purpose     citations   past two decades   size  biosequence databases grow exponentially  computational speed  alignment softwares must  improve  develop  heterogeneous blast hblast  fast parallel search tool   heterogeneous computer  couple cpus  gpus  accelerate blastx  blastp—basic tool  ncbiblast hblast employ  locally decouple seedextension algorithm  better performance  gpus  offer  performance tune mechanism  better efficiency among various cpus  gpus combinations hblast produce identical alignment result  ncbiblast   computational speed  much faster    ncbiblast speedups achieve  hblast  sequential ncbiblastp resp ncbiblastx range mostly     resp      cpu thread   gpus hblast   faster  thread ncbiblastx furthermore hblast   time faster  gpublast",0
20,Rapsearch2,"RAPSearch2: a fast and memory-efficient protein similarity search tool for next-generation sequencing data
With the wide application of next-generation sequencing (NGS) techniques, fast tools for protein similarity search that scale well to large query datasets and large databases are highly desirable. In a previous work, we developed RAPSearch, an algorithm that achieved a ~20–90-fold speedup relative to BLAST while still achieving similar levels of sensitivity for short protein fragments derived from NGS data. RAPSearch, however, requires a substantial memory footprint to identify alignment seeds, due to its use of a suffix array data structure. Here we present RAPSearch2, a new memory-efficient implementation of the RAPSearch algorithm that uses a collision-free hash table to index a similarity search database. The utilization of an optimized data structure further speeds up the similarity search—another 2–3 times. We also implemented multi-threading in RAPSearch2, and the multi-thread modes achieve significant acceleration (e.g. 3.5X for 4-thread mode). RAPSearch2 requires up to 2G memory when running in single thread mode, or up to 3.5G memory when running in 4-thread mode.",Alignment,"rapsearch2  fast  memoryefficient protein similarity search tool  nextgeneration sequence data
  wide application  nextgeneration sequence ngs techniques fast tool  protein similarity search  scale well  large query datasets  large databases  highly desirable   previous work  develop rapsearch  algorithm  achieve  ~fold speedup relative  blast  still achieve similar level  sensitivity  short protein fragment derive  ngs data rapsearch however require  substantial memory footprint  identify alignment seed due   use   suffix array data structure   present rapsearch2  new memoryefficient implementation   rapsearch algorithm  use  collisionfree hash table  index  similarity search database  utilization   optimize data structure  speed   similarity search—another  time  also implement multithreading  rapsearch2   multithread modes achieve significant acceleration    thread mode rapsearch2 require    memory  run  single thread mode     memory  run  thread mode",0
21,USEARCH,"Search and clustering orders of magnitude faster than BLAST.
Biological sequence data is accumulating rapidly, motivating the development of improved high-throughput methods for sequence classification. UBLAST and USEARCH are new algorithms enabling sensitive local and global search of large sequence databases at exceptionally high speeds. They are often orders of magnitude faster than BLAST in practical applications, though sensitivity to distant protein relationships is lower. UCLUST is a new clustering method that exploits USEARCH to assign sequences to clusters. UCLUST offers several advantages over the widely used program CD-HIT, including higher speed, lower memory use, improved sensitivity, clustering at lower identities and classification of much larger datasets.",Alignment,"search  cluster order  magnitude faster  blast
biological sequence data  accumulate rapidly motivate  development  improve highthroughput methods  sequence classification ublast  usearch  new algorithms enable sensitive local  global search  large sequence databases  exceptionally high speed   often order  magnitude faster  blast  practical applications though sensitivity  distant protein relationships  lower uclust   new cluster method  exploit usearch  assign sequence  cluster uclust offer several advantage   widely use program cdhit include higher speed lower memory use improve sensitivity cluster  lower identities  classification  much larger datasets",0
22,VSEARCH,"VSEARCH: a versatile open source tool for metagenomics
VSEARCH is an open source and free of charge multithreaded 64-bit tool for processing and preparing metagenomics, genomics and population genomics nucleotide sequence data. It is designed as an alternative to the widely used USEARCH tool (Edgar, 2010) for which the source code is not publicly available, algorithm details are only rudimentarily described, and only a memory-confined 32-bit version is freely available for academic use. When searching nucleotide sequences, VSEARCH uses a fast heuristic based on words shared by the query and target sequences in order to quickly identify similar sequences, a similar strategy is probably used in USEARCH. VSEARCH then performs optimal global sequence alignment of the query against potential target sequences, using full dynamic programming instead of the seed-and-extend heuristic used by USEARCH. Pairwise alignments are computed in parallel using vectorisation and multiple threads. VSEARCH includes most commands for analysing nucleotide sequences available in USEARCH version 7 and several of those available in USEARCH version 8, including searching (exact or based on global alignment), clustering by similarity (using length pre-sorting, abundance pre-sorting or a user-defined order), chimera detection (reference-based or de novo), dereplication (full length or prefix), pairwise alignment, reverse complementation, sorting, and subsampling. VSEARCH also includes commands for FASTQ file processing, i.e., format detection, filtering, read quality statistics, and merging of paired reads. Furthermore, VSEARCH extends functionality with several new commands and improvements, including shuffling, rereplication, masking of low-complexity sequences with the well-known DUST algorithm, a choice among different similarity definitions, and FASTQ file format conversion. VSEARCH is here shown to be more accurate than USEARCH when performing searching, clustering, chimera detection and subsampling, while on a par with USEARCH for paired-ends read merging. VSEARCH is slower than USEARCH when performing clustering and chimera detection, but significantly faster when performing paired-end reads merging and dereplication.",Alignment,"vsearch  versatile open source tool  metagenomics
vsearch   open source  free  charge multithreaded bite tool  process  prepare metagenomics genomics  population genomics nucleotide sequence data   design   alternative   widely use usearch tool edgar     source code   publicly available algorithm detail   rudimentarily describe    memoryconfined bite version  freely available  academic use  search nucleotide sequence vsearch use  fast heuristic base  word share   query  target sequence  order  quickly identify similar sequence  similar strategy  probably use  usearch vsearch  perform optimal global sequence alignment   query  potential target sequence use full dynamic program instead   seedandextend heuristic use  usearch pairwise alignments  compute  parallel use vectorisation  multiple thread vsearch include  command  analyse nucleotide sequence available  usearch version   several   available  usearch version  include search exact  base  global alignment cluster  similarity use length presorting abundance presorting   userdefined order chimera detection referencebased   novo dereplication full length  prefix pairwise alignment reverse complementation sort  subsampling vsearch also include command  fastq file process  format detection filter read quality statistics  merge  pair read furthermore vsearch extend functionality  several new command  improvements include shuffle rereplication mask  lowcomplexity sequence   wellknown dust algorithm  choice among different similarity definitions  fastq file format conversion vsearch   show    accurate  usearch  perform search cluster chimera detection  subsampling    par  usearch  pairedends read merge vsearch  slower  usearch  perform cluster  chimera detection  significantly faster  perform pairedend read merge  dereplication",0
23,CLUS_GPU-BLASTP,"CLUS_GPU-BLASTP: accelerated protein sequence alignment using GPU-enabled cluster
Basic Local Alignment Search Tool (BLAST) is one of the most frequently used algorithms for bioinformatics applications. In this paper, an accelerated implementation of protein BLAST, i.e., CLUS_GPU-BLASTP for multiple query sequence processing in parallel, on graphical processing unit (GPU)-enabled high-performance cluster is proposed. The experimental setup consisted of a high-performance GPU-enabled cluster. Each compute node of the cluster consisted of two hex-core Intel, Xeon 2.93 GHz processors with 50 GB RAM and 12 MB cache. Each compute node was also equipped with a NVIDIA M2050 GPU. In comparison with the famous GPU-BLAST, our BLAST implementation is 2.1 times faster on single compute node. On a cluster of 12 compute nodes, our implementation gave a speedup of 13.2X. In comparison with standard single-threaded NCBI-BLAST, our implementation achieves a speedup ranging from 7.4X to 8.2X.",Alignment,"clus_gpublastp accelerate protein sequence alignment use gpuenabled cluster
basic local alignment search tool blast  one    frequently use algorithms  bioinformatics applications   paper  accelerate implementation  protein blast  clus_gpublastp  multiple query sequence process  parallel  graphical process unit gpuenabled highperformance cluster  propose  experimental setup consist   highperformance gpuenabled cluster  compute node   cluster consist  two hexcore intel xeon  ghz processors    ram    cache  compute node  also equip   nvidia m2050 gpu  comparison   famous gpublast  blast implementation   time faster  single compute node   cluster   compute nod  implementation give  speedup    comparison  standard singlethreaded ncbiblast  implementation achieve  speedup range    ",0
24,Kraken,"Kraken: ultrafast metagenomic sequence classification using exact alignments
Kraken is an ultrafast and highly accurate program for assigning taxonomic labels to metagenomic DNA sequences. Previous programs designed for this task have been relatively slow and computationally expensive, forcing researchers to use faster abundance estimation programs, which only classify small subsets of metagenomic data. Using exact alignment of k-mers, Kraken achieves classification accuracy comparable to the fastest BLAST program. In its fastest mode, Kraken classifies 100 base pair reads at a rate of over 4.1 million reads per minute, 909 times faster than Megablast and 11 times faster than the abundance estimation program MetaPhlAn.",Classification,"kraken ultrafast metagenomic sequence classification use exact alignments
kraken   ultrafast  highly accurate program  assign taxonomic label  metagenomic dna sequence previous program design   task   relatively slow  computationally expensive force researchers  use faster abundance estimation program   classify small subsets  metagenomic data use exact alignment  kmers kraken achieve classification accuracy comparable   fastest blast program   fastest mode kraken classify  base pair read   rate    million read per minute  time faster  megablast   time faster   abundance estimation program metaphlan",1
25,KSlam,"k-SLAM: accurate and ultra-fast taxonomic classification and gene identification for large metagenomic data sets
k-SLAM is a highly efficient algorithm for the characterization of metagenomic data. Unlike other ultrafast metagenomic classifiers, full sequence alignment is performed allowing for gene identification and variant calling in addition to accurate taxonomic classification. A k-mer based method provides greater taxonomic accuracy than other classifiers and a three orders of magnitude speed increase over alignment based approaches. The use of alignments to find variants and genes along with their taxonomic origins enables novel strains to be characterized. kSLAM’s speed allows a full taxonomic classification and gene identification to be tractable on modern large data sets. A pseudo-assembly method is used to increase classification accuracy by up to 40% for species which have high sequence homology within their genus.",Classification,"kslam accurate  ultrafast taxonomic classification  gene identification  large metagenomic data sets
kslam   highly efficient algorithm   characterization  metagenomic data unlike  ultrafast metagenomic classifiers full sequence alignment  perform allow  gene identification  variant call  addition  accurate taxonomic classification  kmer base method provide greater taxonomic accuracy   classifiers   three order  magnitude speed increase  alignment base approach  use  alignments  find variants  genes along   taxonomic origins enable novel strain   characterize kslams speed allow  full taxonomic classification  gene identification   tractable  modern large data set  pseudoassembly method  use  increase classification accuracy      species   high sequence homology within  genus",1
26,Centrifuge,"Centrifuge: rapid and sensitive classification of metagenomic sequences
Centrifuge is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species on desktop computers. The system uses an indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem. Centrifuge requires a relatively small index (4.2 GB for 4078 bacterial and 200 archaeal genomes) and classifies sequences at very high speed, allowing it to process the millions of reads from a typical high-throughput DNA sequencing run within a few minutes. Together, these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers. Because of its space-optimized indexing schemes, Centrifuge also makes it possible to index the entire NCBI nonredundant nucleotide sequence database (a total of 109 billion bases) with an index size of 69 GB, in contrast to k-mer-based indexing schemes, which require far more extensive space.",Classification,"centrifuge rapid  sensitive classification  metagenomic sequences
centrifuge   novel microbial classification engine  enable rapid accurate  sensitive label  read  quantification  species  desktop computers  system use  index scheme base   burrowswheeler transform bwt   ferraginamanzini  index optimize specifically   metagenomic classification problem centrifuge require  relatively small index     bacterial   archaeal genomes  classify sequence   high speed allow   process  millions  read   typical highthroughput dna sequence run within   minutes together  advance enable timely  accurate analysis  large metagenomics data set  conventional desktop computers    spaceoptimized index scheme centrifuge also make  possible  index  entire ncbi nonredundant nucleotide sequence database  total   billion base   index size     contrast  kmerbased index scheme  require far  extensive space",1
27,Clark,"CLARK: fast and accurate classification of metagenomic and genomic sequences using discriminative k-mers
The problem of supervised DNA sequence classification arises in several fields of computational molecular biology. Although this problem has been extensively studied, it is still computationally challenging due to size of the datasets that modern sequencing technologies can produce. We introduce CLARK a novel approach to classify metagenomic reads at the species or genus level with high accuracy and high speed. Extensive experimental results on various metagenomic samples show that the classification accuracy of CLARK is better or comparable to the best state-of-the-art tools and it is significantly faster than any of its competitors. In its fastest single-threaded mode CLARK classifies, with high accuracy, about 32 million metagenomic short reads per minute. CLARK can also classify BAC clones or transcripts to chromosome arms and centromeric regions. CLARK is a versatile, fast and accurate sequence classification method, especially useful for metagenomics and genomics applications.",Classification,"clark fast  accurate classification  metagenomic  genomic sequence use discriminative kmers
 problem  supervise dna sequence classification arise  several field  computational molecular biology although  problem   extensively study   still computationally challenge due  size   datasets  modern sequence technologies  produce  introduce clark  novel approach  classify metagenomic read   species  genus level  high accuracy  high speed extensive experimental result  various metagenomic sample show   classification accuracy  clark  better  comparable   best stateoftheart tool    significantly faster     competitors   fastest singlethreaded mode clark classify  high accuracy   million metagenomic short read per minute clark  also classify bac clone  transcripts  chromosome arm  centromeric regions clark   versatile fast  accurate sequence classification method especially useful  metagenomics  genomics applications",1
28,kaiju,"Fast and sensitive taxonomic classification for metagenomics with Kaiju.
AMetagenomics emerged as an important field of research not only in microbial ecology but also for human health and disease, and metagenomic studies are performed on increasingly larger scales. While recent taxonomic classification programs achieve high speed by comparing genomic k-mers, they often lack sensitivity for overcoming evolutionary divergence, so that large fractions of the metagenomic reads remain unclassified. Here we present the novel metagenome classifier Kaiju, which finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform. We show in a genome exclusion benchmark that Kaiju classifies reads with higher sensitivity and similar precision compared with current k-mer-based classifiers, especially in genera that are underrepresented in reference databases. We also demonstrate that Kaiju classifies up to 10 times more reads in real metagenomes. Kaiju can process millions of reads per minute and can run on a standard PC.",Classification,"fast  sensitive taxonomic classification  metagenomics  kaiju
ametagenomics emerge   important field  research    microbial ecology  also  human health  disease  metagenomic study  perform  increasingly larger scale  recent taxonomic classification program achieve high speed  compare genomic kmers  often lack sensitivity  overcome evolutionary divergence   large fraction   metagenomic read remain unclassified   present  novel metagenome classifier kaiju  find maximum inexact match   proteinlevel use  burrowswheeler transform  show   genome exclusion benchmark  kaiju classify read  higher sensitivity  similar precision compare  current kmerbased classifiers especially  genera   underrepresented  reference databases  also demonstrate  kaiju classify    time  read  real metagenomes kaiju  process millions  read per minute   run   standard ",1
29,KrakenUniq,"KrakenUniq: confident and fast metagenomics classification using unique k-mer counts
False-positive identifications are a significant problem in metagenomics classification. We present KrakenUniq, a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset. On various test datasets, KrakenUniq gives better recall and precision than other methods and effectively classifies and distinguishes pathogens with low abundance from false positives in infectious disease samples. By using the probabilistic cardinality estimator HyperLogLog, KrakenUniq runs as fast as Kraken and requires little additional memory.",Classification,"krakenuniq confident  fast metagenomics classification use unique kmer counts
falsepositive identifications   significant problem  metagenomics classification  present krakenuniq  novel metagenomics classifier  combine  fast kmerbased classification  kraken   efficient algorithm  assess  coverage  unique kmers find   species   dataset  various test datasets krakenuniq give better recall  precision   methods  effectively classify  distinguish pathogens  low abundance  false positives  infectious disease sample  use  probabilistic cardinality estimator hyperloglog krakenuniq run  fast  kraken  require little additional memory",1
30,LiveKraken,"LiveKraken--real-time metagenomic classification of illumina data
In metagenomics, Kraken is one of the most widely used tools due to its robustness and speed. Yet, the overall turnaround time of metagenomic analysis is hampered by the sequential paradigm of wet and dry lab. In urgent experiments, it can be crucial to gain a timely insight into a dataset. Here, we present LiveKraken, a real-time read classification tool based on the core algorithm of Kraken. LiveKraken uses streams of raw data from Illumina sequencers to classify reads taxonomically. This way, we are able to produce results identical to those of Kraken the moment the sequencer finishes. We are furthermore able to provide comparable results in early stages of a sequencing run, allowing saving up to a week of sequencing time on an Illumina HiSeq in High Throughput Mode. While the number of classified reads grows over time, false classifications appear in negligible numbers and proportions of identified taxa are only affected to a minor extent.",Classification,"livekrakenrealtime metagenomic classification  illumina data
 metagenomics kraken  one    widely use tool due   robustness  speed yet  overall turnaround time  metagenomic analysis  hamper   sequential paradigm  wet  dry lab  urgent experiment    crucial  gain  timely insight   dataset   present livekraken  realtime read classification tool base   core algorithm  kraken livekraken use stream  raw data  illumina sequencers  classify read taxonomically  way   able  produce result identical    kraken  moment  sequencer finish   furthermore able  provide comparable result  early stag   sequence run allow save    week  sequence time   illumina hiseq  high throughput mode   number  classify read grow  time false classifications appear  negligible number  proportion  identify taxa   affect   minor extent",1
31,Marvel,"MARVEL, a Tool for Prediction of Bacteriophage Sequences in Metagenomic Bins
Here we present MARVEL, a tool for prediction of double-stranded DNA bacteriophage sequences in metagenomic bins. MARVEL uses a random forest machine learning approach. We trained the program on a dataset with 1,247 phage and 1,029 bacterial genomes, and tested it on a dataset with 335 bacterial and 177 phage genomes. We show that three simple genomic features extracted from contig sequences were sufficient to achieve a good performance in separating bacterial from phage sequences: gene density, strand shifts, and fraction of significant hits to a viral protein database. We compared the performance of MARVEL to that of VirSorter and VirFinder, two popular programs for predicting viral sequences. Our results show that all three programs have comparable specificity, but MARVEL achieves much better performance on the recall (sensitivity) measure. This means that MARVEL should be able to identify many more phage sequences in metagenomic bins than heretofore has been possible. In a simple test with real data, containing mostly bacterial sequences, MARVEL classified 58 out of 209 bins as phage genomes; other evidence suggests that 57 of these 58 bins are novel phage sequences.",Classification,"marvel  tool  prediction  bacteriophage sequence  metagenomic bins
  present marvel  tool  prediction  doublestranded dna bacteriophage sequence  metagenomic bin marvel use  random forest machine learn approach  train  program   dataset   phage   bacterial genomes  test    dataset   bacterial   phage genomes  show  three simple genomic feature extract  contig sequence  sufficient  achieve  good performance  separate bacterial  phage sequence gene density strand shift  fraction  significant hit   viral protein database  compare  performance  marvel    virsorter  virfinder two popular program  predict viral sequence  result show   three program  comparable specificity  marvel achieve much better performance   recall sensitivity measure  mean  marvel   able  identify many  phage sequence  metagenomic bin  heretofore   possible   simple test  real data contain mostly bacterial sequence marvel classify     bin  phage genomes  evidence suggest      bin  novel phage sequence",1
32,SMART,"Scalable metagenomics alignment research tool (SMART): a scalable, rapid, and complete search heuristic for the classification of metagenomic sequences from complex sequence populations
Next generation sequencing technology has enabled characterization of metagenomics through massively parallel genomic DNA sequencing. The complexity and diversity of environmental samples such as the human gut microflora, combined with the sustained exponential growth in sequencing capacity, has led to the challenge of identifying microbial organisms by DNA sequence. We sought to validate a Scalable Metagenomics Alignment Research Tool (SMART), a novel searching heuristic for shotgun metagenomics sequencing results. After retrieving all genomic DNA sequences from the NCBI GenBank, over 1 × 1011 base pairs of 3.3 × 106 sequences from 9.25 × 105 species were indexed using 4 base pair hashtable shards. A MapReduce searching strategy was used to distribute the search workload in a computing cluster environment. In addition, a one base pair permutation algorithm was used to account for single nucleotide polymorphisms and sequencing errors. Simulated datasets used to evaluate Kraken, a similar metagenomics classification tool, were used to measure and compare precision and accuracy. Finally using a same set of training sequences we compared Kraken, CLARK, and SMART within the same computing environment. Utilizing 12 computational nodes, we completed the classification of all datasets in under 10 min each using exact matching with an average throughput of over 1.95 × 106 reads classified per minute. With permutation matching, we achieved sensitivity greater than 83 % and precision greater than 94 % with simulated datasets at the species classification level. We demonstrated the application of this technique applied to conjunctival and gut microbiome metagenomics sequencing results. In our head to head comparison, SMART and CLARK had similar accuracy gains over Kraken at the species classification level, but SMART required approximately half the amount of RAM of CLARK. SMART is the first scalable, efficient, and rapid metagenomics classification algorithm capable of matching against all the species and sequences present in the NCBI GenBank and allows for a single step classification of microorganisms as well as large plant, mammalian, or invertebrate genomes from which the metagenomic sample may have been derived.",Classification,"scalable metagenomics alignment research tool smart  scalable rapid  complete search heuristic   classification  metagenomic sequence  complex sequence populations
next generation sequence technology  enable characterization  metagenomics  massively parallel genomic dna sequence  complexity  diversity  environmental sample    human gut microflora combine   sustain exponential growth  sequence capacity  lead   challenge  identify microbial organisms  dna sequence  seek  validate  scalable metagenomics alignment research tool smart  novel search heuristic  shotgun metagenomics sequence result  retrieve  genomic dna sequence   ncbi genbank     base pair     sequence     species  index use  base pair hashtable shards  mapreduce search strategy  use  distribute  search workload   compute cluster environment  addition  one base pair permutation algorithm  use  account  single nucleotide polymorphisms  sequence errors simulate datasets use  evaluate kraken  similar metagenomics classification tool  use  measure  compare precision  accuracy finally use   set  train sequence  compare kraken clark  smart within   compute environment utilize  computational nod  complete  classification   datasets    min  use exact match   average throughput      read classify per minute  permutation match  achieve sensitivity greater     precision greater     simulate datasets   species classification level  demonstrate  application   technique apply  conjunctival  gut microbiome metagenomics sequence result   head  head comparison smart  clark  similar accuracy gain  kraken   species classification level  smart require approximately half  amount  ram  clark smart   first scalable efficient  rapid metagenomics classification algorithm capable  match    species  sequence present   ncbi genbank  allow   single step classification  microorganisms  well  large plant mammalian  invertebrate genomes    metagenomic sample may   derive",1
33,Taxonomer,"Taxonomer: an interactive metagenomics analysis portal for universal pathogen detection and host mRNA expression profiling
High-throughput sequencing enables unbiased profiling of microbial communities, universal pathogen detection, and host response to infectious diseases. However, computation times and algorithmic inaccuracies have hindered adoption. We present Taxonomer, an ultrafast, web-tool for comprehensive metagenomics data analysis and interactive results visualization. Taxonomer is unique in providing integrated nucleotide and protein-based classification and simultaneous host messenger RNA (mRNA) transcript profiling. Using real-world case-studies, we show that Taxonomer detects previously unrecognized infections and reveals antiviral host mRNA expression profiles. To facilitate data-sharing across geographic distances in outbreak settings, Taxonomer is publicly available through a web-based user interface. Taxonomer enables rapid, accurate, and interactive analyses of metagenomics data on personal computers and mobile devices. Keywords: Metagenomics, Microbiome, Pathogen detection, Infectious disease diagnostics",Classification,"taxonomer  interactive metagenomics analysis portal  universal pathogen detection  host mrna expression profiling
highthroughput sequence enable unbiased profile  microbial communities universal pathogen detection  host response  infectious diseases however computation time  algorithmic inaccuracies  hinder adoption  present taxonomer  ultrafast webtool  comprehensive metagenomics data analysis  interactive result visualization taxonomer  unique  provide integrate nucleotide  proteinbased classification  simultaneous host messenger rna mrna transcript profile use realworld casestudies  show  taxonomer detect previously unrecognized infections  reveal antiviral host mrna expression profile  facilitate datasharing across geographic distance  outbreak settings taxonomer  publicly available   webbased user interface taxonomer enable rapid accurate  interactive analyse  metagenomics data  personal computers  mobile devices keywords metagenomics microbiome pathogen detection infectious disease diagnostics",1
34,RNA-Code,"RNA-CODE: A Noncoding RNA Classification Tool for Short Reads in NGS Data Lacking Reference Genomes
The number of transcriptomic sequencing projects of various non-model organisms is still accumulating rapidly. As non-coding RNAs (ncRNAs) are highly abundant in living organism and play important roles in many biological processes, identifying fragmentary members of ncRNAs in small RNA-seq data is an important step in post-NGS analysis. However, the state-of-the-art ncRNA search tools are not optimized for next-generation sequencing (NGS) data, especially for very short reads. In this work, we propose and implement a comprehensive ncRNA classification tool (RNA-CODE) for very short reads. RNA-CODE is specifically designed for ncRNA identification in NGS data that lack quality reference genomes. Given a set of short reads, our tool classifies the reads into different types of ncRNA families. The classification results can be used to quantify the expression levels of different types of ncRNAs in RNA-seq data and ncRNA composition profiles in metagenomic data, respectively. The experimental results of applying RNA-CODE to RNA-seq of Arabidopsis and a metagenomic data set sampled from human guts demonstrate that RNA-CODE competes favorably in both sensitivity and specificity with other tools.",Classification,"rnacode  noncoding rna classification tool  short read  ngs data lack reference genomes
 number  transcriptomic sequence project  various nonmodel organisms  still accumulate rapidly  noncoding rnas ncrnas  highly abundant  live organism  play important roles  many biological process identify fragmentary members  ncrnas  small rnaseq data   important step  postngs analysis however  stateoftheart ncrna search tool   optimize  nextgeneration sequence ngs data especially   short read   work  propose  implement  comprehensive ncrna classification tool rnacode   short read rnacode  specifically design  ncrna identification  ngs data  lack quality reference genomes give  set  short read  tool classify  read  different type  ncrna families  classification result   use  quantify  expression level  different type  ncrnas  rnaseq data  ncrna composition profile  metagenomic data respectively  experimental result  apply rnacode  rnaseq  arabidopsis   metagenomic data set sample  human gut demonstrate  rnacode compete favorably   sensitivity  specificity   tool",1
35,MetaOthello,"A novel data structure to support ultra-fast taxonomic classification of metagenomic sequences with k-mer signatures
Metagenomic read classification is a critical step in the identification and quantification of microbial species sampled by high-throughput sequencing. Although many algorithms have been developed to date, they suffer significant memory and/or computational costs. Due to the growing popularity of metagenomic data in both basic science and clinical applications, as well as the increasing volume of data being generated, efficient and accurate algorithms are in high demand. We introduce MetaOthello, a probabilistic hashing classifier for metagenomic sequencing reads. The algorithm employs a novel data structure, called l-Othello, to support efficient querying of a taxon using its k-mer signatures. MetaOthello is an order-of-magnitude faster than the current state-of-the-art algorithms Kraken and Clark, and requires only one-third of the RAM. In comparison to Kaiju, a metagenomic classification tool using protein sequences instead of genomic sequences, MetaOthello is three times faster and exhibits 20–30% higher classification sensitivity. We report comparative analyses of both scalability and accuracy using a number of simulated and empirical datasets.",Classification," novel data structure  support ultrafast taxonomic classification  metagenomic sequence  kmer signatures
metagenomic read classification   critical step   identification  quantification  microbial species sample  highthroughput sequence although many algorithms   develop  date  suffer significant memory andor computational cost due   grow popularity  metagenomic data   basic science  clinical applications  well   increase volume  data  generate efficient  accurate algorithms   high demand  introduce metaothello  probabilistic hash classifier  metagenomic sequence read  algorithm employ  novel data structure call lothello  support efficient query   taxon use  kmer signatures metaothello   orderofmagnitude faster   current stateoftheart algorithms kraken  clark  require  onethird   ram  comparison  kaiju  metagenomic classification tool use protein sequence instead  genomic sequence metaothello  three time faster  exhibit  higher classification sensitivity  report comparative analyse   scalability  accuracy use  number  simulate  empirical datasets",1
36,taxMaps,"taxMaps: comprehensive and highly accurate taxonomic classification of short-read data in reasonable time
High-throughput sequencing is a revolutionary technology for the analysis of metagenomic samples. However, querying large volumes of reads against comprehensive DNA/RNA databases in a sensitive manner can be compute-intensive. Here, we present taxMaps, a highly efficient, sensitive, and fully scalable taxonomic classification tool. Using a combination of simulated and real metagenomics data sets, we demonstrate that taxMaps is more sensitive and more precise than widely used taxonomic classifiers and is capable of delivering classification accuracy comparable to that of BLASTN, but at up to three orders of magnitude less computational cost.",Classification,"taxmaps comprehensive  highly accurate taxonomic classification  shortread data  reasonable time
highthroughput sequence   revolutionary technology   analysis  metagenomic sample however query large volumes  read  comprehensive dnarna databases   sensitive manner   computeintensive   present taxmaps  highly efficient sensitive  fully scalable taxonomic classification tool use  combination  simulate  real metagenomics data set  demonstrate  taxmaps   sensitive   precise  widely use taxonomic classifiers   capable  deliver classification accuracy comparable    blastn     three order  magnitude less computational cost",1
37,MEGAN,"MEGAN analysis of metagenomic data
Metagenomics is the study of the genomic content of a sample of organisms obtained from a common habitat using targeted or random sequencing. Goals include understanding the extent and role of microbial diversity. The taxonomical content of such a sample is usually estimated by comparison against sequence databases of known sequences. Most published studies use the analysis of paired-end reads, complete sequences of environmental fosmid and BAC clones, or environmental assemblies. Emerging sequencing-by-synthesis technologies with very high throughput are paving the way to low-cost random “shotgun” approaches. This paper introduces MEGAN, a new computer program that allows laptop analysis of large metagenomic data sets. In a preprocessing step, the set of DNA sequences is compared against databases of known sequences using BLAST or another comparison tool. MEGAN is then used to compute and explore the taxonomical content of the data set, employing the NCBI taxonomy to summarize and order the results. A simple lowest common ancestor algorithm assigns reads to taxa such that the taxonomical level of the assigned taxon reflects the level of conservation of the sequence. The software allows large data sets to be dissected without the need for assembly or the targeting of specific phylogenetic markers. It provides graphical and statistical output for comparing different data sets. The approach is applied to several data sets, including the Sargasso Sea data set, a recently published metagenomic data set sampled from a mammoth bone, and several complete microbial genomes. Also, simulations that evaluate the performance of the approach for different read lengths are presented.",Classification,"megan analysis  metagenomic data
metagenomics   study   genomic content   sample  organisms obtain   common habitat use target  random sequence goals include understand  extent  role  microbial diversity  taxonomical content    sample  usually estimate  comparison  sequence databases  know sequence  publish study use  analysis  pairedend read complete sequence  environmental fosmid  bac clone  environmental assemblies emerge sequencingbysynthesis technologies   high throughput  pave  way  lowcost random “shotgun” approach  paper introduce megan  new computer program  allow laptop analysis  large metagenomic data set   preprocessing step  set  dna sequence  compare  databases  know sequence use blast  another comparison tool megan   use  compute  explore  taxonomical content   data set employ  ncbi taxonomy  summarize  order  result  simple lowest common ancestor algorithm assign read  taxa    taxonomical level   assign taxon reflect  level  conservation   sequence  software allow large data set   dissect without  need  assembly   target  specific phylogenetic markers  provide graphical  statistical output  compare different data set  approach  apply  several data set include  sargasso sea data set  recently publish metagenomic data set sample   mammoth bone  several complete microbial genomes also simulations  evaluate  performance   approach  different read lengths  present",1
38,MetaShot,"MetaShot: an accurate workflow for taxon classification of host-associated microbiome from shotgun metagenomic data
Shotgun metagenomics by high-throughput sequencing may allow deep and accurate characterization of host-associated total microbiomes, including bacteria, viruses, protists and fungi. However, the analysis of such sequencing data is still extremely challenging in terms of both overall accuracy and computational efficiency, and current methodologies show substantial variability in misclassification rate and resolution at lower taxonomic ranks or are limited to specific life domains (e.g. only bacteria). We present here MetaShot, a workflow for assessing the total microbiome composition from host-associated shotgun sequence data, and show its overall optimal accuracy performance by analyzing both simulated and real datasets.",Classification,"metashot  accurate workflow  taxon classification  hostassociated microbiome  shotgun metagenomic data
shotgun metagenomics  highthroughput sequence may allow deep  accurate characterization  hostassociated total microbiomes include bacteria viruses protists  fungi however  analysis   sequence data  still extremely challenge  term   overall accuracy  computational efficiency  current methodologies show substantial variability  misclassification rate  resolution  lower taxonomic rank   limit  specific life domains   bacteria  present  metashot  workflow  assess  total microbiome composition  hostassociated shotgun sequence data  show  overall optimal accuracy performance  analyze  simulate  real datasets",1
39,RIEMS,"RIEMS: a software pipeline for sensitive and comprehensive taxonomic classification of reads from metagenomics datasets
Fuelled by the advent and subsequent development of next generation sequencing technologies, metagenomics became a powerful tool for the analysis of microbial communities both scientifically and diagnostically. The biggest challenge is the extraction of relevant information from the huge sequence datasets generated for metagenomics studies. Although a plethora of tools are available, data analysis is still a bottleneck. To overcome the bottleneck of data analysis, we developed an automated computational workflow called RIEMS – Reliable Information Extraction from Metagenomic Sequence datasets. RIEMS assigns every individual read sequence within a dataset taxonomically by cascading different sequence analyses with decreasing stringency of the assignments using various software applications. After completion of the analyses, the results are summarised in a clearly structured result protocol organised taxonomically. The high accuracy and performance of RIEMS analyses were proven in comparison with other tools for metagenomics data analysis using simulated sequencing read datasets. RIEMS has the potential to fill the gap that still exists with regard to data analysis for metagenomics studies. The usefulness and power of RIEMS for the analysis of genuine sequencing datasets was demonstrated with an early version of RIEMS in 2011 when it was used to detect the orthobunyavirus sequences leading to the discovery of Schmallenberg virus.",Classification,"riems  software pipeline  sensitive  comprehensive taxonomic classification  read  metagenomics datasets
fuelled   advent  subsequent development  next generation sequence technologies metagenomics become  powerful tool   analysis  microbial communities  scientifically  diagnostically  biggest challenge   extraction  relevant information   huge sequence datasets generate  metagenomics study although  plethora  tool  available data analysis  still  bottleneck  overcome  bottleneck  data analysis  develop  automate computational workflow call riems  reliable information extraction  metagenomic sequence datasets riems assign every individual read sequence within  dataset taxonomically  cascade different sequence analyse  decrease stringency   assignments use various software applications  completion   analyse  result  summarise   clearly structure result protocol organise taxonomically  high accuracy  performance  riems analyse  prove  comparison   tool  metagenomics data analysis use simulate sequence read datasets riems   potential  fill  gap  still exist  regard  data analysis  metagenomics study  usefulness  power  riems   analysis  genuine sequence datasets  demonstrate   early version  riems      use  detect  orthobunyavirus sequence lead   discovery  schmallenberg virus",1
40,GOTTCHA,"Accurate read-based metagenome characterization using a hierarchical suite of unique signatures
A major challenge in the field of shotgun metagenomics is the accurate identification of organisms present within a microbial community, based on classification of short sequence reads. Though existing microbial community profiling methods have attempted to rapidly classify the millions of reads output from modern sequencers, the combination of incomplete databases, similarity among otherwise divergent genomes, errors and biases in sequencing technologies, and the large volumes of sequencing data required for metagenome sequencing has led to unacceptably high false discovery rates (FDR). Here, we present the application of a novel, gene-independent and signature-based metagenomic taxonomic profiling method with significantly and consistently smaller FDR than any other available method. Our algorithm circumvents false positives using a series of non-redundant signature databases and examines Genomic Origins Through Taxonomic CHAllenge (GOTTCHA). GOTTCHA was tested and validated on 20 synthetic and mock datasets ranging in community composition and complexity, was applied successfully to data generated from spiked environmental and clinical samples, and robustly demonstrates superior performance compared with other available tools.",Classification,"accurate readbased metagenome characterization use  hierarchical suite  unique signatures
 major challenge   field  shotgun metagenomics   accurate identification  organisms present within  microbial community base  classification  short sequence read though exist microbial community profile methods  attempt  rapidly classify  millions  read output  modern sequencers  combination  incomplete databases similarity among otherwise divergent genomes errors  bias  sequence technologies   large volumes  sequence data require  metagenome sequence  lead  unacceptably high false discovery rat fdr   present  application   novel geneindependent  signaturebased metagenomic taxonomic profile method  significantly  consistently smaller fdr    available method  algorithm circumvent false positives use  series  nonredundant signature databases  examine genomic origins  taxonomic challenge gottcha gottcha  test  validate   synthetic  mock datasets range  community composition  complexity  apply successfully  data generate  spike environmental  clinical sample  robustly demonstrate superior performance compare   available tool",1
41,Genometa,"Genometa - A Fast and Accurate Classifier for Short Metagenomic Shotgun Reads
Metagenomic studies use high-throughput sequence data to investigate microbial communities in situ. However, considerable challenges remain in the analysis of these data, particularly with regard to speed and reliable analysis of microbial species as opposed to higher level taxa such as phyla. We here present Genometa, a computationally undemanding graphical user interface program that enables identification of bacterial species and gene content from datasets generated by inexpensive high-throughput short read sequencing technologies. Our approach was first verified on two simulated metagenomic short read datasets, detecting 100% and 94% of the bacterial species included with few false positives or false negatives. Subsequent comparative benchmarking analysis against three popular metagenomic algorithms on an Illumina human gut dataset revealed Genometa to attribute the most reads to bacteria at species level (i.e. including all strains of that species) and demonstrate similar or better accuracy than the other programs. Lastly, speed was demonstrated to be many times that of BLAST due to the use of modern short read aligners. Our method is highly accurate if bacteria in the sample are represented by genomes in the reference sequence but cannot find species absent from the reference. This method is one of the most user-friendly and resource efficient approaches and is thus feasible for rapidly analysing millions of short reads on a personal computer.",Classification,"genometa   fast  accurate classifier  short metagenomic shotgun reads
metagenomic study use highthroughput sequence data  investigate microbial communities  situ however considerable challenge remain   analysis   data particularly  regard  speed  reliable analysis  microbial species  oppose  higher level taxa   phyla   present genometa  computationally undemanding graphical user interface program  enable identification  bacterial species  gene content  datasets generate  inexpensive highthroughput short read sequence technologies  approach  first verify  two simulate metagenomic short read datasets detect      bacterial species include   false positives  false negative subsequent comparative benchmarking analysis  three popular metagenomic algorithms   illumina human gut dataset reveal genometa  attribute   read  bacteria  species level  include  strain   species  demonstrate similar  better accuracy    program lastly speed  demonstrate   many time   blast due   use  modern short read aligners  method  highly accurate  bacteria   sample  represent  genomes   reference sequence  cannot find species absent   reference  method  one    userfriendly  resource efficient approach   thus feasible  rapidly analyse millions  short read   personal computer",1
42,LMAT,"Scalable metagenomic taxonomy classification using a reference genome database
Deep metagenomic sequencing of biological samples has the potential to recover otherwise difficult-to-detect microorganisms and accurately characterize biological samples with limited prior knowledge of sample contents. Existing metagenomic taxonomic classification algorithms, however, do not scale well to analyze large metagenomic datasets, and balancing classification accuracy with computational efficiency presents a fundamental challenge. A method is presented to shift computational costs to an off-line computation by creating a taxonomy/genome index that supports scalable metagenomic classification. Scalable performance is demonstrated on real and simulated data to show accurate classification in the presence of novel organisms on samples that include viruses, prokaryotes, fungi and protists. Taxonomic classification of the previously published 150 giga-base Tyrolean Iceman dataset was found to take <20 h on a single node 40 core large memory machine and provide new insights on the metagenomic contents of the sample.",Classification,"scalable metagenomic taxonomy classification use  reference genome database
deep metagenomic sequence  biological sample   potential  recover otherwise difficulttodetect microorganisms  accurately characterize biological sample  limit prior knowledge  sample content exist metagenomic taxonomic classification algorithms however   scale well  analyze large metagenomic datasets  balance classification accuracy  computational efficiency present  fundamental challenge  method  present  shift computational cost   offline computation  create  taxonomygenome index  support scalable metagenomic classification scalable performance  demonstrate  real  simulate data  show accurate classification   presence  novel organisms  sample  include viruses prokaryotes fungi  protists taxonomic classification   previously publish  gigabase tyrolean iceman dataset  find  take     single node  core large memory machine  provide new insights   metagenomic content   sample",1
43,DUDes,"DUDes: a top-down taxonomic profiler for metagenomics
Species identification and quantification are common tasks in metagenomics and pathogen detection studies. The most recent techniques are built on mapping the sequenced reads against a reference database (e.g. whole genomes, marker genes, proteins) followed by application-dependent analysis steps. Although these methods have been proven to be useful in many scenarios, there is still room for improvement in species and strain level detection, mainly for low abundant organisms. We propose a new method: DUDes, a reference-based taxonomic profiler that introduces a novel top-down approach to analyze metagenomic Next-generation sequencing (NGS) samples. Rather than predicting an organism presence in the sample based only on relative abundances, DUDes first identifies possible candidates by comparing the strength of the read mapping in each node of the taxonomic tree in an iterative manner. Instead of using the lowest common ancestor we propose a new approach: the deepest uncommon descendent. We showed in experiments that DUDes works for single and multiple organisms and can identify low abundant taxonomic groups with high precision.",Classification,"dudes  topdown taxonomic profiler  metagenomics
species identification  quantification  common task  metagenomics  pathogen detection study   recent techniques  build  map  sequence read   reference database  whole genomes marker genes proteins follow  applicationdependent analysis step although  methods   prove   useful  many scenarios   still room  improvement  species  strain level detection mainly  low abundant organisms  propose  new method dudes  referencebased taxonomic profiler  introduce  novel topdown approach  analyze metagenomic nextgeneration sequence ngs sample rather  predict  organism presence   sample base   relative abundances dudes first identify possible candidates  compare  strength   read map   node   taxonomic tree   iterative manner instead  use  lowest common ancestor  propose  new approach  deepest uncommon descendent  show  experiment  dudes work  single  multiple organisms   identify low abundant taxonomic group  high precision",1
44,NBC,"NBC: the Naive Bayes Classification tool webserver for taxonomic classification of metagenomic reads.
Datasets from high-throughput sequencing technologies have yielded a vast amount of data about organisms in environmental samples. Yet, it is still a challenge to assess the exact organism content in these samples because the task of taxonomic classification is too computationally complex to annotate all reads in a dataset. An easy-to-use webserver is needed to process these reads. While many methods exist, only a few are publicly available on webservers, and out of those, most do not annotate all reads. We introduce a webserver that implements the naïve Bayes classifier (NBC) to classify all metagenomic reads to their best taxonomic match. Results indicate that NBC can assign next-generation sequencing reads to their taxonomic classification and can find significant populations of genera that other classifiers may miss.",Classification,"nbc  naive bay classification tool webserver  taxonomic classification  metagenomic reads
datasets  highthroughput sequence technologies  yield  vast amount  data  organisms  environmental sample yet   still  challenge  assess  exact organism content   sample   task  taxonomic classification   computationally complex  annotate  read   dataset  easytouse webserver  need  process  read  many methods exist     publicly available  webservers        annotate  read  introduce  webserver  implement  naïve bay classifier nbc  classify  metagenomic read   best taxonomic match result indicate  nbc  assign nextgeneration sequence read   taxonomic classification   find significant populations  genera   classifiers may miss",1
45,Kodoja,"Kodoja: A workflow for virus detection in plants using k-mer analysis of RNA-sequencing data
RNA-sequencing of plant material allows for hypothesis-free detection of multiple viruses simultaneously. This methodology relies on bioinformatics workflows for virus identification. Most workflows are designed for human clinical data, and few go beyond sequence mapping for virus identification. We present a new workflow (Kodoja) for the detection of plant virus sequences in RNA-sequence data. Kodoja uses k-mer profiling at the nucleotide level and sequence mapping at the protein level by integrating two existing tools Kraken and Kaiju. Kodoja was tested on three existing RNA-seq datasets from grapevine, and two new RNA-seq datasets from raspberry. For grapevine, Kodoja was shown to be more sensitive than a method based on contig building and blast alignments (27 viruses detected compared to 19). The application of Kodoja to raspberry, showed that field-grown raspberries were infected by multiple viruses, and that RNA-seq can identify lower amounts of virus material than reverse transcriptase PCR. This work enabled the design of new PCR-primers for detection of Raspberry yellow net virus and Beet ringspot virus. Kodoja is a sensitive method for plant virus discovery in field samples and enables the design of more accurate primers for detection.",VirusDetection,"kodoja  workflow  virus detection  plant use kmer analysis  rnasequencing data
rnasequencing  plant material allow  hypothesisfree detection  multiple viruses simultaneously  methodology rely  bioinformatics workflows  virus identification  workflows  design  human clinical data    beyond sequence map  virus identification  present  new workflow kodoja   detection  plant virus sequence  rnasequence data kodoja use kmer profile   nucleotide level  sequence map   protein level  integrate two exist tool kraken  kaiju kodoja  test  three exist rnaseq datasets  grapevine  two new rnaseq datasets  raspberry  grapevine kodoja  show    sensitive   method base  contig build  blast alignments  viruses detect compare    application  kodoja  raspberry show  fieldgrown raspberries  infect  multiple viruses   rnaseq  identify lower amount  virus material  reverse transcriptase pcr  work enable  design  new pcrprimers  detection  raspberry yellow net virus  beet ringspot virus kodoja   sensitive method  plant virus discovery  field sample  enable  design   accurate primers  detection",2
46,DisCVR,"DisCVR: Rapid viral diagnosis from high-throughput sequencing data
High-throughput sequencing (HTS) enables most pathogens in a clinical sample to be detected from a single analysis, thereby providing novel opportunities for diagnosis, surveillance, and epidemiology. However, this powerful technology is difficult to apply in diagnostic laboratories because of its computational and bioinformatic demands. We have developed DisCVR, which detects known human viruses in clinical samples by matching sample k-mers (twenty-two nucleotide sequences) to k-mers from taxonomically labeled viral genomes. DisCVR was validated using published HTS data for eighty-nine clinical samples from adults with upper respiratory tract infections. These samples had been tested for viruses metagenomically and also by real-time polymerase chain reaction assay, which is the standard diagnostic method. DisCVR detected human viruses with high sensitivity (79%) and specificity (100%), and was able to detect mixed infections. Moreover, it produced results comparable to those in a published metagenomic analysis of 177 blood samples from patients in Nigeria. DisCVR has been designed as a user-friendly tool for detecting human viruses from HTS data using computers with limited RAM and processing power, and includes a graphical user interface to help users interpret and validate the output.",VirusDetection,"discvr rapid viral diagnosis  highthroughput sequence data
highthroughput sequence hts enable  pathogens   clinical sample   detect   single analysis thereby provide novel opportunities  diagnosis surveillance  epidemiology however  powerful technology  difficult  apply  diagnostic laboratories    computational  bioinformatic demand   develop discvr  detect know human viruses  clinical sample  match sample kmers twentytwo nucleotide sequence  kmers  taxonomically label viral genomes discvr  validate use publish hts data  eightynine clinical sample  adults  upper respiratory tract infections  sample   test  viruses metagenomically  also  realtime polymerase chain reaction assay    standard diagnostic method discvr detect human viruses  high sensitivity   specificity    able  detect mix infections moreover  produce result comparable     publish metagenomic analysis   blood sample  patients  nigeria discvr   design   userfriendly tool  detect human viruses  hts data use computers  limit ram  process power  include  graphical user interface  help users interpret  validate  output",2
47,Metavisitor,"Metavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data
Metavisitor is a software package that allows biologists and clinicians without specialized bioinformatics expertise to detect and assemble viral genomes from deep sequence datasets. The package is composed of a set of modular bioinformatic tools and workflows that are implemented in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules. Metavisitor works with DNA, RNA or small RNA sequencing data over a range of read lengths and can use a combination of de novo and guided approaches to assemble genomes from sequencing reads. We show that the software has the potential for quick diagnosis as well as discovery of viruses from a vast array of organisms. Importantly, we provide here executable Metavisitor use cases, which increase the accessibility and transparency of the software, ultimately enabling biologists or clinicians to focus on biological or medical questions.",VirusDetection,"metavisitor  suite  galaxy tool  simple  rapid detection  discovery  viruses  deep sequence data
metavisitor   software package  allow biologists  clinicians without specialize bioinformatics expertise  detect  assemble viral genomes  deep sequence datasets  package  compose   set  modular bioinformatic tool  workflows   implement   galaxy framework use  graphical galaxy workflow editor users  minimal computational skills  use exist metavisitor workflows  adapt   suit specific need  add  modify analysis modules metavisitor work  dna rna  small rna sequence data   range  read lengths   use  combination   novo  guide approach  assemble genomes  sequence read  show   software   potential  quick diagnosis  well  discovery  viruses   vast array  organisms importantly  provide  executable metavisitor use case  increase  accessibility  transparency   software ultimately enable biologists  clinicians  focus  biological  medical question",2
48,VirFind,"Development of a virus detection and discovery pipeline using next generation sequencing
Next generation sequencing (NGS) has revolutionized virus discovery. Notwithstanding, a vertical pipeline, from sample preparation to data analysis, has not been available to the plant virology community. We developed a degenerate oligonucleotide primed RT-PCR method with multiple barcodes for NGS, and constructed VirFind, a bioinformatics tool specifically for virus detection and discovery able to: (i) map and filter out host reads, (ii) deliver files of virus reads with taxonomic information and corresponding Blastn and Blastx reports, and (iii) perform conserved domain search for reads of unknown origin. The pipeline was used to process more than 30 samples resulting in the detection of all viruses known to infect the processed samples, the extension of the genomic sequences of others, and the discovery of several novel viruses. VirFind was tested by four external users with datasets from plants or insects, demonstrating its potential as a universal virus detection and discovery tool.",VirusDetection,"development   virus detection  discovery pipeline use next generation sequencing
next generation sequence ngs  revolutionize virus discovery notwithstanding  vertical pipeline  sample preparation  data analysis    available   plant virology community  develop  degenerate oligonucleotide prim rtpcr method  multiple barcodes  ngs  construct virfind  bioinformatics tool specifically  virus detection  discovery able   map  filter  host read  deliver file  virus read  taxonomic information  correspond blastn  blastx report  iii perform conserve domain search  read  unknown origin  pipeline  use  process    sample result   detection   viruses know  infect  process sample  extension   genomic sequence  others   discovery  several novel viruses virfind  test  four external users  datasets  plant  insects demonstrate  potential   universal virus detection  discovery tool",2
49,VirusFinder,"VirusFinder: software for efficient and accurate detection of viruses and their integration sites in host genomes through next generation sequencing data
Next generation sequencing (NGS) technologies allow us to explore virus interactions with host genomes that lead to carcinogenesis or other diseases; however, this effort is largely hindered by the dearth of efficient computational tools. Here, we present a new tool, VirusFinder, for the identification of viruses and their integration sites in host genomes using NGS data, including whole transcriptome sequencing (RNA-Seq), whole genome sequencing (WGS), and targeted sequencing data. VirusFinder’s unique features include the characterization of insertion loci of virus of arbitrary type in the host genome and high accuracy and computational efficiency as a result of its well-designed pipeline.",VirusDetection,"virusfinder software  efficient  accurate detection  viruses   integration sit  host genomes  next generation sequence data
next generation sequence ngs technologies allow   explore virus interactions  host genomes  lead  carcinogenesis   diseases however  effort  largely hinder   dearth  efficient computational tool   present  new tool virusfinder   identification  viruses   integration sit  host genomes use ngs data include whole transcriptome sequence rnaseq whole genome sequence wgs  target sequence data virusfinders unique feature include  characterization  insertion loci  virus  arbitrary type   host genome  high accuracy  computational efficiency   result   welldesigned pipeline",2
50,VirusSeeker,"VirusSeeker, a computational pipeline for virus discovery and virome composition analysis
The advent of Next Generation Sequencing (NGS) has vastly increased our ability to discover novel viruses and to systematically define the spectrum of viruses present in a given specimen. Such studies have led to the discovery of novel viral pathogens as well as broader associations of the virome with diverse diseases including inflammatory bowel disease, severe acute malnutrition and HIV/AIDS. Critical to the success of these efforts are robust bioinformatic pipelines for rapid classification of microbial sequences. Existing computational tools are typically focused on either eukaryotic virus discovery or virome composition analysis but not both. Here we present VirusSeeker, a BLAST-based NGS data analysis pipeline designed for both purposes. VirusSeeker has been successfully applied in several previously published virome studies. Here we demonstrate the functionality of VirusSeeker in both novel virus discovery and virome composition analysis.",VirusDetection,"virusseeker  computational pipeline  virus discovery  virome composition analysis
 advent  next generation sequence ngs  vastly increase  ability  discover novel viruses   systematically define  spectrum  viruses present   give specimen  study  lead   discovery  novel viral pathogens  well  broader associations   virome  diverse diseases include inflammatory bowel disease severe acute malnutrition  hivaids critical   success   efforts  robust bioinformatic pipelines  rapid classification  microbial sequence exist computational tool  typically focus  either eukaryotic virus discovery  virome composition analysis      present virusseeker  blastbased ngs data analysis pipeline design   purpose virusseeker   successfully apply  several previously publish virome study   demonstrate  functionality  virusseeker   novel virus discovery  virome composition analysis",2
51,Truffle,"Targeted virus detection in next-generation sequencing data using an automated e-probe based approach
The use of next-generation sequencing for plant virus detection is rapidly expanding, necessitating the development of bioinformatic pipelines to support analysis of these large datasets. Pipelines need to be easy implementable to mitigate potential insufficient computational infrastructure and/or skills. In this study user-friendly software was developed for the targeted detection of plant viruses based on e-probes. It can be used for both custom e-probe design, as well as screening preloaded probes against raw NGS data for virus detection. The pipeline was compared to de novo assembly-based virus detection in grapevine and produced comparable results, requiring less time and computational resources. The software, named Truffle, is available for the design and screening of e-probes tailored for user-specific virus species and data, along with preloaded probe-sets for grapevine virus detection.",VirusDetection,"target virus detection  nextgeneration sequence data use  automate eprobe base approach
 use  nextgeneration sequence  plant virus detection  rapidly expand necessitate  development  bioinformatic pipelines  support analysis   large datasets pipelines need   easy implementable  mitigate potential insufficient computational infrastructure andor skills   study userfriendly software  develop   target detection  plant viruses base  eprobes    use   custom eprobe design  well  screen preloaded probe  raw ngs data  virus detection  pipeline  compare   novo assemblybased virus detection  grapevine  produce comparable result require less time  computational resources  software name truffle  available   design  screen  eprobes tailor  userspecific virus species  data along  preloaded probesets  grapevine virus detection",2
52,PhiSpy,"PhiSpy: a novel algorithm for finding prophages in bacterial genomes that combines similarity- and composition-based strategies
Prophages are phages in lysogeny that are integrated into, and replicated as part of, the host bacterial genome. These mobile elements can have tremendous impact on their bacterial hosts’ genomes and phenotypes, which may lead to strain emergence and diversification, increased virulence or antibiotic resistance. However, finding prophages in microbial genomes remains a problem with no definitive solution. The majority of existing tools rely on detecting genomic regions enriched in protein-coding genes with known phage homologs, which hinders the de novo discovery of phage regions. In this study, a weighted phage detection algorithm, PhiSpy was developed based on seven distinctive characteristics of prophages, i.e. protein length, transcription strand directionality, customized AT and GC skew, the abundance of unique phage words, phage insertion points and the similarity of phage proteins. The first five characteristics are capable of identifying prophages without any sequence similarity with known phage genes. PhiSpy locates prophages by ranking genomic regions enriched in distinctive phage traits, which leads to the successful prediction of 94% of prophages in 50 complete bacterial genomes with a 6% false-negative rate and a 0.66% false-positive rate.",VirusDetection,"phispy  novel algorithm  find prophages  bacterial genomes  combine similarity  compositionbased strategies
prophages  phages  lysogeny   integrate   replicate  part   host bacterial genome  mobile elements   tremendous impact   bacterial host genomes  phenotypes  may lead  strain emergence  diversification increase virulence  antibiotic resistance however find prophages  microbial genomes remain  problem   definitive solution  majority  exist tool rely  detect genomic regions enrich  proteincoding genes  know phage homologs  hinder   novo discovery  phage regions   study  weight phage detection algorithm phispy  develop base  seven distinctive characteristics  prophages  protein length transcription strand directionality customize    skew  abundance  unique phage word phage insertion point   similarity  phage proteins  first five characteristics  capable  identify prophages without  sequence similarity  know phage genes phispy locate prophages  rank genomic regions enrich  distinctive phage traits  lead   successful prediction    prophages   complete bacterial genomes    falsenegative rate    falsepositive rate",2
53,PHAST,"PHAST: a fast phage search tool
PHAge Search Tool (PHAST) is a web server designed to rapidly and accurately identify, annotate and graphically display prophage sequences within bacterial genomes or plasmids. It accepts either raw DNA sequence data or partially annotated GenBank formatted data and rapidly performs a number of database comparisons as well as phage ‘cornerstone’ feature identification steps to locate, annotate and display prophage sequences and prophage features. Relative to other prophage identification tools, PHAST is up to 40 times faster and up to 15% more sensitive. It is also able to process and annotate both raw DNA sequence data and Genbank files, provide richly annotated tables on prophage features and prophage ‘quality’ and distinguish between intact and incomplete prophage. PHAST also generates downloadable, high quality, interactive graphics that display all identified prophage components in both circular and linear genomic views.",VirusDetection,"phast  fast phage search tool
phage search tool phast   web server design  rapidly  accurately identify annotate  graphically display prophage sequence within bacterial genomes  plasmids  accept either raw dna sequence data  partially annotate genbank format data  rapidly perform  number  database comparisons  well  phage cornerstone feature identification step  locate annotate  display prophage sequence  prophage feature relative   prophage identification tool phast     time faster      sensitive   also able  process  annotate  raw dna sequence data  genbank file provide richly annotate table  prophage feature  prophage quality  distinguish  intact  incomplete prophage phast also generate downloadable high quality interactive graphics  display  identify prophage components   circular  linear genomic view",2
54,PhageFinder,"Phage_Finder: automated identification and classification of prophage regions in complete bacterial genome sequences
Phage_Finder, a heuristic computer program, was created to identify prophage regions in completed bacterial genomes. Using a test dataset of 42 bacterial genomes whose prophages have been manually identified, Phage_Finder found 91% of the regions, resulting in 7% false positive and 9% false negative prophages. A search of 302 complete bacterial genomes predicted 403 putative prophage regions, accounting for 2.7% of the total bacterial DNA. Analysis of the 285 putative attachment sites revealed tRNAs are targets for integration slightly more frequently (33%) than intergenic (31%) or intragenic (28%) regions, while tmRNAs were targeted in 8% of the regions. The most popular tRNA targets were Arg, Leu, Ser and Thr. Mapping of the insertion point on a consensus tRNA molecule revealed novel insertion points on the 50 side of the D loop, the 30 side of the anticodon loop and the anticodon. A novel method of constructing phylogenetic trees of phages and prophages was developed based on the mean of the BLAST score ratio (BSR) of the phage/ prophage proteomes. This method verified many known bacteriophage groups, making this a useful tool for predicting the relationships of prophages from bacterial genomes.",VirusDetection,"phage_finder automate identification  classification  prophage regions  complete bacterial genome sequences
phage_finder  heuristic computer program  create  identify prophage regions  complete bacterial genomes use  test dataset   bacterial genomes whose prophages   manually identify phage_finder find    regions result   false positive   false negative prophages  search   complete bacterial genomes predict  putative prophage regions account     total bacterial dna analysis    putative attachment sit reveal trnas  target  integration slightly  frequently   intergenic   intragenic  regions  tmrnas  target     regions   popular trna target  arg leu ser  thr map   insertion point   consensus trna molecule reveal novel insertion point    side    loop   side   anticodon loop   anticodon  novel method  construct phylogenetic tree  phages  prophages  develop base   mean   blast score ratio bsr   phage prophage proteomes  method verify many know bacteriophage group make   useful tool  predict  relationships  prophages  bacterial genomes",2
55,VERSE,"VERSE: a novel approach to detect virus integration in host genomes through reference genome customization
Fueled by widespread applications of high-throughput next generation sequencing (NGS) technologies and urgent need to counter threats of pathogenic viruses, large-scale studies were conducted recently to investigate virus integration in host genomes (for example, human tumor genomes) that may cause carcinogenesis or other diseases. A limiting factor in these studies, however, is rapid virus evolution and resulting polymorphisms, which prevent reads from aligning readily to commonly used virus reference genomes, and, accordingly, make virus integration sites difficult to detect. Another confounding factor is host genomic instability as a result of virus insertions. To tackle these challenges and improve our capability to identify cryptic virus-host fusions, we present a new approach that detects Virus intEgration sites through iterative Reference SEquence customization (VERSE). To the best of our knowledge, VERSE is the first approach to improve detection through customizing reference genomes. Using 19 human tumors and cancer cell lines as test data, we demonstrated that VERSE substantially enhanced the sensitivity of virus integration site detection.",VirusDetection,"verse  novel approach  detect virus integration  host genomes  reference genome customization
fueled  widespread applications  highthroughput next generation sequence ngs technologies  urgent need  counter threats  pathogenic viruses largescale study  conduct recently  investigate virus integration  host genomes  example human tumor genomes  may cause carcinogenesis   diseases  limit factor   study however  rapid virus evolution  result polymorphisms  prevent read  align readily  commonly use virus reference genomes  accordingly make virus integration sit difficult  detect another confound factor  host genomic instability   result  virus insertions  tackle  challenge  improve  capability  identify cryptic virushost fusions  present  new approach  detect virus integration sit  iterative reference sequence customization verse   best   knowledge verse   first approach  improve detection  customize reference genomes use  human tumors  cancer cell line  test data  demonstrate  verse substantially enhance  sensitivity  virus integration site detection",2
56,Pathosphere,"Pathosphere.org: pathogen detection and characterization through a web-based, open source informatics platform
The detection of pathogens in complex sample backgrounds has been revolutionized by wide access to next-generation sequencing (NGS) platforms. However, analytical methods to support NGS platforms are not as uniformly available. Pathosphere (found at Pathosphere.org) is a cloud - based open - sourced community tool that allows for communication, collaboration and sharing of NGS analytical tools and data amongst scientists working in academia, industry and government. The architecture allows for users to upload data and run available bioinformatics pipelines without the need for onsite processing hardware or technical support. The pathogen detection capabilities hosted on Pathosphere were tested by analyzing pathogen-containing samples sequenced by NGS with both spiked human samples as well as human and zoonotic host backgrounds. Pathosphere analytical pipelines developed by Edgewood Chemical Biological Center (ECBC) identified spiked pathogens within a common sample analyzed by 454, Ion Torrent, and Illumina sequencing platforms. ECBC pipelines also correctly identified pathogens in human samples containing arenavirus in addition to animal samples containing flavivirus and coronavirus. These analytical methods were limited in the detection of sequences with limited homology to previous annotations within NCBI databases, such as parvovirus. Utilizing the pipeline-hosting adaptability of Pathosphere, the analytical suite was supplemented by analytical pipelines designed by the United States Army Medical Research Insititute of Infectious Diseases and Walter Reed Army Institute of Research (USAMRIID-WRAIR). These pipelines were implemented and detected parvovirus sequence in the sample that the ECBC iterative analysis previously failed to identify. By accurately detecting pathogens in a variety of samples, this work demonstrates the utility of Pathosphere and provides a platform for utilizing, modifying and creating pipelines for a variety of NGS technologies developed to detect pathogens in complex sample backgrounds. These results serve as an exhibition for the existing pipelines and web-based interface of Pathosphere as well as the plug-in adaptability that allows for integration of newer NGS analytical software as it becomes available.",VirusDetection,"pathosphereorg pathogen detection  characterization   webbased open source informatics platform
 detection  pathogens  complex sample background   revolutionize  wide access  nextgeneration sequence ngs platforms however analytical methods  support ngs platforms    uniformly available pathosphere find  pathosphereorg   cloud  base open  source community tool  allow  communication collaboration  share  ngs analytical tool  data amongst scientists work  academia industry  government  architecture allow  users  upload data  run available bioinformatics pipelines without  need  onsite process hardware  technical support  pathogen detection capabilities host  pathosphere  test  analyze pathogencontaining sample sequence  ngs   spike human sample  well  human  zoonotic host background pathosphere analytical pipelines develop  edgewood chemical biological center ecbc identify spike pathogens within  common sample analyze   ion torrent  illumina sequence platforms ecbc pipelines also correctly identify pathogens  human sample contain arenavirus  addition  animal sample contain flavivirus  coronavirus  analytical methods  limit   detection  sequence  limit homology  previous annotations within ncbi databases   parvovirus utilize  pipelinehosting adaptability  pathosphere  analytical suite  supplement  analytical pipelines design   unite state army medical research insititute  infectious diseases  walter reed army institute  research usamriidwrair  pipelines  implement  detect parvovirus sequence   sample   ecbc iterative analysis previously fail  identify  accurately detect pathogens   variety  sample  work demonstrate  utility  pathosphere  provide  platform  utilize modify  create pipelines   variety  ngs technologies develop  detect pathogens  complex sample background  result serve   exhibition   exist pipelines  webbased interface  pathosphere  well   plugin adaptability  allow  integration  newer ngs analytical software   become available",2
57,Decontaminer,"DecontaMiner: A Pipeline for the Detection and Analysis of Contaminating Sequences in Human NGS Sequencing Data
Reads alignment is an essential step of next generation sequencing) data analyses. One challenging issue is represented by unmapped reads that are usually discarded and considered as not informative. Instead, it is important to fully understand the source of those reads, to assess the quality of the whole experiment. Moreover, it is of interest to get some insights on possible “contamination” from non-human sequences (e.g., viruses, bacteria, and fungi). Contamination may take place during the experimental procedures leading to sequencing, or be due to the presence of microorganisms infecting the sampled tissues. Here we propose a pipeline for the detection of viral, bacterial, and fungi contamination in human sequenced data. Similarities between input reads (query) and putative contaminating organism sequences (subject) are detected using a local alignment strategy (MegaBLAST). For each organism database DecontaMiner provides two main output files: one containing all the reads matching only a single organism; the second one containing the “ambiguous” matching reads. In both files, data is sorted by organism and classified by taxonomic group. Low quality, unaligned sequences, and those discarded by user criteria are also provided as output. Other information and summary statistics on the number of matched/filtered/discarded reads and organisms are generated. This pipeline has successfully detected foreign sequences in human Cancer RNA-seq data.",VirusDetection,"decontaminer  pipeline   detection  analysis  contaminate sequence  human ngs sequence data
reads alignment   essential step  next generation sequence data analyse one challenge issue  represent  unmapped read   usually discard  consider   informative instead   important  fully understand  source   read  assess  quality   whole experiment moreover    interest  get  insights  possible “contamination”  nonhuman sequence  viruses bacteria  fungi contamination may take place   experimental procedures lead  sequence   due   presence  microorganisms infect  sample tissue   propose  pipeline   detection  viral bacterial  fungi contamination  human sequence data similarities  input read query  putative contaminate organism sequence subject  detect use  local alignment strategy megablast   organism database decontaminer provide two main output file one contain   read match   single organism  second one contain  “ambiguous” match read   file data  sort  organism  classify  taxonomic group low quality unaligned sequence   discard  user criteria  also provide  output  information  summary statistics   number  matchedfiltereddiscarded read  organisms  generate  pipeline  successfully detect foreign sequence  human cancer rnaseq data",2
58,ART-DECO,"ART-DeCo: easy tool for detection and characterization of cross-contamination of DNA samples in diagnostic next-generation sequencing analysis
Next-generation sequencing (NGS) is routinely used for constitutional genetic analysis. However, cross-contamination between samples constitutes a major risk that could impact the results of the analysis. We have developed ART-DeCo, a tool using the allelic ratio (AR) of the Single Nucleotide Polymorphisms sequenced with regions of interest. When a sample is contaminated by DNA with a different genotype, unexpected ARs are obtained, which are in turn used for detection of contamination with a screening test, followed by identification and quantification of the contaminant. Following optimization, ART-DeCo was applied to 2222 constitutional DNA samples. The screening test was positive for 191 samples. In 33 cases (contamination percentages: 1.3% to 29.2%), the contaminant was identified and was mostly located in adjacent wells. Three other positive cases were due to barcoding errors or mixture of two DNA samples. Interestingly, the last contaminated sample corresponded to a bone marrow transplant recipient. Lastly, no contaminant was identified in 154 weakly positive ( < 4%) samples that were considered to be irrelevant to constitutional genetic analysis. ART-DeCo lends itself to mandatory quality control procedures, also highlighting the delicate steps of library preparation, resulting in practice improvement.",VirusDetection,"artdeco easy tool  detection  characterization  crosscontamination  dna sample  diagnostic nextgeneration sequence analysis
nextgeneration sequence ngs  routinely use  constitutional genetic analysis however crosscontamination  sample constitute  major risk  could impact  result   analysis   develop artdeco  tool use  allelic ratio    single nucleotide polymorphisms sequence  regions  interest   sample  contaminate  dna   different genotype unexpected ars  obtain    turn use  detection  contamination   screen test follow  identification  quantification   contaminant follow optimization artdeco  apply   constitutional dna sample  screen test  positive   sample   case contamination percentages     contaminant  identify   mostly locate  adjacent well three  positive case  due  barcoding errors  mixture  two dna sample interestingly  last contaminate sample correspond   bone marrow transplant recipient lastly  contaminant  identify   weakly positive    sample   consider   irrelevant  constitutional genetic analysis artdeco lend   mandatory quality control procedures also highlight  delicate step  library preparation result  practice improvement",2
59,PaPrBaG,"PaPrBaG: A machine learning approach for the detection of novel pathogens from NGS data
The reliable detection of novel bacterial pathogens from next-generation sequencing data is a key challenge for microbial diagnostics. Current computational tools usually rely on sequence similarity and often fail to detect novel species when closely related genomes are unavailable or missing from the reference database. Here we present the machine learning based approach PaPrBaG (Pathogenicity Prediction for Bacterial Genomes). PaPrBaG overcomes genetic divergence by training on a wide range of species with known pathogenicity phenotype. To that end we compiled a comprehensive list of pathogenic and non-pathogenic bacteria with human host, using various genome metadata in conjunction with a rule-based protocol. A detailed comparative study reveals that PaPrBaG has several advantages over sequence similarity approaches. Most importantly, it always provides a prediction whereas other approaches discard a large number of sequencing reads with low similarity to currently known reference genomes. Furthermore, PaPrBaG remains reliable even at very low genomic coverages. CombiningPaPrBaG with existing approaches further improves prediction results.",VirusDetection,"paprbag  machine learn approach   detection  novel pathogens  ngs data
 reliable detection  novel bacterial pathogens  nextgeneration sequence data   key challenge  microbial diagnostics current computational tool usually rely  sequence similarity  often fail  detect novel species  closely relate genomes  unavailable  miss   reference database   present  machine learn base approach paprbag pathogenicity prediction  bacterial genomes paprbag overcome genetic divergence  train   wide range  species  know pathogenicity phenotype   end  compile  comprehensive list  pathogenic  nonpathogenic bacteria  human host use various genome metadata  conjunction   rulebased protocol  detail comparative study reveal  paprbag  several advantage  sequence similarity approach  importantly  always provide  prediction whereas  approach discard  large number  sequence read  low similarity  currently know reference genomes furthermore paprbag remain reliable even   low genomic coverages combiningpaprbag  exist approach  improve prediction result",2
60,VirVarSeq,"VirVarSeq: a low-frequency virus variant detection pipeline for Illumina sequencing using adaptive base-calling accuracy filtering
In virology, massively parallel sequencing (MPS) opens many opportunities for studying viral quasi-species, e.g. in HIV-1- and HCV-infected patients. This is essential for understanding pathways to resistance, which can substantially improve treatment. Although MPS platforms allow in-depth characterization of sequence variation, their measurements still involve substantial technical noise. For Illumina sequencing, single base substitutions are the main error source and impede powerful assessment of low-frequency mutations. Fortunately, base calls are complemented with quality scores (Qs) that are useful for differentiating errors from the real low-frequency mutations. A variant calling tool, Q-cpileup, is proposed, which exploits the Qs of nucleotides in a filtering strategy to increase specificity. The tool is imbedded in an open-source pipeline, VirVarSeq, which allows variant calling starting from fastq files. Using both plasmid mixtures and clinical samples, we show that Q-cpileup is able to reduce the number of false-positive findings. The filtering strategy is adaptive and provides an optimized threshold for individual samples in each sequencing run. Additionally, linkage information is kept between single-nucleotide polymorphisms as variants are called at the codon level. This enables virologists to have an immediate biological interpretation of the reported variants with respect to their antiviral drug responses. A comparison with existing SNP caller tools reveals that calling variants at the codon level with Q-cpileup results in an outstanding sensitivity while maintaining a good specificity for variants with frequencies down to 0.5%.",VirusDetection,"virvarseq  lowfrequency virus variant detection pipeline  illumina sequence use adaptive basecalling accuracy filtering
 virology massively parallel sequence mps open many opportunities  study viral quasispecies   hiv  hcvinfected patients   essential  understand pathways  resistance   substantially improve treatment although mps platforms allow indepth characterization  sequence variation  measurements still involve substantial technical noise  illumina sequence single base substitutions   main error source  impede powerful assessment  lowfrequency mutations fortunately base call  complement  quality score    useful  differentiate errors   real lowfrequency mutations  variant call tool qcpileup  propose  exploit    nucleotides   filter strategy  increase specificity  tool  imbed   opensource pipeline virvarseq  allow variant call start  fastq file use  plasmid mixtures  clinical sample  show  qcpileup  able  reduce  number  falsepositive find  filter strategy  adaptive  provide  optimize threshold  individual sample   sequence run additionally linkage information  keep  singlenucleotide polymorphisms  variants  call   codon level  enable virologists    immediate biological interpretation   report variants  respect   antiviral drug responses  comparison  exist snp caller tool reveal  call variants   codon level  qcpileup result   outstanding sensitivity  maintain  good specificity  variants  frequencies   ",2
61,Giant Virus Finder,"The ""Giant Virus Finder"" discovers an abundance of giant viruses in the Antarctic dry valleys
Mimivirus was identified in 2003 from a biofilm of anindustrial water-cooling tower in England. Later, numerous new giant viruses were found in oceans and freshwater habitats, some of them having 2,500 genes.We have demonstrated their likely presence in four soil samples taken from the Kutch Desert (Gujarat, India). Here we describe a bioinformatics work-flow, called the ‘‘Giant Virus Finder’’ that is capable of discovering the likely presence ofthe genomes of giant viruses in metagenomic shotgun-sequenced datasets. The new workflow is applied to numerous hot and cold desert soil samples as well as some tundra- and forest soils.We show that most of these samples contain giant viruses, especially in the Antarctic dry valleys. The results imply that giant viruses could be frequent not only in aqueous habitats, but in a wide spectrum of soils on our planet.",VirusDetection," ""giant virus finder"" discover  abundance  giant viruses   antarctic dry valleys
mimivirus  identify     biofilm  anindustrial watercooling tower  england later numerous new giant viruses  find  oceans  freshwater habitats      geneswe  demonstrate  likely presence  four soil sample take   kutch desert gujarat india   describe  bioinformatics workflow call  giant virus finder   capable  discover  likely presence ofthe genomes  giant viruses  metagenomic shotgunsequenced datasets  new workflow  apply  numerous hot  cold desert soil sample  well   tundra  forest soilswe show     sample contain giant viruses especially   antarctic dry valleys  result imply  giant viruses could  frequent    aqueous habitats    wide spectrum  soil   planet",2
62,IMSA,"IMSA: Integrated Metagenomic Sequence Analysis for Identification of Exogenous Reads in a Host Genomic Background
Metagenomics, the study of microbial genomes within diverse environments, is a rapidly developing field. The identification of microbial sequences within a host organism enables the study of human intestinal, respiratory, and skin microbiota, and has allowed the identification of novel viruses in diseases such as Merkel cell carcinoma. There are few publicly available tools for metagenomic high throughput sequence analysis. We present Integrated Metagenomic Sequence Analysis (IMSA), a flexible, fast, and robust computational analysis pipeline that is available for public use. IMSA takes input sequence from high throughput datasets and uses a user-defined host database to filter out host sequence. IMSA then aligns the filtered reads to a user-defined universal database to characterize exogenous reads within the host background. IMSA assigns a score to each node of the taxonomy based on read frequency, and can output this as a taxonomy report suitable for cluster analysis or as a taxonomy map (TaxMap). IMSA also outputs the specific sequence reads assigned to a taxon of interest for downstream analysis. We demonstrate the use of IMSA to detect pathogens and normal flora within sequence data from a primary human cervical cancer carrying HPV16, a primary human cutaneous squamous cell carcinoma carrying HPV 16, the CaSki cell line carrying HPV16, and the HeLa cell line carrying HPV18.",VirusDetection,"imsa integrate metagenomic sequence analysis  identification  exogenous read   host genomic background
metagenomics  study  microbial genomes within diverse environments   rapidly develop field  identification  microbial sequence within  host organism enable  study  human intestinal respiratory  skin microbiota   allow  identification  novel viruses  diseases   merkel cell carcinoma    publicly available tool  metagenomic high throughput sequence analysis  present integrate metagenomic sequence analysis imsa  flexible fast  robust computational analysis pipeline   available  public use imsa take input sequence  high throughput datasets  use  userdefined host database  filter  host sequence imsa  align  filter read   userdefined universal database  characterize exogenous read within  host background imsa assign  score   node   taxonomy base  read frequency   output    taxonomy report suitable  cluster analysis    taxonomy map taxmap imsa also output  specific sequence read assign   taxon  interest  downstream analysis  demonstrate  use  imsa  detect pathogens  normal flora within sequence data   primary human cervical cancer carry hpv16  primary human cutaneous squamous cell carcinoma carry hpv   caski cell line carry hpv16   hela cell line carry hpv18",2
63,DeepVirFinder,"Identifying viruses from metagenomic data using deep learning
The recent development of metagenomic sequencing makes it possible to massively sequence microbial genomes including viral genomes without the need for laboratory culture. Existing reference-based and gene homology-based methods are not efﬁcient in identifying unknown viruses or short viral sequences from metagenomic data. Methods: Here we developed a reference-free and alignment-free machine learning method, DeepVirFinder, for identifying viral sequences in metagenomic data using deep learning. Trained based on sequences from viral RefSeq discovered before May 2015, and evaluated on those discovered after that date, DeepVirFinder outperformed the state-of-the-art method VirFinder at all contig lengths, achieving AUROC 0.93, 0.95, 0.97, and 0.98 for 300, 500, 1000, and 3000 bp sequences respectively. Enlarging the training data with additional millions of puriﬁed viral sequences from metavirome samples further improved the accuracy for identifying virus groups that are under-represented. Applying DeepVirFinder to real human gut metagenomic samples, we identiﬁed 51,138 viral sequences belonging to 175 bins in patients with colorectal carcinoma (CRC). Ten bins were found associated with the cancer status, suggesting viruses may play important roles in CRC. Powered by deep learning and high throughput sequencing metagenomic data, DeepVirFinder signiﬁcantly improved the accuracy of viral identiﬁcation and will assist the study of viruses in the era of metagenomics.",VirusIdentification,"identify viruses  metagenomic data use deep learning
 recent development  metagenomic sequence make  possible  massively sequence microbial genomes include viral genomes without  need  laboratory culture exist referencebased  gene homologybased methods   efﬁcient  identify unknown viruses  short viral sequence  metagenomic data methods   develop  referencefree  alignmentfree machine learn method deepvirfinder  identify viral sequence  metagenomic data use deep learn train base  sequence  viral refseq discover  may   evaluate   discover   date deepvirfinder outperform  stateoftheart method virfinder   contig lengths achieve auroc             sequence respectively enlarge  train data  additional millions  puriﬁed viral sequence  metavirome sample  improve  accuracy  identify virus group   underrepresented apply deepvirfinder  real human gut metagenomic sample  identiﬁed  viral sequence belong   bin  patients  colorectal carcinoma crc ten bin  find associate   cancer status suggest viruses may play important roles  crc power  deep learn  high throughput sequence metagenomic data deepvirfinder signiﬁcantly improve  accuracy  viral identiﬁcation   assist  study  viruses   era  metagenomics",3
64,ViraMiner,"ViraMiner: Deep learning on raw DNA sequences for identifying viral genomes in human samples
Despite its clinical importance, detection of highly divergent or yet unknown viruses is a major challenge. When human samples are sequenced, conventional alignments classify many assembled contigs as “unknown” since many of the sequences are not similar to known genomes. In this work, we developed ViraMiner, a deep learning-based method to identify viruses in various human biospecimens. ViraMiner contains two branches of Convolutional Neural Networks designed to detect both patterns and pattern-frequencies on raw metagenomics contigs. The training dataset included sequences obtained from 19 metagenomic experiments which were analyzed and labeled by BLAST. The model achieves significantly improved accuracy compared to other machine learning methods for viral genome classification. Using 300 bp contigs ViraMiner achieves 0.923 area under the ROC curve. To our knowledge, this is the first machine learning methodology that can detect the presence of viral sequences among raw metagenomic contigs from diverse human samples. We suggest that the proposed model captures different types of information of genome composition, and can be used as a recommendation system to further investigate sequences labeled as “unknown” by conventional alignment methods. Exploring these highly-divergent viruses, in turn, can enhance our knowledge of infectious causes of diseases.",VirusIdentification,"viraminer deep learn  raw dna sequence  identify viral genomes  human samples
despite  clinical importance detection  highly divergent  yet unknown viruses   major challenge  human sample  sequence conventional alignments classify many assemble contigs  “unknown” since many   sequence   similar  know genomes   work  develop viraminer  deep learningbased method  identify viruses  various human biospecimens viraminer contain two branch  convolutional neural network design  detect  pattern  patternfrequencies  raw metagenomics contigs  train dataset include sequence obtain   metagenomic experiment   analyze  label  blast  model achieve significantly improve accuracy compare   machine learn methods  viral genome classification use   contigs viraminer achieve  area   roc curve   knowledge    first machine learn methodology   detect  presence  viral sequence among raw metagenomic contigs  diverse human sample  suggest   propose model capture different type  information  genome composition    use   recommendation system   investigate sequence label  “unknown”  conventional alignment methods explore  highlydivergent viruses  turn  enhance  knowledge  infectious cause  diseases",3
65,VirFinder,"VirFinder: a novel k-mer based tool for identifying viral sequences from assembled metagenomic data
Identifying viral sequences in mixed metagenomes containing both viral and host contigs is a critical first step in analyzing the viral component of samples. Current tools for distinguishing prokaryotic virus and host contigs primarily use gene-based similarity approaches. Such approaches can significantly limit results especially for short contigs that have few predicted proteins or lack proteins with similarity to previously known viruses. We have developed VirFinder, the first k-mer frequency based, machine learning method for virus contig identification that entirely avoids gene-based similarity searches. VirFinder instead identifies viral sequences based on our empirical observation that viruses and hosts have discernibly different k-mer signatures. VirFinder’s performance in correctly identifying viral sequences was tested by training its machine learning model on sequences from host and viral genomes sequenced before 1 January 2014 and evaluating on sequences obtained after 1 January 2014. VirFinder had significantly better rates of identifying true viral contigs (true positive rates (TPRs)) than VirSorter, the current state-of-the-art gene-based virus classification tool, when evaluated with either contigs subsampled from complete genomes or assembled from a simulated human gut metagenome. For example, for contigs subsampled from complete genomes, VirFinder had 78-, 2.4-, and 1.8-fold higher TPRs than VirSorter for 1, 3, and 5 kb contigs, respectively, at the same false positive rates as VirSorter (0, 0.003, and 0.006, respectively), thus VirFinder works considerably better for small contigs than VirSorter. VirFinder furthermore identified several recently sequenced virus genomes (after 1 January 2014) that VirSorter did not and that have no nucleotide similarity to previously sequenced viruses, demonstrating VirFinder’s potential advantage in identifying novel viral sequences. Application of VirFinder to a set of human gut metagenomes from healthy and liver cirrhosis patients reveals higher viral diversity in healthy individuals than cirrhosis patients. We also identified contig bins containing crAssphage-like contigs with higher abundance in healthy patients and a putative Veillonella genus prophage associated with cirrhosis patients. This innovative k-mer based tool complements gene-based approaches and will significantly improve prokaryotic viral sequence identification, especially for metagenomic-based studies of viral ecology.",VirusIdentification,"virfinder  novel kmer base tool  identify viral sequence  assemble metagenomic data
identifying viral sequence  mix metagenomes contain  viral  host contigs   critical first step  analyze  viral component  sample current tool  distinguish prokaryotic virus  host contigs primarily use genebased similarity approach  approach  significantly limit result especially  short contigs    predict proteins  lack proteins  similarity  previously know viruses   develop virfinder  first kmer frequency base machine learn method  virus contig identification  entirely avoid genebased similarity search virfinder instead identify viral sequence base   empirical observation  viruses  host  discernibly different kmer signatures virfinders performance  correctly identify viral sequence  test  train  machine learn model  sequence  host  viral genomes sequence   january   evaluate  sequence obtain   january  virfinder  significantly better rat  identify true viral contigs true positive rat tprs  virsorter  current stateoftheart genebased virus classification tool  evaluate  either contigs subsampled  complete genomes  assemble   simulate human gut metagenome  example  contigs subsampled  complete genomes virfinder     fold higher tprs  virsorter       contigs respectively    false positive rat  virsorter     respectively thus virfinder work considerably better  small contigs  virsorter virfinder furthermore identify several recently sequence virus genomes   january   virsorter       nucleotide similarity  previously sequence viruses demonstrate virfinders potential advantage  identify novel viral sequence application  virfinder   set  human gut metagenomes  healthy  liver cirrhosis patients reveal higher viral diversity  healthy individuals  cirrhosis patients  also identify contig bin contain crassphagelike contigs  higher abundance  healthy patients   putative veillonella genus prophage associate  cirrhosis patients  innovative kmer base tool complement genebased approach   significantly improve prokaryotic viral sequence identification especially  metagenomicbased study  viral ecology",3
66,VirNet,"VirNet: Deep attention model for viral reads identification
Metagenomics shows a promising understanding of function and diversity of the microbial communities due to the difficulty of studying microorganism with pure culture isolation. Moreover, the viral identification is considered one of the essential steps in studying microbial communities. Several studies show different methods to identify viruses in mixed metagenomic data using homology and statistical techniques. These techniques have many limitations due to viral genome diversity. In this work, we propose a deep attention model for viral identification of metagenomic data. For testing purpose, we generated fragments of viruses and bacteria from RefSeq genomes with different lengths to find the best hyperparameters for our model. Then, we simulated both microbiome and virome high throughput data from our test dataset with aim of validating our approach. We compared our tool to the state-of-the-art statistical tool for viral identification and found the performance of VirNet much better regarding accuracy on the same testing data.",VirusIdentification,"virnet deep attention model  viral read identification
metagenomics show  promise understand  function  diversity   microbial communities due   difficulty  study microorganism  pure culture isolation moreover  viral identification  consider one   essential step  study microbial communities several study show different methods  identify viruses  mix metagenomic data use homology  statistical techniques  techniques  many limitations due  viral genome diversity   work  propose  deep attention model  viral identification  metagenomic data  test purpose  generate fragment  viruses  bacteria  refseq genomes  different lengths  find  best hyperparameters   model   simulate  microbiome  virome high throughput data   test dataset  aim  validate  approach  compare  tool   stateoftheart statistical tool  viral identification  find  performance  virnet much better regard accuracy    test data",3
67,VirSorter,"VirSorter: mining viral signal from microbial genomic data
Viruses of microbes impact all ecosystems where microbes drive key energy and substrate transformations including the oceans, humans and industrial fermenters. However, despite this recognized importance, our understanding of viral diversity and impacts remains limited by too few model systems and reference genomes. One way to fill these gaps in our knowledge of viral diversity is through the detection of viral signal in microbial genomic data. While multiple approaches have been developed and applied for the detection of prophages (viral genomes integrated in a microbial genome), new types of microbial genomic data are emerging that are more fragmented and larger scale, such as Single-cell Amplified Genomes (SAGs) of uncultivated organisms or genomic fragments assembled from metagenomic sequencing. Here, we present VirSorter, a tool designed to detect viral signal in these different types of microbial sequence data in both a reference-dependent and reference-independent manner, leveraging probabilistic models and extensive virome data to maximize detection of novel viruses. Performance testing shows that VirSorter’s prophage prediction capability compares to that of available prophage predictors for complete genomes, but is superior in predicting viral sequences outside of a host genome (i.e., from extrachromosomal prophages, lytic infections, or partially assembled prophages). Furthermore, VirSorter outperforms existing tools for fragmented genomic and metagenomic datasets, and can identify viral signal in assembled sequence (contigs) as short as 3kb, while providing near-perfect identification (>95% Recall and 100% Precision) on contigs of at least 10kb. Because VirSorter scales to large datasets, it can also be used in “reverse” to more confidently identify viral sequence in viral metagenomes by sorting away cellular DNA whether derived from gene transfer agents, generalized transduction or contamination. Finally, VirSorter is made available through the iPlant Cyberinfrastructure that provides a web-based user interface interconnected with the required computing resources. VirSorter thus complements existing prophage prediction softwares to better leverage fragmented, SAG and metagenomic datasets in a way that will scale to modern sequencing. Given these features, VirSorter should enable the discovery of new viruses in microbial datasets, and further our understanding of uncultivated viral communities across diverse ecosystems.",VirusIdentification,"virsorter mine viral signal  microbial genomic data
viruses  microbes impact  ecosystems  microbes drive key energy  substrate transformations include  oceans humans  industrial fermenters however despite  recognize importance  understand  viral diversity  impact remain limit    model systems  reference genomes one way  fill  gap   knowledge  viral diversity    detection  viral signal  microbial genomic data  multiple approach   develop  apply   detection  prophages viral genomes integrate   microbial genome new type  microbial genomic data  emerge    fragment  larger scale   singlecell amplify genomes sag  uncultivated organisms  genomic fragment assemble  metagenomic sequence   present virsorter  tool design  detect viral signal   different type  microbial sequence data    referencedependent  referenceindependent manner leverage probabilistic model  extensive virome data  maximize detection  novel viruses performance test show  virsorters prophage prediction capability compare    available prophage predictors  complete genomes   superior  predict viral sequence outside   host genome   extrachromosomal prophages lytic infections  partially assemble prophages furthermore virsorter outperform exist tool  fragment genomic  metagenomic datasets   identify viral signal  assemble sequence contigs  short  3kb  provide nearperfect identification  recall   precision  contigs   least 10kb  virsorter scale  large datasets   also  use  “reverse”   confidently identify viral sequence  viral metagenomes  sort away cellular dna whether derive  gene transfer agents generalize transduction  contamination finally virsorter  make available   iplant cyberinfrastructure  provide  webbased user interface interconnect   require compute resources virsorter thus complement exist prophage prediction softwares  better leverage fragment sag  metagenomic datasets   way   scale  modern sequence give  feature virsorter  enable  discovery  new viruses  microbial datasets    understand  uncultivated viral communities across diverse ecosystems",3
68,VirusDetect,"VirusDetect: An automated pipeline for efficient virus discovery using deep sequencing of small RNAs
Accurate detection of viruses in plants and animals is critical for agriculture production and human health. Deep sequencing and assembly of virus-derived small interfering RNAs has proven to be a highly eﬃcient approach for virus discovery. Here we present VirusDetect, a bioinformatics pipeline that can eﬃciently analyze largescale small RNA (sRNA) datasets for both known and novel virus identiﬁcation. VirusDetect performs both reference-guided assemblies through aligning sRNA sequences to a curated virus reference database and de novo assemblies of sRNA sequences with automated parameter optimization and the option of host sRNA subtraction. The assembled contigs are compared to a curated and classiﬁed reference virus database for known and novel virus identiﬁcation, and evaluated for their sRNA size proﬁles to identify novel viruses. Extensive evaluations using plant and insect sRNA datasets suggest that VirusDetect is highly sensitive and eﬃcient in identifying known and novel viruses.",VirusIdentification,"virusdetect  automate pipeline  efficient virus discovery use deep sequence  small rnas
accurate detection  viruses  plant  animals  critical  agriculture production  human health deep sequence  assembly  virusderived small interfere rnas  prove    highly eﬃcient approach  virus discovery   present virusdetect  bioinformatics pipeline   eﬃciently analyze largescale small rna srna datasets   know  novel virus identiﬁcation virusdetect perform  referenceguided assemblies  align srna sequence   curated virus reference database   novo assemblies  srna sequence  automate parameter optimization   option  host srna subtraction  assemble contigs  compare   curated  classiﬁed reference virus database  know  novel virus identiﬁcation  evaluate   srna size proﬁles  identify novel viruses extensive evaluations use plant  insect srna datasets suggest  virusdetect  highly sensitive  eﬃcient  identify know  novel viruses",3
69,VIP,"VIP: an integrated pipeline for metagenomics of virus identification and discovery
Identification and discovery of viruses using next-generation sequencing technology is a fast-developing area with potential wide application in clinical diagnostics, public health monitoring and novel virus discovery. However, tremendous sequence data from NGS study has posed great challenge both in accuracy and velocity for application of NGS study. Here we describe VIP (“Virus Identification Pipeline”), a one-touch computational pipeline for virus identification and discovery from metagenomic NGS data. VIP performs the following steps to achieve its goal: (i) map and filter out background-related reads, (ii) extensive classification of reads on the basis of nucleotide and remote amino acid homology, (iii) multiple k-mer based de novo assembly and phylogenetic analysis to provide evolutionary insight. We validated the feasibility and veracity of this pipeline with sequencing results of various types of clinical samples and public datasets. VIP has also contributed to timely virus diagnosis (~10 min) in acutely ill patients, demonstrating its potential in the performance of unbiased NGS-based clinical studies with demand of short turnaround time.",VirusIdentification,"vip  integrate pipeline  metagenomics  virus identification  discovery
identification  discovery  viruses use nextgeneration sequence technology   fastdeveloping area  potential wide application  clinical diagnostics public health monitor  novel virus discovery however tremendous sequence data  ngs study  pose great challenge   accuracy  velocity  application  ngs study   describe vip “virus identification pipeline”  onetouch computational pipeline  virus identification  discovery  metagenomic ngs data vip perform  follow step  achieve  goal  map  filter  backgroundrelated read  extensive classification  read   basis  nucleotide  remote amino acid homology iii multiple kmer base  novo assembly  phylogenetic analysis  provide evolutionary insight  validate  feasibility  veracity   pipeline  sequence result  various type  clinical sample  public datasets vip  also contribute  timely virus diagnosis ~ min  acutely ill patients demonstrate  potential   performance  unbiased ngsbased clinical study  demand  short turnaround time",3
70,VSDToolkit,"An internet-based bioinformatics toolkit for plant biosecurity diagnosis and surveillance of viruses and viroids
Detection and preventing entry of exotic viruses and viroids at the border is critical for protecting plant industries trade worldwide. Existing post entry quarantine screening protocols rely on time-consuming biological indicators and/or molecular assays that require knowledge of infecting viral pathogens. Plants have developed the ability to recognise and respond to viral infections through Dicer-like enzymes that cleave viral sequences into specific small RNA products. Many studies reported the use of a broad range of small RNAs encompassing the product sizes of several Dicer enzymes involved in distinct biological pathways. Here we optimise the assembly of viral sequences by using specific small RNA subsets. Results: We sequenced the small RNA fractions of 21 plants held at quarantine glasshouse facilities in Australia and New Zealand. Benchmarking of several de novo assembler tools yielded SPAdes using a kmer of 19 to produce the best assembly outcomes. We also found that de novo assembly using 21–25 nt small RNAs can result in chimeric assemblies of viral sequences and plant host sequences. Such non-specific assemblies can be resolved by using 21–22 nt or 24 nt small RNAs subsets. Among the 21 selected samples, we identified contigs with sequence similarity to 18 viruses and 3 viroids in 13 samples. Most of the viruses were assembled using only 21–22 nt long virus-derived siRNAs (viRNAs), except for one Citrus endogenous pararetrovirus that was more efficiently assembled using 24 nt long viRNAs. All three viroids found in this study were fully assembled using either 21–22 nt or 24 nt viRNAs. Optimised analysis workflows were customised within the Yabi web-based analytical environment. We present a fully automated viral surveillance and diagnosis web-based bioinformatics toolkit that provides a flexible, user-friendly, robust and scalable interface for the discovery and diagnosis of viral pathogens. Conclusions: We have implemented an automated viral surveillance and diagnosis (VSD) bioinformatics toolkit that produces improved viruses and viroid sequence assemblies. The VSD toolkit provides several optimised and reusable workflows applicable to distinct viral pathogens. We envisage that this resource will facilitate the surveillance and diagnosis viral pathogens in plants, insects and invertebrates.",VirusIdentification," internetbased bioinformatics toolkit  plant biosecurity diagnosis  surveillance  viruses  viroids
detection  prevent entry  exotic viruses  viroids   border  critical  protect plant industries trade worldwide exist post entry quarantine screen protocols rely  timeconsuming biological indicators andor molecular assay  require knowledge  infect viral pathogens plant  develop  ability  recognise  respond  viral infections  dicerlike enzymes  cleave viral sequence  specific small rna products many study report  use   broad range  small rnas encompass  product size  several dicer enzymes involve  distinct biological pathways   optimise  assembly  viral sequence  use specific small rna subsets result  sequence  small rna fraction   plant hold  quarantine glasshouse facilities  australia  new zealand benchmarking  several  novo assembler tool yield spade use  kmer    produce  best assembly outcomes  also find   novo assembly use   small rnas  result  chimeric assemblies  viral sequence  plant host sequence  nonspecific assemblies   resolve  use      small rnas subsets among   select sample  identify contigs  sequence similarity   viruses   viroids   sample    viruses  assemble use    long virusderived sirnas virnas except  one citrus endogenous pararetrovirus    efficiently assemble use   long virnas  three viroids find   study  fully assemble use either      virnas optimise analysis workflows  customise within  yabi webbased analytical environment  present  fully automate viral surveillance  diagnosis webbased bioinformatics toolkit  provide  flexible userfriendly robust  scalable interface   discovery  diagnosis  viral pathogens conclusions   implement  automate viral surveillance  diagnosis vsd bioinformatics toolkit  produce improve viruses  viroid sequence assemblies  vsd toolkit provide several optimise  reusable workflows applicable  distinct viral pathogens  envisage   resource  facilitate  surveillance  diagnosis viral pathogens  plant insects  invertebrates",3
71,VirusHunter,"Identification of novel viruses using VirusHunter--an automated data analysis pipeline
Quick and accurate identification of microbial pathogens is essential for both diagnosis and response to emerging infectious diseases. The advent of next-generation sequencing technology offers an unprecedented platform for rapid sequencing-based identification of novel viruses. We have developed a customized bioinformatics data analysis pipeline, VirusHunter, for the analysis of Roche/454 and other long read Next generation sequencing platform data. To illustrate the utility of VirusHunter, we performed Roche/454 GS FLX titanium sequencing on two unclassified virus isolates from the World Reference Center for Emerging Viruses and Arboviruses (WRCEVA). VirusHunter identified sequences derived from a novel bunyavirus and a novel reovirus in the two samples respectively. Further sequence analysis demonstrated that the viruses were novel members of the Phlebovirus and Orbivirus genera. Both Phlebovirus and Orbivirus genera include many economic important viruses or serious human pathogens.",VirusIdentification,"identification  novel viruses use virushunteran automate data analysis pipeline
quick  accurate identification  microbial pathogens  essential   diagnosis  response  emerge infectious diseases  advent  nextgeneration sequence technology offer  unprecedented platform  rapid sequencingbased identification  novel viruses   develop  customize bioinformatics data analysis pipeline virushunter   analysis  roche   long read next generation sequence platform data  illustrate  utility  virushunter  perform roche  flx titanium sequence  two unclassified virus isolate   world reference center  emerge viruses  arboviruses wrceva virushunter identify sequence derive   novel bunyavirus   novel reovirus   two sample respectively  sequence analysis demonstrate   viruses  novel members   phlebovirus  orbivirus genera  phlebovirus  orbivirus genera include many economic important viruses  serious human pathogens",3
72,Vipie,"Vipie: web pipeline for parallel characterization of viral populations from multiple NGS samples
Next generation sequencing (NGS) technology allows laboratories to investigate virome composition in clinical and environmental samples in a culture-independent way. There is a need for bioinformatic tools capable of parallel processing of virome sequencing data by exactly identical methods: this is especially important in studies of multifactorial diseases, or in parallel comparison of laboratory protocols. We have developed a web-based application allowing direct upload of sequences from multiple virome samples using custom parameters. The samples are then processed in parallel using an identical protocol, and can be easily reanalyzed. The pipeline performs de-novo assembly, taxonomic classification of viruses as well as sample analyses based on user-defined grouping categories. Tables of virus abundance are produced from cross-validation by remapping the sequencing reads to a union of all observed reference viruses. In addition, read sets and reports are created after processing unmapped reads against known human and bacterial ribosome references. Secured interactive results are dynamically plotted with population and diversity charts, clustered heatmaps and a sortable and searchable abundance table. The Vipie web application is a unique tool for multi-sample metagenomic analysis of viral data, producing searchable hits tables, interactive population maps, alpha diversity measures and clustered heatmaps that are grouped in applicable custom sample categories. Known references such as human genome and bacterial ribosomal genes are optionally removed from unmapped (‘dark matter’) reads. Secured results are accessible and shareable on modern browsers",VirusIdentification,"vipie web pipeline  parallel characterization  viral populations  multiple ngs samples
next generation sequence ngs technology allow laboratories  investigate virome composition  clinical  environmental sample   cultureindependent way    need  bioinformatic tool capable  parallel process  virome sequence data  exactly identical methods   especially important  study  multifactorial diseases   parallel comparison  laboratory protocols   develop  webbased application allow direct upload  sequence  multiple virome sample use custom parameters  sample   process  parallel use  identical protocol    easily reanalyzed  pipeline perform denovo assembly taxonomic classification  viruses  well  sample analyse base  userdefined group categories table  virus abundance  produce  crossvalidation  remapping  sequence read   union   observe reference viruses  addition read set  report  create  process unmapped read  know human  bacterial ribosome reference secure interactive result  dynamically plot  population  diversity chart cluster heatmaps   sortable  searchable abundance table  vipie web application   unique tool  multisample metagenomic analysis  viral data produce searchable hit table interactive population map alpha diversity measure  cluster heatmaps   group  applicable custom sample categories know reference   human genome  bacterial ribosomal genes  optionally remove  unmapped dark matter read secure result  accessible  shareable  modern browsers",3
73,ViromeScan,"ViromeScan: a new tool for metagenomic viral community profiling
Bioinformatics tools available for metagenomic sequencing analysis are principally devoted to the identification of microorganisms populating an ecological niche, but they usually do not consider viruses. Only some software have been designed to profile the viral sequences, however they are not efficient in the characterization of viruses in the context of complex communities, like the intestinal microbiota, containing bacteria, archeabacteria, eukaryotic microorganisms and viruses. In any case, a comprehensive description of the host-microbiota interactions can not ignore the profile of eukaryotic viruses within the virome, as viruses are definitely critical for the regulation of the host immunophenotype. ViromeScan is an innovative metagenomic analysis tool that characterizes the taxonomy of the virome directly from raw data of next-generation sequencing. The tool uses hierarchical databases for eukaryotic viruses to unambiguously assign reads to viral species more accurately and >1000 fold faster than other existing approaches. We validated ViromeScan on synthetic microbial communities and applied it on metagenomic samples of the Human Microbiome Project, providing a sensitive eukaryotic virome profiling of different human body sites. ViromeScan allows the user to explore and taxonomically characterize the virome from metagenomic reads, efficiently denoising samples from reads of other microorganisms. This implies that users can fully characterize the microbiome, including bacteria and viruses, by shotgun metagenomic sequencing followed by different bioinformatic pipelines.",VirusIdentification,"viromescan  new tool  metagenomic viral community profiling
bioinformatics tool available  metagenomic sequence analysis  principally devote   identification  microorganisms populate  ecological niche   usually   consider viruses   software   design  profile  viral sequence however    efficient   characterization  viruses   context  complex communities like  intestinal microbiota contain bacteria archeabacteria eukaryotic microorganisms  viruses   case  comprehensive description   hostmicrobiota interactions   ignore  profile  eukaryotic viruses within  virome  viruses  definitely critical   regulation   host immunophenotype viromescan   innovative metagenomic analysis tool  characterize  taxonomy   virome directly  raw data  nextgeneration sequence  tool use hierarchical databases  eukaryotic viruses  unambiguously assign read  viral species  accurately   fold faster   exist approach  validate viromescan  synthetic microbial communities  apply   metagenomic sample   human microbiome project provide  sensitive eukaryotic virome profile  different human body sit viromescan allow  user  explore  taxonomically characterize  virome  metagenomic read efficiently denoising sample  read   microorganisms  imply  users  fully characterize  microbiome include bacteria  viruses  shotgun metagenomic sequence follow  different bioinformatic pipelines",3
74,Genome Detective,"Genome Detective: an automated system for virus identification from high-throughput sequencing data
Genome Detective is an easy to use web-based software application that assembles the genomes of viruses quickly and accurately. The application uses a novel alignment method that constructs genomes by reference-based linking of de novo contigs by combining amino-acids and nucleotide scores. The software was optimized using synthetic datasets to represent the great diversity of virus genomes. The application was then validated with next generation sequencing data of hundreds of viruses. User time is minimal and it is limited to the time required to upload the data.",VirusIdentification,"genome detective  automate system  virus identification  highthroughput sequence data
genome detective   easy  use webbased software application  assemble  genomes  viruses quickly  accurately  application use  novel alignment method  construct genomes  referencebased link   novo contigs  combine aminoacids  nucleotide score  software  optimize use synthetic datasets  represent  great diversity  virus genomes  application   validate  next generation sequence data  hundreds  viruses user time  minimal    limit   time require  upload  data",3
75,VaPid,"VAPiD: a lightweight cross-platform viral annotation pipeline and identification tool to facilitate virus genome submissions to NCBI GenBank
With sequencing technologies becoming cheaper and easier to use, more groups are able to obtain whole genome sequences of viruses of public health and scientific importance. Submission of genomic data to NCBI GenBank is a requirement prior to publication and plays a critical role in making scientific data publicly available. GenBank currently has automatic prokaryotic and eukaryotic genome annotation pipelines but has no viral annotation pipeline beyond influenza virus. Annotation and submission of viral genome sequence is a non-trivial task, especially for groups that do not routinely interact with GenBank for data submissions. We present Viral Annotation Pipeline and iDentification (VAPiD), a portable and lightweight command-line tool for annotation and GenBank deposition of viral genomes. VAPiD supports annotation of nearly all unsegmented viral genomes. The pipeline has been validated on human immunodeficiency virus, human parainfluenza virus 1–4, human metapneumovirus, human coronaviruses (229E/OC43/NL63/HKU1/SARS/MERS), human enteroviruses/rhinoviruses, measles virus, mumps virus, Hepatitis A-E Virus, Chikungunya virus, dengue virus, and West Nile virus, as well the human polyomaviruses BK/JC/MCV, human adenoviruses, and human papillomaviruses. The program can handle individual or batch submissions of different viruses to GenBank and correctly annotates multiple viruses, including those that contain ribosomal slippage or RNA editing without prior knowledge of the virus to be annotated. VAPiD is programmed in Python and is compatible with Windows, Linux, and Mac OS systems. We have created a portable, lightweight, user-friendly, internet-enabled, open-source, command-line genome annotation and submission package to facilitate virus genome submissions to NCBI GenBank.",VirusIdentification,"vapid  lightweight crossplatform viral annotation pipeline  identification tool  facilitate virus genome submissions  ncbi genbank
 sequence technologies become cheaper  easier  use  group  able  obtain whole genome sequence  viruses  public health  scientific importance submission  genomic data  ncbi genbank   requirement prior  publication  play  critical role  make scientific data publicly available genbank currently  automatic prokaryotic  eukaryotic genome annotation pipelines    viral annotation pipeline beyond influenza virus annotation  submission  viral genome sequence   nontrivial task especially  group    routinely interact  genbank  data submissions  present viral annotation pipeline  identification vapid  portable  lightweight commandline tool  annotation  genbank deposition  viral genomes vapid support annotation  nearly  unsegmented viral genomes  pipeline   validate  human immunodeficiency virus human parainfluenza virus  human metapneumovirus human coronaviruses 229eoc43nl63hku1sarsmers human enterovirusesrhinoviruses measles virus mumps virus hepatitis  virus chikungunya virus dengue virus  west nile virus  well  human polyomaviruses bkjcmcv human adenoviruses  human papillomaviruses  program  handle individual  batch submissions  different viruses  genbank  correctly annotate multiple viruses include   contain ribosomal slippage  rna edit without prior knowledge   virus   annotate vapid  program  python   compatible  windows linux  mac  systems   create  portable lightweight userfriendly internetenabled opensource commandline genome annotation  submission package  facilitate virus genome submissions  ncbi genbank",3
76,VirusSeq,"VirusSeq: software to identify viruses and their integration sites using next-generation sequencing of human cancer tissue
We developed a new algorithmic method, VirusSeq, for detecting known viruses and their integration sites in the human genome using next-generation sequencing data. We evaluated VirusSeq on whole-transcriptome sequencing (RNA-Seq) data of 256 human cancer samples from The Cancer Genome Atlas. Using these data, we showed that VirusSeq accurately detects the known viruses and their integration sites with high sensitivity and specificity. VirusSeq can also perform this function using whole-genome sequencing data of human tissue.",VirusIdentification,"virusseq software  identify viruses   integration sit use nextgeneration sequence  human cancer tissue
 develop  new algorithmic method virusseq  detect know viruses   integration sit   human genome use nextgeneration sequence data  evaluate virusseq  wholetranscriptome sequence rnaseq data   human cancer sample   cancer genome atlas use  data  show  virusseq accurately detect  know viruses   integration sit  high sensitivity  specificity virusseq  also perform  function use wholegenome sequence data  human tissue",3
77,PAIPline,"PAIPline: pathogen identification in metagenomic and clinical next generation sequencing samples
Next generation sequencing (NGS) has provided researchers with a powerful tool to characterize metagenomic and clinical samples in research and diagnostic settings. NGS allows an open view into samples useful for pathogen detection in an unbiased fashion and without prior hypothesis about possible causative agents. However, NGS datasets for pathogen detection come with different obstacles, such as a very unfavorable ratio of pathogen to host reads. Alongside often appearing false positives and irrelevant organisms, such as contaminants, tools are often challenged by samples with low pathogen loads and might not report organisms present below a certain threshold. Furthermore, some metagenomic profiling tools are only focused on one particular set of pathogens, for example bacteria. We present PAIPline, a bioinformatics pipeline specifically designed to address problems associated with detecting pathogens in diagnostic samples. PAIPline particularly focuses on userfriendliness and encapsulates all necessary steps from preprocessing to resolution of ambiguous reads and filtering up to visualization in a single tool. In contrast to existing tools, PAIPline is more specific while maintaining sensitivity. This is shown in a comparative evaluation where PAIPline was benchmarked along other well-known metagenomic profiling tools on previously published well-characterized datasets. Additionally, as part of an international cooperation project, PAIPline was applied to an outbreak sample of hemorrhagic fevers of then unknown etiology. The presented results show that PAIPline can serve as a robust, reliable, user-friendly, adaptable and generalizable stand-alone software for diagnostics from NGS samples and as a stepping stone for further downstream analyses.",VirusIdentification,"paipline pathogen identification  metagenomic  clinical next generation sequence samples
next generation sequence ngs  provide researchers   powerful tool  characterize metagenomic  clinical sample  research  diagnostic settings ngs allow  open view  sample useful  pathogen detection   unbiased fashion  without prior hypothesis  possible causative agents however ngs datasets  pathogen detection come  different obstacles     unfavorable ratio  pathogen  host read alongside often appear false positives  irrelevant organisms   contaminants tool  often challenge  sample  low pathogen load  might  report organisms present   certain threshold furthermore  metagenomic profile tool   focus  one particular set  pathogens  example bacteria  present paipline  bioinformatics pipeline specifically design  address problems associate  detect pathogens  diagnostic sample paipline particularly focus  userfriendliness  encapsulate  necessary step  preprocessing  resolution  ambiguous read  filter   visualization   single tool  contrast  exist tool paipline   specific  maintain sensitivity   show   comparative evaluation  paipline  benchmarked along  wellknown metagenomic profile tool  previously publish wellcharacterized datasets additionally  part   international cooperation project paipline  apply   outbreak sample  hemorrhagic fevers   unknown etiology  present result show  paipline  serve   robust reliable userfriendly adaptable  generalizable standalone software  diagnostics  ngs sample    step stone   downstream analyse",3
78,GATK PathSeq,"GATK PathSeq: a customizable computational tool for the discovery and identification of microbial sequences in libraries from eukaryotic hosts
We present an updated version of our computational pipeline, PathSeq, for the discovery and identification of microbial sequences in genomic and transcriptomic libraries from eukaryotic hosts. This pipeline is available in the Genome Analysis Toolkit (GATK) as a suite of configurable tools that can report the microbial composition of DNA or RNA short-read sequencing samples and identify unknown sequences for downstream assembly of novel organisms. GATK PathSeq enables sample analysis in minutes at low cost. In addition, these tools are built with the GATK engine and Apache Spark framework, providing robust, rapid parallelization of read quality filtering, host subtraction and microbial alignment in workstation, cluster and cloud environments.",VirusIdentification,"gatk pathseq  customizable computational tool   discovery  identification  microbial sequence  libraries  eukaryotic hosts
 present  update version   computational pipeline pathseq   discovery  identification  microbial sequence  genomic  transcriptomic libraries  eukaryotic host  pipeline  available   genome analysis toolkit gatk   suite  configurable tool   report  microbial composition  dna  rna shortread sequence sample  identify unknown sequence  downstream assembly  novel organisms gatk pathseq enable sample analysis  minutes  low cost  addition  tool  build   gatk engine  apache spark framework provide robust rapid parallelization  read quality filter host subtraction  microbial alignment  workstation cluster  cloud environments",3
79,RINS,"Rapid identification of non-human sequences in high-throughput sequencing datasets
Rapid identification of non-human sequences (RINS) is an intersection-based pathogen detection workflow that utilizes a user-provided custom reference genome set for identification of non-human sequences in deep sequencing datasets. In <2 h, RINS correctly identified the known virus in the dataset SRR73726 and is compatible with any computer capable of running the prerequisite alignment and assembly programs. RINS accurately identifies sequencing reads from intact or mutated non-human genomes in a dataset and robustly generates contigs with these non-human sequences",VirusIdentification,"rapid identification  nonhuman sequence  highthroughput sequence datasets
rapid identification  nonhuman sequence rins   intersectionbased pathogen detection workflow  utilize  userprovided custom reference genome set  identification  nonhuman sequence  deep sequence datasets    rins correctly identify  know virus   dataset srr73726   compatible   computer capable  run  prerequisite alignment  assembly program rins accurately identify sequence read  intact  mutate nonhuman genomes   dataset  robustly generate contigs   nonhuman sequence",3
80,SURPI,"A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples
Unbiased next-generation sequencing (NGS) approaches enable comprehensive pathogen detection in the clinical microbiology laboratory and have numerous applications for public health surveillance, outbreak investigation, and the diagnosis of infectious diseases. However, practical deployment of the technology is hindered by the bioinformatics challenge of analyzing results accurately and in a clinically relevant timeframe. Here we describe SURPI (""sequence-based ultrarapid pathogen identification""), a computational pipeline for pathogen identification from complex metagenomic NGS data generated from clinical samples, and demonstrate use of the pipeline in the analysis of 237 clinical samples comprising more than 1.1 billion sequences. Deployable on both cloud-based and standalone servers, SURPI leverages two state-of-the-art aligners for accelerated analyses, SNAP and RAPSearch, which are as accurate as existing bioinformatics tools but orders of magnitude faster in performance. In fast mode, SURPI detects viruses and bacteria by scanning data sets of 7-500 million reads in 11 min to 5 h, while in comprehensive mode, all known microorganisms are identified, followed by de novo assembly and protein homology searches for divergent viruses in 50 min to 16 h. SURPI has also directly contributed to real-time microbial diagnosis in acutely ill patients, underscoring its potential key role in the development of unbiased NGS-based clinical assays in infectious diseases that demand rapid turnaround times.",VirusIdentification," cloudcompatible bioinformatics pipeline  ultrarapid pathogen identification  nextgeneration sequence  clinical samples
unbiased nextgeneration sequence ngs approach enable comprehensive pathogen detection   clinical microbiology laboratory   numerous applications  public health surveillance outbreak investigation   diagnosis  infectious diseases however practical deployment   technology  hinder   bioinformatics challenge  analyze result accurately    clinically relevant timeframe   describe surpi ""sequencebased ultrarapid pathogen identification""  computational pipeline  pathogen identification  complex metagenomic ngs data generate  clinical sample  demonstrate use   pipeline   analysis   clinical sample comprise    billion sequence deployable   cloudbased  standalone servers surpi leverage two stateoftheart aligners  accelerate analyse snap  rapsearch    accurate  exist bioinformatics tool  order  magnitude faster  performance  fast mode surpi detect viruses  bacteria  scan data set   million read   min      comprehensive mode  know microorganisms  identify follow   novo assembly  protein homology search  divergent viruses   min    surpi  also directly contribute  realtime microbial diagnosis  acutely ill patients underscore  potential key role   development  unbiased ngsbased clinical assay  infectious diseases  demand rapid turnaround time",3
81,MetaViC,"Metaviromics Reveals Unknown Viral Diversity in the Biting Midge Culicoides impunctatus
Biting midges (Culicoides species) are vectors of arboviruses and were responsible for the emergence and spread of Schmallenberg virus (SBV) in Europe in 2011 and are likely to be involved in the emergence of other arboviruses in Europe. Improved surveillance and better understanding of risks require a better understanding of the circulating viral diversity in these biting insects. In this study, we expand the sequence space of RNA viruses by identifying a number of novel RNA viruses from Culicoides impunctatus (biting midge) using a meta-transcriptomic approach. A novel metaviromic pipeline called MetaViC was developed specifically to identify novel virus sequence signatures from high throughput sequencing (HTS) datasets in the absence of a known host genome. MetaViC is a protein centric pipeline that looks for specific protein signatures in the reads and contigs generated as part of the pipeline. Several novel viruses, including an alphanodavirus with both segments, a novel relative of the Hubei sobemo-like virus 49, two rhabdo-like viruses and a chuvirus, were identified in the Scottish midge samples. The newly identified viruses were found to be phylogenetically distinct to those previous known. These findings expand our current knowledge of viral diversity in arthropods and especially in these understudied disease vectors.",VirusIdentification,"metaviromics reveal unknown viral diversity   bite midge culicoides impunctatus
biting midges culicoides species  vectors  arboviruses   responsible   emergence  spread  schmallenberg virus sbv  europe     likely   involve   emergence   arboviruses  europe improve surveillance  better understand  risk require  better understand   circulate viral diversity   bite insects   study  expand  sequence space  rna viruses  identify  number  novel rna viruses  culicoides impunctatus bite midge use  metatranscriptomic approach  novel metaviromic pipeline call metavic  develop specifically  identify novel virus sequence signatures  high throughput sequence hts datasets   absence   know host genome metavic   protein centric pipeline  look  specific protein signatures   read  contigs generate  part   pipeline several novel viruses include  alphanodavirus   segment  novel relative   hubei sobemolike virus  two rhabdolike viruses   chuvirus  identify   scottish midge sample  newly identify viruses  find   phylogenetically distinct   previous know  find expand  current knowledge  viral diversity  arthropods  especially   understudy disease vectors",3
82,MePIC,"MePIC, metagenomic pathogen identification for clinical specimens
Next-generation DNA sequencing technologies have led to a new method of identifying the causative agents of infectious diseases. The analysis comprises three steps. First, DNA/RNA is extracted and extensively sequenced from a specimen that includes the pathogen, human tissue and commensal microorganisms. Second, the sequenced reads are matched with a database of known sequences, and the organisms from which the individual reads were derived are inferred. Last, the percentages of the organisms' genomic sequences in the specimen (i.e., the metagenome) are estimated, and the pathogen is identified. The first and last steps have become easy due to the development of benchtop sequencers and metagenomic software. To facilitate the middle step, which requires computational resources and skill, we developed a cloud-computing pipeline, MePIC: ""Metagenomic Pathogen Identification for Clinical specimens."" In the pipeline, unnecessary bases are trimmed off the reads, and human reads are removed. For the remaining reads, similar sequences are searched in the database of known nucleotide sequences. The search is drastically sped up by using a cloud-computing system. The webpage interface can be used easily by clinicians and epidemiologists. We believe that the use of the MePIC pipeline will promote metagenomic pathogen identification and improve the understanding of infectious diseases.",VirusIdentification,"mepic metagenomic pathogen identification  clinical specimens
nextgeneration dna sequence technologies  lead   new method  identify  causative agents  infectious diseases  analysis comprise three step first dnarna  extract  extensively sequence   specimen  include  pathogen human tissue  commensal microorganisms second  sequence read  match   database  know sequence   organisms    individual read  derive  infer last  percentages   organisms' genomic sequence   specimen   metagenome  estimate   pathogen  identify  first  last step  become easy due   development  benchtop sequencers  metagenomic software  facilitate  middle step  require computational resources  skill  develop  cloudcomputing pipeline mepic ""metagenomic pathogen identification  clinical specimens""   pipeline unnecessary base  trim   read  human read  remove   remain read similar sequence  search   database  know nucleotide sequence  search  drastically speed   use  cloudcomputing system  webpage interface   use easily  clinicians  epidemiologists  believe   use   mepic pipeline  promote metagenomic pathogen identification  improve  understand  infectious diseases",3
83,VIRALpro,"VIRALpro: a tool to identify viral capsid and tail sequences 
Not only sequence data continue to outpace annotation information, but also the problem is further exacerbated when organisms are underrepresented in the annotation databases. This is the case with non-human-pathogenic viruses which occur frequently in metagenomic projects. Thus, there is a need for tools capable of detecting and classifying viral sequences. We describe VIRALpro a new effective tool for identifying capsid and tail protein sequences, which are the cornerstones toward viral sequence annotation and viral genome classification.",VirusIdentification,"viralpro  tool  identify viral capsid  tail sequence 
  sequence data continue  outpace annotation information  also  problem   exacerbate  organisms  underrepresented   annotation databases    case  nonhumanpathogenic viruses  occur frequently  metagenomic project thus    need  tool capable  detect  classify viral sequence  describe viralpro  new effective tool  identify capsid  tail protein sequence    cornerstones toward viral sequence annotation  viral genome classification",3
84,READSCAN,"READSCAN: a fast and scalable pathogen discovery program with accurate genome relative abundance estimation
READSCAN is a highly scalable parallel program to identify non-host sequences (of potential pathogen origin) and estimate their genome relative abundance in high-throughput sequence datasets. READSCAN accurately classified human and viral sequences on a 20.1 million reads simulated dataset in <27 min using a small Beowulf compute cluster with 16 nodes",VirusIdentification,"readscan  fast  scalable pathogen discovery program  accurate genome relative abundance estimation
readscan   highly scalable parallel program  identify nonhost sequence  potential pathogen origin  estimate  genome relative abundance  highthroughput sequence datasets readscan accurately classify human  viral sequence    million read simulate dataset   min use  small beowulf compute cluster   nod",3
85,MagicBLAST,"Magic-BLAST, an accurate RNA-seq aligner for long and short reads
Next-generation sequencing technologies can produce tens of millions of reads, often paired-end, from transcripts or genomes. But few programs can align RNA on the genome and accurately discover introns, especially with long reads. We introduce Magic-BLAST, a new aligner based on ideas from the Magic pipeline. Magic-BLAST uses innovative techniques that include the optimization of a spliced alignment score and selective masking during seed selection. We evaluate the performance of Magic-BLAST to accurately map short or long sequences and its ability to discover introns on real RNA-seq data sets from PacBio, Roche and Illumina runs, and on six benchmarks, and compare it to other popular aligners. Additionally, we look at alignments of human idealized RefSeq mRNA sequences perfectly matching the genome. We show that Magic-BLAST is the best at intron discovery over a wide range of conditions and the best at mapping reads longer than 250 bases, from any platform. It is versatile and robust to high levels of mismatches or extreme base composition, and reasonably fast. It can align reads to a BLAST database or a FASTA file. It can accept a FASTQ file as input or automatically retrieve an accession from the SRA repository at the NCBI.",Mapping,"magicblast  accurate rnaseq aligner  long  short reads
nextgeneration sequence technologies  produce tens  millions  read often pairedend  transcripts  genomes   program  align rna   genome  accurately discover introns especially  long read  introduce magicblast  new aligner base  ideas   magic pipeline magicblast use innovative techniques  include  optimization   splice alignment score  selective mask  seed selection  evaluate  performance  magicblast  accurately map short  long sequence   ability  discover introns  real rnaseq data set  pacbio roche  illumina run   six benchmarks  compare    popular aligners additionally  look  alignments  human idealize refseq mrna sequence perfectly match  genome  show  magicblast   best  intron discovery   wide range  condition   best  map read longer   base   platform   versatile  robust  high level  mismatch  extreme base composition  reasonably fast   align read   blast database   fasta file   accept  fastq file  input  automatically retrieve  accession   sra repository   ncbi",4
86,MGmapper,"MGmapper: Reference based mapping and taxonomy annotation of metagenomics sequence reads
An increasing amount of species and gene identification studies rely on the use of next generation sequence analysis of either single isolate or metagenomics samples. Several methods are available to perform taxonomic annotations and a previous metagenomics benchmark study has shown that a vast number of false positive species annotations are a problem unless thresholds or post-processing are applied to differentiate between correct and false annotations. MGmapper is a package to process raw next generation sequence data and perform reference based sequence assignment, followed by a post-processing analysis to produce reliable taxonomy annotation at species and strain level resolution. An in-vitro bacterial mock community sample comprised of 8 genuses, 11 species and 12 strains was previously used to benchmark metagenomics classification methods. After applying a post-processing filter, we obtained 100% correct taxonomy assignments at species and genus level. A sensitivity and precision at 75% was obtained for strain level annotations. A comparison between MGmapper and Kraken at species level, shows MGmapper assigns taxonomy at species level using 84.8% of the sequence reads, compared to 70.5% for Kraken and both methods identified all species with no false positives. Extensive read count statistics are provided in plain text and excel sheets for both rejected and accepted taxonomy annotations.",Mapping,"mgmapper reference base map  taxonomy annotation  metagenomics sequence reads
 increase amount  species  gene identification study rely   use  next generation sequence analysis  either single isolate  metagenomics sample several methods  available  perform taxonomic annotations   previous metagenomics benchmark study  show   vast number  false positive species annotations   problem unless thresholds  postprocessing  apply  differentiate  correct  false annotations mgmapper   package  process raw next generation sequence data  perform reference base sequence assignment follow   postprocessing analysis  produce reliable taxonomy annotation  species  strain level resolution  invitro bacterial mock community sample comprise   genuses  species   strain  previously use  benchmark metagenomics classification methods  apply  postprocessing filter  obtain  correct taxonomy assignments  species  genus level  sensitivity  precision    obtain  strain level annotations  comparison  mgmapper  kraken  species level show mgmapper assign taxonomy  species level use    sequence read compare    kraken   methods identify  species   false positives extensive read count statistics  provide  plain text  excel sheet   reject  accept taxonomy annotations",4
87,BWA,"Tandem repeats analysis in DNA sequences based on improved Burrows-Wheeler transform
The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ∼10–20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.",Mapping,"tandem repeat analysis  dna sequence base  improve burrowswheeler transform
 enormous amount  short read generate   new dna sequence technologies call   development  fast  accurate read alignment program  first generation  hash tablebased methods   develop include maq   accurate feature rich  fast enough  align short read   single individual however maq   support gap alignment  singleend read  make  unsuitable  alignment  longer read  indels may occur frequently  speed  maq  also  concern   alignment  scale    resequencing  hundreds  individuals  implement burrowswheeler alignment tool bwa  new read alignment package   base  backward search  burrowswheeler transform bwt  efficiently align short sequence read   large reference sequence    human genome allow mismatch  gap bwa support  base space read   illumina sequence machine  color space read   solid machine evaluations   simulate  real data suggest  bwa   faster  maq  achieve similar accuracy  addition bwa output alignment   new standard sam sequence alignmentmap format variant call   downstream analyse   alignment   achieve   open source samtools software package",4
88,SOAP2,"SOAP2: an improved ultrafast tool for short read alignment
SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20–30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome.",Mapping,"soap2  improve ultrafast tool  short read alignment
soap2   significantly improve version   short oligonucleotide alignment program   reduce computer memory usage  increase alignment speed   unprecedented rate  use  burrow wheeler transformation bwt compression index  substitute  seed strategy  index  reference sequence   main memory  test    whole human genome  find   new algorithm reduce memory usage       improve alignment speed   time soap2  compatible   single  pairedend read additionally  tool  support multiple text  compress file format  consensus builder  also  develop  consensus assembly  snp detection  alignment  short read   reference genome",4
89,MAQ,"Mapping short DNA sequencing reads and calling variants using mapping quality scores
New sequencing technologies promise a new era in the use of DNA sequence. However, some of these technologies produce very short reads, typically of a few tens of base pairs, and to use these reads effectively requires new algorithms and software. In particular, there is a major issue in efficiently aligning short reads to a reference genome and handling ambiguity or lack of accuracy in this alignment. Here we introduce the concept of mapping quality, a measure of the confidence that a read actually comes from the position it is aligned to by the mapping algorithm. We describe the software MAQ that can build assemblies by mapping shotgun short reads to a reference genome, using quality scores to derive genotype calls of the consensus sequence of a diploid genome, e.g., from a human sample. MAQ makes full use of mate-pair information and estimates the error probability of each read alignment. Error probabilities are also derived for the final genotype calls, using a Bayesian statistical model that incorporates the mapping qualities, error probabilities from the raw sequence quality scores, sampling of the two haplotypes, and an empirical model for correlated errors at a site. Both read mapping and genotype calling are evaluated on simulated data and real data. MAQ is accurate, efficient, versatile, and user-friendly.",Mapping,"map short dna sequence read  call variants use map quality scores
new sequence technologies promise  new era   use  dna sequence however    technologies produce  short read typically    tens  base pair   use  read effectively require new algorithms  software  particular    major issue  efficiently align short read   reference genome  handle ambiguity  lack  accuracy   alignment   introduce  concept  map quality  measure   confidence   read actually come   position   align    map algorithm  describe  software maq   build assemblies  map shotgun short read   reference genome use quality score  derive genotype call   consensus sequence   diploid genome    human sample maq make full use  matepair information  estimate  error probability   read alignment error probabilities  also derive   final genotype call use  bayesian statistical model  incorporate  map qualities error probabilities   raw sequence quality score sample   two haplotypes   empirical model  correlate errors   site  read map  genotype call  evaluate  simulate data  real data maq  accurate efficient versatile  userfriendly",4
90,RMAP,"Using quality scores and longer reads improves accuracy of Solexa read mapping
Second-generation sequencing has the potential to revolutionize genomics and impact all areas of biomedical science. New technologies will make re-sequencing widely available for such applications as identifying genome variations or interrogating the oligonucleotide content of a large sample (e.g. ChIP-sequencing). The increase in speed, sensitivity and availability of sequencing technology brings demand for advances in computational technology to perform associated analysis tasks. The Solexa/Illumina 1G sequencer can produce tens of millions of reads, ranging in length from ~25–50 nt, in a single experiment. Accurately mapping the reads back to a reference genome is a critical task in almost all applications. Two sources of information that are often ignored when mapping reads from the Solexa technology are the 3' ends of longer reads, which contain a much higher frequency of sequencing errors, and the base-call quality scores. To investigate whether these sources of information can be used to improve accuracy when mapping reads, we developed the RMAP tool, which can map reads having a wide range of lengths and allows base-call quality scores to determine which positions in each read are more important when mapping. We applied RMAP to analyze data re-sequenced from two human BAC regions for varying read lengths, and varying criteria for use of quality scores. Our results indicate that significant gains in Solexa read mapping performance can be achieved by considering the information in 3' ends of longer reads, and appropriately using the basecall quality scores. The RMAP tool we have developed will enable researchers to effectively exploit this information in targeted re-sequencing projects.",Mapping,"use quality score  longer read improve accuracy  solexa read mapping
secondgeneration sequence   potential  revolutionize genomics  impact  areas  biomedical science new technologies  make resequencing widely available   applications  identify genome variations  interrogate  oligonucleotide content   large sample  chipsequencing  increase  speed sensitivity  availability  sequence technology bring demand  advance  computational technology  perform associate analysis task  solexaillumina  sequencer  produce tens  millions  read range  length  ~    single experiment accurately map  read back   reference genome   critical task  almost  applications two source  information   often ignore  map read   solexa technology   ' end  longer read  contain  much higher frequency  sequence errors   basecall quality score  investigate whether  source  information   use  improve accuracy  map read  develop  rmap tool   map read   wide range  lengths  allow basecall quality score  determine  position   read   important  map  apply rmap  analyze data resequenced  two human bac regions  vary read lengths  vary criteria  use  quality score  result indicate  significant gain  solexa read map performance   achieve  consider  information  ' end  longer read  appropriately use  basecall quality score  rmap tool   develop  enable researchers  effectively exploit  information  target resequencing project",4
91,GSNAP,"Fast and SNP-tolerant detection of complex variants and splicing in short reads 
Next-generation sequencing captures sequence differences in reads relative to a reference genome or transcriptome, including splicing events and complex variants involving multiple mismatches and long indels. We present computational methods for fast detection of complex variants and splicing in short reads, based on a successively constrained search process of merging and filtering position lists from a genomic index. Our methods are implemented in GSNAP (Genomic Short-read Nucleotide Alignment Program), which can align both single- and paired-end reads as short as 14 nt and of arbitrarily long length. It can detect shortand long-distance splicing, including interchromosomal splicing, in individual reads, using probabilistic models or a database of known splice sites. Our program also permits SNP-tolerant alignment to a reference space of all possible combinations of major and minor alleles, and can align reads from bisulfite-treated DNA for the study of methylation state. In comparison testing, GSNAP has speeds comparable to existing programs, especially in reads of ≥70 nt and is fastest in detecting complex variants with four or more mismatches or insertions of 1–9 nt and deletions of 1–30 nt. Although SNP tolerance does not increase alignment yield substantially, it affects alignment results in 7–8% of transcriptional reads, typically by revealing alternate genomic mappings for a read. Simulations of bisulfiteconverted DNA show a decrease in identifying genomic positions uniquely in 6% of 36 nt reads and 3% of 70 nt reads.",Mapping,"fast  snptolerant detection  complex variants  splice  short read 
nextgeneration sequence capture sequence differences  read relative   reference genome  transcriptome include splice events  complex variants involve multiple mismatch  long indels  present computational methods  fast detection  complex variants  splice  short read base   successively constrain search process  merge  filter position list   genomic index  methods  implement  gsnap genomic shortread nucleotide alignment program   align  single  pairedend read  short      arbitrarily long length   detect shortand longdistance splice include interchromosomal splice  individual read use probabilistic model   database  know splice sit  program also permit snptolerant alignment   reference space   possible combinations  major  minor alleles   align read  bisulfitetreated dna   study  methylation state  comparison test gsnap  speed comparable  exist program especially  read  ≥    fastest  detect complex variants  four   mismatch  insertions     deletions    although snp tolerance   increase alignment yield substantially  affect alignment result    transcriptional read typically  reveal alternate genomic mappings   read simulations  bisulfiteconverted dna show  decrease  identify genomic position uniquely      read      read",4
92,Stampy,"Stampy: a statistical algorithm for sensitive and fast mapping of Illumina sequence reads
High-volume sequencing of DNA and RNA is now within reach of any research laboratory and is quickly becoming established as a key research tool. In many workflows, each of the short sequences (“reads”) resulting from a sequencing run are first “mapped” (aligned) to a reference sequence to infer the read from which the genomic location derived, a challenging task because of the high data volumes and often large genomes. Existing read mapping software excel in either speed (e.g., BWA, Bowtie, ELAND) or sensitivity (e.g., Novoalign), but not in both. In addition, performance often deteriorates in the presence of sequence variation, particularly so for short insertions and deletions (indels). Here, we present a read mapper, Stampy, which uses a hybrid mapping algorithm and a detailed statistical model to achieve both speed and sensitivity, particularly when reads include sequence variation. This results in a higher useable sequence yield and improved accuracy compared to that of existing software.",Mapping,"stampy  statistical algorithm  sensitive  fast map  illumina sequence reads
highvolume sequence  dna  rna   within reach   research laboratory   quickly become establish   key research tool  many workflows    short sequence “reads” result   sequence run  first “mapped” align   reference sequence  infer  read    genomic location derive  challenge task    high data volumes  often large genomes exist read map software excel  either speed  bwa bowtie eland  sensitivity  novoalign      addition performance often deteriorate   presence  sequence variation particularly   short insertions  deletions indels   present  read mapper stampy  use  hybrid map algorithm   detail statistical model  achieve  speed  sensitivity particularly  read include sequence variation  result   higher useable sequence yield  improve accuracy compare    exist software",4
93,Shrimp2,"SHRiMP2: sensitive yet practical SHort Read Mapping
We report on a major update (version 2) of the original SHort Read Mapping Program (SHRiMP). SHRiMP2 primarily targets mapping sensitivity, and is able to achieve high accuracy at a very reasonable speed. SHRiMP2 supports both letter space and color space (AB/SOLiD) reads, enables for direct alignment of paired reads and uses parallel computation to fully utilize multi-core architectures.",Mapping,"shrimp2 sensitive yet practical short read mapping
 report   major update version    original short read map program shrimp shrimp2 primarily target map sensitivity   able  achieve high accuracy    reasonable speed shrimp2 support  letter space  color space absolid read enable  direct alignment  pair read  use parallel computation  fully utilize multicore architectures",4
94,mrsFAST,"mrsFAST-Ultra: a compact, SNP-aware mapper for high performance sequencing applications
High throughput sequencing (HTS) platforms generate unprecedented amounts of data that introduce challenges for processing and downstream analysis. While tools that report the ‘best’ mapping location of each read provide a fast way to process HTS data, they are not suitable for many types of downstream analysis such as structural variation detection, where it is important to report multiple mapping loci for each read. For this purpose we introduce mrsFAST-Ultra, a fast, cache oblivious, SNP-aware aligner that can handle the multi-mapping of HTS reads very efficiently. mrsFAST-Ultra improves mrsFAST, our first cache oblivious read aligner capable of handling multi-mapping reads, through new and compact index structures that reduce not only the overall memory usage but also the number of CPU operations per alignment. In fact the size of the index generated by mrsFAST-Ultra is 10 times smaller than that of mrsFAST. As importantly, mrsFAST-Ultra introduces new features such as being able to (i) obtain the best mapping loci for each read, and (ii) return all reads that have at most n mapping loci (within an error threshold), together with these loci, for any user specified n. Furthermore, mrsFAST-Ultra is SNPaware, i.e. it can map reads to reference genome while discounting the mismatches that occur at common SNP locations provided by db-SNP; this significantly increases the number of reads that can be mapped to the reference genome. Notice that all of the above features are implemented within the index structure and are not simple post-processing steps and thus are performed highly efficiently. Finally, mrsFAST-Ultra utilizes multiple available cores and processors and can be tuned for various memory settings. Our results show that mrsFAST-Ultra is roughly five times faster than its predecessor mrsFAST. In comparison to newly enhanced popular tools such as Bowtie2, it is more sensitive (it can report 10 times or more mappings per read) and much faster (six times or more) in the multi-mapping mode. Furthermore, mrsFAST-Ultra has an index size of 2GB for the entire human reference genome, which is roughly half of that of Bowtie2.",Mapping,"mrsfastultra  compact snpaware mapper  high performance sequence applications
high throughput sequence hts platforms generate unprecedented amount  data  introduce challenge  process  downstream analysis  tool  report  best map location   read provide  fast way  process hts data    suitable  many type  downstream analysis   structural variation detection    important  report multiple map loci   read   purpose  introduce mrsfastultra  fast cache oblivious snpaware aligner   handle  multimapping  hts read  efficiently mrsfastultra improve mrsfast  first cache oblivious read aligner capable  handle multimapping read  new  compact index structure  reduce    overall memory usage  also  number  cpu operations per alignment  fact  size   index generate  mrsfastultra   time smaller    mrsfast  importantly mrsfastultra introduce new feature    able   obtain  best map loci   read   return  read      map loci within  error threshold together   loci   user specify  furthermore mrsfastultra  snpaware    map read  reference genome  discount  mismatch  occur  common snp locations provide  dbsnp  significantly increase  number  read    map   reference genome notice      feature  implement within  index structure    simple postprocessing step  thus  perform highly efficiently finally mrsfastultra utilize multiple available core  processors    tune  various memory settings  result show  mrsfastultra  roughly five time faster   predecessor mrsfast  comparison  newly enhance popular tool   bowtie2    sensitive   report  time   mappings per read  much faster six time     multimapping mode furthermore mrsfastultra   index size  2gb   entire human reference genome   roughly half    bowtie2",4
95,BAM-matcher,"BAM-matcher: a tool for rapid NGS sample matching 
The standard method used by high-throughput genome sequencing facilities for detecting mislabelled samples is to use independently generated high-density SNP data to determine sample identity. However, as it has now become commonplace to have multiple samples sequenced from the same source, such as for analysis of somatic variants using matched tumour and normal samples, we can directly use the genotype information inherent in the sequence data to match samples and thus bypass the need for additional laboratory testing. Here we present BAM-matcher, a tool that can rapidly determine whether two BAM files represent samples from the same biological source by comparing their genotypes. BAM-matcher is designed to be simple to use, provides easily interpretable results, and is suitable for deployment at early stages of data processing pipelines.",Mapping,"bammatcher  tool  rapid ngs sample match 
 standard method use  highthroughput genome sequence facilities  detect mislabelled sample   use independently generate highdensity snp data  determine sample identity however     become commonplace   multiple sample sequence    source    analysis  somatic variants use match tumour  normal sample   directly use  genotype information inherent   sequence data  match sample  thus bypass  need  additional laboratory test   present bammatcher  tool   rapidly determine whether two bam file represent sample    biological source  compare  genotypes bammatcher  design   simple  use provide easily interpretable result   suitable  deployment  early stag  data process pipelines",4
96,TAGdb,"Targeted identification of genomic regions using TAGdb
The introduction of second generation sequencing technology has enabled the cost effective sequencing of genomes and the identification of large numbers of genes and gene promoters. However, the assembly of DNA sequences to create a representation of the complete genome sequence remains costly, especially for the larger and more complex plant genomes. We have developed an online database, TAGdb, that enables researchers to identify paired read sequences that share identity with a submitted query sequence. These tags can be used to design oligonucleotide primers for the PCR amplification of the region in the target genome. The ability to produce large numbers of paired read genome tags using second generation sequencing provides a cost effective method for the identification of genes and promoters in large, complex or orphan species without the need for whole genome assembly.",Mapping,"target identification  genomic regions use tagdb
 introduction  second generation sequence technology  enable  cost effective sequence  genomes   identification  large number  genes  gene promoters however  assembly  dna sequence  create  representation   complete genome sequence remain costly especially   larger   complex plant genomes   develop  online database tagdb  enable researchers  identify pair read sequence  share identity   submit query sequence  tag   use  design oligonucleotide primers   pcr amplification   region   target genome  ability  produce large number  pair read genome tag use second generation sequence provide  cost effective method   identification  genes  promoters  large complex  orphan species without  need  whole genome assembly",4
97,HGA,"HGA: de novo genome assembly method for bacterial genomes using high coverage short sequencing reads.
Current high-throughput sequencing technologies generate large numbers of relatively short and error-prone reads, making the de novo assembly problem challenging. Although high quality assemblies can be obtained by assembling multiple paired-end libraries with both short and long insert sizes, the latter are costly to generate. Recently, GAGE-B study showed that a remarkably good assembly quality can be obtained for bacterial genomes by state-of-the-art assemblers run on a single short-insert library with very high coverage. In this paper, we introduce a novel hierarchical genome assembly (HGA) methodology that takes further advantage of such very high coverage by independently assembling disjoint subsets of reads, combining assemblies of the subsets, and finally re-assembling the combined contigs along with the original reads. We empirically evaluated this methodology for 8 leading assemblers using 7 GAGE-B bacterial datasets consisting of 100 bp Illumina HiSeq and 250 bp Illumina MiSeq reads, with coverage ranging from 100x– ∼200x. The results show that for all evaluated datasets and using most evaluated assemblers (that were used to assemble the disjoint subsets), HGA leads to a significant improvement in the quality of the assembly based on N50 and corrected N50 metrics.",Assembly,"hga  novo genome assembly method  bacterial genomes use high coverage short sequence reads
current highthroughput sequence technologies generate large number  relatively short  errorprone read make   novo assembly problem challenge although high quality assemblies   obtain  assemble multiple pairedend libraries   short  long insert size  latter  costly  generate recently gageb study show   remarkably good assembly quality   obtain  bacterial genomes  stateoftheart assemblers run   single shortinsert library   high coverage   paper  introduce  novel hierarchical genome assembly hga methodology  take  advantage    high coverage  independently assemble disjoint subsets  read combine assemblies   subsets  finally reassemble  combine contigs along   original read  empirically evaluate  methodology   lead assemblers use  gageb bacterial datasets consist    illumina hiseq    illumina miseq read  coverage range     result show    evaluate datasets  use  evaluate assemblers   use  assemble  disjoint subsets hga lead   significant improvement   quality   assembly base  n50  correct n50 metrics",5
98,Velvet,"Velvet: algorithms for de novo short read assembly using de Bruijn graphs
We have developed a new set of algorithms, collectively called “Velvet,” to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25–50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of ∼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.",Assembly,"velvet algorithms   novo short read assembly use  bruijn graphs
  develop  new set  algorithms collectively call “velvet”  manipulate  bruijn graph  genomic sequence assembly   bruijn graph   compact representation base  short word kmers   ideal  high coverage  short read   data set apply velvet   short read  pairedends information  one  produce contigs  significant length    n50 length  simulations  prokaryotic data   n50  simulate mammalian bacs  apply  real solexa data set without read pair velvet generate contigs      prokaryote      mammalian bac  close agreement   simulate result without readpair information velvet represent  new approach  assembly   leverage  short read  combination  read pair  produce useful assemblies",5
99,MetaVelvet,"MetaVelvet: an extension of Velvet assembler to de novo metagenome assembly from short sequence reads
An important step in ‘metagenomics’ analysis is the assembly of multiple genomes from mixed sequence reads of multiple species in a microbial community. Most conventional pipelines use a single-genome assembler with carefully optimized parameters. A limitation of a single-genome assembler for de novo metagenome assembly is that sequences of highly abundant species are likely misidentified as repeats in a single genome, resulting in a number of small fragmented scaffolds. We extended a single-genome assembler for short reads, known as ‘Velvet’, to metagenome assembly, which we called ‘MetaVelvet’, for mixed short reads of multiple species. Our fundamental concept was to first decompose a de Bruijn graph constructed from mixed short reads into individual sub-graphs, and second, to build scaffolds based on each decomposed de Bruijn sub-graph as an isolate species genome. We made use of two features, the coverage (abundance) difference and graph connectivity, for the decomposition of the de Bruijn graph. For simulated datasets, MetaVelvet succeeded in generating significantly higher N50 scores than any single-genome assemblers. MetaVelvet also reconstructed relatively low-coverage genome sequences as scaffolds. On real datasets of human gut microbial read data, MetaVelvet produced longer scaffolds and increased the number of predicted genes.",Assembly,"metavelvet  extension  velvet assembler   novo metagenome assembly  short sequence reads
 important step  metagenomics analysis   assembly  multiple genomes  mix sequence read  multiple species   microbial community  conventional pipelines use  singlegenome assembler  carefully optimize parameters  limitation   singlegenome assembler   novo metagenome assembly   sequence  highly abundant species  likely misidentified  repeat   single genome result   number  small fragment scaffold  extend  singlegenome assembler  short read know  velvet  metagenome assembly   call metavelvet  mix short read  multiple species  fundamental concept   first decompose   bruijn graph construct  mix short read  individual subgraphs  second  build scaffold base   decompose  bruijn subgraph   isolate species genome  make use  two feature  coverage abundance difference  graph connectivity   decomposition    bruijn graph  simulate datasets metavelvet succeed  generate significantly higher n50 score   singlegenome assemblers metavelvet also reconstruct relatively lowcoverage genome sequence  scaffold  real datasets  human gut microbial read data metavelvet produce longer scaffold  increase  number  predict genes",5
100,Rnnotator,"Rnnotator: an automated de novo transcriptome assembly pipeline from stranded RNA-Seq reads
Comprehensive annotation and quantification of transcriptomes are outstanding problems in functional genomics. While high throughput mRNA sequencing (RNA-Seq) has emerged as a powerful tool for addressing these problems, its success is dependent upon the availability and quality of reference genome sequences, thus limiting the organisms to which it can be applied. Here, we describe Rnnotator, an automated software pipeline that generates transcript models by de novo assembly of RNA-Seq data without the need for a reference genome. We have applied the Rnnotator assembly pipeline to two yeast transcriptomes and compared the results to the reference gene catalogs of these organisms. The contigs produced by Rnnotator are highly accurate (95%) and reconstruct full-length genes for the majority of the existing gene models (54.3%). Furthermore, our analyses revealed many novel transcribed regions that are absent from well annotated genomes, suggesting Rnnotator serves as a complementary approach to analysis based on a reference genome for comprehensive transcriptomics. These results demonstrate that the Rnnotator pipeline is able to reconstruct full-length transcripts in the absence of a complete reference genome.",Assembly,"rnnotator  automate  novo transcriptome assembly pipeline  strand rnaseq reads
comprehensive annotation  quantification  transcriptomes  outstanding problems  functional genomics  high throughput mrna sequence rnaseq  emerge   powerful tool  address  problems  success  dependent upon  availability  quality  reference genome sequence thus limit  organisms      apply   describe rnnotator  automate software pipeline  generate transcript model   novo assembly  rnaseq data without  need   reference genome   apply  rnnotator assembly pipeline  two yeast transcriptomes  compare  result   reference gene catalog   organisms  contigs produce  rnnotator  highly accurate   reconstruct fulllength genes   majority   exist gene model  furthermore  analyse reveal many novel transcribe regions   absent  well annotate genomes suggest rnnotator serve   complementary approach  analysis base   reference genome  comprehensive transcriptomics  result demonstrate   rnnotator pipeline  able  reconstruct fulllength transcripts   absence   complete reference genome",5
101,Taipan,"A fast hybrid short read fragment assembly algorithm 
The shorter and vastly more numerous reads produced by second-generation sequencing technologies require new tools that can assemble massive numbers of reads in reasonable time. Existing short-read assembly tools can be classified into two categories: greedy extension-based and graph-based. While the graph-based approaches are generally superior in terms of assembly quality, the computer resources required for building and storing a huge graph are very high. In this article, we present Taipan, an assembly algorithm which can be viewed as a hybrid of these two approaches. Taipan uses greedy extensions for contig construction but at each step realizes enough of the corresponding read graph to make better decisions as to how assembly should continue. We show that this approach can achieve an assembly quality at least as good as the graph-based approaches used in the popular Edena and Velvet assembly tools using a moderate amount of computing resources.",Assembly," fast hybrid short read fragment assembly algorithm 
 shorter  vastly  numerous read produce  secondgeneration sequence technologies require new tool   assemble massive number  read  reasonable time exist shortread assembly tool   classify  two categories greedy extensionbased  graphbased   graphbased approach  generally superior  term  assembly quality  computer resources require  build  store  huge graph   high   article  present taipan  assembly algorithm    view   hybrid   two approach taipan use greedy extensions  contig construction    step realize enough   correspond read graph  make better decisions    assembly  continue  show   approach  achieve  assembly quality  least  good   graphbased approach use   popular edena  velvet assembly tool use  moderate amount  compute resources",5
102,AByss,"De novo transcriptome assembly with ABySS
Whole transcriptome shotgun sequencing data from non-normalized samples offer unique opportunities to study the metabolic states of organisms. One can deduce gene expression levels using sequence coverage as a surrogate, identify coding changes or discover novel isoforms or transcripts. Especially for discovery of novel events, de novo assembly of transcriptomes is desirable. Transcriptome from tumor tissue of a patient with follicular lymphoma was sequenced with 36 base pair (bp) single- and paired-end reads on the Illumina Genome Analyzer II platform. We assembled ∼194 million reads using ABySS into 66 921 contigs 100 bp or longer, with a maximum contig length of 10 951 bp, representing over 30 million base pairs of unique transcriptome sequence, or roughly 1% of the genome.",Assembly," novo transcriptome assembly  abyss
whole transcriptome shotgun sequence data  nonnormalized sample offer unique opportunities  study  metabolic state  organisms one  deduce gene expression level use sequence coverage   surrogate identify cod change  discover novel isoforms  transcripts especially  discovery  novel events  novo assembly  transcriptomes  desirable transcriptome  tumor tissue   patient  follicular lymphoma  sequence   base pair  single  pairedend read   illumina genome analyzer  platform  assemble  million read use aby    contigs    longer   maximum contig length     represent   million base pair  unique transcriptome sequence  roughly    genome",5
103,ALLPATHS,"ALLPATHS: de novo assembly of whole-genome shotgun microreads
New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun “microreads.” For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80× coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.",Assembly,"allpaths  novo assembly  wholegenome shotgun microreads
new dna sequence technologies deliver data  dramatically lower cost  demand new analytical methods  take full advantage    short read   produce  provide  initial theoretical solution   challenge   novo assembly  wholegenome shotgun “microreads”   genomes  size      generate highquality assemblies   coverage  pair base simulate read model  real illuminasolexa read  bacterial genomes  campylobacter jejuni  escherichia coli assemble optimally yield single perfect contigs  larger genomes yield assemblies   highly connect  accurate assemblies  present   graph form  retain intrinsic ambiguities    arise  polymorphism thereby provide information    absent  previous genome assemblies    jejuni   coli  assembly graph   single edge encompass  entire genome larger genomes produce  complicate graph   vast majority   base   assemblies  present  long edge   nearly always perfect  describe  general method  genome assembly    apply   type  dna sequence data   short read data  also conventional sequence read",5
104,Oases,"Oases: robust de novo RNA-seq assembly across the dynamic range of expression levels.
High-throughput sequencing has made the analysis of new model organisms more affordable. Although assembling a new genome can still be costly and difficult, it is possible to use RNA-seq to sequence mRNA. In the absence of a known genome, it is necessary to assemble these sequences de novo, taking into account possible alternative isoforms and the dynamic range of expression values. We present a software package named Oases designed to heuristically assemble RNA-seq reads in the absence of a reference genome, across a broad spectrum of expression values and in presence of alternative isoforms. It achieves this by using an array of hash lengths, a dynamic filtering of noise, a robust resolution of alternative splicing events and the efficient merging of multiple assemblies. It was tested on human and mouse RNA-seq data and is shown to improve significantly on the transABySS and Trinity de novo transcriptome assemblers.",Assembly,"oases robust  novo rnaseq assembly across  dynamic range  expression levels
highthroughput sequence  make  analysis  new model organisms  affordable although assemble  new genome  still  costly  difficult   possible  use rnaseq  sequence mrna   absence   know genome   necessary  assemble  sequence  novo take  account possible alternative isoforms   dynamic range  expression value  present  software package name oases design  heuristically assemble rnaseq read   absence   reference genome across  broad spectrum  expression value   presence  alternative isoforms  achieve   use  array  hash lengths  dynamic filter  noise  robust resolution  alternative splice events   efficient merge  multiple assemblies   test  human  mouse rnaseq data   show  improve significantly   transabyss  trinity  novo transcriptome assemblers",5
105,SPADES,"SPAdes: A New Genome Assembly Algorithm and Its Applications to Single-Cell Sequencing
The lion's share of bacteria in various environments cannot be cloned in the laboratory and thus cannot be sequenced using existing technologies. A major goal of single-cell genomics is to complement gene-centric metagenomic data with whole-genome assemblies of uncultivated organisms. Assembly of single-cell data is challenging because of highly non-uniform read coverage as well as elevated levels of sequencing errors and chimeric reads. We describe SPAdes, a new assembler for both single-cell and standard (multicell) assembly, and demonstrate that it improves on the recently released E+V−SC assembler (specialized for single-cell data) and on popular assemblers Velvet and SoapDeNovo (for multicell data). SPAdes generates single-cell assemblies, providing information about genomes of uncultivatable bacteria that vastly exceeds what may be obtained via traditional metagenomics studies.",Assembly,"spade  new genome assembly algorithm   applications  singlecell sequencing
 lion' share  bacteria  various environments cannot  clone   laboratory  thus cannot  sequence use exist technologies  major goal  singlecell genomics   complement genecentric metagenomic data  wholegenome assemblies  uncultivated organisms assembly  singlecell data  challenge   highly nonuniform read coverage  well  elevate level  sequence errors  chimeric read  describe spade  new assembler   singlecell  standard multicell assembly  demonstrate   improve   recently release evsc assembler specialize  singlecell data   popular assemblers velvet  soapdenovo  multicell data spade generate singlecell assemblies provide information  genomes  uncultivatable bacteria  vastly exceed  may  obtain via traditional metagenomics study",5
106,SOAPdenovo2,"SOAPdenovo2: an empirically improved memory-efficient short-read de novo assembler
There is a rapidly increasing amount of de novo genome assembly using next-generation sequencing (NGS) short reads; however, several big challenges remain to be overcome in order for this to be efficient and accurate. SOAPdenovo has been successfully applied to assemble many published genomes, but it still needs improvement in continuity, accuracy and coverage, especially in repeat regions. To overcome these challenges, we have developed its successor, SOAPdenovo2, which has the advantage of a new algorithm design that reduces memory consumption in graph construction, resolves more repeat regions in contig assembly, increases coverage and length in scaffold construction, improves gap closing, and optimizes for large genome. Benchmark using the Assemblathon1 and GAGE datasets showed that SOAPdenovo2 greatly surpasses its predecessor SOAPdenovo and is competitive to other assemblers on both assembly length and accuracy. We also provide an updated assembly version of the 2008 Asian (YH) genome using SOAPdenovo2. Here, the contig and scaffold N50 of the YH genome were ∼20.9 kbp and ∼22 Mbp, respectively, which is 3-fold and 50-fold longer than the first published version. The genome coverage increased from 81.16% to 93.91%, and memory consumption was ∼2/3 lower during the point of largest memory consumption.",Assembly,"soapdenovo2  empirically improve memoryefficient shortread  novo assembler
   rapidly increase amount   novo genome assembly use nextgeneration sequence ngs short read however several big challenge remain   overcome  order     efficient  accurate soapdenovo   successfully apply  assemble many publish genomes   still need improvement  continuity accuracy  coverage especially  repeat regions  overcome  challenge   develop  successor soapdenovo2    advantage   new algorithm design  reduce memory consumption  graph construction resolve  repeat regions  contig assembly increase coverage  length  scaffold construction improve gap close  optimize  large genome benchmark use  assemblathon1  gage datasets show  soapdenovo2 greatly surpass  predecessor soapdenovo   competitive   assemblers   assembly length  accuracy  also provide  update assembly version    asian  genome use soapdenovo2   contig  scaffold n50    genome   kbp   mbp respectively   fold  fold longer   first publish version  genome coverage increase      memory consumption   lower   point  largest memory consumption",5
107,SSAKE,"Assembling millions of short DNA sequences using SSAKE.
Novel DNA sequencing technologies with the potential for up to three orders magnitude more sequence throughput than conventional Sanger sequencing are emerging. The instrument now available from Solexa Ltd, produces millions of short DNA sequences of 25 nt each. Due to ubiquitous repeats in large genomes and the inability of short sequences to uniquely and unambiguously characterize them, the short read length limits applicability for de novo sequencing. However, given the sequencing depth and the throughput of this instrument, stringent assembly of highly identical sequences can be achieved. We describe SSAKE, a tool for aggressively assembling millions of short nucleotide sequences by progressively searching through a prefix tree for the longest possible overlap between any two sequences. SSAKE is designed to help leverage the information from short sequence reads by stringently assembling them into contiguous sequences that can be used to characterize novel sequencing targets.",Assembly,"assemble millions  short dna sequence use ssake
novel dna sequence technologies   potential    three order magnitude  sequence throughput  conventional sanger sequence  emerge  instrument  available  solexa ltd produce millions  short dna sequence     due  ubiquitous repeat  large genomes   inability  short sequence  uniquely  unambiguously characterize   short read length limit applicability   novo sequence however give  sequence depth   throughput   instrument stringent assembly  highly identical sequence   achieve  describe ssake  tool  aggressively assemble millions  short nucleotide sequence  progressively search   prefix tree   longest possible overlap   two sequence ssake  design  help leverage  information  short sequence read  stringently assemble   contiguous sequence    use  characterize novel sequence target",5
108,GAML,"GAML: Genome Assembly by Maximum Likelihood
Resolution of repeats and scaffolding of shorter contigs are critical parts of genome assembly. Modern assemblers usually perform such steps by heuristics, often tailored to a particular technology for producing paired or long reads. We propose a new framework that allows systematic combination of diverse sequencing datasets into a single assembly. We achieve this by searching for an assembly with the maximum likelihood in a probabilistic model capturing error rate, insert lengths, and other characteristics of the sequencing technology used to produce each dataset. We have implemented a prototype genome assembler GAML that can use any combination of insert sizes with Illumina or 454 reads, as well as PacBio reads. Our experiments show that we can assemble short genomes with N50 sizes and error rates comparable to ALLPATHS-LG or Cerulean. While ALLPATHS-LG and Cerulean require each a specific combination of datasets, GAML works on any combination. We have introduced a new probabilistic approach to genome assembly and demonstrated that this approach can lead to superior results when used to combine diverse set of datasets from different sequencing technologies.",Assembly,"gaml genome assembly  maximum likelihood
resolution  repeat  scaffold  shorter contigs  critical part  genome assembly modern assemblers usually perform  step  heuristics often tailor   particular technology  produce pair  long read  propose  new framework  allow systematic combination  diverse sequence datasets   single assembly  achieve   search   assembly   maximum likelihood   probabilistic model capture error rate insert lengths   characteristics   sequence technology use  produce  dataset   implement  prototype genome assembler gaml   use  combination  insert size  illumina   read  well  pacbio read  experiment show    assemble short genomes  n50 size  error rat comparable  allpathslg  cerulean  allpathslg  cerulean require   specific combination  datasets gaml work   combination   introduce  new probabilistic approach  genome assembly  demonstrate   approach  lead  superior result  use  combine diverse set  datasets  different sequence technologies",5
109,SKESA,"SKESA: strategic k-mer extension for scrupulous assemblies
SKESA is a DeBruijn graph-based de-novo assembler designed for assembling reads of microbial genomes sequenced using Illumina. Comparison with SPAdes and MegaHit shows that SKESA produces assemblies that have high sequence quality and contiguity, handles low-level contamination in reads, is fast, and produces an identical assembly for the same input when assembled multiple times with the same or different compute resources. SKESA has been used for assembling over 272,000 read sets in the Sequence Read Archive at NCBI and for real-time pathogen detection.",Assembly,"skesa strategic kmer extension  scrupulous assemblies
skesa   debruijn graphbased denovo assembler design  assemble read  microbial genomes sequence use illumina comparison  spade  megahit show  skesa produce assemblies   high sequence quality  contiguity handle lowlevel contamination  read  fast  produce  identical assembly    input  assemble multiple time     different compute resources skesa   use  assemble   read set   sequence read archive  ncbi   realtime pathogen detection",5
110,Euler-sr,"Short read fragment assembly of bacterial genomes
In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.",Assembly,"short read fragment assembly  bacterial genomes
  last year highthroughput sequence technologies  progress  proofofconcept  production quality   methods produce highquality read   yet  produce read comparable  length  sangerbased sequence current fragment assembly algorithms   implement  optimize  matepaired sangerbased read  thus   perform well  short read produce  short read technologies  present  new eulerian assembler  generate nearly optimal short read assemblies  bacterial genomes  describe  approach  assemble read   case   popular hybrid protocol  short  long sangerbased read  combine",5
111,ALLPATHS2,"ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads
We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8% (ALLPATHS2), 68.7% (Velvet), and 42.1% (EULER-SR).",Assembly,"allpaths  small genomes assemble accurately   high continuity  short pair reads
 demonstrate  genome sequence approach finish quality   generate  short pair read use  base fragment   base jump read  five microbial genomes  vary  composition  size     allpaths2 generate assemblies  long accurate contigs  scaffold velvet  eulersr  less accurate  example  escherichia coli  fraction   stretch   perfect   allpaths2  velvet   eulersr",5
112,VCAKE,"Extending assembly of short DNA sequences to handle error 
Inexpensive de novo genome sequencing, particularly in organisms with small genomes, is now possible using several new sequencing technologies. Some of these technologies such as that from Illumina's Solexa Sequencing, produce high genomic coverage by generating a very large number of small reads (∼30 bp). While prior work shows that partial assembly can be performed by k-mer extension in error-free reads, this algorithm is unsuccessful with the sequencing error rates found in practice. We present VCAKE (Verified Consensus Assembly by K-mer Extension), a modification of simple k-mer extension that overcomes error by using high depth coverage. Though it is a simple modification of a previous approach, we show significant improvements in assembly results on simulated and experimental datasets that include error.",Assembly,"extend assembly  short dna sequence  handle error 
inexpensive  novo genome sequence particularly  organisms  small genomes   possible use several new sequence technologies    technologies     illumina' solexa sequence produce high genomic coverage  generate   large number  small read    prior work show  partial assembly   perform  kmer extension  errorfree read  algorithm  unsuccessful   sequence error rat find  practice  present vcake verify consensus assembly  kmer extension  modification  simple kmer extension  overcome error  use high depth coverage though    simple modification   previous approach  show significant improvements  assembly result  simulate  experimental datasets  include error",5
113,SHARCGS,"SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing
The latest revolution in the DNA sequencing field has been brought about by the development of automated sequencers that are capable of generating giga base pair data sets quickly and at low cost. Applications of such technologies seem to be limited to resequencing and transcript discovery, due to the shortness of the generated reads. In order to extend the fields of application to de novo sequencing, we developed the SHARCGS algorithm to assemble short-read (25–40-mer) data with high accuracy and speed. The efficiency of SHARCGS was tested on BAC inserts from three eukaryotic species, on two yeast chromosomes, and on two bacterial genomes (Haemophilus influenzae, Escherichia coli). We show that 30-mer-based BAC assemblies have N50 sizes >20 kbp for Drosophila and Arabidopsis and >4 kbp for human in simulations taking missing reads and wrong base calls into account. We assembled 949,974 contigs with length >50 bp, and only one single contig could not be aligned error-free against the reference sequences. We generated 36-mer reads for the genome of Helicobacter acinonychis on the Illumina 1G sequencing instrument and assembled 937 contigs covering 98% of the genome with an N50 size of 3.7 kbp. With the exception of five contigs that differ in 1–4 positions relative to the reference sequence, all contigs matched the genome error-free. Thus, SHARCGS is a suitable tool for fully exploiting novel sequencing technologies by assembling sequence contigs de novo with high confidence and by outperforming existing assembly algorithms in terms of speed and accuracy.",Assembly,"sharcgs  fast  highly accurate shortread assembly algorithm   novo genomic sequencing
 latest revolution   dna sequence field   bring    development  automate sequencers   capable  generate giga base pair data set quickly   low cost applications   technologies seem   limit  resequencing  transcript discovery due   shortness   generate read  order  extend  field  application   novo sequence  develop  sharcgs algorithm  assemble shortread mer data  high accuracy  speed  efficiency  sharcgs  test  bac insert  three eukaryotic species  two yeast chromosomes   two bacterial genomes haemophilus influenzae escherichia coli  show  merbased bac assemblies  n50 size  kbp  drosophila  arabidopsis   kbp  human  simulations take miss read  wrong base call  account  assemble  contigs  length     one single contig could   align errorfree   reference sequence  generate mer read   genome  helicobacter acinonychis   illumina  sequence instrument  assemble  contigs cover    genome   n50 size   kbp   exception  five contigs  differ   position relative   reference sequence  contigs match  genome errorfree thus sharcgs   suitable tool  fully exploit novel sequence technologies  assemble sequence contigs  novo  high confidence   outperform exist assembly algorithms  term  speed  accuracy",5
114,Edena,"De novo bacterial genome sequencing: Millions of very short reads assembled on a desktop computer
Novel high-throughput DNA sequencing technologies allow researchers to characterize a bacterial genome during a single experiment and at a moderate cost. However, the increase in sequencing throughput that is allowed by using such platforms is obtained at the expense of individual sequence read length, which must be assembled into longer contigs to be exploitable. This study focuses on the Illumina sequencing platform that produces millions of very short sequences that are 35 bases in length. We propose a de novo assembler software that is dedicated to process such data. Based on a classical overlap graph representation and on the detection of potentially spurious reads, our software generates a set of accurate contigs of several kilobases that cover most of the bacterial genome. The assembly results were validated by comparing data sets that were obtained experimentally for Staphylococcus aureus strain MW2 and Helicobacter acinonychis strain Sheeba with that of their published genomes acquired by conventional sequencing of 1.5- to 3.0-kb fragments. We also provide indications that the broad coverage achieved by high-throughput sequencing might allow for the detection of clonal polymorphisms in the set of DNA molecules being sequenced.",Assembly," novo bacterial genome sequence millions   short read assemble   desktop computer
novel highthroughput dna sequence technologies allow researchers  characterize  bacterial genome   single experiment    moderate cost however  increase  sequence throughput   allow  use  platforms  obtain   expense  individual sequence read length  must  assemble  longer contigs   exploitable  study focus   illumina sequence platform  produce millions   short sequence    base  length  propose   novo assembler software   dedicate  process  data base   classical overlap graph representation    detection  potentially spurious read  software generate  set  accurate contigs  several kilobases  cover    bacterial genome  assembly result  validate  compare data set   obtain experimentally  staphylococcus aureus strain mw2  helicobacter acinonychis strain sheeba     publish genomes acquire  conventional sequence     fragment  also provide indications   broad coverage achieve  highthroughput sequence might allow   detection  clonal polymorphisms   set  dna molecules  sequence",5
115,IDBA-a,"IDBA – A Practical Iterative de Bruijn Graph De Novo Assembler
The de Bruijn graph assembly approach breaks reads into k-mers before assembling them into contigs. The string graph approach forms contigs by connecting two reads with k or more overlapping nucleotides. Both approaches must deal with the following problems: false-positive vertices, due to erroneous reads; gap problem, due to non-uniform coverage; branching problem, due to erroneous reads and repeat regions. A proper choice of k is crucial but for single k there is always a trade-off: a small k favors the situation of erroneous reads and non-uniform coverage, and a large k favors short repeat regions. We propose an iterative de Bruijn graph approach iterating from small to large k exploring the advantages of the in between values. Our IDBA outperforms the existing algorithms by constructing longer contigs with similar accuracy and using less memory, both with real and simulated data. The running time of the algorithm is comparable to existing algorithms.",Assembly,"idba   practical iterative  bruijn graph  novo assembler
  bruijn graph assembly approach break read  kmers  assemble   contigs  string graph approach form contigs  connect two read     overlap nucleotides  approach must deal   follow problems falsepositive vertices due  erroneous read gap problem due  nonuniform coverage branch problem due  erroneous read  repeat regions  proper choice    crucial   single    always  tradeoff  small  favor  situation  erroneous read  nonuniform coverage   large  favor short repeat regions  propose  iterative  bruijn graph approach iterate  small  large  explore  advantage     value  idba outperform  exist algorithms  construct longer contigs  similar accuracy  use less memory   real  simulate data  run time   algorithm  comparable  exist algorithms",5
116,IDBA-tran,"IDBA-tran: a more robust de novo de Bruijn graph assembler for transcriptomes with uneven expression levels.
RNA sequencing based on next-generation sequencing technology is effective for analyzing transcriptomes. Like de novo genome assembly, de novo transcriptome assembly does not rely on any reference genome or additional annotation information, but is more difficult. In particular, isoforms can have very uneven expression levels (e.g. 1:100), which make it very difficult to identify low-expressed isoforms. One challenge is to remove erroneous vertices/edges with high multiplicity (produced by high-expressed isoforms) in the de Bruijn graph without removing correct ones with not-so-high multiplicity from low-expressed isoforms. Failing to do so will result in the loss of low-expressed isoforms or having complicated subgraphs with transcripts of different genes mixed together due to erroneous vertices/edges. Unlike existing tools, which remove erroneous vertices/edges with multiplicities lower than a global threshold, we use a probabilistic progressive approach to iteratively remove them with local thresholds. This enables us to decompose the graph into disconnected components, each containing a few genes, if not a single gene, while retaining many correct vertices/edges of low-expressed isoforms. Combined with existing techniques, IDBA-Tran is able to assemble both high-expressed and low-expressed transcripts and outperform existing assemblers in terms of sensitivity and specificity for both simulated and real data.",Assembly,"idbatran   robust  novo  bruijn graph assembler  transcriptomes  uneven expression levels
rna sequence base  nextgeneration sequence technology  effective  analyze transcriptomes like  novo genome assembly  novo transcriptome assembly   rely   reference genome  additional annotation information    difficult  particular isoforms    uneven expression level    make   difficult  identify lowexpressed isoforms one challenge   remove erroneous verticesedges  high multiplicity produce  highexpressed isoforms    bruijn graph without remove correct ones  notsohigh multiplicity  lowexpressed isoforms fail     result   loss  lowexpressed isoforms   complicate subgraphs  transcripts  different genes mix together due  erroneous verticesedges unlike exist tool  remove erroneous verticesedges  multiplicities lower   global threshold  use  probabilistic progressive approach  iteratively remove   local thresholds  enable   decompose  graph  disconnect components  contain   genes    single gene  retain many correct verticesedges  lowexpressed isoforms combine  exist techniques idbatran  able  assemble  highexpressed  lowexpressed transcripts  outperform exist assemblers  term  sensitivity  specificity   simulate  real data",5
117,Meta-IDBA,"Meta-IDBA: a de Novo assembler for metagenomic data.
Next-generation sequencing techniques allow us to generate reads from a microbial environment in order to analyze the microbial community. However, assembling of a set of mixed reads from different species to form contigs is a bottleneck of metagenomic research. Although there are many assemblers for assembling reads from a single genome, there are no assemblers for assembling reads in metagenomic data without reference genome sequences. Moreover, the performances of these assemblers on metagenomic data are far from satisfactory, because of the existence of common regions in the genomes of subspecies and species, which make the assembly problem much more complicated. We introduce the Meta-IDBA algorithm for assembling reads in metagenomic data, which contain multiple genomes from different species. There are two core steps in Meta-IDBA. It first tries to partition the de Bruijn graph into isolated components of different species based on an important observation. Then, for each component, it captures the slight variants of the genomes of subspecies from the same species by multiple alignments and represents the genome of one species, using a consensus sequence. Comparison of the performances of Meta-IDBA and existing assemblers, such as Velvet and Abyss for different metagenomic datasets shows that Meta-IDBA can reconstruct longer contigs with similar accuracy.",Assembly,"metaidba   novo assembler  metagenomic data
nextgeneration sequence techniques allow   generate read   microbial environment  order  analyze  microbial community however assemble   set  mix read  different species  form contigs   bottleneck  metagenomic research although   many assemblers  assemble read   single genome    assemblers  assemble read  metagenomic data without reference genome sequence moreover  performances   assemblers  metagenomic data  far  satisfactory    existence  common regions   genomes  subspecies  species  make  assembly problem much  complicate  introduce  metaidba algorithm  assemble read  metagenomic data  contain multiple genomes  different species   two core step  metaidba  first try  partition   bruijn graph  isolate components  different species base   important observation    component  capture  slight variants   genomes  subspecies    species  multiple alignments  represent  genome  one species use  consensus sequence comparison   performances  metaidba  exist assemblers   velvet  aby  different metagenomic datasets show  metaidba  reconstruct longer contigs  similar accuracy",5
118,IDBA-UD,"IDBA-UD: a de novo assembler for single-cell and metagenomic sequencing data with highly uneven depth
Next-generation sequencing allows us to sequence reads from a microbial environment using single-cell sequencing or metagenomic sequencing technologies. However, both technologies suffer from the problem that sequencing depth of different regions of a genome or genomes from different species are highly uneven. Most existing genome assemblers usually have an assumption that sequencing depths are even. These assemblers fail to construct correct long contigs. We introduce the IDBA-UD algorithm that is based on the de Bruijn graph approach for assembling reads from single-cell sequencing or metagenomic sequencing technologies with uneven sequencing depths. Several non-trivial techniques have been employed to tackle the problems. Instead of using a simple threshold, we use multiple depthrelative thresholds to remove erroneous k-mers in both low-depth and high-depth regions. The technique of local assembly with paired-end information is used to solve the branch problem of low-depth short repeat regions. To speed up the process, an error correction step is conducted to correct reads of high-depth regions that can be aligned to highconfident contigs. Comparison of the performances of IDBA-UD and existing assemblers (Velvet, Velvet-SC, SOAPdenovo and Meta-IDBA) for different datasets, shows that IDBA-UD can reconstruct longer contigs with higher accuracy.",Assembly,"idbaud   novo assembler  singlecell  metagenomic sequence data  highly uneven depth
nextgeneration sequence allow   sequence read   microbial environment use singlecell sequence  metagenomic sequence technologies however  technologies suffer   problem  sequence depth  different regions   genome  genomes  different species  highly uneven  exist genome assemblers usually   assumption  sequence depths  even  assemblers fail  construct correct long contigs  introduce  idbaud algorithm   base    bruijn graph approach  assemble read  singlecell sequence  metagenomic sequence technologies  uneven sequence depths several nontrivial techniques   employ  tackle  problems instead  use  simple threshold  use multiple depthrelative thresholds  remove erroneous kmers   lowdepth  highdepth regions  technique  local assembly  pairedend information  use  solve  branch problem  lowdepth short repeat regions  speed   process  error correction step  conduct  correct read  highdepth regions    align  highconfident contigs comparison   performances  idbaud  exist assemblers velvet velvetsc soapdenovo  metaidba  different datasets show  idbaud  reconstruct longer contigs  higher accuracy",5
119,T-IDBA,"T-IDBA: A de novo Iterative de Bruijn Graph Assembler for Transcriptome
RNA-seq data produced by next-generation sequencing technology is a useful tool for analyzing transcriptomes. However, existing de novo transcriptome assemblers do not fully utilize the properties of transcriptomes and may result in short contigs because of the splicing nature (shared exons) of the genes. We propose the T-IDBA algorithm to reconstruct expressed isoforms without reference genome. By using pair-end information to solve the problem of long repeats in different genes and branching in the same gene due to alternative splicing, the graph can be decomposed into small components, each corresponds to a gene. The most possible isoforms with sufficient support from the pair-end reads will be found heuristically. In practice, our de novo transcriptome assembler, T-IDBA, outperforms Abyss substantially in terms of sensitivity and precision for both simulated and real data.",Assembly,"tidba   novo iterative  bruijn graph assembler  transcriptome
rnaseq data produce  nextgeneration sequence technology   useful tool  analyze transcriptomes however exist  novo transcriptome assemblers   fully utilize  properties  transcriptomes  may result  short contigs    splice nature share exons   genes  propose  tidba algorithm  reconstruct express isoforms without reference genome  use pairend information  solve  problem  long repeat  different genes  branch    gene due  alternative splice  graph   decompose  small components  correspond   gene   possible isoforms  sufficient support   pairend read   find heuristically  practice   novo transcriptome assembler tidba outperform aby substantially  term  sensitivity  precision   simulate  real data",5
120,Pasha,"Parallelized short read assembly of large genomes using de Bruijn graphs
Next-generation sequencing technologies have given rise to the explosive increase in DNA sequencing throughput, and have promoted the recent development of de novo short read assemblers. However, existing assemblers require high execution times and a large amount of compute resources to assemble large genomes from quantities of short reads. We present PASHA, a parallelized short read assembler using de Bruijn graphs, which takes advantage of hybrid computing architectures consisting of both shared-memory multi-core CPUs and distributed-memory compute clusters to gain efficiency and scalability. Evaluation using three small-scale real paired-end datasets shows that PASHA is able to produce more contiguous high-quality assemblies in shorter time compared to three leading assemblers: Velvet, ABySS and SOAPdenovo. PASHA's scalability for large genome datasets is demonstrated with human genome assembly. Compared to ABySS, PASHA achieves competitive assembly quality with faster execution speed on the same compute resources, yielding an NG50 contig size of 503 with the longest correct contig size of 18,252, and an NG50 scaffold size of 2,294. Moreover, the human assembly is completed in about 21 hours with only modest compute resources. Developing parallel assemblers for large genomes has been garnering significant research efforts due to the explosive size growth of high-throughput short read datasets. By employing hybrid parallelism consisting of multi-threading on multi-core CPUs and message passing on compute clusters, PASHA is able to assemble the human genome with high quality and in reasonable time using modest compute resources.",Assembly,"parallelize short read assembly  large genomes use  bruijn graphs
nextgeneration sequence technologies  give rise   explosive increase  dna sequence throughput   promote  recent development   novo short read assemblers however exist assemblers require high execution time   large amount  compute resources  assemble large genomes  quantities  short read  present pasha  parallelize short read assembler use  bruijn graph  take advantage  hybrid compute architectures consist   sharedmemory multicore cpus  distributedmemory compute cluster  gain efficiency  scalability evaluation use three smallscale real pairedend datasets show  pasha  able  produce  contiguous highquality assemblies  shorter time compare  three lead assemblers velvet aby  soapdenovo pasha' scalability  large genome datasets  demonstrate  human genome assembly compare  aby pasha achieve competitive assembly quality  faster execution speed    compute resources yield  ng50 contig size     longest correct contig size     ng50 scaffold size   moreover  human assembly  complete    hours   modest compute resources develop parallel assemblers  large genomes   garner significant research efforts due   explosive size growth  highthroughput short read datasets  employ hybrid parallelism consist  multithreading  multicore cpus  message pass  compute cluster pasha  able  assemble  human genome  high quality   reasonable time use modest compute resources",5
121,Ray,"Ray: Simultaneous Assembly of Reads from a Mix of High-Throughput Sequencing Technologies
An accurate genome sequence of a desired species is now a pre-requisite for genome research. An important step in obtaining a high-quality genome sequence is to correctly assemble short reads into longer sequences accurately representing contiguous genomic regions. Current sequencing technologies continue to offer increases in throughput, and corresponding reductions in cost and time. Unfortunately, the benefit of obtaining a large number of reads is complicated by sequencing errors, with different biases being observed with each platform. Although software are available to assemble reads for each individual system, no procedure has been proposed for high-quality simultaneous assembly based on reads from a mix of different technologies. In this paper, we describe a parallel short-read assembler, called Ray, which has been developed to assemble reads obtained from a combination of sequencing platforms. We compared its performance to other assemblers on simulated and real datasets. We used a combination of Roche/454 and Illumina reads to assemble three different genomes. We showed that mixing sequencing technologies systematically reduces the number of contigs and the number of errors.",Assembly,"ray simultaneous assembly  read   mix  highthroughput sequence technologies
 accurate genome sequence   desire species    prerequisite  genome research  important step  obtain  highquality genome sequence   correctly assemble short read  longer sequence accurately represent contiguous genomic regions current sequence technologies continue  offer increase  throughput  correspond reductions  cost  time unfortunately  benefit  obtain  large number  read  complicate  sequence errors  different bias  observe   platform although software  available  assemble read   individual system  procedure   propose  highquality simultaneous assembly base  read   mix  different technologies   paper  describe  parallel shortread assembler call ray    develop  assemble read obtain   combination  sequence platforms  compare  performance   assemblers  simulate  real datasets  use  combination  roche  illumina read  assemble three different genomes  show  mix sequence technologies systematically reduce  number  contigs   number  errors",5
122,Ray-meta,"Ray Meta: scalable de novo metagenome assembly and profiling
Voluminous parallel sequencing datasets, especially metagenomic experiments, require distributed computing for de novo assembly and taxonomic profiling. Ray Meta is a massively distributed metagenome assembler that is coupled with Ray Communities, which profiles microbiomes based on uniquely-colored k-mers. It can accurately assemble and profile a three billion read metagenomic experiment representing 1,000 bacterial genomes of uneven proportions in 15 hours with 1,024 processor cores, using only 1.5 GB per core. The software will facilitate the processing of large and complex datasets, and will help in generating biological insights for specific environments.",Assembly,"ray meta scalable  novo metagenome assembly  profiling
voluminous parallel sequence datasets especially metagenomic experiment require distribute compute   novo assembly  taxonomic profile ray meta   massively distribute metagenome assembler   couple  ray communities  profile microbiomes base  uniquelycolored kmers   accurately assemble  profile  three billion read metagenomic experiment represent  bacterial genomes  uneven proportion   hours   processor core use    per core  software  facilitate  process  large  complex datasets   help  generate biological insights  specific environments",5
123,MegaHIt,"MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph
MEGAHIT is a NGS de novo assembler for assembling large and complex metagenomics data in a time- and cost-efficient manner. It finished assembling a soil metagenomics dataset with 252 Gbps in 44.1 and 99.6 h on a single computing node with and without a graphics processing unit, respectively. MEGAHIT assembles the data as a whole, i.e. no pre-processing like partitioning and normalization was needed. When compared with previous methods on assembling the soil data, MEGAHIT generated a three-time larger assembly, with longer contig N50 and average contig length; furthermore, 55.8% of the reads were aligned to the assembly, giving a fourfold improvement.",Assembly,"megahit  ultrafast singlenode solution  large  complex metagenomics assembly via succinct  bruijn graph
megahit   ngs  novo assembler  assemble large  complex metagenomics data   time  costefficient manner  finish assemble  soil metagenomics dataset   gbps        single compute node   without  graphics process unit respectively megahit assemble  data   whole   preprocessing like partition  normalization  need  compare  previous methods  assemble  soil data megahit generate  threetime larger assembly  longer contig n50  average contig length furthermore    read  align   assembly give  fourfold improvement",5
124,Omega,"Omega: an overlap-graph de novo assembler for metagenomics
Metagenomic sequencing allows reconstruction of microbial genomes directly from environmental samples. Omega ( o verlap-graph me ta g enome a ssembler) was developed for assembling and scaffolding Illumina sequencing data of microbial communities. Omega found overlaps between reads using a prefix/suffix hash table. The overlap graph of reads was simplified by removing transitive edges and trimming short branches. Unitigs were generated based on minimum cost flow analysis of the overlap graph and then merged to contigs and scaffolds using mate-pair information. In comparison with three de Bruijn graph assemblers (SOAPdenovo, IDBA-UD and MetaVelvet), Omega provided comparable overall performance on a HiSeq 100-bp dataset and superior performance on a MiSeq 300-bp dataset. In comparison with Celera on the MiSeq dataset, Omega provided more continuous assemblies overall using a fraction of the computing time of existing overlap-layout-consensus assemblers. This indicates Omega can more efficiently assemble longer Illumina reads, and at deeper coverage, for metagenomic datasets.",Assembly,"omega  overlapgraph  novo assembler  metagenomics
metagenomic sequence allow reconstruction  microbial genomes directly  environmental sample omega   verlapgraph    enome  ssembler  develop  assemble  scaffold illumina sequence data  microbial communities omega find overlap  read use  prefixsuffix hash table  overlap graph  read  simplify  remove transitive edge  trim short branch unitigs  generate base  minimum cost flow analysis   overlap graph   merge  contigs  scaffold use matepair information  comparison  three  bruijn graph assemblers soapdenovo idbaud  metavelvet omega provide comparable overall performance   hiseq  dataset  superior performance   miseq  dataset  comparison  celera   miseq dataset omega provide  continuous assemblies overall use  fraction   compute time  exist overlaplayoutconsensus assemblers  indicate omega   efficiently assemble longer illumina read   deeper coverage  metagenomic datasets",5
125,Genovo,"Genovo: de novo assembly for metagenomes
Next-generation sequencing technologies produce a large number of noisy reads from the DNA in a sample. Metagenomics and population sequencing aim to recover the genomic sequences of the species in the sample, which could be of high diversity. Methods geared towards single sequence reconstruction are not sensitive enough when applied in this setting. We introduce a generative probabilistic model of read generation from environmental samples and present Genovo, a novel de novo sequence assembler that discovers likely sequence reconstructions under the model. A nonparametric prior accounts for the unknown number of genomes in the sample. Inference is performed by applying a series of hill-climbing steps iteratively until convergence. We compare the performance of Genovo to three other short read assembly programs in a series of synthetic experiments and across nine metagenomic datasets created using the 454 platform, the largest of which has 311k reads. Genovo's reconstructions cover more bases and recover more genes than the other methods, even for low-abundance sequences, and yield a higher assembly score.",Assembly,"genovo  novo assembly  metagenomes
nextgeneration sequence technologies produce  large number  noisy read   dna   sample metagenomics  population sequence aim  recover  genomic sequence   species   sample  could   high diversity methods gear towards single sequence reconstruction   sensitive enough  apply   set  introduce  generative probabilistic model  read generation  environmental sample  present genovo  novel  novo sequence assembler  discover likely sequence reconstructions   model  nonparametric prior account   unknown number  genomes   sample inference  perform  apply  series  hillclimbing step iteratively  convergence  compare  performance  genovo  three  short read assembly program   series  synthetic experiment  across nine metagenomic datasets create use   platform  largest     read genovo' reconstructions cover  base  recover  genes    methods even  lowabundance sequence  yield  higher assembly score",5
126,metaSPAdes,"metaSPAdes: a new versatile metagenomic assembler
While metagenomics has emerged as a technology of choice for analyzing bacterial populations, the assembly of metagenomic data remains challenging, thus stifling biological discoveries. Moreover, recent studies revealed that complex bacterial populations may be composed from dozens of related strains, thus further amplifying the challenge of metagenomic assembly. metaSPAdes addresses various challenges of metagenomic assembly by capitalizing on computational ideas that proved to be useful in assemblies of single cells and highly polymorphic diploid genomes. We benchmark metaSPAdes against other state-of-the-art metagenome assemblers and demonstrate that it results in high-quality assemblies across diverse data sets.",Assembly,"metaspades  new versatile metagenomic assembler
 metagenomics  emerge   technology  choice  analyze bacterial populations  assembly  metagenomic data remain challenge thus stifle biological discoveries moreover recent study reveal  complex bacterial populations may  compose  dozens  relate strain thus  amplify  challenge  metagenomic assembly metaspades address various challenge  metagenomic assembly  capitalize  computational ideas  prove   useful  assemblies  single cells  highly polymorphic diploid genomes  benchmark metaspades   stateoftheart metagenome assemblers  demonstrate   result  highquality assemblies across diverse data set",5
127,VirusTap,"VirusTAP: Viral Genome-Targeted Assembly Pipeline
Although next-generation sequencing (NGS) technology provides a comprehensive means with which to identify potential pathogens from clinical specimens, simple and user-friendly bioinformatics pipelines are expected to obtain the entire viral genome sequence, subsequently providing traceability, based on extensive molecular phylogenetic analyses. We have developed a web-based integrated NGS analysis tool for the viral genome (virus genome-targeted assembly pipeline: VirusTAP), which includes extensive sequence subtraction of host- or bacteria-related NGS reads prior to de novo assembly, leading to the prompt and accurate assembly of viral genome sequences from metagenomic NGS reads. ",Assembly,"virustap viral genometargeted assembly pipeline
although nextgeneration sequence ngs technology provide  comprehensive mean    identify potential pathogens  clinical specimens simple  userfriendly bioinformatics pipelines  expect  obtain  entire viral genome sequence subsequently provide traceability base  extensive molecular phylogenetic analyse   develop  webbased integrate ngs analysis tool   viral genome virus genometargeted assembly pipeline virustap  include extensive sequence subtraction  host  bacteriarelated ngs read prior   novo assembly lead   prompt  accurate assembly  viral genome sequence  metagenomic ngs read ",5
128,SearchSmallRNA,"SearchSmallRNA: a graphical interface tool for the assemblage of viral genomes using small RNA libraries data
Next-generation parallel sequencing (NGS) allows the identification of viral pathogens by sequencing the small RNAs of infected hosts. Thus, viral genomes may be assembled from host immune response products without prior virus enrichment, amplification or purification. However, mapping of the vast information obtained presents a bioinformatics challenge. In order to by pass the need of line command and basic bioinformatics knowledge, we develop a mapping software with a graphical interface to the assemblage of viral genomes from small RNA dataset obtained by NGS. SearchSmallRNA was developed in JAVA language version 7 using NetBeans IDE 7.1 software. The program also allows the analysis of the viral small interfering RNAs (vsRNAs) profile; providing an overview of the size distribution and other features of the vsRNAs produced in infected cells.  The program performs comparisons between each read sequenced present in a library and a chosen reference genome. Reads showing Hamming distances smaller or equal to an allowed mismatched will be selected as positives and used to the assemblage of a long nucleotide genome sequence. In order to validate the software, distinct analysis using NGS dataset obtained from HIV and two plant viruses were used to reconstruct viral whole genomes. SearchSmallRNA program was able to reconstructed viral genomes using NGS of small RNA dataset with high degree of reliability so it will be a valuable tool for viruses sequencing and discovery. It is accessible and free to all research communities and has the advantage to have an easy-to-use graphical interface.",Assembly,"searchsmallrna  graphical interface tool   assemblage  viral genomes use small rna libraries data
nextgeneration parallel sequence ngs allow  identification  viral pathogens  sequence  small rnas  infect host thus viral genomes may  assemble  host immune response products without prior virus enrichment amplification  purification however map   vast information obtain present  bioinformatics challenge  order   pass  need  line command  basic bioinformatics knowledge  develop  map software   graphical interface   assemblage  viral genomes  small rna dataset obtain  ngs searchsmallrna  develop  java language version  use netbeans ide  software  program also allow  analysis   viral small interfere rnas vsrnas profile provide  overview   size distribution   feature   vsrnas produce  infect cells   program perform comparisons   read sequence present   library   choose reference genome read show ham distance smaller  equal   allow mismatch   select  positives  use   assemblage   long nucleotide genome sequence  order  validate  software distinct analysis use ngs dataset obtain  hiv  two plant viruses  use  reconstruct viral whole genomes searchsmallrna program  able  reconstruct viral genomes use ngs  small rna dataset  high degree  reliability      valuable tool  viruses sequence  discovery   accessible  free   research communities    advantage    easytouse graphical interface",5
129,drVM,"drVM: a new tool for efficient genome assembly of known eukaryotic viruses from metagenomes.
Virus discovery using high-throughput next-generation sequencing has become more commonplace. However, although analysis of deep next-generation sequencing data allows us to identity potential pathogens, the entire analytical procedure requires competency in the bioinformatics domain, which includes implementing proper software packages and preparing prerequisite databases. Simple and user-friendly bioinformatics pipelines are urgently required to obtain complete viral genome sequences from metagenomic data. This manuscript presents a pipeline, drVM (detect and reconstruct known viral genomes from metagenomes), for rapid viral read identification, genus-level read partition, read normalization, de novo assembly, sequence annotation, and coverage profiling. The first two procedures and sequence annotation rely on known viral genomes as a reference database. drVM was validated via the analysis of over 300 sequencing runs generated by Illumina and Ion Torrent platforms to provide complete viral genome assemblies for a variety of virus types including DNA viruses, RNA viruses, and retroviruses. drVM was compared with other viral detection tools to demonstrate its merits in terms of viral genome completeness and reduced computation time. This substantiates the platform's potential to produce prompt and accurate viral genome sequences from clinical samples.",Assembly,"drvm  new tool  efficient genome assembly  know eukaryotic viruses  metagenomes
virus discovery use highthroughput nextgeneration sequence  become  commonplace however although analysis  deep nextgeneration sequence data allow   identity potential pathogens  entire analytical procedure require competency   bioinformatics domain  include implement proper software package  prepare prerequisite databases simple  userfriendly bioinformatics pipelines  urgently require  obtain complete viral genome sequence  metagenomic data  manuscript present  pipeline drvm detect  reconstruct know viral genomes  metagenomes  rapid viral read identification genuslevel read partition read normalization  novo assembly sequence annotation  coverage profile  first two procedures  sequence annotation rely  know viral genomes   reference database drvm  validate via  analysis    sequence run generate  illumina  ion torrent platforms  provide complete viral genome assemblies   variety  virus type include dna viruses rna viruses  retroviruses drvm  compare   viral detection tool  demonstrate  merit  term  viral genome completeness  reduce computation time  substantiate  platform' potential  produce prompt  accurate viral genome sequence  clinical sample",5
130,InteMap,"InteMAP: Integrated metagenomic assembly pipeline for NGS short reads
Next-generation sequencing (NGS) has greatly facilitated metagenomic analysis but also raised new challenges for metagenomic DNA sequence assembly, owing to its high-throughput nature and extremely short reads generated by sequencers such as Illumina. To date, how to generate a high-quality draft assembly for metagenomic sequencing projects has not been fully addressed. We conducted a comprehensive assessment on state-of-the-art de novo assemblers and revealed that the performance of each assembler depends critically on the sequencing depth. To address this problem, we developed a pipeline named InteMAP to integrate three assemblers, ABySS, IDBA-UD and CABOG, which were found to complement each other in assembling metagenomic sequences. Making a decision of which assembling approaches to use according to the sequencing coverage estimation algorithm for each short read, the pipeline presents an automatic platform suitable to assemble real metagenomic NGS data with uneven coverage distribution of sequencing depth. By comparing the performance of InteMAP with current assemblers on both synthetic and real NGS metagenomic data, we demonstrated that InteMAP achieves better performance with a longer total contig length and higher contiguity, and contains more genes than others. We developed a de novo pipeline, named InteMAP, that integrates existing tools for metagenomics assembly. The pipeline outperforms previous assembly methods on metagenomic assembly by providing a longer total contig length, a higher contiguity and covering more genes. InteMAP, therefore, could potentially be a useful tool for the research community of metagenomics.",Assembly,"intemap integrate metagenomic assembly pipeline  ngs short reads
nextgeneration sequence ngs  greatly facilitate metagenomic analysis  also raise new challenge  metagenomic dna sequence assembly owe   highthroughput nature  extremely short read generate  sequencers   illumina  date   generate  highquality draft assembly  metagenomic sequence project    fully address  conduct  comprehensive assessment  stateoftheart  novo assemblers  reveal   performance   assembler depend critically   sequence depth  address  problem  develop  pipeline name intemap  integrate three assemblers aby idbaud  cabog   find  complement    assemble metagenomic sequence make  decision   assemble approach  use accord   sequence coverage estimation algorithm   short read  pipeline present  automatic platform suitable  assemble real metagenomic ngs data  uneven coverage distribution  sequence depth  compare  performance  intemap  current assemblers   synthetic  real ngs metagenomic data  demonstrate  intemap achieve better performance   longer total contig length  higher contiguity  contain  genes  others  develop   novo pipeline name intemap  integrate exist tool  metagenomics assembly  pipeline outperform previous assembly methods  metagenomic assembly  provide  longer total contig length  higher contiguity  cover  genes intemap therefore could potentially   useful tool   research community  metagenomics",5
131,DBG2OLC,"DBG2OLC: Efficient Assembly of Large Genomes Using Long Erroneous Reads of the Third Generation Sequencing Technologies
The highly anticipated transition from next generation sequencing (NGS) to third generation sequencing (3GS) has been difficult primarily due to high error rates and excessive sequencing cost. The high error rates make the assembly of long erroneous reads of large genomes challenging because existing software solutions are often overwhelmed by error correction tasks. Here we report a hybrid assembly approach that simultaneously utilizes NGS and 3GS data to address both issues. We gain advantages from three general and basic design principles: (i) Compact representation of the long reads leads to efficient alignments. (ii) Base-level errors can be skipped; structural errors need to be detected and corrected. (iii) Structurally correct 3GS reads are assembled and polished. In our implementation, preassembled NGS contigs are used to derive the compact representation of the long reads, motivating an algorithmic conversion from a de Bruijn graph to an overlap graph, the two major assembly paradigms. Moreover, since NGS and 3GS data can compensate for each other, our hybrid assembly approach reduces both of their sequencing requirements. Experiments show that our software is able to assemble mammalian-sized genomes orders of magnitude more quickly than existing methods without consuming a lot of memory, while saving about half of the sequencing cost.",Assembly,"dbg2olc efficient assembly  large genomes use long erroneous read   third generation sequence technologies
 highly anticipate transition  next generation sequence ngs  third generation sequence 3gs   difficult primarily due  high error rat  excessive sequence cost  high error rat make  assembly  long erroneous read  large genomes challenge  exist software solutions  often overwhelm  error correction task   report  hybrid assembly approach  simultaneously utilize ngs  3gs data  address  issue  gain advantage  three general  basic design principles  compact representation   long read lead  efficient alignments  baselevel errors   skip structural errors need   detect  correct iii structurally correct 3gs read  assemble  polish   implementation preassemble ngs contigs  use  derive  compact representation   long read motivate  algorithmic conversion    bruijn graph   overlap graph  two major assembly paradigms moreover since ngs  3gs data  compensate     hybrid assembly approach reduce    sequence requirements experiment show   software  able  assemble mammaliansized genomes order  magnitude  quickly  exist methods without consume  lot  memory  save  half   sequence cost",5
132,MaSuRcA,"The MaSuRCA genome assembler
Second-generation sequencing technologies produce high coverage of the genome by short reads at a low cost, which has prompted development of new assembly methods. In particular, multiple algorithms based on de Bruijn graphs have been shown to be effective for the assembly problem. In this article, we describe a new hybrid approach that has the computational efficiency of de Bruijn graph methods and the flexibility of overlap-based assembly strategies, and which allows variable read lengths while tolerating a significant level of sequencing error. Our method transforms large numbers of paired-end reads into a much smaller number of longer ‘super-reads’. The use of super-reads allows us to assemble combinations of Illumina reads of differing lengths together with longer reads from 454 and Sanger sequencing technologies, making it one of the few assemblers capable of handling such mixtures. We call our system the Maryland Super-Read Celera Assembler (abbreviated MaSuRCA and pronounced ‘mazurka’). We evaluate the performance of MaSuRCA against two of the most widely used assemblers for Illumina data, Allpaths-LG and SOAPdenovo2, on two datasets from organisms for which high-quality assemblies are available: the bacterium Rhodobacter sphaeroides and chromosome 16 of the mouse genome. We show that MaSuRCA performs on par or better than Allpaths-LG and significantly better than SOAPdenovo on these data, when evaluated against the finished sequence. We then show that MaSuRCA can significantly improve its assemblies when the original data are augmented with long reads.",Assembly," masurca genome assembler
secondgeneration sequence technologies produce high coverage   genome  short read   low cost   prompt development  new assembly methods  particular multiple algorithms base   bruijn graph   show   effective   assembly problem   article  describe  new hybrid approach    computational efficiency   bruijn graph methods   flexibility  overlapbased assembly strategies   allow variable read lengths  tolerate  significant level  sequence error  method transform large number  pairedend read   much smaller number  longer superreads  use  superreads allow   assemble combinations  illumina read  differ lengths together  longer read    sanger sequence technologies make  one    assemblers capable  handle  mixtures  call  system  maryland superread celera assembler abbreviate masurca  pronounce mazurka  evaluate  performance  masurca  two    widely use assemblers  illumina data allpathslg  soapdenovo2  two datasets  organisms   highquality assemblies  available  bacterium rhodobacter sphaeroides  chromosome    mouse genome  show  masurca perform  par  better  allpathslg  significantly better  soapdenovo   data  evaluate   finish sequence   show  masurca  significantly improve  assemblies   original data  augment  long read",5
133,TIGER,"TIGER: tiled iterative genome assembler
With the cost reduction of the next-generation sequencing (NGS) technologies, genomics has provided us with an unprecedented opportunity to understand fundamental questions in biology and elucidate human diseases. De novo genome assembly is one of the most important steps to reconstruct the sequenced genome. However, most de novo assemblers require enormous amount of computational resource, which is not accessible for most research groups and medical personnel. We have developed a novel de novo assembly framework, called Tiger, which adapts to available computing resources by iteratively decomposing the assembly problem into sub-problems. Our method is also flexible to embed different assemblers for various types of target genomes. Using the sequence data from a human chromosome, our results show that Tiger can achieve much better NG50s, better genome coverage, and slightly higher errors, as compared to Velvet and SOAPdenovo, using modest amount of memory that are available in commodity computers today. Most state-of-the-art assemblers that can achieve relatively high assembly quality need excessive amount of computing resource (in particular, memory) that is not available to most researchers to achieve high quality results. Tiger provides the only known viable path to utilize NGS de novo assemblers that require more memory than that is present in available computers. Evaluation results demonstrate the feasibility of getting better quality results with low memory footprint and the scalability of using distributed commodity computers.",Assembly,"tiger tile iterative genome assembler
  cost reduction   nextgeneration sequence ngs technologies genomics  provide    unprecedented opportunity  understand fundamental question  biology  elucidate human diseases  novo genome assembly  one    important step  reconstruct  sequence genome however   novo assemblers require enormous amount  computational resource    accessible   research group  medical personnel   develop  novel  novo assembly framework call tiger  adapt  available compute resources  iteratively decompose  assembly problem  subproblems  method  also flexible  embed different assemblers  various type  target genomes use  sequence data   human chromosome  result show  tiger  achieve much better ng50s better genome coverage  slightly higher errors  compare  velvet  soapdenovo use modest amount  memory   available  commodity computers today  stateoftheart assemblers   achieve relatively high assembly quality need excessive amount  compute resource  particular memory    available   researchers  achieve high quality result tiger provide   know viable path  utilize ngs  novo assemblers  require  memory    present  available computers evaluation result demonstrate  feasibility  get better quality result  low memory footprint   scalability  use distribute commodity computers",5
134,PANDAseq,"PANDAseq: paired-end assembler for illumina sequences
Illumina paired-end reads are used to analyse microbial communities by targeting amplicons of the 16S rRNA gene. Publicly available tools are needed to assemble overlapping paired-end reads while correcting mismatches and uncalled bases; many errors could be corrected to obtain higher sequence yields using quality information. PANDAseq assembles paired-end reads rapidly and with the correction of most errors. Uncertain error corrections come from reads with many low-quality bases identified by upstream processing. Benchmarks were done using real error masks on simulated data, a pure source template, and a pooled template of genomic DNA from known organisms. PANDAseq assembled reads more rapidly and with reduced error incorporation compared to alternative methods. PANDAseq rapidly assembles sequences and scales to billions of paired-end reads. Assembly of control libraries showed a 4-50% increase in the number of assembled sequences over naïve assembly with negligible loss of ""good"" sequence.",Assembly,"pandaseq pairedend assembler  illumina sequences
illumina pairedend read  use  analyse microbial communities  target amplicons    rrna gene publicly available tool  need  assemble overlap pairedend read  correct mismatch  uncalled base many errors could  correct  obtain higher sequence yield use quality information pandaseq assemble pairedend read rapidly    correction   errors uncertain error corrections come  read  many lowquality base identify  upstream process benchmarks   use real error mask  simulate data  pure source template   pool template  genomic dna  know organisms pandaseq assemble read  rapidly   reduce error incorporation compare  alternative methods pandaseq rapidly assemble sequence  scale  billions  pairedend read assembly  control libraries show   increase   number  assemble sequence  naïve assembly  negligible loss  ""good"" sequence",5
135,ARACHNE,"ARACHNE: A Whole-Genome Shotgun Assembler
We describe a new computer system, calledARACHNE, for assembling genome sequence using paired-end whole-genome shotgun reads. ARACHNEhas several key features, including an efficient and sensitive procedure for finding read overlaps, a procedure for scoring overlaps that achieves high accuracy by correcting errors before assembly, read merger based on forward-reverse links, and detection of repeat contigs by forward-reverse link inconsistency. To testARACHNE, we created simulated reads providing ∼10-fold coverage of the genomes of H. influenzae, S. cerevisiae, and D. melanogaster, as well as human chromosomes 21 and 22. The assemblies of these simulated reads yielded nearly complete coverage of the respective genomes, with a small number of contigs joined into a smaller number of supercontigs (or scaffolds). For example, analysis of the D. melanogaster genome yielded ∼98% coverage with an N50 contig length of 324 kb and an N50 supercontig length of 5143 kb. The assembly accuracy was high, although not perfect: small errors occurred at a frequency of roughly 1 per 1 Mb (typically, deletion of ∼1 kb in size), with a very small number of other misassemblies. The assembly was rapid: the Drosophilaassembly required only 21 hours on a single 667 MHz processor and used 8.4 Gb of memory.",Assembly,"arachne  wholegenome shotgun assembler
 describe  new computer system calledarachne  assemble genome sequence use pairedend wholegenome shotgun read arachnehas several key feature include  efficient  sensitive procedure  find read overlap  procedure  score overlap  achieve high accuracy  correct errors  assembly read merger base  forwardreverse link  detection  repeat contigs  forwardreverse link inconsistency  testarachne  create simulate read provide fold coverage   genomes   influenzae  cerevisiae   melanogaster  well  human chromosomes     assemblies   simulate read yield nearly complete coverage   respective genomes   small number  contigs join   smaller number  supercontigs  scaffold  example analysis    melanogaster genome yield  coverage   n50 contig length      n50 supercontig length     assembly accuracy  high although  perfect small errors occur   frequency  roughly  per   typically deletion     size    small number   misassemblies  assembly  rapid  drosophilaassembly require   hours   single  mhz processor  use    memory",5
136,Minimus,"Minimus: a fast, lightweight genome assembler
Genome assemblers have grown very large and complex in response to the need for algorithms to handle the challenges of large whole-genome sequencing projects. Many of the most common uses of assemblers, however, are best served by a simpler type of assembler that requires fewer software components, uses less memory, and is far easier to install and run. We have developed the Minimus assembler to address these issues, and tested it on a range of assembly problems. We show that Minimus performs well on several small assembly tasks, including the assembly of viral genomes, individual genes, and BAC clones. In addition, we evaluate Minimus' performance in assembling bacterial genomes in order to assess its suitability as a component of a larger assembly pipeline. We show that, unlike other software currently used for these tasks, Minimus produces significantly fewer assembly errors, at the cost of generating a more fragmented assembly.",Assembly,"minimus  fast lightweight genome assembler
genome assemblers  grow  large  complex  response   need  algorithms  handle  challenge  large wholegenome sequence project many    common use  assemblers however  best serve   simpler type  assembler  require fewer software components use less memory   far easier  install  run   develop  minimus assembler  address  issue  test    range  assembly problems  show  minimus perform well  several small assembly task include  assembly  viral genomes individual genes  bac clone  addition  evaluate minimus' performance  assemble bacterial genomes  order  assess  suitability   component   larger assembly pipeline  show  unlike  software currently use   task minimus produce significantly fewer assembly errors   cost  generate   fragment assembly",5
137,SSPACE,"Scaffolding pre-assembled contigs using SSPACE.
De novo assembly tools play a main role in reconstructing genomes from next-generation sequencing (NGS) data and usually yield a number of contigs. Using paired-read sequencing data it is possible to assess the order, distance and orientation of contigs and combine them into so-called scaffolds. Although the latter process is a crucial step in finishing genomes, scaffolding algorithms are often built-in functions in de novo assembly tools and cannot be independently controlled. We here present a new tool, called SSPACE, which is a stand-alone scaffolder of pre-assembled contigs using paired-read data. Main features are: a short runtime, multiple library input of paired-end and/or mate pair datasets and possible contig extension with unmapped sequence reads. SSPACE shows promising results on both prokaryote and eukaryote genomic testsets where the amount of initial contigs was reduced by at least 75%.",Assembly,"scaffold preassemble contigs use sspace
 novo assembly tool play  main role  reconstruct genomes  nextgeneration sequence ngs data  usually yield  number  contigs use pairedread sequence data   possible  assess  order distance  orientation  contigs  combine   socalled scaffold although  latter process   crucial step  finish genomes scaffold algorithms  often builtin function   novo assembly tool  cannot  independently control   present  new tool call sspace    standalone scaffolder  preassemble contigs use pairedread data main feature   short runtime multiple library input  pairedend andor mate pair datasets  possible contig extension  unmapped sequence read sspace show promise result   prokaryote  eukaryote genomic testsets   amount  initial contigs  reduce   least ",5
138,SSPACE-LongRead,"SSPACE-LongRead: scaffolding bacterial draft genomes using long read sequence information
The recent introduction of the Pacific Biosciences RS single molecule sequencing technology has opened new doors to scaffolding genome assemblies in a cost-effective manner. The long read sequence information is promised to enhance the quality of incomplete and inaccurate draft assemblies constructed from Next Generation Sequencing (NGS) data. Here we propose a novel hybrid assembly methodology that aims to scaffold pre-assembled contigs in an iterative manner using PacBio RS long read information as a backbone. On a test set comprising six bacterial draft genomes, assembled using either a single Illumina MiSeq or Roche 454 library, we show that even a 50× coverage of uncorrected PacBio RS long reads is sufficient to drastically reduce the number of contigs. Comparisons to the AHA scaffolder indicate our strategy is better capable of producing (nearly) complete bacterial genomes. The current work describes our SSPACE-LongRead software which is designed to upgrade incomplete draft genomes using single molecule sequences. We conclude that the recent advances of the PacBio sequencing technology and chemistry, in combination with the limited computational resources required to run our program, allow to scaffold genomes in a fast and reliable manner.",Assembly,"sspacelongread scaffold bacterial draft genomes use long read sequence information
 recent introduction   pacific biosciences  single molecule sequence technology  open new doors  scaffold genome assemblies   costeffective manner  long read sequence information  promise  enhance  quality  incomplete  inaccurate draft assemblies construct  next generation sequence ngs data   propose  novel hybrid assembly methodology  aim  scaffold preassemble contigs   iterative manner use pacbio  long read information   backbone   test set comprise six bacterial draft genomes assemble use either  single illumina miseq  roche  library  show  even   coverage  uncorrected pacbio  long read  sufficient  drastically reduce  number  contigs comparisons   aha scaffolder indicate  strategy  better capable  produce nearly complete bacterial genomes  current work describe  sspacelongread software   design  upgrade incomplete draft genomes use single molecule sequence  conclude   recent advance   pacbio sequence technology  chemistry  combination   limit computational resources require  run  program allow  scaffold genomes   fast  reliable manner",5
139,ccTSA,"ccTSA: a coverage-centric threaded sequence assembler
De novo sequencing, a process to find the whole genome or the regions of a species without references, requires much higher computational power compared to mapped sequencing with references. The advent and continuous evolution of next-generation sequencing technologies further stress the demands of high-throughput processing of myriads of short DNA fragments. Recently announced sequence assemblers, such as Velvet, SOAPdenovo, and ABySS, all exploit parallelism to meet these computational demands since contemporary computer systems primarily rely on scaling the number of computing cores to improve performance. However, most of them are not tailored to exploit the full potential of these systems, leading to suboptimal performance. In this paper, we present ccTSA, a parallel sequence assembler that utilizes coverage to prune k-mers, find preferred edges, and resolve conflicts in preferred edges between k-mers. We minimize computation dependencies between threads to effectively parallelize k-mer processing. We also judiciously allocate and reuse memory space in order to lower memory usage and further improve sequencing speed. The results of ccTSA are compelling such that it runs several times faster than other assemblers while providing comparable quality values such as N50.",Assembly,"cctsa  coveragecentric thread sequence assembler
 novo sequence  process  find  whole genome   regions   species without reference require much higher computational power compare  map sequence  reference  advent  continuous evolution  nextgeneration sequence technologies  stress  demand  highthroughput process  myriads  short dna fragment recently announce sequence assemblers   velvet soapdenovo  aby  exploit parallelism  meet  computational demand since contemporary computer systems primarily rely  scale  number  compute core  improve performance however      tailor  exploit  full potential   systems lead  suboptimal performance   paper  present cctsa  parallel sequence assembler  utilize coverage  prune kmers find prefer edge  resolve conflict  prefer edge  kmers  minimize computation dependencies  thread  effectively parallelize kmer process  also judiciously allocate  reuse memory space  order  lower memory usage   improve sequence speed  result  cctsa  compel    run several time faster   assemblers  provide comparable quality value   n50",5
140,Readjoiner,"Readjoiner: a fast and memory efficient string graph-based sequence assembler
Ongoing improvements in throughput of the next-generation sequencing technologies challenge the current generation of de novo sequence assemblers. Most recent sequence assemblers are based on the construction of a de Bruijn graph. An alternative framework of growing interest is the assembly string graph, not necessitating a division of the reads into k-mers, but requiring fast algorithms for the computation of suffix-prefix matches among all pairs of reads. Here we present efficient methods for the construction of a string graph from a set of sequencing reads. Our approach employs suffix sorting and scanning methods to compute suffix-prefix matches. Transitive edges are recognized and eliminated early in the process and the graph is efficiently constructed including irreducible edges only. Our suffix-prefix match determination and string graph construction algorithms have been implemented in the software package Readjoiner. Comparison with existing string graph-based assemblers shows that Readjoiner is faster and more space efficient.",Assembly,"readjoiner  fast  memory efficient string graphbased sequence assembler
ongoing improvements  throughput   nextgeneration sequence technologies challenge  current generation   novo sequence assemblers  recent sequence assemblers  base   construction    bruijn graph  alternative framework  grow interest   assembly string graph  necessitate  division   read  kmers  require fast algorithms   computation  suffixprefix match among  pair  read   present efficient methods   construction   string graph   set  sequence read  approach employ suffix sort  scan methods  compute suffixprefix match transitive edge  recognize  eliminate early   process   graph  efficiently construct include irreducible edge   suffixprefix match determination  string graph construction algorithms   implement   software package readjoiner comparison  exist string graphbased assemblers show  readjoiner  faster   space efficient",5
141,Bracken,"Bracken: estimating species abundance in metagenomics data
Metagenomic experiments attempt to characterize microbial communities using high-throughput DNA sequencing. Identification of the microorganisms in a sample provides information about the genetic profile, population structure, and role of microorganisms within an environment. Until recently, most metagenomics studies focused on high-level characterization at the level of phyla, or alternatively sequenced the 16S ribosomal RNA gene that is present in bacterial species. As the cost of sequencing has fallen, though, metagenomics experiments have increasingly used unbiased shotgun sequencing to capture all the organisms in a sample. This approach requires a method for estimating abundance directly from the raw read data. Here we describe a fast, accurate new method that computes the abundance at the species level using the reads collected in a metagenomics experiment. Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN) uses the taxonomic assignments made by Kraken, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above. We demonstrate that Bracken can produce accurate species- and genus-level abundance estimates even when a sample contains multiple near-identical species.",AbundanceEstimation,"bracken estimate species abundance  metagenomics data
metagenomic experiment attempt  characterize microbial communities use highthroughput dna sequence identification   microorganisms   sample provide information   genetic profile population structure  role  microorganisms within  environment  recently  metagenomics study focus  highlevel characterization   level  phyla  alternatively sequence   ribosomal rna gene   present  bacterial species   cost  sequence  fall though metagenomics experiment  increasingly use unbiased shotgun sequence  capture   organisms   sample  approach require  method  estimate abundance directly   raw read data   describe  fast accurate new method  compute  abundance   species level use  read collect   metagenomics experiment bracken bayesian reestimation  abundance  classification  kraken use  taxonomic assignments make  kraken   fast readlevel classifier along  information   genomes   estimate abundance   species level  genus level    demonstrate  bracken  produce accurate species  genuslevel abundance estimate even   sample contain multiple nearidentical species",6
142,mOTUs2,"Microbial abundance, activity and population genomic profiling with mOTUs2
Metagenomic sequencing has greatly improved our ability to profile the composition of environmental and host-associated microbial communities. However, the dependency of most methods on reference genomes, which are currently unavailable for a substantial fraction of microbial species, introduces estimation biases. We present an updated and functionally extended tool based on universal (i.e., reference-independent), phylogenetic marker gene (MG)-based operational taxonomic units (mOTUs) enabling the profiling of >7700 microbial species. As more than 30% of them could not previously be quantified at this taxonomic resolution, relative abundance estimates based on mOTUs are more accurate compared to other methods. As a new feature, we show that mOTUs, which are based on essential housekeeping genes, are demonstrably well-suited for quantification of basal transcriptional activity of community members. Furthermore, single nucleotide variation profiles estimated using mOTUs reflect those from whole genomes, which allows for comparing microbial strain populations (e.g., across different human body sites).",AbundanceEstimation,"microbial abundance activity  population genomic profile  motus2
metagenomic sequence  greatly improve  ability  profile  composition  environmental  hostassociated microbial communities however  dependency   methods  reference genomes   currently unavailable   substantial fraction  microbial species introduce estimation bias  present  update  functionally extend tool base  universal  referenceindependent phylogenetic marker gene mgbased operational taxonomic units motus enable  profile   microbial species       could  previously  quantify   taxonomic resolution relative abundance estimate base  motus   accurate compare   methods   new feature  show  motus   base  essential housekeep genes  demonstrably wellsuited  quantification  basal transcriptional activity  community members furthermore single nucleotide variation profile estimate use motus reflect   whole genomes  allow  compare microbial strain populations  across different human body sit",6
143,DiTASiC,"Abundance estimation and differential testing on strain level in metagenomics data
Current metagenomics approaches allow analyzing the composition of microbial communities at high resolution. Important changes to the composition are known to even occur on strain level and to go hand in hand with changes in disease or ecological state. However, specific challenges arise for strain level analysis due to highly similar genome sequences present. Only a limited number of tools approach taxa abundance estimation beyond species level and there is a strong need for dedicated tools for strain resolution and differential abundance testing. We present DiTASiC (Differential Taxa Abundance including Similarity Correction) as a novel approach for quantification and differential assessment of individual taxa in metagenomics samples. We introduce a generalized linear model for the resolution of shared read counts which cause a significant bias on strain level. Further, we capture abundance estimation uncertainties, which play a crucial role in differential abundance analysis. A novel statistical framework is built, which integrates the abundance variance and infers abundance distributions for differential testing sensitive to strain level. As a result, we obtain highly accurate abundance estimates down to sub-strain level and enable fine-grained resolution of strain clusters. We demonstrate the relevance of read ambiguity resolution and integration of abundance uncertainties for differential analysis. Accurate detections of even small changes are achieved and false-positives are significantly reduced. Superior performance is shown on latest benchmark sets of various complexities and in comparison to existing methods.",AbundanceEstimation,"abundance estimation  differential test  strain level  metagenomics data
current metagenomics approach allow analyze  composition  microbial communities  high resolution important change   composition  know  even occur  strain level    hand  hand  change  disease  ecological state however specific challenge arise  strain level analysis due  highly similar genome sequence present   limit number  tool approach taxa abundance estimation beyond species level     strong need  dedicate tool  strain resolution  differential abundance test  present ditasic differential taxa abundance include similarity correction   novel approach  quantification  differential assessment  individual taxa  metagenomics sample  introduce  generalize linear model   resolution  share read count  cause  significant bias  strain level   capture abundance estimation uncertainties  play  crucial role  differential abundance analysis  novel statistical framework  build  integrate  abundance variance  infer abundance distributions  differential test sensitive  strain level   result  obtain highly accurate abundance estimate   substrain level  enable finegrained resolution  strain cluster  demonstrate  relevance  read ambiguity resolution  integration  abundance uncertainties  differential analysis accurate detections  even small change  achieve  falsepositives  significantly reduce superior performance  show  latest benchmark set  various complexities   comparison  exist methods",6
144,FastViromeExplorer,"FastViromeExplorer: a pipeline for virus and phage identification and abundance profiling in metagenomics data
With the increase in the availability of metagenomic data generated by next generation sequencing, there is an urgent need for fast and accurate tools for identifying viruses in host-associated and environmental samples. In this paper, we developed a stand-alone pipeline called FastViromeExplorer for the detection and abundance quantification of viruses and phages in large metagenomic datasets by performing rapid searches of virus and phage sequence databases. Both simulated and real data from human microbiome and ocean environmental samples are used to validate FastViromeExplorer as a reliable tool to quickly and accurately identify viruses and their abundances in large datasets.",AbundanceEstimation,"fastviromeexplorer  pipeline  virus  phage identification  abundance profile  metagenomics data
  increase   availability  metagenomic data generate  next generation sequence    urgent need  fast  accurate tool  identify viruses  hostassociated  environmental sample   paper  develop  standalone pipeline call fastviromeexplorer   detection  abundance quantification  viruses  phages  large metagenomic datasets  perform rapid search  virus  phage sequence databases  simulate  real data  human microbiome  ocean environmental sample  use  validate fastviromeexplorer   reliable tool  quickly  accurately identify viruses   abundances  large datasets",6
145,Kallisto,"Near-optimal probabilistic RNA-seq quantification
We present kallisto, an RNA-seq quantification program that is two orders of magnitude faster than previous approaches and achieves similar accuracy. Kallisto pseudoaligns reads to a reference, producing a list of transcripts that are compatible with each read while avoiding alignment of individual bases. We use kallisto to analyze 30 million unaligned paired-end RNA-seq reads in <10 min on a standard laptop computer. This removes a major computational bottleneck in RNA-seq analysis.",AbundanceEstimation,"nearoptimal probabilistic rnaseq quantification
 present kallisto  rnaseq quantification program   two order  magnitude faster  previous approach  achieve similar accuracy kallisto pseudoaligns read   reference produce  list  transcripts   compatible   read  avoid alignment  individual base  use kallisto  analyze  million unaligned pairedend rnaseq read   min   standard laptop computer  remove  major computational bottleneck  rnaseq analysis",6
146,GASiC,"Metagenomic abundance estimation and diagnostic testing on species level
One goal of sequencing-based metagenomic community analysis is the quantitative taxonomic assessment of microbial community compositions. In particular, relative quantification of taxons is of high relevance for metagenomic diagnostics or microbial community comparison. However, the majority of existing approaches quantify at low resolution (e.g. at phylum level), rely on the existence of special genes (e.g. 16S), or have severe problems discerning species with highly similar genome sequences. Yet, problems as metagenomic diagnostics require accurate quantification on species level. We developed Genome Abundance Similarity Correction (GASiC), a method to estimate true genome abundances via read alignment by considering reference genome similarities in a non-negative LASSO approach. We demonstrate GASiC’s superior performance over existing methods on simulated benchmark data as well as on real data. In addition, we present applications to datasets of both bacterial DNA and viral RNA source. We further discuss our approach as an alternative to PCR-based DNA quantification.",AbundanceEstimation,"metagenomic abundance estimation  diagnostic test  species level
one goal  sequencingbased metagenomic community analysis   quantitative taxonomic assessment  microbial community compositions  particular relative quantification  taxons   high relevance  metagenomic diagnostics  microbial community comparison however  majority  exist approach quantify  low resolution   phylum level rely   existence  special genes     severe problems discern species  highly similar genome sequence yet problems  metagenomic diagnostics require accurate quantification  species level  develop genome abundance similarity correction gasic  method  estimate true genome abundances via read alignment  consider reference genome similarities   nonnegative lasso approach  demonstrate gasics superior performance  exist methods  simulate benchmark data  well   real data  addition  present applications  datasets   bacterial dna  viral rna source   discuss  approach   alternative  pcrbased dna quantification",6
147,Salmon,"Salmon: Accurate, Versatile and Ultrafast Quantification from RNA-seq Data using Lightweight-Alignment
Transcript quantification is a central task in the analysis of RNA-seq data. Accurate computational methods for the quantification of transcript abundances are essential for downstream analysis. However, most existing approaches are much slower than is necessary for their degree of accuracy. We introduce Salmon, a novel method and software tool for transcript quantification that exhibits state-of-the-art accuracy while being significantly faster than most other tools. Salmon achieves this through the combined application of a two-phase inference procedure, a reduced data representation, and a novel lightweight read alignment algorithm.",AbundanceEstimation,"salmon accurate versatile  ultrafast quantification  rnaseq data use lightweightalignment
transcript quantification   central task   analysis  rnaseq data accurate computational methods   quantification  transcript abundances  essential  downstream analysis however  exist approach  much slower   necessary   degree  accuracy  introduce salmon  novel method  software tool  transcript quantification  exhibit stateoftheart accuracy   significantly faster    tool salmon achieve    combine application   twophase inference procedure  reduce data representation   novel lightweight read alignment algorithm",6
148,Sailfish,"Sailfish enables alignment-free isoform quantification from RNA-seq reads using lightweight algorithms
We introduce Sailfish, a computational method for quantifying the abundance of previously annotated RNA isoforms from RNA-seq data. Because Sailfish entirely avoids mapping reads, a time-consuming step in all current methods, it provides quantification estimates much faster than do existing approaches (typically 20 times faster) without loss of accuracy. By facilitating frequent reanalysis of data and reducing the need to optimize parameters, Sailfish exemplifies the potential of lightweight algorithms for efficiently processing sequencing reads.",AbundanceEstimation,"sailfish enable alignmentfree isoform quantification  rnaseq read use lightweight algorithms
 introduce sailfish  computational method  quantify  abundance  previously annotate rna isoforms  rnaseq data  sailfish entirely avoid map read  timeconsuming step   current methods  provide quantification estimate much faster   exist approach typically  time faster without loss  accuracy  facilitate frequent reanalysis  data  reduce  need  optimize parameters sailfish exemplify  potential  lightweight algorithms  efficiently process sequence read",6
149,ROCker,"ROCker: accurate detection and quantification of target genes in short-read metagenomic data sets by modeling sliding-window bitscores
Functional annotation of metagenomic and metatranscriptomic data sets relies on similarity searches based on e-value thresholds resulting in an unknown number of false positive and negative matches. To overcome these limitations, we introduce ROCker, aimed at identifying position-specific, most-discriminant thresholds in sliding windows along the sequence of a target protein, accounting for non-discriminative domains shared by unrelated proteins. ROCker employs the receiver operating characteristic (ROC) curve to minimize false discovery rate (FDR) and calculate the best thresholds based on how simulated shotgun metagenomic reads of known composition map onto well-curated reference protein sequences and thus, differs from HMM profiles and related methods. We showcase ROCker using ammonia monooxygenase (amoA) and nitrous oxide reductase (nosZ) genes, mediating oxidation of ammonia and the reduction of the potent greenhouse gas, N2O, to inert N2, respectively. ROCker typically showed 60-fold lower FDR when compared to the common practice of using fixed e-values. Previously uncounted ‘atypical’ nosZ genes were found to be two times more abundant, on average, than their typical counterparts in most soil metagenomes and the abundance of bacterial amoA was quantified against the highly-related particulate methane monooxygenase (pmoA). Therefore, ROCker can reliably detect and quantify target genes in short-read metagenomes.",AbundanceEstimation,"rocker accurate detection  quantification  target genes  shortread metagenomic data set  model slidingwindow bitscores
functional annotation  metagenomic  metatranscriptomic data set rely  similarity search base  evalue thresholds result   unknown number  false positive  negative match  overcome  limitations  introduce rocker aim  identify positionspecific mostdiscriminant thresholds  slide windows along  sequence   target protein account  nondiscriminative domains share  unrelated proteins rocker employ  receiver operate characteristic roc curve  minimize false discovery rate fdr  calculate  best thresholds base   simulate shotgun metagenomic read  know composition map onto wellcurated reference protein sequence  thus differ  hmm profile  relate methods  showcase rocker use ammonia monooxygenase amoa  nitrous oxide reductase nosz genes mediate oxidation  ammonia   reduction   potent greenhouse gas n2o  inert  respectively rocker typically show fold lower fdr  compare   common practice  use fix evalues previously uncounted atypical nosz genes  find   two time  abundant  average   typical counterparts   soil metagenomes   abundance  bacterial amoa  quantify   highlyrelated particulate methane monooxygenase pmoa therefore rocker  reliably detect  quantify target genes  shortread metagenomes",6
150,GAAS,"The GAAS metagenomic tool and its estimations of viral and microbial average genome size in four major biomes
Metagenomic studies characterize both the composition and diversity of uncultured viral and microbial communities. BLAST-based comparisons have typically been used for such analyses; however, sampling biases, high percentages of unknown sequences, and the use of arbitrary thresholds to find significant similarities can decrease the accuracy and validity of estimates. Here, we present Genome relative Abundance and Average Size (GAAS), a complete software package that provides improved estimates of community composition and average genome length for metagenomes in both textual and graphical formats. GAAS implements a novel methodology to control for sampling bias via length normalization, to adjust for multiple BLAST similarities by similarity weighting, and to select significant similarities using relative alignment lengths. In benchmark tests, the GAAS method was robust to both high percentages of unknown sequences and to variations in metagenomic sequence read lengths. Re-analysis of the Sargasso Sea virome using GAAS indicated that standard methodologies for metagenomic analysis may dramatically underestimate the abundance and importance of organisms with small genomes in environmental systems. Using GAAS, we conducted a meta-analysis of microbial and viral average genome lengths in over 150 metagenomes from four biomes to determine whether genome lengths vary consistently between and within biomes, and between microbial and viral communities from the same environment. Significant differences between biomes and within aquatic sub-biomes (oceans, hypersaline systems, freshwater, and microbialites) suggested that average genome length is a fundamental property of environments driven by factors at the sub-biome level. The behavior of paired viral and microbial metagenomes from the same environment indicated that microbial and viral average genome sizes are independent of each other, but indicative of community responses to stressors and environmental conditions.",AbundanceEstimation," gaas metagenomic tool   estimations  viral  microbial average genome size  four major biomes
metagenomic study characterize   composition  diversity  uncultured viral  microbial communities blastbased comparisons  typically  use   analyse however sample bias high percentages  unknown sequence   use  arbitrary thresholds  find significant similarities  decrease  accuracy  validity  estimate   present genome relative abundance  average size gaas  complete software package  provide improve estimate  community composition  average genome length  metagenomes   textual  graphical format gaas implement  novel methodology  control  sample bias via length normalization  adjust  multiple blast similarities  similarity weight   select significant similarities use relative alignment lengths  benchmark test  gaas method  robust   high percentages  unknown sequence   variations  metagenomic sequence read lengths reanalysis   sargasso sea virome use gaas indicate  standard methodologies  metagenomic analysis may dramatically underestimate  abundance  importance  organisms  small genomes  environmental systems use gaas  conduct  metaanalysis  microbial  viral average genome lengths    metagenomes  four biomes  determine whether genome lengths vary consistently   within biomes   microbial  viral communities    environment significant differences  biomes  within aquatic subbiomes oceans hypersaline systems freshwater  microbialites suggest  average genome length   fundamental property  environments drive  factor   subbiome level  behavior  pair viral  microbial metagenomes    environment indicate  microbial  viral average genome size  independent     indicative  community responses  stressors  environmental condition",6
151,GRAMMy,"Accurate genome relative abundance estimation based on shotgun metagenomic reads
Accurate estimation of microbial community composition based on metagenomic sequencing data is fundamental for subsequent metagenomics analysis. Prevalent estimation methods are mainly based on directly summarizing alignment results or its variants; often result in biased and/or unstable estimates. We have developed a unified probabilistic framework (named GRAMMy) by explicitly modeling read assignment ambiguities, genome size biases and read distributions along the genomes. Maximum likelihood method is employed to compute Genome Relative Abundance of microbial communities using the Mixture Model theory (GRAMMy). GRAMMy has been demonstrated to give estimates that are accurate and robust across both simulated and real read benchmark datasets. We applied GRAMMy to a collection of 34 metagenomic read sets from four metagenomics projects and identified 99 frequent species (minimally 0.5% abundant in at least 50% of the data- sets) in the human gut samples. Our results show substantial improvements over previous studies, such as adjusting the over-estimated abundance for Bacteroides species for human gut samples, by providing a new reference-based strategy for metagenomic sample comparisons. GRAMMy can be used flexibly with many read assignment tools (mapping, alignment or composition-based) even with low-sensitivity mapping results from huge short-read datasets. It will be increasingly useful as an accurate and robust tool for abundance estimation with the growing size of read sets and the expanding database of reference genomes.",AbundanceEstimation,"accurate genome relative abundance estimation base  shotgun metagenomic reads
accurate estimation  microbial community composition base  metagenomic sequence data  fundamental  subsequent metagenomics analysis prevalent estimation methods  mainly base  directly summarize alignment result   variants often result  bias andor unstable estimate   develop  unify probabilistic framework name grammy  explicitly model read assignment ambiguities genome size bias  read distributions along  genomes maximum likelihood method  employ  compute genome relative abundance  microbial communities use  mixture model theory grammy grammy   demonstrate  give estimate   accurate  robust across  simulate  real read benchmark datasets  apply grammy   collection   metagenomic read set  four metagenomics project  identify  frequent species minimally  abundant   least    data set   human gut sample  result show substantial improvements  previous study   adjust  overestimate abundance  bacteroides species  human gut sample  provide  new referencebased strategy  metagenomic sample comparisons grammy   use flexibly  many read assignment tool map alignment  compositionbased even  lowsensitivity map result  huge shortread datasets    increasingly useful   accurate  robust tool  abundance estimation   grow size  read set   expand database  reference genomes",6
152,MetaID,"MetaID: a novel method for identification and quantification of metagenomic samples
Advances in next-generation sequencing (NGS) technology has provided us with an opportunity to analyze and evaluate the rich microbial communities present in all natural environments. The shorter reads obtained from the shortgun technology has paved the way for determining the taxonomic profile of a community by simply aligning the reads against the available reference genomes. While several computational methods are available for taxonomic profiling at the genus- and species-level, none of these methods are effective at the strain-level identification due to the increasing difficulty in detecting variation at that level. Here, we present MetaID, an alignment-free n-gram based approach that can accurately identify microorganisms at the strain level and estimate the abundance of each organism in a sample, given a metagenomic sequencing dataset. MetaID is an n-gram based method that calculates the profile of unique and common n-grams from the dataset of 2,031 prokaryotic genomes and assigns weights to each n-gram using a scoring function. This scoring function assigns higher weightage to the n-grams that appear in fewer genomes and vice versa; thus, allows for effective use of both unique and common n-grams for species identification. Our 10-fold cross-validation results on a simulated dataset show a remarkable accuracy of 99.7% at the strain-level identification of the organisms in gut microbiome. We also demonstrated that our model shows impressive performance even by using only 25% or 50% of the genome sequences for modeling. In addition to identification of the species, our method can also estimate the relative abundance of each species in the simulated metagenomic samples. The generic approach employed in this method can be applied for accurate identification of a wide variety of microbial species (viruses, prokaryotes and eukaryotes) present in any environmental sample. The proposed scoring function and approach is able to accurately identify and estimate the entire taxa in any metagenomic community. The weights assigned to the common n-grams by our scoring function are precisely calibrated to match the reads up to the strain level. Our multipronged validation tests demonstrate that MetaID is sufficiently robust to accurately identify and estimate the abundance of each taxon in any natural environment even when using incomplete or partially sequenced genomes.",AbundanceEstimation,"metaid  novel method  identification  quantification  metagenomic samples
advances  nextgeneration sequence ngs technology  provide    opportunity  analyze  evaluate  rich microbial communities present   natural environments  shorter read obtain   shortgun technology  pave  way  determine  taxonomic profile   community  simply align  read   available reference genomes  several computational methods  available  taxonomic profile   genus  specieslevel none   methods  effective   strainlevel identification due   increase difficulty  detect variation   level   present metaid  alignmentfree ngram base approach   accurately identify microorganisms   strain level  estimate  abundance   organism   sample give  metagenomic sequence dataset metaid   ngram base method  calculate  profile  unique  common ngrams   dataset   prokaryotic genomes  assign weight   ngram use  score function  score function assign higher weightage   ngrams  appear  fewer genomes  vice versa thus allow  effective use   unique  common ngrams  species identification  fold crossvalidation result   simulate dataset show  remarkable accuracy     strainlevel identification   organisms  gut microbiome  also demonstrate   model show impressive performance even  use       genome sequence  model  addition  identification   species  method  also estimate  relative abundance   species   simulate metagenomic sample  generic approach employ   method   apply  accurate identification   wide variety  microbial species viruses prokaryotes  eukaryotes present   environmental sample  propose score function  approach  able  accurately identify  estimate  entire taxa   metagenomic community  weight assign   common ngrams   score function  precisely calibrate  match  read    strain level  multipronged validation test demonstrate  metaid  sufficiently robust  accurately identify  estimate  abundance   taxon   natural environment even  use incomplete  partially sequence genomes",6
153,STAMP,"STAMP: statistical analysis of taxonomic and functional profiles
STAMP is a graphical software package that provides statistical hypothesis tests and exploratory plots for analysing taxonomic and functional profiles. It supports tests for comparing pairs of samples or samples organized into two or more treatment groups. Effect sizes and confidence intervals are provided to allow critical assessment of the biological relevancy of test results. A user-friendly graphical interface permits easy exploration of statistical results and generation of publication-quality plots.",AbundanceEstimation,"stamp statistical analysis  taxonomic  functional profiles
stamp   graphical software package  provide statistical hypothesis test  exploratory plot  analyse taxonomic  functional profile  support test  compare pair  sample  sample organize  two   treatment group effect size  confidence intervals  provide  allow critical assessment   biological relevancy  test result  userfriendly graphical interface permit easy exploration  statistical result  generation  publicationquality plot",6
154,imGLAD,"imGLAD: accurate detection and quantification of target organisms in metagenomes
Accurate detection of target microbial species in metagenomic datasets from environmental samples remains limited because the limit of detection of current methods is typically inaccessible and the frequency of false-positives, resulting from inadequate identification of regions of the genome that are either too highly conserved to be diagnostic (e.g., rRNA genes) or prone to frequent horizontal genetic exchange (e.g., mobile elements) remains unknown. To overcome these limitations, we introduce imGLAD, which aims to detect (target) genomic sequences in metagenomic datasets. imGLAD achieves high accuracy because it uses the sequence-discrete population concept for discriminating between metagenomic reads originating from the target organism compared to reads from co-occurring close relatives, masks regions of the genome that are not informative using the MyTaxa engine, and models both the sequencing breadth and depth to determine relative abundance and limit of detection. We validated imGLAD by analyzing metagenomic datasets derived from spinach leaves inoculated with the enteric pathogen Escherichia coli O157:H7 and showed that its limit of detection can be comparable to that of PCR-based approaches for these samples.",AbundanceEstimation,"imglad accurate detection  quantification  target organisms  metagenomes
accurate detection  target microbial species  metagenomic datasets  environmental sample remain limit   limit  detection  current methods  typically inaccessible   frequency  falsepositives result  inadequate identification  regions   genome   either  highly conserve   diagnostic  rrna genes  prone  frequent horizontal genetic exchange  mobile elements remain unknown  overcome  limitations  introduce imglad  aim  detect target genomic sequence  metagenomic datasets imglad achieve high accuracy   use  sequencediscrete population concept  discriminate  metagenomic read originate   target organism compare  read  cooccurring close relatives mask regions   genome    informative use  mytaxa engine  model   sequence breadth  depth  determine relative abundance  limit  detection  validate imglad  analyze metagenomic datasets derive  spinach leave inoculate   enteric pathogen escherichia coli o157h7  show   limit  detection   comparable    pcrbased approach   sample",6
155,MetaPhlAn2,"MetaPhlAn2 for enhanced metagenomic taxonomic profiling
MetaPhlAn (metagenomic phylogenetic analysis)1 is a method for characterizing the taxonomic profiles of whole-metagenome shotgun (WMS) samples that has been used successfully in large-scale microbial community studies2,3. This work complements the original species-level profiling method with a system for eukaryotic and viral quantitation, strain-level identification and strain tracking. These and other extensions make the MetaPhlAn2 computational package (http://segatalab.cibio.unitn.it/tools/metaphlan2/ and Supplementary Software)MetaPhlAn (metagenomic phylogenetic analysis)1 is a method for characterizing the taxonomic profiles of whole-metagenome shotgun (WMS) samples that has been used successfully in large-scale microbial community studies2,3. This work complements the original species-level profiling method with a system for eukaryotic and viral quantitation, strain-level identification and strain tracking. These and other extensions make the MetaPhlAn2 computational package an efficient tool for mining WMS samples.an efficient tool for mining WMS samples. Our method infers the presence and read coverage of clade-specific markers to unequivocally detect the taxonomic clades present in a microbiome sample and estimate their relative abundance1. MetaPhlAn2 includes an expanded set of ∼1 million markers (184 ± 45 for each bacterial species) from >7,500 species, based on the approximately tenfold increase in the number of sequenced genomes in the past 2 years. Subspecies markers enable strain-level analyses, and quasi-markers improve accuracy and allow the detection of viruses and eukaryotic microbes",AbundanceEstimation,"metaphlan2  enhance metagenomic taxonomic profiling
metaphlan metagenomic phylogenetic analysis   method  characterize  taxonomic profile  wholemetagenome shotgun wms sample    use successfully  largescale microbial community studies2  work complement  original specieslevel profile method   system  eukaryotic  viral quantitation strainlevel identification  strain track    extensions make  metaphlan2 computational package   supplementary softwaremetaphlan metagenomic phylogenetic analysis   method  characterize  taxonomic profile  wholemetagenome shotgun wms sample    use successfully  largescale microbial community studies2  work complement  original specieslevel profile method   system  eukaryotic  viral quantitation strainlevel identification  strain track    extensions make  metaphlan2 computational package  efficient tool  mine wms samplesan efficient tool  mine wms sample  method infer  presence  read coverage  cladespecific markers  unequivocally detect  taxonomic clades present   microbiome sample  estimate  relative abundance1 metaphlan2 include  expand set   million markers  ±    bacterial species   species base   approximately tenfold increase   number  sequence genomes   past  years subspecies markers enable strainlevel analyse  quasimarkers improve accuracy  allow  detection  viruses  eukaryotic microbes",6
156,MetaPhyler,"MetaPhyler: Taxonomic profiling for metagenomic sequences
A major goal of metagenomics is to characterize the microbial diversity of an environment. The most popular approach relies on 16S rRNA sequencing, however this approach can generate biased estimates due to differences in the copy number of the 16S rRNA gene between even closely related organisms, and due to PCR artifacts. The taxonomic composition can also be determined from whole-metagenome sequencing data by matching individual sequences against a database of reference genes. One major limitation of prior methods used for this purpose is the use of a universal classification threshold for all genes at all taxonomic levels. We propose that better classification results can be obtained by tuning the taxonomic classifier to each matching length, reference gene, and taxonomic level. We present a novel taxonomic profiler MetaPhyler, which uses marker genes as a taxonomic reference. Results on simulated datasets demonstrate that MetaPhyler outperforms other tools commonly used in this context (CARMA, Megan and PhymmBL). We also present interesting results obtained by applying MetaPhyler to a real metagenomic dataset.",AbundanceEstimation,"metaphyler taxonomic profile  metagenomic sequences
 major goal  metagenomics   characterize  microbial diversity   environment   popular approach rely   rrna sequence however  approach  generate bias estimate due  differences   copy number    rrna gene  even closely relate organisms  due  pcr artifacts  taxonomic composition  also  determine  wholemetagenome sequence data  match individual sequence   database  reference genes one major limitation  prior methods use   purpose   use   universal classification threshold   genes   taxonomic level  propose  better classification result   obtain  tune  taxonomic classifier   match length reference gene  taxonomic level  present  novel taxonomic profiler metaphyler  use marker genes   taxonomic reference result  simulate datasets demonstrate  metaphyler outperform  tool commonly use   context carma megan  phymmbl  also present interest result obtain  apply metaphyler   real metagenomic dataset",6
157,TAxyPro,"Protein signature-based estimation of metagenomic abundances including all domains of life and viruses
Metagenome analysis requires tools that can estimate the taxonomic abundances in anonymous sequence data over the whole range of biological entities. Because there is usually no prior knowledge about the data composition, not only all domains of life but also viruses have to be included in taxonomic profiling. Such a full-range approach, however, is difficult to realize owing to the limited coverage of available reference data. In particular, archaea and viruses are generally not well represented by current genome databases. We introduce a novel approach to taxonomic profiling of metagenomes that is based on mixture model analysis of protein signatures. Our results on simulated and real data reveal the difficulties of the existing methods when measuring achaeal or viral abundances and show the overall good profiling performance of the protein-based mixture model. As an application example, we provide a large-scale analysis of data from the Human Microbiome Project. This demonstrates the utility of our method as a first instance profiling tool for a fast estimate of the community structure.",AbundanceEstimation,"protein signaturebased estimation  metagenomic abundances include  domains  life  viruses
metagenome analysis require tool   estimate  taxonomic abundances  anonymous sequence data   whole range  biological entities    usually  prior knowledge   data composition    domains  life  also viruses    include  taxonomic profile   fullrange approach however  difficult  realize owe   limit coverage  available reference data  particular archaea  viruses  generally  well represent  current genome databases  introduce  novel approach  taxonomic profile  metagenomes   base  mixture model analysis  protein signatures  result  simulate  real data reveal  difficulties   exist methods  measure achaeal  viral abundances  show  overall good profile performance   proteinbased mixture model   application example  provide  largescale analysis  data   human microbiome project  demonstrate  utility   method   first instance profile tool   fast estimate   community structure",6
158,Trimmomatic,"Trimmomatic: a flexible trimmer for Illumina sequence data
Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested.",Trimming,"trimmomatic  flexible trimmer  illumina sequence data
although many nextgeneration sequence ngs read preprocessing tool already exist  could  find  tool  combination  tool  meet  requirements  term  flexibility correct handle  pairedend data  high performance   develop trimmomatic    flexible  efficient preprocessing tool  could correctly handle pairedend data  value  ngs read preprocessing  demonstrate   referencebased  referencefree task trimmomatic  show  produce output    least competitive    many case superior   produce   tool   scenarios test",7
159,SeqPurge,"SeqPurge: highly-sensitive adapter trimming for paired-end NGS data
Trimming of adapter sequences from short read data is a common preprocessing step during NGS data analysis. When performing paired-end sequencing, the overlap between forward and reverse read can be used to identify excess adapter sequences. This is exploited by several previously published adapter trimming tools. However, our evaluation on amplicon-based data shows that most of the current tools are not able to remove all adapter sequences and that adapter contamination may even lead to spurious variant calls. Here we present SeqPurge (https://github.com/imgag/ngs-bits), a highly-sensitive adapter trimmer that uses a probabilistic approach to detect the overlap between forward and reverse reads of Illumina sequencing data. SeqPurge can detect very short adapter sequences, even if only one base long. Compared to other adapter trimmers specifically designed for paired-end data, we found that SeqPurge achieves a higher sensitivity. The number of remaining adapter bases after trimming is reduced by up to 90 %, depending on the compared tool. In simulations with different error rates, we found that SeqPurge is also the most error-tolerant adapter trimmer in the comparison. SeqPurge achieves a very high sensitivity and a high error-tolerance, combined with a specificity and runtime that are comparable to other state-of-the-art adapter trimmers. The very good adapter trimming performance, complemented with additional features such as quality-based trimming and basic quality control, makes SeqPurge an excellent choice for the pre-processing of paired-end NGS data.",Trimming,"seqpurge highlysensitive adapter trim  pairedend ngs data
trimming  adapter sequence  short read data   common preprocessing step  ngs data analysis  perform pairedend sequence  overlap  forward  reverse read   use  identify excess adapter sequence   exploit  several previously publish adapter trim tool however  evaluation  ampliconbased data show     current tool   able  remove  adapter sequence   adapter contamination may even lead  spurious variant call   present seqpurge   highlysensitive adapter trimmer  use  probabilistic approach  detect  overlap  forward  reverse read  illumina sequence data seqpurge  detect  short adapter sequence even   one base long compare   adapter trimmers specifically design  pairedend data  find  seqpurge achieve  higher sensitivity  number  remain adapter base  trim  reduce      depend   compare tool  simulations  different error rat  find  seqpurge  also   errortolerant adapter trimmer   comparison seqpurge achieve   high sensitivity   high errortolerance combine   specificity  runtime   comparable   stateoftheart adapter trimmers   good adapter trim performance complement  additional feature   qualitybased trim  basic quality control make seqpurge  excellent choice   preprocessing  pairedend ngs data",7
160,Skewer,"Skewer: a fast and accurate adapter trimmer for next-generation sequencing paired-end reads
Adapter trimming is a prerequisite step for analyzing next-generation sequencing (NGS) data when the reads are longer than the target DNA/RNA fragments. Although typically used in small RNA sequencing, adapter trimming is also used widely in other applications, such as genome DNA sequencing and transcriptome RNA/cDNA sequencing, where fragments shorter than a read are sometimes obtained because of the limitations of NGS protocols. For the newly emerged Nextera long mate-pair (LMP) protocol, junction adapters are located in the middle of all properly constructed fragments; hence, adapter trimming is essential to gain the correct paired reads. However, our investigations have shown that few adapter trimming tools meet both efficiency and accuracy requirements simultaneously. The performances of these tools can be even worse for paired-end and/or mate-pair sequencing. To improve the efficiency of adapter trimming, we devised a novel algorithm, the bit-masked k-difference matching algorithm, which has O(k n) expected time with O(m) space, where k is the maximum number of differences allowed, n is the read length, and m is the adapter length. This algorithm makes it possible to fully enumerate all candidates that meet a specified threshold, e.g. error ratio, within a short period of time. To improve the accuracy of this algorithm, we designed a simple and easy-to-explain statistical scoring scheme to evaluate candidates in the pattern matching step. We also devised scoring schemes to fully exploit the paired-end/mate-pair information when it is applicable. All these features have been implemented in an industry-standard tool named Skewer (https://sourceforge.net/projects/skewer). Experiments on simulated data, real data of small RNA sequencing, paired-end RNA sequencing, and Nextera LMP sequencing showed that Skewer outperforms all other similar tools that have the same utility. Further, Skewer is considerably faster than other tools that have comparative accuracies; namely, one times faster for single-end sequencing, more than 12 times faster for paired-end sequencing, and 49% faster for LMP sequencing. Skewer achieved as yet unmatched accuracies for adapter trimming with low time bound.",Trimming,"skewer  fast  accurate adapter trimmer  nextgeneration sequence pairedend reads
adapter trim   prerequisite step  analyze nextgeneration sequence ngs data   read  longer   target dnarna fragment although typically use  small rna sequence adapter trim  also use widely   applications   genome dna sequence  transcriptome rnacdna sequence  fragment shorter   read  sometimes obtain    limitations  ngs protocols   newly emerge nextera long matepair lmp protocol junction adapters  locate   middle   properly construct fragment hence adapter trim  essential  gain  correct pair read however  investigations  show   adapter trim tool meet  efficiency  accuracy requirements simultaneously  performances   tool   even worse  pairedend andor matepair sequence  improve  efficiency  adapter trim  devise  novel algorithm  bitmasked kdifference match algorithm     expect time   space     maximum number  differences allow    read length     adapter length  algorithm make  possible  fully enumerate  candidates  meet  specify threshold  error ratio within  short period  time  improve  accuracy   algorithm  design  simple  easytoexplain statistical score scheme  evaluate candidates   pattern match step  also devise score scheme  fully exploit  pairedendmatepair information    applicable   feature   implement   industrystandard tool name skewer  experiment  simulate data real data  small rna sequence pairedend rna sequence  nextera lmp sequence show  skewer outperform   similar tool     utility  skewer  considerably faster   tool   comparative accuracies namely one time faster  singleend sequence    time faster  pairedend sequence   faster  lmp sequence skewer achieve  yet unmatched accuracies  adapter trim  low time bind",7
161,Ktrim,"Ktrim: an extra-fast and accurate adapter- and quality-trimmer for sequencing data
Next-generation sequencing (NGS) data frequently suffer from poor-quality cycles and adapter contaminations therefore need to be preprocessed before downstream analyses. With the ever-growing throughput and read length of modern sequencers, the preprocessing step turns to be a bottleneck in data analysis due to unmet performance of current tools. Extra-fast and accurate adapter- and quality-trimming tools for sequencing data preprocessing are therefore still of urgent demand. Ktrim was developed in this work. Key features of Ktrim include: built-in support to adapters of common library preparation kits; supports user-supplied, customized adapter sequences; supports both paired-end and single-end data; supports parallelization to accelerate the analysis. Ktrim was ∼2–18 times faster than current tools and also showed high accuracy when applied on the testing datasets. Ktrim could thus serve as a valuable and efficient tool for short-read NGS data preprocessing.",Trimming,"ktrim  extrafast  accurate adapter  qualitytrimmer  sequence data
nextgeneration sequence ngs data frequently suffer  poorquality cycle  adapter contaminations therefore need   preprocessed  downstream analyse   evergrowing throughput  read length  modern sequencers  preprocessing step turn    bottleneck  data analysis due  unmet performance  current tool extrafast  accurate adapter  qualitytrimming tool  sequence data preprocessing  therefore still  urgent demand ktrim  develop   work key feature  ktrim include builtin support  adapters  common library preparation kit support usersupplied customize adapter sequence support  pairedend  singleend data support parallelization  accelerate  analysis ktrim   time faster  current tool  also show high accuracy  apply   test datasets ktrim could thus serve   valuable  efficient tool  shortread ngs data preprocessing",7
162,ConDetri-a,"ConDeTri--a content dependent read trimmer for Illumina data
During the last few years, DNA and RNA sequencing have started to play an increasingly important role in biological and medical applications, especially due to the greater amount of sequencing data yielded from the new sequencing machines and the enormous decrease in sequencing costs. Particularly, Illumina/Solexa sequencing has had an increasing impact on gathering data from model and non-model organisms. However, accurate and easy to use tools for quality filtering have not yet been established. We present ConDeTri, a method for content dependent read trimming for next generation sequencing data using quality scores of each individual base. The main focus of the method is to remove sequencing errors from reads so that sequencing reads can be standardized. Another aspect of the method is to incorporate read trimming in next-generation sequencing data processing and analysis pipelines. It can process single-end and paired-end sequence data of arbitrary length and it is independent from sequencing coverage and user interaction. ConDeTri is able to trim and remove reads with low quality scores to save computational time and memory usage during de novo assemblies. Low coverage or large genome sequencing projects will especially gain from trimming reads. The method can easily be incorporated into preprocessing and analysis pipelines for Illumina data.",Trimming,"condetria content dependent read trimmer  illumina data
  last  years dna  rna sequence  start  play  increasingly important role  biological  medical applications especially due   greater amount  sequence data yield   new sequence machine   enormous decrease  sequence cost particularly illuminasolexa sequence    increase impact  gather data  model  nonmodel organisms however accurate  easy  use tool  quality filter   yet  establish  present condetri  method  content dependent read trim  next generation sequence data use quality score   individual base  main focus   method   remove sequence errors  read   sequence read   standardize another aspect   method   incorporate read trim  nextgeneration sequence data process  analysis pipelines   process singleend  pairedend sequence data  arbitrary length    independent  sequence coverage  user interaction condetri  able  trim  remove read  low quality score  save computational time  memory usage   novo assemblies low coverage  large genome sequence project  especially gain  trim read  method  easily  incorporate  preprocessing  analysis pipelines  illumina data",7
163,InfoTrim,"InfoTrim: A DNA read quality trimmer using entropy
Biological DNA reads are often trimmed before mapping, genome assembly, and other tasks to improve the quality of the results. Biological sequence complexity relates to alignment quality as low complexity regions can align poorly. There are many read trimmers, but many do not use sequence complexity for trimming. Alignment of reads generated from whole genome bisulfite sequencing is especially challenging since bisulfite treated reads tend to reduce sequence complexity. InfoTrim, a new read trimmer, was created to explore these issues. It is evaluated against five other trimmers using four read mappers on real and simulated bisulfite treated DNA data. InfoTrim has reasonable results.",Trimming,"infotrim  dna read quality trimmer use entropy
biological dna read  often trim  map genome assembly   task  improve  quality   result biological sequence complexity relate  alignment quality  low complexity regions  align poorly   many read trimmers  many   use sequence complexity  trim alignment  read generate  whole genome bisulfite sequence  especially challenge since bisulfite treat read tend  reduce sequence complexity infotrim  new read trimmer  create  explore  issue   evaluate  five  trimmers use four read mappers  real  simulate bisulfite treat dna data infotrim  reasonable result",7
164,CutAdapt,"Cutadapt Removes Adapter Sequences From High-Throughput Sequencing Reads
When small RNA is sequenced on current sequencing machines, the resulting reads are usually longer than the RNA and therefore contain parts of the 3' adapter. That adapter must be found and removed error-tolerantly from each read before read mapping. Previous solutions are either hard to use or do not offer required features, in particular support for color space data. As an easy to use alternative, we developed the command-line tool cutadapt, which supports 454, Illumina and SOLiD (color space) data, offers two adapter trimming algorithms, and has other useful features.",Trimming,"cutadapt remove adapter sequence  highthroughput sequence reads
 small rna  sequence  current sequence machine  result read  usually longer   rna  therefore contain part   ' adapter  adapter must  find  remove errortolerantly   read  read map previous solutions  either hard  use    offer require feature  particular support  color space data   easy  use alternative  develop  commandline tool cutadapt  support  illumina  solid color space data offer two adapter trim algorithms    useful feature",7
165,AlienTrimmer,"AlienTrimmer: A tool to quickly and accurately trim off multiple short contaminant sequences from high-throughput sequencing reads
Contaminant oligonucleotide sequences such as primers and adapters can occur in both ends of high-throughput sequencing (HTS) reads. AlienTrimmer was developed in order to detect and remove such contaminants. Based on the decomposition of specified alien nucleotide sequences into k-mers, AlienTrimmer is able to determine whether such alien k-mers are occurring in one or in both read ends by using a simple polynomial algorithm. Therefore, AlienTrimmer can process typical HTS single- or paired-end files with millions of reads in several minutes with very low computer resources. Based on the analysis of both simulated and real-case Illumina®, 454™ and Ion Torrent™ read data, we show that AlienTrimmer performs with excellent accuracy and speed in comparison with other trimming tools.",Trimming,"alientrimmer  tool  quickly  accurately trim  multiple short contaminant sequence  highthroughput sequence reads
contaminant oligonucleotide sequence   primers  adapters  occur   end  highthroughput sequence hts read alientrimmer  develop  order  detect  remove  contaminants base   decomposition  specify alien nucleotide sequence  kmers alientrimmer  able  determine whether  alien kmers  occur  one    read end  use  simple polynomial algorithm therefore alientrimmer  process typical hts single  pairedend file  millions  read  several minutes   low computer resources base   analysis   simulate  realcase illumina® ™  ion torrent™ read data  show  alientrimmer perform  excellent accuracy  speed  comparison   trim tool",7
166,PE-Trimmer,"An Efficient Trimming Algorithm based on Multi-Feature Fusion Scoring Model for NGS Data
Next-generation sequencing (NGS) has enabled an exponential growth rate of sequencing data. However, several sequence artifacts, including error reads (base calling errors and small insertions or deletions) and poor quality reads, which can impose significant impact on the downstream sequence processing and analysis. Here, we present PE-Trimmer, a sensitive and special trimming algorithm for NGS sequence. First, PE-Trimmer removes technical sequences in paired-end reads based on the characteristics of low quality reads in NGS data. Second, PE-Trimmer determines the range of reads that need to be trimmed according to the quality score statistics histogram of reads in the library. To improve the accuracy of this algorithm, we design a light-weight and easy-to-explain scoring model to evaluate candidates in the pattern of trimming step. Finally, PE-Trimmer selects the appropriate trimming strategy to process the low quality reads based on the location determined by the scoring model. PE-Trimmer is able to locate and remove adapter residues from the paired-end reads. It is easily configurable and offers superior throughput in the multi-threaded mode. We test PE-Trimmer on five datasets, and compare it with the current five latest methods. The experimental results demonstrate that PE-Trimmer produces more superior results, compared with other trimmers.",Trimming," efficient trim algorithm base  multifeature fusion score model  ngs data
nextgeneration sequence ngs  enable  exponential growth rate  sequence data however several sequence artifacts include error read base call errors  small insertions  deletions  poor quality read   impose significant impact   downstream sequence process  analysis   present petrimmer  sensitive  special trim algorithm  ngs sequence first petrimmer remove technical sequence  pairedend read base   characteristics  low quality read  ngs data second petrimmer determine  range  read  need   trim accord   quality score statistics histogram  read   library  improve  accuracy   algorithm  design  lightweight  easytoexplain score model  evaluate candidates   pattern  trim step finally petrimmer select  appropriate trim strategy  process  low quality read base   location determine   score model petrimmer  able  locate  remove adapter residues   pairedend read   easily configurable  offer superior throughput   multithreaded mode  test petrimmer  five datasets  compare    current five latest methods  experimental result demonstrate  petrimmer produce  superior result compare   trimmers",7
167,cutPrimers,"cutPrimers: A New Tool for Accurate Cutting of Primers from Reads of Targeted Next Generation Sequencing
Cutting of primers from reads is an important step of processing targeted amplicon-based next generation sequencing data. Existing tools are adapted for cutting of one or several primer/adapter sequences from reads and removing all of their occurrences. Also most of the existing tools use kmers and may cut only part of primers or primers with studied sequence of gene. Because of this, use of such programs leads to incorrect trimming, reduction of coverage, and increase in the number of false-positive and/or false-negative results. We have developed a new tool named cutPrimers for accurate cutting of any number of primers from reads. Using sequencing reads that were obtained during study of BRCA1/2 genes, we compared it with cutadapt, AlienTrimmer, and BBDuk. All of them trimmed reads in such a way that coverage of at least two amplicons decreased to unacceptable level (<30 reads) comparing with reads trimmed with cutPrimers. At the same time, Trimmomatic and AlienTrimmer cut all occurrences of primer sequences, so the length of the remaining reads was less than prospective.",Trimming,"cutprimers  new tool  accurate cut  primers  read  target next generation sequencing
cutting  primers  read   important step  process target ampliconbased next generation sequence data exist tool  adapt  cut  one  several primeradapter sequence  read  remove    occurrences also    exist tool use kmers  may cut  part  primers  primers  study sequence  gene    use   program lead  incorrect trim reduction  coverage  increase   number  falsepositive andor falsenegative result   develop  new tool name cutprimers  accurate cut   number  primers  read use sequence read   obtain  study  brca1 genes  compare   cutadapt alientrimmer  bbduk    trim read    way  coverage   least two amplicons decrease  unacceptable level  read compare  read trim  cutprimers    time trimmomatic  alientrimmer cut  occurrences  primer sequence   length   remain read  less  prospective",7
168,AfterQC,"AfterQC: automatic filtering, trimming, error removing and quality control for fastq data
Some applications, especially those clinical applications requiring high accuracy of sequencing data, usually have to face the troubles caused by unavoidable sequencing errors. Several tools have been proposed to profile the sequencing quality, but few of them can quantify or correct the sequencing errors. This unmet requirement motivated us to develop AfterQC, a tool with functions to profile sequencing errors and correct most of them, plus highly automated quality control and data filtering features. Different from most tools, AfterQC analyses the overlapping of paired sequences for pair-end sequencing data. Based on overlapping analysis, AfterQC can detect and cut adapters, and furthermore it gives a novel function to correct wrong bases in the overlapping regions. Another new feature is to detect and visualise sequencing bubbles, which can be commonly found on the flowcell lanes and may raise sequencing errors. Besides normal per cycle quality and base content plotting, AfterQC also provides features like polyX (a long sub-sequence of a same base X) filtering, automatic trimming and K-MER based strand bias profiling. For each single or pair of FastQ files, AfterQC filters out bad reads, detects and eliminates sequencer’s bubble effects, trims reads at front and tail, detects the sequencing errors and corrects part of them, and finally outputs clean data and generates HTML reports with interactive figures. AfterQC can run in batch mode with multiprocess support, it can run with a single FastQ file, a single pair of FastQ files (for pair-end sequencing), or a folder for all included FastQ files to be processed automatically. Based on overlapping analysis, AfterQC can estimate the sequencing error rate and profile the error transform distribution. The results of our error profiling tests show that the error distribution is highly platform dependent. Much more than just another new quality control (QC) tool, AfterQC is able to perform quality control, data filtering, error profiling and base correction automatically. Experimental results show that AfterQC can help to eliminate the sequencing errors for pair-end sequencing data to provide much cleaner outputs, and consequently help to reduce the false-positive variants, especially for the low-frequency somatic mutations. While providing rich configurable options, AfterQC can detect and set all the options automatically and require no argument in most cases.",QualityControl,"afterqc automatic filter trim error remove  quality control  fastq data
 applications especially  clinical applications require high accuracy  sequence data usually   face  trouble cause  unavoidable sequence errors several tool   propose  profile  sequence quality      quantify  correct  sequence errors  unmet requirement motivate   develop afterqc  tool  function  profile sequence errors  correct    plus highly automate quality control  data filter feature different   tool afterqc analyse  overlap  pair sequence  pairend sequence data base  overlap analysis afterqc  detect  cut adapters  furthermore  give  novel function  correct wrong base   overlap regions another new feature   detect  visualise sequence bubble    commonly find   flowcell lanes  may raise sequence errors besides normal per cycle quality  base content plot afterqc also provide feature like polyx  long subsequence    base  filter automatic trim  kmer base strand bias profile   single  pair  fastq file afterqc filter  bad read detect  eliminate sequencers bubble effect trim read  front  tail detect  sequence errors  correct part    finally output clean data  generate html report  interactive figure afterqc  run  batch mode  multiprocess support   run   single fastq file  single pair  fastq file  pairend sequence   folder   include fastq file   process automatically base  overlap analysis afterqc  estimate  sequence error rate  profile  error transform distribution  result   error profile test show   error distribution  highly platform dependent much    another new quality control  tool afterqc  able  perform quality control data filter error profile  base correction automatically experimental result show  afterqc  help  eliminate  sequence errors  pairend sequence data  provide much cleaner output  consequently help  reduce  falsepositive variants especially   lowfrequency somatic mutations  provide rich configurable options afterqc  detect  set   options automatically  require  argument   case",8
169,NGSQC,"NGS QC Toolkit: a toolkit for quality control of next generation sequencing data.
Next generation sequencing (NGS) technologies provide a high-throughput means to generate large amount of sequence data. However, quality control (QC) of sequence data generated from these technologies is extremely important for meaningful downstream analysis. Further, highly efficient and fast processing tools are required to handle the large volume of datasets. Here, we have developed an application, NGS QC Toolkit, for quality check and filtering of high-quality data. This toolkit is a standalone and open source application freely available at http://www.nipgr.res.in/ngsqctoolkit.html. All the tools in the application have been implemented in Perl programming language. The toolkit is comprised of user-friendly tools for QC of sequencing data generated using Roche 454 and Illumina platforms, and additional tools to aid QC (sequence format converter and trimming tools) and analysis (statistics tools). A variety of options have been provided to facilitate the QC at user-defined parameters. The toolkit is expected to be very useful for the QC of NGS data to facilitate better downstream analysis.",QualityControl,"ngs  toolkit  toolkit  quality control  next generation sequence data
next generation sequence ngs technologies provide  highthroughput mean  generate large amount  sequence data however quality control   sequence data generate   technologies  extremely important  meaningful downstream analysis  highly efficient  fast process tool  require  handle  large volume  datasets    develop  application ngs  toolkit  quality check  filter  highquality data  toolkit   standalone  open source application freely available     tool   application   implement  perl program language  toolkit  comprise  userfriendly tool    sequence data generate use roche   illumina platforms  additional tool  aid  sequence format converter  trim tool  analysis statistics tool  variety  options   provide  facilitate    userdefined parameters  toolkit  expect    useful     ngs data  facilitate better downstream analysis",8
170,QC-Chain,"QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data
Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7–8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis.",QualityControl,"qcchain fast  holistic quality control method  nextgeneration sequence data
nextgeneration sequence ngs technologies   widely use  life sciences however several kinds  sequence artifacts include lowquality read  contaminate read  find   quite common  raw sequence data  compromise downstream analysis therefore quality control   essential  raw ngs data however although   ngs data quality control tool  publicly available   two limitations first  process speed could  cope   rapid increase  large data volume second  respect  remove  contaminate read none   could identify contaminate source  novo   rely heavily  prior information   contaminate species   usually  available  advance   report qcchain  fast accurate  holistic ngs data qualitycontrol method  tool synergeticly comprise  userfriendly tool   quality assessment  trim  raw read use parallelqc  fast read process tool  identification quantification  filtration  unknown contamination  get highquality clean read   optimize base  parallel computation   process speed  significantly higher    methods experiment  simulate  real ngs data  show  read  low sequence quality could  identify  filter possible contaminate source could  identify  quantify  novo accurately  quickly comparison  raw read  process read also show  subsequent analyse genome assembly gene prediction gene annotation etc result base  process read improve significantly  completeness  accuracy  regard  process speed qcchain achieve  time speedup base  parallel computation  compare  traditional methods therefore qcchain   fast  useful quality control tool  read quality process   novo contamination filtration  ngs read  could significantly facilitate downstream analysis",8
171,SAMStat,"SAMStat: monitoring biases in next generation sequencing data
The sequence alignment/map format (SAM) is a commonly used format to store the alignments between millions of short reads and a reference genome. Often certain positions within the reads are inherently more likely to contain errors due to the protocols used to prepare the samples. Such biases can have adverse effects on both mapping rate and accuracy. To understand the relationship between potential protocol biases and poor mapping we wrote SAMstat, a simple C program plotting nucleotide overrepresentation and other statistics in mapped and unmapped reads in a concise html page. Collecting such statistics also makes it easy to highlight problems in the data processing and enables non-experts to track data quality over time. We demonstrate that studying sequence features in mapped data can be used to identify biases particular to one sequencing protocol. Once identified, such biases can be considered in the downstream analysis or even be removed by read trimming or filtering techniques.",QualityControl,"samstat monitor bias  next generation sequence data
 sequence alignmentmap format sam   commonly use format  store  alignments  millions  short read   reference genome often certain position within  read  inherently  likely  contain errors due   protocols use  prepare  sample  bias   adverse effect   map rate  accuracy  understand  relationship  potential protocol bias  poor map  write samstat  simple  program plot nucleotide overrepresentation   statistics  map  unmapped read   concise html page collect  statistics also make  easy  highlight problems   data process  enable nonexperts  track data quality  time  demonstrate  study sequence feature  map data   use  identify bias particular  one sequence protocol  identify  bias   consider   downstream analysis  even  remove  read trim  filter techniques",8
172,ClinQC,"ClinQC: a tool for quality control and cleaning of Sanger and NGS data in clinical research
Traditional Sanger sequencing has been used as a gold standard method for genetic testing in clinic to perform single gene test, which has been a cumbersome and expensive method to test several genes in heterogeneous disease such as cancer. With the advent of Next Generation Sequencing technologies, which produce data on unprecedented speed in a cost effective manner have overcome the limitation of Sanger sequencing. Therefore, for the efficient and affordable genetic testing, Next Generation Sequencing has been used as a complementary method with Sanger sequencing for disease causing mutation identification and confirmation in clinical research. However, in order to identify the potential disease causing mutations with great sensitivity and specificity it is essential to ensure high quality sequencing data. Therefore, integrated software tools are lacking which can analyze Sanger and NGS data together and eliminate platform specific sequencing errors, low quality reads and support the analysis of several sample/patients data set in a single run. We have developed ClinQC, a flexible and user-friendly pipeline for format conversion, quality control, trimming and filtering of raw sequencing data generated from Sanger sequencing and three NGS sequencing platforms including Illumina, 454 and Ion Torrent. First, ClinQC convert input read files from their native formats to a common FASTQ format and remove adapters, and PCR primers. Next, it split bar-coded samples, filter duplicates, contamination and low quality sequences and generates a QC report. ClinQC output high quality reads in FASTQ format with Sanger quality encoding, which can be directly used in down-stream analysis. It can analyze hundreds of sample/patients data in a single run and generate unified output files for both Sanger and NGS sequencing data. Our tool is expected to be very useful for quality control and format conversion of Sanger and NGS data to facilitate improved downstream analysis and mutation screening.",QualityControl,"clinqc  tool  quality control  clean  sanger  ngs data  clinical research
traditional sanger sequence   use   gold standard method  genetic test  clinic  perform single gene test     cumbersome  expensive method  test several genes  heterogeneous disease   cancer   advent  next generation sequence technologies  produce data  unprecedented speed   cost effective manner  overcome  limitation  sanger sequence therefore   efficient  affordable genetic test next generation sequence   use   complementary method  sanger sequence  disease cause mutation identification  confirmation  clinical research however  order  identify  potential disease cause mutations  great sensitivity  specificity   essential  ensure high quality sequence data therefore integrate software tool  lack   analyze sanger  ngs data together  eliminate platform specific sequence errors low quality read  support  analysis  several samplepatients data set   single run   develop clinqc  flexible  userfriendly pipeline  format conversion quality control trim  filter  raw sequence data generate  sanger sequence  three ngs sequence platforms include illumina   ion torrent first clinqc convert input read file   native format   common fastq format  remove adapters  pcr primers next  split barcoded sample filter duplicate contamination  low quality sequence  generate   report clinqc output high quality read  fastq format  sanger quality encode    directly use  downstream analysis   analyze hundreds  samplepatients data   single run  generate unify output file   sanger  ngs sequence data  tool  expect    useful  quality control  format conversion  sanger  ngs data  facilitate improve downstream analysis  mutation screen",8
173,UrQt,"UrQt: an efficient software for the Unsupervised Quality trimming of NGS data.
Quality control is a necessary step of any Next Generation Sequencing analysis. Although customary, this step still requires manual interventions to empirically choose tuning parameters according to various quality statistics. Moreover, current quality control procedures that provide a “good quality” data set, are not optimal and discard many informative nucleotides. To address these drawbacks, we present a new quality control method, implemented in UrQt software, for Unsupervised Quality trimming of Next Generation Sequencing reads. Our trimming procedure relies on a well-defined probabilistic framework to detect the best segmentation between two segments of unreliable nucleotides, framing a segment of informative nucleotides. Our software only requires one user-friendly parameter to define the minimal quality threshold (phred score) to consider a nucleotide to be informative, which is independent of both the experiment and the quality of the data. This procedure is implemented in C++ in an efficient and parallelized software with a low memory footprint. We tested the performances of UrQt compared to the best-known trimming programs, on seven RNA and DNA sequencing experiments and demonstrated its optimality in the resulting tradeoff between the number of trimmed nucleotides and the quality objective. By finding the best segmentation to delimit a segment of good quality nucleotides, UrQt greatly increases the number of reads and of nucleotides that can be retained for a given quality objective.",QualityControl,"urqt  efficient software   unsupervised quality trim  ngs data
quality control   necessary step   next generation sequence analysis although customary  step still require manual interventions  empirically choose tune parameters accord  various quality statistics moreover current quality control procedures  provide  “good quality” data set   optimal  discard many informative nucleotides  address  drawbacks  present  new quality control method implement  urqt software  unsupervised quality trim  next generation sequence read  trim procedure rely   welldefined probabilistic framework  detect  best segmentation  two segment  unreliable nucleotides frame  segment  informative nucleotides  software  require one userfriendly parameter  define  minimal quality threshold phred score  consider  nucleotide   informative   independent    experiment   quality   data  procedure  implement     efficient  parallelize software   low memory footprint  test  performances  urqt compare   bestknown trim program  seven rna  dna sequence experiment  demonstrate  optimality   result tradeoff   number  trim nucleotides   quality objective  find  best segmentation  delimit  segment  good quality nucleotides urqt greatly increase  number  read   nucleotides    retain   give quality objective",8
174,FQC,"FQC Dashboard: integrates FastQC results into a web-based, interactive, and extensible FASTQ quality control tool
FQC is software that facilitates quality control of FASTQ files by carrying out a QC protocol using FastQC, parsing results, and aggregating quality metrics into an interactive dashboard designed to richly summarize individual sequencing runs. The dashboard groups samples in dropdowns for navigation among the data sets, utilizes human-readable configuration files to manipulate the pages and tabs, and is extensible with CSV data.",QualityControl,"fqc dashboard integrate fastqc result   webbased interactive  extensible fastq quality control tool
fqc  software  facilitate quality control  fastq file  carry    protocol use fastqc parse result  aggregate quality metrics   interactive dashboard design  richly summarize individual sequence run  dashboard group sample  dropdowns  navigation among  data set utilize humanreadable configuration file  manipulate  page  tabs   extensible  csv data",8
175,KAT,"KAT: a K-mer analysis toolkit to quality control NGS datasets and genome assemblies 
De novo assembly of whole genome shotgun (WGS) next-generation sequencing (NGS) data benefits from high-quality input with high coverage. However, in practice, determining the quality and quantity of useful reads quickly and in a reference-free manner is not trivial. Gaining a better understanding of the WGS data, and how that data is utilized by assemblers, provides useful insights that can inform the assembly process and result in better assemblies.  We present the K-mer Analysis Toolkit (KAT): a multi-purpose software toolkit for reference-free quality control (QC) of WGS reads and de novo genome assemblies, primarily via their k-mer frequencies and GC composition. KAT enables users to assess levels of errors, bias and contamination at various stages of the assembly process. In this paper we highlight KAT’s ability to provide valuable insights into assembly composition and quality of genome assemblies through pairwise comparison of k-mers present in both input reads and the assemblies",QualityControl,"kat  kmer analysis toolkit  quality control ngs datasets  genome assemblies 
 novo assembly  whole genome shotgun wgs nextgeneration sequence ngs data benefit  highquality input  high coverage however  practice determine  quality  quantity  useful read quickly    referencefree manner   trivial gain  better understand   wgs data    data  utilize  assemblers provide useful insights   inform  assembly process  result  better assemblies   present  kmer analysis toolkit kat  multipurpose software toolkit  referencefree quality control   wgs read   novo genome assemblies primarily via  kmer frequencies   composition kat enable users  assess level  errors bias  contamination  various stag   assembly process   paper  highlight kats ability  provide valuable insights  assembly composition  quality  genome assemblies  pairwise comparison  kmers present   input read   assemblies",8
176,NGS-QC generator,"NGS-QC Generator: A Quality Control System for ChIP-Seq and Related Deep Sequencing-Generated Datasets
The combination of massive parallel sequencing with a variety of modern DNA/RNA enrichment technologies provides means for interrogating functional protein–genome interactions (ChIP-seq), genome-wide transcriptional activity (RNA-seq; GRO-seq), chromatin accessibility (DNase-seq, FAIRE-seq, MNase-seq), and more recently the three-dimensional organization of chromatin (Hi-C, ChIA-PET). In systems biology-based approaches several of these readouts are generally cumulated with the aim of describing living systems through a reconstitution of the genome-regulatory functions. However, an issue that is often underestimated is that conclusions drawn from such multidimensional analyses of NGS-derived datasets critically depend on the quality of the compared datasets. To address this problem, we have developed the NGS-QC Generator, a quality control system that infers quality descriptors for any kind of ChIP-sequencing and related datasets. In this chapter we provide a detailed protocol for (1) assessing quality descriptors with the NGS-QC Generator; (2) to interpret the generated reports; and (3) to explore the database of QC indicators (www.ngs-qc.org) for >21,000 publicly available datasets.",QualityControl,"ngsqc generator  quality control system  chipseq  relate deep sequencinggenerated datasets
 combination  massive parallel sequence   variety  modern dnarna enrichment technologies provide mean  interrogate functional proteingenome interactions chipseq genomewide transcriptional activity rnaseq groseq chromatin accessibility dnaseseq faireseq mnaseseq   recently  threedimensional organization  chromatin hic chiapet  systems biologybased approach several   readouts  generally cumulate   aim  describe live systems   reconstitution   genomeregulatory function however  issue   often underestimate   conclusions draw   multidimensional analyse  ngsderived datasets critically depend   quality   compare datasets  address  problem   develop  ngsqc generator  quality control system  infer quality descriptors   kind  chipsequencing  relate datasets   chapter  provide  detail protocol   assess quality descriptors   ngsqc generator   interpret  generate report    explore  database   indicators wwwngsqcorg   publicly available datasets",8
177,EasyQC,"EasyQC: Tool with Interactive User Interface for Efficient Next-Generation Sequencing Data Quality Control
The advent of next-generation sequencing (NGS) technologies has revolutionized the world of genomic research. Millions of sequences are generated in a short period of time and they provide intriguing insights to the researcher. Many NGS platforms have evolved over a period of time and their efficiency has been ever increasing. Still, primarily because of the chemistry, glitch in the sequencing machine and human handling errors, some artifacts tend to exist in the final sequence data set. These sequence errors have a profound impact on the downstream analyses and may provide misleading information. Hence, filtering of these erroneous reads has become inevitable and myriad of tools are available for this purpose. However, many of them are accessible as a command line interface that requires the user to enter each command manually. Here, we report EasyQC, a tool for NGS data quality control (QC) with a graphical user interface providing options to carry out trimming of NGS reads based on quality, length, homopolymer, and ambiguous bases. EasyQC also possesses features such as format converter, paired end merger, adapter trimmer, and a graph generator that generates quality distribution, length distribution, GC content, and base composition graphs. Comparison of raw and processed sequence data sets using EasyQC suggested significant increase in overall quality of the sequences. Testing of EasyQC using NGS data sets on a standalone desktop proved to be relatively faster. EasyQC is developed using PERL modules and can be executed in Windows and Linux platforms. With the various QC features, easy interface for end users, and cross-platform compatibility, EasyQC would be a valuable addition to the already existing tools facilitating better downstream analyses.",QualityControl,"easyqc tool  interactive user interface  efficient nextgeneration sequence data quality control
 advent  nextgeneration sequence ngs technologies  revolutionize  world  genomic research millions  sequence  generate   short period  time   provide intrigue insights   researcher many ngs platforms  evolve   period  time   efficiency   ever increase still primarily    chemistry glitch   sequence machine  human handle errors  artifacts tend  exist   final sequence data set  sequence errors   profound impact   downstream analyse  may provide mislead information hence filter   erroneous read  become inevitable  myriad  tool  available   purpose however many    accessible   command line interface  require  user  enter  command manually   report easyqc  tool  ngs data quality control    graphical user interface provide options  carry  trim  ngs read base  quality length homopolymer  ambiguous base easyqc also possess feature   format converter pair end merger adapter trimmer   graph generator  generate quality distribution length distribution  content  base composition graph comparison  raw  process sequence data set use easyqc suggest significant increase  overall quality   sequence test  easyqc use ngs data set   standalone desktop prove   relatively faster easyqc  develop use perl modules    execute  windows  linux platforms   various  feature easy interface  end users  crossplatform compatibility easyqc would   valuable addition   already exist tool facilitate better downstream analyse",8
178,FastQ Screen,"FastQ Screen: A tool for multi-genome mapping and quality control
DNA sequencing analysis typically involves mapping reads to just one reference genome. Mapping against multiple genomes is necessary, however, when the genome of origin requires confirmation. Mapping against multiple genomes is also advisable for detecting contamination or for identifying sample swaps which, if left undetected, may lead to incorrect experimental conclusions. Consequently, we present FastQ Screen, a tool to validate the origin of DNA samples by quantifying the proportion of reads that map to a panel of reference genomes. FastQ Screen is intended to be used routinely as a quality control measure and for analysing samples in which the origin of the DNA is uncertain or has multiple sources.",QualityControl,"fastq screen  tool  multigenome map  quality control
dna sequence analysis typically involve map read   one reference genome map  multiple genomes  necessary however   genome  origin require confirmation map  multiple genomes  also advisable  detect contamination   identify sample swap   leave undetected may lead  incorrect experimental conclusions consequently  present fastq screen  tool  validate  origin  dna sample  quantify  proportion  read  map   panel  reference genomes fastq screen  intend   use routinely   quality control measure   analyse sample    origin   dna  uncertain   multiple source",8
179,antiSMASH,"antiSMASH: rapid identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genome sequences
Bacterial and fungal secondary metabolism is a rich source of novel bioactive compounds with potential pharmaceutical applications as antibiotics, anti-tumor drugs or cholesterol-lowering drugs. To find new drug candidates, microbiologists are increasingly relying on sequencing genomes of a wide variety of microbes. However, rapidly and reliably pinpointing all the potential gene clusters for secondary metabolites in dozens of newly sequenced genomes has been extremely challenging, due to their biochemical heterogeneity, the presence of unknown enzymes and the dispersed nature of the necessary specialized bioinformatics tools and resources. Here, we present antiSMASH (antibiotics & Secondary Metabolite Analysis Shell), the first comprehensive pipeline capable of identifying biosynthetic loci covering the whole range of known secondary metabolite compound classes (polyketides, non-ribosomal peptides, terpenes, aminoglycosides, aminocoumarins, indolocarbazoles, lantibiotics, bacteriocins, nucleosides, beta-lactams, butyrolactones, siderophores, melanins and others). It aligns the identified regions at the gene cluster level to their nearest relatives from a database containing all other known gene clusters, and integrates or cross-links all previously available secondary-metabolite specific gene analysis methods in one interactive view. antiSMASH is available at http://antismash.secondarymetabolites.org .",Annotation,"antismash rapid identification annotation  analysis  secondary metabolite biosynthesis gene cluster  bacterial  fungal genome sequences
bacterial  fungal secondary metabolism   rich source  novel bioactive compound  potential pharmaceutical applications  antibiotics antitumor drug  cholesterollowering drug  find new drug candidates microbiologists  increasingly rely  sequence genomes   wide variety  microbes however rapidly  reliably pinpoint   potential gene cluster  secondary metabolites  dozens  newly sequence genomes   extremely challenge due   biochemical heterogeneity  presence  unknown enzymes   disperse nature   necessary specialize bioinformatics tool  resources   present antismash antibiotics  secondary metabolite analysis shell  first comprehensive pipeline capable  identify biosynthetic loci cover  whole range  know secondary metabolite compound class polyketides nonribosomal peptides terpenes aminoglycosides aminocoumarins indolocarbazoles lantibiotics bacteriocins nucleosides betalactams butyrolactones siderophores melanins  others  align  identify regions   gene cluster level   nearest relatives   database contain   know gene cluster  integrate  crosslinks  previously available secondarymetabolite specific gene analysis methods  one interactive view antismash  available   ",9
180,CSN,"CSN: unsupervised approach for inferring biological networks based on the genome alone
Most organisms cannot be cultivated, as they live in unique ecological conditions that cannot be mimicked in the lab. Understanding the functionality of those organisms’ genes and their interactions by performing large-scale measurements of transcription levels, protein-protein interactions or metabolism, is extremely difficult and, in some cases, impossible. Thus, efficient algorithms for deciphering genome functionality based only on the genomic sequences with no other experimental measurements are needed. In this study, we describe a novel algorithm that infers gene networks that we name Common Substring Network (CSN). The algorithm enables inferring novel regulatory relations among genes based only on the genomic sequence of a given organism and partial homolog/ortholog-based functional annotation. It can specifically infer the functional annotation of genes with unknown homology. This approach is based on the assumption that related genes, not necessarily homologs, tend to share sub-sequences, which may be related to common regulatory mechanisms, similar functionality of encoded proteins, common evolutionary history, and more. We demonstrate that CSNs, which are based on S. cerevisiae and E. coli genomes, have properties similar to ‘traditional’ biological networks inferred from experiments. Highly expressed genes tend to have higher degree nodes in the CSN, genes with similar protein functionality tend to be closer, and the CSN graph exhibits a powerlaw degree distribution. Also, we show how the CSN can be used for predicting gene interactions and functions. The reported results suggest that ‘silent’ code inside the transcript can help to predict central features of biological networks and gene function. This approach can help researchers to understand the genome of novel microorganisms, analyze metagenomic data, and can help to decipher new gene functions",Annotation,"csn unsupervised approach  infer biological network base   genome alone
 organisms cannot  cultivate   live  unique ecological condition  cannot  mimic   lab understand  functionality   organisms genes   interactions  perform largescale measurements  transcription level proteinprotein interactions  metabolism  extremely difficult    case impossible thus efficient algorithms  decipher genome functionality base    genomic sequence    experimental measurements  need   study  describe  novel algorithm  infer gene network   name common substring network csn  algorithm enable infer novel regulatory relations among genes base    genomic sequence   give organism  partial homologorthologbased functional annotation   specifically infer  functional annotation  genes  unknown homology  approach  base   assumption  relate genes  necessarily homologs tend  share subsequences  may  relate  common regulatory mechanisms similar functionality  encode proteins common evolutionary history    demonstrate  csns   base   cerevisiae   coli genomes  properties similar  traditional biological network infer  experiment highly express genes tend   higher degree nod   csn genes  similar protein functionality tend   closer   csn graph exhibit  powerlaw degree distribution also  show   csn   use  predict gene interactions  function  report result suggest  silent code inside  transcript  help  predict central feature  biological network  gene function  approach  help researchers  understand  genome  novel microorganisms analyze metagenomic data   help  decipher new gene function",9
181,CAVA,"CSN and CAVA: variant annotation tools for rapid, robust next-generation sequencing analysis in the clinical setting
Next-generation sequencing (NGS) offers unprecedented opportunities to expand clinical genomics. It also presents challenges with respect to integration with data from other sequencing methods and historical data. Provision of consistent, clinically applicable variant annotation of NGS data has proved difficult, particularly of indels, an important variant class in clinical genomics. Annotation in relation to a reference genome sequence, the DNA strand of coding transcripts and potential alternative variant representations has not been well addressed. Here we present tools that address these challenges to provide rapid, standardized, clinically appropriate annotation of NGS data in line with existing clinical standards. We developed a clinical sequencing nomenclature (CSN), a fixed variant annotation consistent with the principles of the Human Genome Variation Society (HGVS) guidelines, optimized for automated variant annotation of NGS data. To deliver high-throughput CSN annotation we created CAVA (Clinical Annotation of VAriants), a fast, lightweight tool designed for easy incorporation into NGS pipelines. CAVA allows transcript specification, appropriately accommodates the strand of a gene transcript and flags variants with alternative annotations to facilitate clinical interpretation and comparison with other datasets. We evaluated CAVA in exome data and a clinical BRCA1/BRCA2 gene testing pipeline. CAVA generated CSN calls for 10,313,034 variants in the ExAC database in 13.44 hours, and annotated the ICR1000 exome series in 6.5 hours. Evaluation of 731 different indels from a single individual revealed 92 % had alternative representations in left aligned and right aligned data. Annotation of left aligned data, as performed by many annotation tools, would thus give clinically discrepant annotation for the 339 (46 %) indels in genes transcribed from the forward DNA strand. By contrast, CAVA provides the correct clinical annotation for all indels. CAVA also flagged the 370 indels with alternative representations of a different functional class, which may profoundly influence clinical interpretation. CAVA annotation of 50 BRCA1/BRCA2 gene mutations from a clinical pipeline gave 100 % concordance with Sanger data; only 8/25 BRCA2 mutations were correctly clinically annotated by other tools. CAVA is a freely available tool that provides rapid, robust, high-throughput clinical annotation of NGS data, using a standardized clinical sequencing nomenclature.",Annotation,"csn  cava variant annotation tool  rapid robust nextgeneration sequence analysis   clinical setting
nextgeneration sequence ngs offer unprecedented opportunities  expand clinical genomics  also present challenge  respect  integration  data   sequence methods  historical data provision  consistent clinically applicable variant annotation  ngs data  prove difficult particularly  indels  important variant class  clinical genomics annotation  relation   reference genome sequence  dna strand  cod transcripts  potential alternative variant representations    well address   present tool  address  challenge  provide rapid standardize clinically appropriate annotation  ngs data  line  exist clinical standards  develop  clinical sequence nomenclature csn  fix variant annotation consistent   principles   human genome variation society hgvs guidelines optimize  automate variant annotation  ngs data  deliver highthroughput csn annotation  create cava clinical annotation  variants  fast lightweight tool design  easy incorporation  ngs pipelines cava allow transcript specification appropriately accommodate  strand   gene transcript  flag variants  alternative annotations  facilitate clinical interpretation  comparison   datasets  evaluate cava  exome data   clinical brca1brca2 gene test pipeline cava generate csn call   variants   exac database   hours  annotate  icr1000 exome series   hours evaluation   different indels   single individual reveal    alternative representations  leave align  right align data annotation  leave align data  perform  many annotation tool would thus give clinically discrepant annotation      indels  genes transcribe   forward dna strand  contrast cava provide  correct clinical annotation   indels cava also flag   indels  alternative representations   different functional class  may profoundly influence clinical interpretation cava annotation   brca1brca2 gene mutations   clinical pipeline give   concordance  sanger data   brca2 mutations  correctly clinically annotate   tool cava   freely available tool  provide rapid robust highthroughput clinical annotation  ngs data use  standardize clinical sequence nomenclature",9
182,VariantDB,"VariantDB: a flexible annotation and filtering portal for next generation sequencing data
Interpretation of the multitude of variants obtained from next generation sequencing (NGS) is labor intensive and complex. Web-based interfaces such as Galaxy streamline the generation of variant lists but lack flexibility in the downstream annotation and filtering that are necessary to identify causative variants in medical genomics. To this end, we built VariantDB, a web-based interactive annotation and filtering platform that automatically annotates variants with allele frequencies, functional impact, pathogenicity predictions and pathway information. VariantDB allows filtering by all annotations, under dominant, recessive or de novo inheritance models",Annotation,"variantdb  flexible annotation  filter portal  next generation sequence data
interpretation   multitude  variants obtain  next generation sequence ngs  labor intensive  complex webbased interfaces   galaxy streamline  generation  variant list  lack flexibility   downstream annotation  filter   necessary  identify causative variants  medical genomics   end  build variantdb  webbased interactive annotation  filter platform  automatically annotate variants  allele frequencies functional impact pathogenicity predictions  pathway information variantdb allow filter   annotations  dominant recessive   novo inheritance model",9
183,MGRASt,"MG-RAST, a Metagenomics Service for Analysis of Microbial Community Structure and Function
Approaches in molecular biology, particularly those that deal with high-throughput sequencing of entire microbial communities (the field of metagenomics), are rapidly advancing our understanding of the composition and functional content of microbial communities involved in climate change, environmental pollution, human health, biotechnology, etc. Metagenomics provides researchers with the most complete picture of the taxonomic (i.e., what organisms are there) and functional (i.e., what are those organisms doing) composition of natively sampled microbial communities, making it possible to perform investigations that include organisms that were previously intractable to laboratory-controlled culturing; currently, these constitute the vast majority of all microbes on the planet. All organisms contained in environmental samples are sequenced in a culture-independent manner, most often with 16S ribosomal amplicon methods to investigate the taxonomic or whole-genome shotgun-based methods to investigate the functional content of sampled communities. Metagenomics allows researchers to characterize the community composition and functional content of microbial communities, but it cannot show which functional processes are active; however, near parallel developments in transcriptomics promise a dramatic increase in our knowledge in this area as well. Since 2008, MG-RAST (Meyer et al., BMC Bioinformatics 9:386, 2008) has served as a public resource for annotation and analysis of metagenomic sequence data, providing a repository that currently houses more than 150,000 data sets (containing 60+ tera-base-pairs) with more than 23,000 publically available. MG-RAST, or the metagenomics RAST (rapid annotation using subsystems technology) server makes it possible for users to upload raw metagenomic sequence data in (preferably) fastq or fasta format. Assessments of sequence quality, annotation with respect to multiple reference databases, are performed automatically with minimal input from the user (see Subheading 4 at the end of this chapter for more details). Post-annotation analysis and visualization are also possible, directly through the web interface, or with tools like matR (metagenomic analysis tools for R, covered later in this chapter) that utilize the MG-RAST API (http://api.metagenomics.anl.gov/api.html) to easily download data from any stage in the MG-RAST processing pipeline. Over the years, MG-RAST has undergone substantial revisions to keep pace with the dramatic growth in the number, size, and types of sequence data that accompany constantly evolving developments in metagenomics and related -omic sciences (e.g., metatranscriptomics).",Annotation,"mgrast  metagenomics service  analysis  microbial community structure  function
approaches  molecular biology particularly   deal  highthroughput sequence  entire microbial communities  field  metagenomics  rapidly advance  understand   composition  functional content  microbial communities involve  climate change environmental pollution human health biotechnology etc metagenomics provide researchers    complete picture   taxonomic   organisms    functional     organisms  composition  natively sample microbial communities make  possible  perform investigations  include organisms   previously intractable  laboratorycontrolled culture currently  constitute  vast majority   microbes   planet  organisms contain  environmental sample  sequence   cultureindependent manner  often   ribosomal amplicon methods  investigate  taxonomic  wholegenome shotgunbased methods  investigate  functional content  sample communities metagenomics allow researchers  characterize  community composition  functional content  microbial communities   cannot show  functional process  active however near parallel developments  transcriptomics promise  dramatic increase   knowledge   area  well since  mgrast meyer   bmc bioinformatics    serve   public resource  annotation  analysis  metagenomic sequence data provide  repository  currently house    data set contain  terabasepairs     publically available mgrast   metagenomics rast rapid annotation use subsystems technology server make  possible  users  upload raw metagenomic sequence data  preferably fastq  fasta format assessments  sequence quality annotation  respect  multiple reference databases  perform automatically  minimal input   user see subheading    end   chapter   detail postannotation analysis  visualization  also possible directly   web interface   tool like matr metagenomic analysis tool   cover later   chapter  utilize  mgrast api   easily download data   stage   mgrast process pipeline   years mgrast  undergo substantial revisions  keep pace   dramatic growth   number size  type  sequence data  accompany constantly evolve developments  metagenomics  relate omic sciences  metatranscriptomics",9
184,VaPid,"VAPiD: a lightweight cross-platform viral annotation pipeline and identification tool to facilitate virus genome submissions to NCBI GenBank
With sequencing technologies becoming cheaper and easier to use, more groups are able to obtain whole genome sequences of viruses of public health and scientific importance. Submission of genomic data to NCBI GenBank is a requirement prior to publication and plays a critical role in making scientific data publicly available. GenBank currently has automatic prokaryotic and eukaryotic genome annotation pipelines but has no viral annotation pipeline beyond influenza virus. Annotation and submission of viral genome sequence is a non-trivial task, especially for groups that do not routinely interact with GenBank for data submissions. We present Viral Annotation Pipeline and iDentification (VAPiD), a portable and lightweight command-line tool for annotation and GenBank deposition of viral genomes. VAPiD supports annotation of nearly all unsegmented viral genomes. The pipeline has been validated on human immunodeficiency virus, human parainfluenza virus 1–4, human metapneumovirus, human coronaviruses (229E/OC43/NL63/HKU1/SARS/MERS), human enteroviruses/rhinoviruses, measles virus, mumps virus, Hepatitis A-E Virus, Chikungunya virus, dengue virus, and West Nile virus, as well the human polyomaviruses BK/JC/MCV, human adenoviruses, and human papillomaviruses. The program can handle individual or batch submissions of different viruses to GenBank and correctly annotates multiple viruses, including those that contain ribosomal slippage or RNA editing without prior knowledge of the virus to be annotated. VAPiD is programmed in Python and is compatible with Windows, Linux, and Mac OS systems. We have created a portable, lightweight, user-friendly, internet-enabled, open-source, command-line genome annotation and submission package to facilitate virus genome submissions to NCBI GenBank.",Annotation,"vapid  lightweight crossplatform viral annotation pipeline  identification tool  facilitate virus genome submissions  ncbi genbank
 sequence technologies become cheaper  easier  use  group  able  obtain whole genome sequence  viruses  public health  scientific importance submission  genomic data  ncbi genbank   requirement prior  publication  play  critical role  make scientific data publicly available genbank currently  automatic prokaryotic  eukaryotic genome annotation pipelines    viral annotation pipeline beyond influenza virus annotation  submission  viral genome sequence   nontrivial task especially  group    routinely interact  genbank  data submissions  present viral annotation pipeline  identification vapid  portable  lightweight commandline tool  annotation  genbank deposition  viral genomes vapid support annotation  nearly  unsegmented viral genomes  pipeline   validate  human immunodeficiency virus human parainfluenza virus  human metapneumovirus human coronaviruses 229eoc43nl63hku1sarsmers human enterovirusesrhinoviruses measles virus mumps virus hepatitis  virus chikungunya virus dengue virus  west nile virus  well  human polyomaviruses bkjcmcv human adenoviruses  human papillomaviruses  program  handle individual  batch submissions  different viruses  genbank  correctly annotate multiple viruses include   contain ribosomal slippage  rna edit without prior knowledge   virus   annotate vapid  program  python   compatible  windows linux  mac  systems   create  portable lightweight userfriendly internetenabled opensource commandline genome annotation  submission package  facilitate virus genome submissions  ncbi genbank",9
185,DNAScan,"DNAscan: personal computer compatible NGS analysis, annotation and visualisation
Next Generation Sequencing (NGS) is a commonly used technology for studying the genetic basis of biological processes and it underpins the aspirations of precision medicine. However, there are significant challenges when dealing with NGS data. Firstly, a huge number of bioinformatics tools for a wide range of uses exist, therefore it is challenging to design an analysis pipeline. Secondly, NGS analysis is computationally intensive, requiring expensive infrastructure, and many medical and research centres do not have adequate high performance computing facilities and cloud computing is not always an option due to privacy and ownership issues. Finally, the interpretation of the results is not trivial and most available pipelines lack the utilities to favour this crucial step. We have therefore developed a fast and efficient bioinformatics pipeline that allows for the analysis of DNA sequencing data, while requiring little computational effort and memory usage. DNAscan can analyse a whole exome sequencing sample in 1 h and a 40x whole genome sequencing sample in 13 h, on a midrange computer. The pipeline can look for single nucleotide variants, small indels, structural variants, repeat expansions and viral genetic material (or any other organism). Its results are annotated using a customisable variety of databases and are available for an on-the-fly visualisation with a local deployment of the gene.iobio platform. DNAscan is implemented in Python. DNAscan is an extremely fast and computationally efficient pipeline for analysis, visualization and interpretation of NGS data. It is designed to provide a powerful and easy-to-use tool for applications in biomedical research and diagnostic medicine, at minimal computational cost. Its comprehensive approach will maximise the potential audience of users, bringing such analyses within the reach of non-specialist laboratories, and those from centres with limited funding available.",Annotation,"dnascan personal computer compatible ngs analysis annotation  visualisation
next generation sequence ngs   commonly use technology  study  genetic basis  biological process   underpin  aspirations  precision medicine however   significant challenge  deal  ngs data firstly  huge number  bioinformatics tool   wide range  use exist therefore   challenge  design  analysis pipeline secondly ngs analysis  computationally intensive require expensive infrastructure  many medical  research centre    adequate high performance compute facilities  cloud compute   always  option due  privacy  ownership issue finally  interpretation   result   trivial   available pipelines lack  utilities  favour  crucial step   therefore develop  fast  efficient bioinformatics pipeline  allow   analysis  dna sequence data  require little computational effort  memory usage dnascan  analyse  whole exome sequence sample       whole genome sequence sample      midrange computer  pipeline  look  single nucleotide variants small indels structural variants repeat expansions  viral genetic material    organism  result  annotate use  customisable variety  databases   available   onthefly visualisation   local deployment   geneiobio platform dnascan  implement  python dnascan   extremely fast  computationally efficient pipeline  analysis visualization  interpretation  ngs data   design  provide  powerful  easytouse tool  applications  biomedical research  diagnostic medicine  minimal computational cost  comprehensive approach  maximise  potential audience  users bring  analyse within  reach  nonspecialist laboratories    centre  limit fund available",9
186,Savant,"Savant: genome browser for high-throughput sequencing data
The advent of high-throughput sequencing (HTS) technologies has made it affordable to sequence many individuals' genomes. Simultaneously the computational analysis of the large volumes of data generated by the new sequencing machines remains a challenge. While a plethora of tools are available to map the resulting reads to a reference genome, and to conduct primary analysis of the mappings, it is often necessary to visually examine the results and underlying data to confirm predictions and understand the functional effects, especially in the context of other datasets. We introduce Savant, the Sequence Annotation, Visualization and ANalysis Tool, a desktop visualization and analysis browser for genomic data. Savant was developed for visualizing and analyzing HTS data, with special care taken to enable dynamic visualization in the presence of gigabases of genomic reads and references the size of the human genome. Savant supports the visualization of genome-based sequence, point, interval and continuous datasets, and multiple visualization modes that enable easy identification of genomic variants (including single nucleotide polymorphisms, structural and copy number variants), and functional genomic information (e.g. peaks in ChIP-seq data) in the context of genomic annotations.",Annotation,"savant genome browser  highthroughput sequence data
 advent  highthroughput sequence hts technologies  make  affordable  sequence many individuals' genomes simultaneously  computational analysis   large volumes  data generate   new sequence machine remain  challenge   plethora  tool  available  map  result read   reference genome   conduct primary analysis   mappings   often necessary  visually examine  result  underlie data  confirm predictions  understand  functional effect especially   context   datasets  introduce savant  sequence annotation visualization  analysis tool  desktop visualization  analysis browser  genomic data savant  develop  visualize  analyze hts data  special care take  enable dynamic visualization   presence  gigabases  genomic read  reference  size   human genome savant support  visualization  genomebased sequence point interval  continuous datasets  multiple visualization modes  enable easy identification  genomic variants include single nucleotide polymorphisms structural  copy number variants  functional genomic information  peak  chipseq data   context  genomic annotations",9
187,FastAnnotator,"FastAnnotator- an efficient transcript annotation web tool
Recent developments in high-throughput sequencing (HTS) technologies have made it feasible to sequence the complete transcriptomes of non-model organisms or metatranscriptomes from environmental samples. The challenge after generating hundreds of millions of sequences is to annotate these transcripts and classify the transcripts based on their putative functions. Because many biological scientists lack the knowledge to install Linux-based software packages or maintain databases used for transcript annotation, we developed an automatic annotation tool with an easy-to-use interface. To elucidate the potential functions of gene transcripts, we integrated well-established annotation tools: Blast2GO, PRIAM and RPS BLAST in a web-based service, FastAnnotator, which can assign Gene Ontology (GO) terms, Enzyme Commission numbers (EC numbers) and functional domains to query sequences. Using six transcriptome sequence datasets as examples, we demonstrated the ability of FastAnnotator to assign functional annotations. FastAnnotator annotated 88.1% and 81.3% of the transcripts from the well-studied organisms Caenorhabditis elegans and Streptococcus parasanguinis, respectively. Furthermore, FastAnnotator annotated 62.9%, 20.4%, 53.1% and 42.0% of the sequences from the transcriptomes of sweet potato, clam, amoeba, and Trichomonas vaginalis, respectively, which lack reference genomes. We demonstrated that FastAnnotator can complete the annotation process in a reasonable amount of time and is suitable for the annotation of transcriptomes from model organisms or organisms for which annotated reference genomes are not avaiable. The sequencing process no longer represents the bottleneck in the study of genomics, and automatic annotation tools have become invaluable as the annotation procedure has become the limiting step. We present FastAnnotator, which was an automated annotation web tool designed to efficiently annotate sequences with their gene functions, enzyme functions or domains. FastAnnotator is useful in transcriptome studies and especially for those focusing on non-model organisms or metatranscriptomes. FastAnnotator does not require local installation.",Annotation,"fastannotator  efficient transcript annotation web tool
recent developments  highthroughput sequence hts technologies  make  feasible  sequence  complete transcriptomes  nonmodel organisms  metatranscriptomes  environmental sample  challenge  generate hundreds  millions  sequence   annotate  transcripts  classify  transcripts base   putative function  many biological scientists lack  knowledge  install linuxbased software package  maintain databases use  transcript annotation  develop  automatic annotation tool   easytouse interface  elucidate  potential function  gene transcripts  integrate wellestablished annotation tool blast2go priam  rps blast   webbased service fastannotator   assign gene ontology  term enzyme commission number  number  functional domains  query sequence use six transcriptome sequence datasets  examples  demonstrate  ability  fastannotator  assign functional annotations fastannotator annotate      transcripts   wellstudied organisms caenorhabditis elegans  streptococcus parasanguinis respectively furthermore fastannotator annotate        sequence   transcriptomes  sweet potato clam amoeba  trichomonas vaginalis respectively  lack reference genomes  demonstrate  fastannotator  complete  annotation process   reasonable amount  time   suitable   annotation  transcriptomes  model organisms  organisms   annotate reference genomes   avaiable  sequence process  longer represent  bottleneck   study  genomics  automatic annotation tool  become invaluable   annotation procedure  become  limit step  present fastannotator    automate annotation web tool design  efficiently annotate sequence   gene function enzyme function  domains fastannotator  useful  transcriptome study  especially   focus  nonmodel organisms  metatranscriptomes fastannotator   require local installation",9
188,VMGAP,"TheViral MetaGenome Annotation Pipeline(VMGAP):an automated tool for the functional annotation of viral Metagenomic shotgun sequencing data
In the past few years, the field of metagenomics has been growing at an accelerated pace, particularly in response to advancements in new sequencing technologies. The large volume of sequence data from novel organisms generated by metagenomic projects has triggered the development of specialized databases and tools focused on particular groups of organisms or data types. Here we describe a pipeline for the functional annotation of viral metagenomic sequence data. The Viral MetaGenome Annotation Pipeline (VMGAP) pipeline takes advantage of a number of specialized databases, such as collections of mobile genetic elements and environmental metagenomes to improve the classification and functional prediction of viral gene products. The pipeline assigns a functional term to each predicted protein sequence following a suite of comprehensive analyses whose results are ranked according to a priority rules hierarchy. Additional annotation is provided in the form of enzyme commission (EC) numbers, GO/MeGO terms and Hidden Markov Models together with supporting evidence.",Annotation,"theviral metagenome annotation pipelinevmgapan automate tool   functional annotation  viral metagenomic shotgun sequence data
  past  years  field  metagenomics   grow   accelerate pace particularly  response  advancements  new sequence technologies  large volume  sequence data  novel organisms generate  metagenomic project  trigger  development  specialize databases  tool focus  particular group  organisms  data type   describe  pipeline   functional annotation  viral metagenomic sequence data  viral metagenome annotation pipeline vmgap pipeline take advantage   number  specialize databases   collections  mobile genetic elements  environmental metagenomes  improve  classification  functional prediction  viral gene products  pipeline assign  functional term   predict protein sequence follow  suite  comprehensive analyse whose result  rank accord   priority rule hierarchy additional annotation  provide   form  enzyme commission  number gomego term  hide markov model together  support evidence",9
189,PyroBayes,"Pyrobayes: an improved base caller for SNP discovery in pyrosequences
Previously reported applications of the 454 Life Sciences pyrosequencing technology have relied on deep sequence coverage for accurate polymorphism discovery because of frequent insertion and deletion sequence errors. Here we report a new base calling program, Pyrobayes, for pyrosequencing reads. Pyrobayes permits accurate single-nucleotide polymorphism (SNP) calling in resequencing applications, even in shallow read coverage, primarily because it produces more confident base calls than the native base calling program.",SNPDiscovery,"pyrobayes  improve base caller  snp discovery  pyrosequences
previously report applications    life sciences pyrosequencing technology  rely  deep sequence coverage  accurate polymorphism discovery   frequent insertion  deletion sequence errors   report  new base call program pyrobayes  pyrosequencing read pyrobayes permit accurate singlenucleotide polymorphism snp call  resequencing applications even  shallow read coverage primarily   produce  confident base call   native base call program",10
190,AutoSNP,"Redundancy based detection of sequence polymorphisms in expressed sequence tag data using autoSNP 
AutoSNP is a program to detect single nucleotide polymorphisms (SNPs) and insertion/deletion polymorphisms (indels) in expressed sequence tag (EST) data. The program uses d2cluster and cap3 to cluster and align EST sequences, and uses redundancy to differentiate between candidate SNPs and sequence errors. Candidate polymorphisms are identified as occurring in multiple reads within an alignment. For each candidate SNP, two measures of confidence are calculated, the redundancy of the polymorphism at a SNP locus and the co segregation of the candidate SNP with other SNPs in the alignment.",SNPDiscovery,"redundancy base detection  sequence polymorphisms  express sequence tag data use autosnp 
autosnp   program  detect single nucleotide polymorphisms snps  insertiondeletion polymorphisms indels  express sequence tag est data  program use d2cluster  cap3  cluster  align est sequence  use redundancy  differentiate  candidate snps  sequence errors candidate polymorphisms  identify  occur  multiple read within  alignment   candidate snp two measure  confidence  calculate  redundancy   polymorphism   snp locus    segregation   candidate snp   snps   alignment",10
191,ATLAS-SNP2,"A SNP discovery method to assess variant allele probability from next-generation resequencing data
Accurate identification of genetic variants from next-generation sequencing (NGS) data is essential for immediate large-scale genomic endeavors such as the 1000 Genomes Project, and is crucial for further genetic analysis based on the discoveries. The key challenge in single nucleotide polymorphism (SNP) discovery is to distinguish true individual variants (occurring at a low frequency) from sequencing errors (often occurring at frequencies orders of magnitude higher). Therefore, knowledge of the error probabilities of base calls is essential. We have developed Atlas-SNP2, a computational tool that detects and accounts for systematic sequencing errors caused by context-related variables in a logistic regression model learned from training data sets. Subsequently, it estimates the posterior error probability for each substitution through a Bayesian formula that integrates prior knowledge of the overall sequencing error probability and the estimated SNP rate with the results from the logistic regression model for the given substitutions. The estimated posterior SNP probability can be used to distinguish true SNPs from sequencing errors. Validation results show that Atlas-SNP2 achieves a false-positive rate of lower than 10%, with an ∼5% or lower false-negative rate.",SNPDiscovery," snp discovery method  assess variant allele probability  nextgeneration resequencing data
accurate identification  genetic variants  nextgeneration sequence ngs data  essential  immediate largescale genomic endeavor     genomes project   crucial   genetic analysis base   discoveries  key challenge  single nucleotide polymorphism snp discovery   distinguish true individual variants occur   low frequency  sequence errors often occur  frequencies order  magnitude higher therefore knowledge   error probabilities  base call  essential   develop atlassnp2  computational tool  detect  account  systematic sequence errors cause  contextrelated variables   logistic regression model learn  train data set subsequently  estimate  posterior error probability   substitution   bayesian formula  integrate prior knowledge   overall sequence error probability   estimate snp rate   result   logistic regression model   give substitutions  estimate posterior snp probability   use  distinguish true snps  sequence errors validation result show  atlassnp2 achieve  falsepositive rate  lower       lower falsenegative rate",10
192,SNPServer,"SNPServer: a real-time SNP discovery tool
SNPServer is a real-time flexible tool for the discovery of SNPs (single nucleotide polymorphisms) within DNA sequence data. The program uses BLAST, to identify related sequences, and CAP3, to cluster and align these sequences. The alignments are parsed to the SNP discovery software autoSNP, a program that detects SNPs and insertion/deletion polymorphisms (indels). Alternatively, lists of related sequences or pre-assembled sequences may be entered for SNP discovery. SNPServer and autoSNP use redundancy to differentiate between candidate SNPs and sequence errors. For each candidate SNP, two measures of confidence are calculated, the redundancy of the polymorphism at a SNP locus and the co-segregation of the candidate SNP with other SNPs in the alignment.",SNPDiscovery,"snpserver  realtime snp discovery tool
snpserver   realtime flexible tool   discovery  snps single nucleotide polymorphisms within dna sequence data  program use blast  identify relate sequence  cap3  cluster  align  sequence  alignments  parse   snp discovery software autosnp  program  detect snps  insertiondeletion polymorphisms indels alternatively list  relate sequence  preassemble sequence may  enter  snp discovery snpserver  autosnp use redundancy  differentiate  candidate snps  sequence errors   candidate snp two measure  confidence  calculate  redundancy   polymorphism   snp locus   cosegregation   candidate snp   snps   alignment",10
193,snp-search,"snp-search: simple processing, manipulation and searching of SNPs from high-throughput sequencing
A typical bacterial pathogen genome mapping project can identify thousands of single nucleotide polymorphisms (SNP). Interpreting SNP data is complex and it is difficult to conceptualise the data contained within the large flat files that are the typical output from most SNP calling algorithms. One solution to this problem is to construct a database that can be queried using simple commands so that SNP interrogation and output is both easy and comprehensible. Here we present snp-search, a tool that manages SNP data and allows for manipulation and searching of SNP data. After creation of a SNP database from a VCF file, snp-search can be used to convert the selected SNP data into FASTA sequences, construct phylogenies, look for unique SNPs, and output contextual information about each SNP. The FASTA output from snp-search is particularly useful for the generation of robust phylogenetic trees that are based on SNP differences across the conserved positions in whole genomes. Queries can be designed to answer critical genomic questions such as the association of SNPs with particular phenotypes. snp-search is a tool that manages SNP data and outputs useful information which can be used to test important biological hypotheses",SNPDiscovery,"snpsearch simple process manipulation  search  snps  highthroughput sequencing
 typical bacterial pathogen genome map project  identify thousands  single nucleotide polymorphisms snp interpret snp data  complex    difficult  conceptualise  data contain within  large flat file    typical output   snp call algorithms one solution   problem   construct  database    query use simple command   snp interrogation  output   easy  comprehensible   present snpsearch  tool  manage snp data  allow  manipulation  search  snp data  creation   snp database   vcf file snpsearch   use  convert  select snp data  fasta sequence construct phylogenies look  unique snps  output contextual information   snp  fasta output  snpsearch  particularly useful   generation  robust phylogenetic tree   base  snp differences across  conserve position  whole genomes query   design  answer critical genomic question    association  snps  particular phenotypes snpsearch   tool  manage snp data  output useful information    use  test important biological hypotheses",10
194,NGS-SNP,"In-depth annotation of SNPs arising from resequencing projects using NGS-SNP
NGS-SNP is a collection of command-line scripts for providing rich annotations for SNPs identified by the sequencing of whole genomes from any organism with reference sequences in Ensembl. Included among the annotations, several of which are not available from any existing SNP annotation tools, are the results of detailed comparisons with orthologous sequences. These comparisons can, for example, identify SNPs that affect conserved residues, or alter residues or genes linked to phenotypes in another species. ",SNPDiscovery,"indepth annotation  snps arise  resequencing project use ngssnp
ngssnp   collection  commandline script  provide rich annotations  snps identify   sequence  whole genomes   organism  reference sequence  ensembl include among  annotations several     available   exist snp annotation tool   result  detail comparisons  orthologous sequence  comparisons   example identify snps  affect conserve residues  alter residues  genes link  phenotypes  another species ",10
195,Varscan2,"VarScan 2: somatic mutation and copy number alteration discovery in cancer by exome sequencing
Cancer is a disease driven by genetic variation and mutation. Exome sequencing can be utilized for discovering these variants and mutations across hundreds of tumors. Here we present an analysis tool, VarScan 2, for the detection of somatic mutations and copy number alterations (CNAs) in exome data from tumor–normal pairs. Unlike most current approaches, our algorithm reads data from both samples simultaneously; a heuristic and statistical algorithm detects sequence variants and classifies them by somatic status (germline, somatic, or LOH); while a comparison of normalized read depth delineates relative copy number changes. We apply these methods to the analysis of exome sequence data from 151 high-grade ovarian tumors characterized as part of the Cancer Genome Atlas (TCGA). We validated some 7790 somatic coding mutations, achieving 93% sensitivity and 85% precision for single nucleotide variant (SNV) detection. Exome-based CNA analysis identified 29 large-scale alterations and 619 focal events per tumor on average. As in our previous analysis of these data, we observed frequent amplification of oncogenes (e.g., CCNE1, MYC) and deletion of tumor suppressors (NF1, PTEN, and CDKN2A). We searched for additional recurrent focal CNAs using the correlation matrix diagonal segmentation (CMDS) algorithm, which identified 424 significant events affecting 582 genes. Taken together, our results demonstrate the robust performance of VarScan 2 for somatic mutation and CNA detection and shed new light on the landscape of genetic alterations in ovarian cancer.",SNPDiscovery,"varscan  somatic mutation  copy number alteration discovery  cancer  exome sequencing
cancer   disease drive  genetic variation  mutation exome sequence   utilize  discover  variants  mutations across hundreds  tumors   present  analysis tool varscan    detection  somatic mutations  copy number alterations cnas  exome data  tumornormal pair unlike  current approach  algorithm read data   sample simultaneously  heuristic  statistical algorithm detect sequence variants  classify   somatic status germline somatic  loh   comparison  normalize read depth delineate relative copy number change  apply  methods   analysis  exome sequence data   highgrade ovarian tumors characterize  part   cancer genome atlas tcga  validate   somatic cod mutations achieve  sensitivity   precision  single nucleotide variant snv detection exomebased cna analysis identify  largescale alterations   focal events per tumor  average    previous analysis   data  observe frequent amplification  oncogenes  ccne1 myc  deletion  tumor suppressors nf1 pten  cdkn2a  search  additional recurrent focal cnas use  correlation matrix diagonal segmentation cmds algorithm  identify  significant events affect  genes take together  result demonstrate  robust performance  varscan   somatic mutation  cna detection  shed new light   landscape  genetic alterations  ovarian cancer",10
196,ngs_backbone,"ngs_backbone: a pipeline for read cleaning, mapping and SNP calling using Next Generation Sequence
The possibilities offered by next generation sequencing (NGS) platforms are revolutionizing biotechnological laboratories. Moreover, the combination of NGS sequencing and affordable high-throughput genotyping technologies is facilitating the rapid discovery and use of SNPs in non-model species. However, this abundance of sequences and polymorphisms creates new software needs. To fulfill these needs, we have developed a powerful, yet easy-to-use application. The ngs_backbone software is a parallel pipeline capable of analyzing Sanger, 454, Illumina and SOLiD (Sequencing by Oligonucleotide Ligation and Detection) sequence reads. Its main supported analyses are: read cleaning, transcriptome assembly and annotation, read mapping and single nucleotide polymorphism (SNP) calling and selection. In order to build a truly useful tool, the software development was paired with a laboratory experiment. All public tomato Sanger EST reads plus 14.2 million Illumina reads were employed to test the tool and predict polymorphism in tomato. The cleaned reads were mapped to the SGN tomato transcriptome obtaining a coverage of 4.2 for Sanger and 8.5 for Illumina. 23,360 single nucleotide variations (SNVs) were predicted. A total of 76 SNVs were experimentally validated, and 85% were found to be real. ngs_backbone is a new software package capable of analyzing sequences produced by NGS technologies and predicting SNVs with great accuracy. In our tomato example, we created a highly polymorphic collection of SNVs that will be a useful resource for tomato researchers and breeders",SNPDiscovery,"ngs_backbone  pipeline  read clean map  snp call use next generation sequence
 possibilities offer  next generation sequence ngs platforms  revolutionize biotechnological laboratories moreover  combination  ngs sequence  affordable highthroughput genotyping technologies  facilitate  rapid discovery  use  snps  nonmodel species however  abundance  sequence  polymorphisms create new software need  fulfill  need   develop  powerful yet easytouse application  ngs_backbone software   parallel pipeline capable  analyze sanger  illumina  solid sequence  oligonucleotide ligation  detection sequence read  main support analyse  read clean transcriptome assembly  annotation read map  single nucleotide polymorphism snp call  selection  order  build  truly useful tool  software development  pair   laboratory experiment  public tomato sanger est read plus  million illumina read  employ  test  tool  predict polymorphism  tomato  clean read  map   sgn tomato transcriptome obtain  coverage    sanger    illumina  single nucleotide variations snvs  predict  total   snvs  experimentally validate    find   real ngs_backbone   new software package capable  analyze sequence produce  ngs technologies  predict snvs  great accuracy   tomato example  create  highly polymorphic collection  snvs     useful resource  tomato researchers  breeders",10
197,SNPhood,"SNPhood: investigate, quantify and visualise the epigenomic neighbourhood of SNPs using NGS data
The vast majority of the many thousands of disease-associated single nucleotide polymorphisms (SNPs) lie in the non-coding part of the genome. They are likely to affect regulatory elements, such as enhancers and promoters, rather than the function of a protein. To understand the molecular mechanisms underlying genetic diseases, it is therefore increasingly important to study the effect of a SNP on nearby molecular traits such as chromatin or transcription factor binding. We developed SNPhood, a user-friendly Bioconductor R package to investigate, quantify and visualise the local epigenetic neighbourhood of a set of SNPs in terms of chromatin marks or TF binding sites using data from NGS experiments.",SNPDiscovery,"snphood investigate quantify  visualise  epigenomic neighbourhood  snps use ngs data
 vast majority   many thousands  diseaseassociated single nucleotide polymorphisms snps lie   noncoding part   genome   likely  affect regulatory elements   enhancers  promoters rather   function   protein  understand  molecular mechanisms underlie genetic diseases   therefore increasingly important  study  effect   snp  nearby molecular traits   chromatin  transcription factor bind  develop snphood  userfriendly bioconductor  package  investigate quantify  visualise  local epigenetic neighbourhood   set  snps  term  chromatin mark   bind sit use data  ngs experiment",10
198,Fast-GBS,"Fast-GBS: a new pipeline for the efficient and highly accurate calling of SNPs from genotyping-by-sequencing data
Next-generation sequencing (NGS) technologies have accelerated considerably the investigation into the composition of genomes and their functions. Genotyping-by-sequencing (GBS) is a genotyping approach that makes use of NGS to rapidly and economically scan a genome. It has been shown to allow the simultaneous discovery and genotyping of thousands to millions of SNPs across a wide range of species. For most users, the main challenge in GBS is the bioinformatics analysis of the large amount of sequence information derived from sequencing GBS libraries in view of calling alleles at SNP loci. Herein we describe a new GBS bioinformatics pipeline, Fast-GBS, designed to provide highly accurate genotyping, to require modest computing resources and to offer ease of use. Fast-GBS is built upon standard bioinformatics language and file formats, is capable of handling data from different sequencing platforms, is capable of detecting different kinds of variants (SNPs, MNPs, and Indels). To illustrate its performance, we called variants in three collections of samples (soybean, barley, and potato) that cover a range of different genome sizes, levels of genome complexity, and ploidy. Within these small sets of samples, we called 35 k, 32 k and 38 k SNPs for soybean, barley and potato, respectively. To assess genotype accuracy, we compared these GBS-derived SNP genotypes with independent data sets obtained from whole-genome sequencing or SNP arrays. This analysis yielded estimated accuracies of 98.7, 95.2, and 94% for soybean, barley, and potato, respectively. We conclude that Fast-GBS provides a highly efficient and reliable tool for calling SNPs from GBS data.",SNPDiscovery,"fastgbs  new pipeline   efficient  highly accurate call  snps  genotypingbysequencing data
nextgeneration sequence ngs technologies  accelerate considerably  investigation   composition  genomes   function genotypingbysequencing gbs   genotyping approach  make use  ngs  rapidly  economically scan  genome    show  allow  simultaneous discovery  genotyping  thousands  millions  snps across  wide range  species   users  main challenge  gbs   bioinformatics analysis   large amount  sequence information derive  sequence gbs libraries  view  call alleles  snp loci herein  describe  new gbs bioinformatics pipeline fastgbs design  provide highly accurate genotyping  require modest compute resources   offer ease  use fastgbs  build upon standard bioinformatics language  file format  capable  handle data  different sequence platforms  capable  detect different kinds  variants snps mnps  indels  illustrate  performance  call variants  three collections  sample soybean barley  potato  cover  range  different genome size level  genome complexity  ploidy within  small set  sample  call        snps  soybean barley  potato respectively  assess genotype accuracy  compare  gbsderived snp genotypes  independent data set obtain  wholegenome sequence  snp array  analysis yield estimate accuracies       soybean barley  potato respectively  conclude  fastgbs provide  highly efficient  reliable tool  call snps  gbs data",10
199,DeepVariant,"A universal SNP and small-indel variant caller using deep neural networks
Despite rapid advances in sequencing technologies, accurately calling genetic variants present in an individual genome from billions of short, errorful sequence reads remains challenging. Here we show that a deep convolutional neural network can call genetic variation in aligned next-generation sequencing read data by learning statistical relationships between images of read pileups around putative variant and true genotype calls. The approach, called DeepVariant, outperforms existing state-of-the-art tools. The learned model generalizes across genome builds and mammalian species, allowing nonhuman sequencing projects to benefit from the wealth of human ground-truth data. We further show that DeepVariant can learn to call variants in a variety of sequencing technologies and experimental designs, including deep whole genomes from 10X Genomics and Ion Ampliseq exomes, highlighting the benefits of using more automated and generalizable techniques for variant calling.",SNPDiscovery," universal snp  smallindel variant caller use deep neural networks
despite rapid advance  sequence technologies accurately call genetic variants present   individual genome  billions  short errorful sequence read remain challenge   show   deep convolutional neural network  call genetic variation  align nextgeneration sequence read data  learn statistical relationships  image  read pileups around putative variant  true genotype call  approach call deepvariant outperform exist stateoftheart tool  learn model generalize across genome build  mammalian species allow nonhuman sequence project  benefit   wealth  human groundtruth data   show  deepvariant  learn  call variants   variety  sequence technologies  experimental design include deep whole genomes   genomics  ion ampliseq exomes highlight  benefit  use  automate  generalizable techniques  variant call",10
200,SNPAAmapper,"SNPAAMapper: An efficient genome-wide SNP variant analysis pipeline for next-generation sequencing data
Many NGS analysis tools focusing on read alignment and variant calling functions for exome sequencing data have been developed in recent years. However, publicly available tools dealing with the downstream analysis of genome-wide variants are fewer and have limited functionality. We developed SNPAAMapper, a novel variant analysis pipeline that can effectively classify variants by region (e.g. CDS, UTRs, intron, upstream, downstream), predict amino acid change type (e.g. synonymous, non-synonymous mutation), and prioritize mutation effects (e.g. CDS versus UTRs). Additional functionality afforded by our pipeline includes: checking variants at exon/intron junctions, customized homozygosity and allele frequency cutoff parameters, and annotation of known variants with dbSNP information, listing original and mutated amino acid sequences containing variants. The final result is reported in a spreadsheet format table containing all variant associated information and prioritized amino acids effects for investigators to examine.",SNPDiscovery,"snpaamapper  efficient genomewide snp variant analysis pipeline  nextgeneration sequence data
many ngs analysis tool focus  read alignment  variant call function  exome sequence data   develop  recent years however publicly available tool deal   downstream analysis  genomewide variants  fewer   limit functionality  develop snpaamapper  novel variant analysis pipeline   effectively classify variants  region  cds utrs intron upstream downstream predict amino acid change type  synonymous nonsynonymous mutation  prioritize mutation effect  cds versus utrs additional functionality afford   pipeline include check variants  exonintron junctions customize homozygosity  allele frequency cutoff parameters  annotation  know variants  dbsnp information list original  mutate amino acid sequence contain variants  final result  report   spreadsheet format table contain  variant associate information  prioritize amino acids effect  investigators  examine",10
201,SNP-ML,"Machine learning as an effective method for identifying true SNPs in polyploid plants
Single nucleotide polymorphisms (SNPs) have many advantages as molecular markers since they are ubiquitous and codominant. However, the discovery of true SNPs in polyploid species is difficult. Peanut (Arachis hypogaea L.) is an allopolyploid, which has a very low rate of true SNP calling. A large set of true and false SNPs identified from the Axiom_Arachis 58k array was leveraged to train machine-learning models to enable identification of true SNPs directly from sequence data to reduce ascertainment bias. These models achieved accuracy rates above 80% using real peanut RNA sequencing (RNA-seq) and whole-genome shotgun (WGS) resequencing data, which is higher than previously reported for polyploids and at least a twofold improvement for peanut. A 48K SNP array, Axiom_Arachis2, was designed using this approach resulting in 75% accuracy of calling SNPs from different tetraploid peanut genotypes. Using the method to simulate SNP variation in several polyploids, models achieved >98% accuracy in selecting true SNPs. Additionally, models built with simulated genotypes were able to select true SNPs at >80% accuracy using real peanut data. This work accomplished the objective to create an effective approach for calling highly reliable SNPs from polyploids using machine learning. A novel tool was developed for predicting true SNPs from sequence data, designated as SNP machine learning (SNP-ML), using the described models. The SNP-ML additionally provides functionality to train new models not included in this study for customized use, designated SNP machine learner (SNP-MLer).",SNPDiscovery,"machine learn   effective method  identify true snps  polyploid plants
single nucleotide polymorphisms snps  many advantage  molecular markers since   ubiquitous  codominant however  discovery  true snps  polyploid species  difficult peanut arachis hypogaea    allopolyploid     low rate  true snp call  large set  true  false snps identify   axiom_arachis  array  leverage  train machinelearning model  enable identification  true snps directly  sequence data  reduce ascertainment bias  model achieve accuracy rat   use real peanut rna sequence rnaseq  wholegenome shotgun wgs resequencing data   higher  previously report  polyploids   least  twofold improvement  peanut   snp array axiom_arachis2  design use  approach result   accuracy  call snps  different tetraploid peanut genotypes use  method  simulate snp variation  several polyploids model achieve  accuracy  select true snps additionally model build  simulate genotypes  able  select true snps   accuracy use real peanut data  work accomplish  objective  create  effective approach  call highly reliable snps  polyploids use machine learn  novel tool  develop  predict true snps  sequence data designate  snp machine learn snpml use  describe model  snpml additionally provide functionality  train new model  include   study  customize use designate snp machine learner snpmler",10
202,Bambino,"Bambino: a variant detector and alignment viewer for next-generation sequencing data in the SAM/BAM format
Bambino is a variant detector and graphical alignment viewer for next-generation sequencing data in the SAM/BAM format, which is capable of pooling data from multiple source files. The variant detector takes advantage of SAM-specific annotations, and produces detailed output suitable for genotyping and identification of somatic mutations. The assembly viewer can display reads in the context of either a user-provided or automatically generated reference sequence, retrieve genome annotation features from a UCSC genome annotation database, display histograms of non-reference allele frequencies, and predict protein-coding changes caused by SNPs.",Visualization,"bambino  variant detector  alignment viewer  nextgeneration sequence data   sambam format
bambino   variant detector  graphical alignment viewer  nextgeneration sequence data   sambam format   capable  pool data  multiple source file  variant detector take advantage  samspecific annotations  produce detail output suitable  genotyping  identification  somatic mutations  assembly viewer  display read   context  either  userprovided  automatically generate reference sequence retrieve genome annotation feature   ucsc genome annotation database display histograms  nonreference allele frequencies  predict proteincoding change cause  snps",11
203,consed,"Consed: a graphical editor for next-generation sequencing
The rapid growth of DNA sequencing throughput in recent years implies that graphical interfaces for viewing and correcting errors must now handle large numbers of reads, efficiently pinpoint regions of interest and automate as many tasks as possible. We have adapted consed to reflect this. To allow full-feature editing of large datasets while keeping memory requirements low, we developed a viewer, bamScape, that reads billion-read BAM files, identifies and displays problem areas for user review and launches the consed graphical editor on user-selected regions, allowing, in addition to longstanding consed capabilities such as assembly editing, a variety of new features including direct editing of the reference sequence, variant and error detection, display of annotation tracks and the ability to simultaneously process a group of reads. Many batch processing capabilities have been added.",Visualization,"con  graphical editor  nextgeneration sequencing
 rapid growth  dna sequence throughput  recent years imply  graphical interfaces  view  correct errors must  handle large number  read efficiently pinpoint regions  interest  automate  many task  possible   adapt con  reflect   allow fullfeature edit  large datasets  keep memory requirements low  develop  viewer bamscape  read billionread bam file identify  display problem areas  user review  launch  con graphical editor  userselected regions allow  addition  longstanding con capabilities   assembly edit  variety  new feature include direct edit   reference sequence variant  error detection display  annotation track   ability  simultaneously process  group  read many batch process capabilities   add",11
204,Tablet,"Tablet—next generation sequence assembly visualization
Tablet is a lightweight, high-performance graphical viewer for next-generation sequence assemblies and alignments. Supporting a range of input assembly formats, Tablet provides high-quality visualizations showing data in packed or stacked views, allowing instant access and navigation to any region of interest, and whole contig overviews and data summaries. Tablet is both multi-core aware and memory efficient, allowing it to handle assemblies containing millions of reads, even on a 32-bit desktop machine.",Visualization,"tablet—next generation sequence assembly visualization
tablet   lightweight highperformance graphical viewer  nextgeneration sequence assemblies  alignments support  range  input assembly format tablet provide highquality visualizations show data  pack  stack view allow instant access  navigation   region  interest  whole contig overviews  data summaries tablet   multicore aware  memory efficient allow   handle assemblies contain millions  read even   bite desktop machine",11
205,IGV,"Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration
Data visualization is an essential component of genomic data analysis. However, the size and diversity of the data sets produced by today’s sequencing and array-based profiling methods present major challenges to visualization tools. The Integrative Genomics Viewer (IGV) is a high-performance viewer that efficiently handles large heterogeneous data sets, while providing a smooth and intuitive user experience at all levels of genome resolution. A key characteristic of IGV is its focus on the integrative nature of genomic studies, with support for both array-based and next-generation sequencing data, and the integration of clinical and phenotypic data. Although IGV is often used to view genomic data from public sources, its primary emphasis is to support researchers who wish to visualize and explore their own data sets or those from colleagues. To that end, IGV supports flexible loading of local and remote data sets, and is optimized to provide high-performance data visualization and exploration on standard desktop systems.",Visualization,"integrative genomics viewer igv highperformance genomics data visualization  exploration
data visualization   essential component  genomic data analysis however  size  diversity   data set produce  todays sequence  arraybased profile methods present major challenge  visualization tool  integrative genomics viewer igv   highperformance viewer  efficiently handle large heterogeneous data set  provide  smooth  intuitive user experience   level  genome resolution  key characteristic  igv   focus   integrative nature  genomic study  support   arraybased  nextgeneration sequence data   integration  clinical  phenotypic data although igv  often use  view genomic data  public source  primary emphasis   support researchers  wish  visualize  explore   data set    colleagues   end igv support flexible load  local  remote data set   optimize  provide highperformance data visualization  exploration  standard desktop systems",11
206,MagicViewer,"MagicViewer: integrated solution for next-generation sequencing data visualization and genetic variation detection and annotation
New sequencing technologies, such as Roche 454, ABI SOLiD and Illumina, have been increasingly developed at an astounding pace with the advantages of high throughput, reduced time and cost. To satisfy the impending need for deciphering the large-scale data generated from next-generation sequencing, an integrated software MagicViewer is developed to easily visualize short read mapping, identify and annotate genetic variation based on the reference genome. MagicViewer provides a user-friendly environment in which large-scale short reads can be displayed in a zoomable interface under user-defined color scheme through an operating system-independent manner. Meanwhile, it also holds a versatile computational pipeline for genetic variation detection, filtration, annotation and visualization, providing details of search option, functional classification, subset selection, sequence association and primer design. In conclusion, MagicViewer is a sophisticated assembly visualization and genetic variation annotation tool for next-generation sequencing data, which can be widely used in a variety of sequencing-based researches, including genome re-sequencing and transcriptome studies.",Visualization,"magicviewer integrate solution  nextgeneration sequence data visualization  genetic variation detection  annotation
new sequence technologies   roche  abi solid  illumina   increasingly develop   astound pace   advantage  high throughput reduce time  cost  satisfy  impend need  decipher  largescale data generate  nextgeneration sequence  integrate software magicviewer  develop  easily visualize short read map identify  annotate genetic variation base   reference genome magicviewer provide  userfriendly environment   largescale short read   display   zoomable interface  userdefined color scheme   operate systemindependent manner meanwhile  also hold  versatile computational pipeline  genetic variation detection filtration annotation  visualization provide detail  search option functional classification subset selection sequence association  primer design  conclusion magicviewer   sophisticate assembly visualization  genetic variation annotation tool  nextgeneration sequence data    widely use   variety  sequencingbased research include genome resequencing  transcriptome study",11
207,EagleView,"EagleView: a genome assembly viewer for next-generation sequencing technologies
The emergence of high-throughput next-generation sequencing technologies (e.g., 454 Life Sciences [Roche], Illumina sequencing [formerly Solexa sequencing]) has dramatically sped up whole-genome de novo sequencing and resequencing. While the low cost of these sequencing technologies provides an unparalleled opportunity for genome-wide polymorphism discovery, the analysis of the new data types and huge data volume poses formidable informatics challenges for base calling, read alignment and genome assembly, polymorphism detection, as well as data visualization. We introduce a new data integration and visualization tool EagleView to facilitate data analyses, visual validation, and hypothesis generation. EagleView can handle a large genome assembly of millions of reads. It supports a compact assembly view, multiple navigation modes, and a pinpoint view of technology-specific trace information. Moreover, EagleView supports viewing coassembly of mixed-type reads from different technologies and supports integrating genome feature annotations into genome assemblies. EagleView has been used in our own lab and by over 100 research labs worldwide for next-generation sequence analyses.",Visualization,"eagleview  genome assembly viewer  nextgeneration sequence technologies
 emergence  highthroughput nextgeneration sequence technologies   life sciences roche illumina sequence formerly solexa sequence  dramatically speed  wholegenome  novo sequence  resequencing   low cost   sequence technologies provide  unparalleled opportunity  genomewide polymorphism discovery  analysis   new data type  huge data volume pose formidable informatics challenge  base call read alignment  genome assembly polymorphism detection  well  data visualization  introduce  new data integration  visualization tool eagleview  facilitate data analyse visual validation  hypothesis generation eagleview  handle  large genome assembly  millions  read  support  compact assembly view multiple navigation modes   pinpoint view  technologyspecific trace information moreover eagleview support view coassembly  mixedtype read  different technologies  support integrate genome feature annotations  genome assemblies eagleview   use    lab     research labs worldwide  nextgeneration sequence analyse",11
208,Icarus,"Icarus: visualizer for de novo assembly evaluation
Data visualization plays an increasingly important role in NGS data analysis. With advances in both sequencing and computational technologies, it has become a new bottleneck in genomics studies. Indeed, evaluation of de novo genome assemblies is one of the areas that can benefit from the visualization. However, even though multiple quality assessment methods are now available, existing visualization tools are hardly suitable for this purpose. Here, we present Icarus—a novel genome visualizer for accurate assessment and analysis of genomic draft assemblies, which is based on the tool QUAST. Icarus can be used in studies where a related reference genome is available, as well as for non-model organisms. The tool is available online and as a standalone application.",Visualization,"icarus visualizer   novo assembly evaluation
data visualization play  increasingly important role  ngs data analysis  advance   sequence  computational technologies   become  new bottleneck  genomics study indeed evaluation   novo genome assemblies  one   areas   benefit   visualization however even though multiple quality assessment methods   available exist visualization tool  hardly suitable   purpose   present icarus— novel genome visualizer  accurate assessment  analysis  genomic draft assemblies   base   tool quast icarus   use  study   relate reference genome  available  well   nonmodel organisms  tool  available online    standalone application",11
209,MapView,"MapView: visualization of short reads alignment on a desktop computer 
We introduce a new visual analytics tool named MapView to facilitate the representation of large-scale short reads alignment data and genetic variation analysis. MapView can handle hundreds of millions of short reads on a desktop computer with limited memory. It supports a compact alignment view for both single-end and paired end short reads, multiple navigation and zoom modes and multi-thread processing. Moreover, MapView offers automated genetic variation detection. MapView has been used in our lab and by over 10 research labs worldwide.",Visualization,"mapview visualization  short read alignment   desktop computer 
 introduce  new visual analytics tool name mapview  facilitate  representation  largescale short read alignment data  genetic variation analysis mapview  handle hundreds  millions  short read   desktop computer  limit memory  support  compact alignment view   singleend  pair end short read multiple navigation  zoom modes  multithread process moreover mapview offer automate genetic variation detection mapview   use   lab     research labs worldwide",11
210,MetaSee,"MetaSee: an interactive and extendable visualization toolbox for metagenomic sample analysis and comparison
The NGS (next generation sequencing)-based metagenomic data analysis is becoming the mainstream for the study of microbial communities. Faced with a large amount of data in metagenomic research, effective data visualization is important for scientists to effectively explore, interpret and manipulate such rich information. The visualization of the metagenomic data, especially multi-sample data, is one of the most critical challenges. The different data sample sources, sequencing approaches and heterogeneous data formats make robust and seamless data visualization difficult. Moreover, researchers have different focuses on metagenomic studies: taxonomical or functional, sample-centric or genome-centric, single sample or multiple samples, etc. However, current efforts in metagenomic data visualization cannot fulfill all of these needs, and it is extremely hard to organize all of these visualization effects in a systematic manner. An extendable, interactive visualization tool would be the method of choice to fulfill all of these visualization needs. In this paper, we have present MetaSee, an extendable toolbox that facilitates the interactive visualization of metagenomic samples of interests. The main components of MetaSee include: (I) a core visualization engine that is composed of different views for comparison of multiple samples: Global view, Phylogenetic view, Sample view and Taxa view, as well as link-out for more in-depth analysis; (II) front-end user interface with real metagenomic models that connect to the above core visualization engine and (III) open-source portal for the development of plug-ins for MetaSee. This integrative visualization tool not only provides the visualization effects, but also enables researchers to perform in-depth analysis of the metagenomic samples of interests. Moreover, its open-source portal allows for the design of plug-ins for MetaSee, which would facilitate the development of any additional visualization effects.",Visualization,"metasee  interactive  extendable visualization toolbox  metagenomic sample analysis  comparison
 ngs next generation sequencingbased metagenomic data analysis  become  mainstream   study  microbial communities face   large amount  data  metagenomic research effective data visualization  important  scientists  effectively explore interpret  manipulate  rich information  visualization   metagenomic data especially multisample data  one    critical challenge  different data sample source sequence approach  heterogeneous data format make robust  seamless data visualization difficult moreover researchers  different focus  metagenomic study taxonomical  functional samplecentric  genomecentric single sample  multiple sample etc however current efforts  metagenomic data visualization cannot fulfill    need    extremely hard  organize    visualization effect   systematic manner  extendable interactive visualization tool would   method  choice  fulfill    visualization need   paper   present metasee  extendable toolbox  facilitate  interactive visualization  metagenomic sample  interest  main components  metasee include   core visualization engine   compose  different view  comparison  multiple sample global view phylogenetic view sample view  taxa view  well  linkout   indepth analysis  frontend user interface  real metagenomic model  connect    core visualization engine  iii opensource portal   development  plugins  metasee  integrative visualization tool   provide  visualization effect  also enable researchers  perform indepth analysis   metagenomic sample  interest moreover  opensource portal allow   design  plugins  metasee  would facilitate  development   additional visualization effect",11
211,Strainer,"Strainer: software for analysis of population variation in community genomic datasets
Metagenomic analyses of microbial communities that are comprehensive enough to provide multiple samples of most loci in the genomes of the dominant organism types will also reveal patterns of genetic variation within natural populations. New bioinformatic tools will enable visualization and comprehensive analysis of this sequence variation and inference of recent evolutionary and ecological processes. We have developed a software package for analysis and visualization of genetic variation in populations and reconstruction of strain variants from otherwise co-assembled sequences. Sequencing reads can be clustered by matching patterns of single nucleotide polymorphisms to generate predicted gene and protein variant sequences, identify conserved intergenic regulatory sequences, and determine the quantity and distribution of recombination events. The Strainer software, a first generation metagenomic bioinformatics tool, facilitates comprehension and analysis of heterogeneity intrinsic in natural communities. The program reveals the degree of clustering among closely related sequence variants and provides a rapid means to generate gene and protein sequences for functional, ecological, and evolutionary analyses.",Visualization,"strainer software  analysis  population variation  community genomic datasets
metagenomic analyse  microbial communities   comprehensive enough  provide multiple sample   loci   genomes   dominant organism type  also reveal pattern  genetic variation within natural populations new bioinformatic tool  enable visualization  comprehensive analysis   sequence variation  inference  recent evolutionary  ecological process   develop  software package  analysis  visualization  genetic variation  populations  reconstruction  strain variants  otherwise coassembled sequence sequence read   cluster  match pattern  single nucleotide polymorphisms  generate predict gene  protein variant sequence identify conserve intergenic regulatory sequence  determine  quantity  distribution  recombination events  strainer software  first generation metagenomic bioinformatics tool facilitate comprehension  analysis  heterogeneity intrinsic  natural communities  program reveal  degree  cluster among closely relate sequence variants  provide  rapid mean  generate gene  protein sequence  functional ecological  evolutionary analyse",11
212,Krona,"Interactive metagenomic visualization in a Web browser
A critical output of metagenomic studies is the estimation of abundances of taxonomical or functional groups. The inherent uncertainty in assignments to these groups makes it important to consider both their hierarchical contexts and their prediction confidence. The current tools for visualizing metagenomic data, however, omit or distort quantitative hierarchical relationships and lack the facility for displaying secondary variables. Here we present Krona, a new visualization tool that allows intuitive exploration of relative abundances and confidences within the complex hierarchies of metagenomic classifications. Krona combines a variant of radial, space-filling displays with parametric coloring and interactive polar-coordinate zooming. The HTML5 and JavaScript implementation enables fully interactive charts that can be explored with any modern Web browser, without the need for installed software or plug-ins. This Web-based architecture also allows each chart to be an independent document, making them easy to share via e-mail or post to a standard Web server. To illustrate Krona's utility, we describe its application to various metagenomic data sets and its compatibility with popular metagenomic analysis tools. Krona is both a powerful metagenomic visualization tool and a demonstration of the potential of HTML5 for highly accessible bioinformatic visualizations. Its rich and interactive displays facilitate more informed interpretations of metagenomic analyses, while its implementation as a browser-based application makes it extremely portable and easily adopted into existing analysis packages",Visualization,"interactive metagenomic visualization   web browser
 critical output  metagenomic study   estimation  abundances  taxonomical  functional group  inherent uncertainty  assignments   group make  important  consider   hierarchical contexts   prediction confidence  current tool  visualize metagenomic data however omit  distort quantitative hierarchical relationships  lack  facility  display secondary variables   present krona  new visualization tool  allow intuitive exploration  relative abundances  confidences within  complex hierarchies  metagenomic classifications krona combine  variant  radial spacefilling display  parametric color  interactive polarcoordinate zoom  html5  javascript implementation enable fully interactive chart    explore   modern web browser without  need  instal software  plugins  webbased architecture also allow  chart    independent document make  easy  share via email  post   standard web server  illustrate krona' utility  describe  application  various metagenomic data set   compatibility  popular metagenomic analysis tool krona    powerful metagenomic visualization tool   demonstration   potential  html5  highly accessible bioinformatic visualizations  rich  interactive display facilitate  inform interpretations  metagenomic analyse   implementation   browserbased application make  extremely portable  easily adopt  exist analysis package",11
213,iTOL,"Interactive Tree Of Life (iTOL): an online tool for phylogenetic tree display and annotation 
Interactive Tree Of Life (iTOL) is a web-based tool for the display, manipulation and annotation of phylogenetic trees. Trees can be interactively pruned and re-rooted. Various types of data such as genome sizes or protein domain repertoires can be mapped onto the tree. Export to several bitmap and vector graphics formats is supported.",Visualization,"interactive tree  life itol  online tool  phylogenetic tree display  annotation 
interactive tree  life itol   webbased tool   display manipulation  annotation  phylogenetic tree tree   interactively prune  rerooted various type  data   genome size  protein domain repertoires   map onto  tree export  several bitmap  vector graphics format  support",11
214,REAPR,"REAPR: a universal tool for genome assembly evaluation
Methods to reliably assess the accuracy of genome sequence data are lacking. Currently completeness is only described qualitatively and mis-assemblies are overlooked. Here we present REAPR, a tool that precisely identifies errors in genome assemblies without the need for a reference sequence. We have validated REAPR on complete genomes or de novo assemblies from bacteria, malaria and Caenorhabditis elegans, and demonstrate that 86% and 82% of the human and mouse reference genomes are error-free, respectively. When applied to an ongoing genome project, REAPR provides corrected assembly statistics allowing the quantitative comparison of multiple assemblies.",AssemblyEvaluation,"reapr  universal tool  genome assembly evaluation
methods  reliably assess  accuracy  genome sequence data  lack currently completeness   describe qualitatively  misassemblies  overlook   present reapr  tool  precisely identify errors  genome assemblies without  need   reference sequence   validate reapr  complete genomes   novo assemblies  bacteria malaria  caenorhabditis elegans  demonstrate       human  mouse reference genomes  errorfree respectively  apply   ongoing genome project reapr provide correct assembly statistics allow  quantitative comparison  multiple assemblies",12
215,QUAST-LG,"Versatile genome assembly evaluation with QUAST-LG
The emergence of high-throughput sequencing technologies revolutionized genomics in early 2000s. The next revolution came with the era of long-read sequencing. These technological advances along with novel computational approaches became the next step towards the automatic pipelines capable to assemble nearly complete mammalian-size genomes. In this manuscript, we demonstrate performance of the state-of-the-art genome assembly software on six eukaryotic datasets sequenced using different technologies. To evaluate the results, we developed QUAST-LG—a tool that compares large genomic de novo assemblies against reference sequences and computes relevant quality metrics. Since genomes generally cannot be reconstructed completely due to complex repeat patterns and low coverage regions, we introduce a concept of upper bound assembly for a given genome and set of reads, and compute theoretical limits on assembly correctness and completeness. Using QUAST-LG, we show how close the assemblies are to the theoretical optimum, and how far this optimum is from the finished reference.",AssemblyEvaluation,"versatile genome assembly evaluation  quastlg
 emergence  highthroughput sequence technologies revolutionize genomics  early   next revolution come   era  longread sequence  technological advance along  novel computational approach become  next step towards  automatic pipelines capable  assemble nearly complete mammaliansize genomes   manuscript  demonstrate performance   stateoftheart genome assembly software  six eukaryotic datasets sequence use different technologies  evaluate  result  develop quastlg— tool  compare large genomic  novo assemblies  reference sequence  compute relevant quality metrics since genomes generally cannot  reconstruct completely due  complex repeat pattern  low coverage regions  introduce  concept  upper bind assembly   give genome  set  read  compute theoretical limit  assembly correctness  completeness use quastlg  show  close  assemblies    theoretical optimum   far  optimum    finish reference",12
216,ALE,"ALE: a generic assembly likelihood evaluation framework for assessing the accuracy of genome and metagenome assemblies
Researchers need general purpose methods for objectively evaluating the accuracy of single and metagenome assemblies and for automatically detecting any errors they may contain. Current methods do not fully meet this need because they require a reference, only consider one of the many aspects of assembly quality or lack statistical justification, and none are designed to evaluate metagenome assemblies. In this article, we present an Assembly Likelihood Evaluation (ALE) framework that overcomes these limitations, systematically evaluating the accuracy of an assembly in a reference-independent manner using rigorous statistical methods. This framework is comprehensive, and integrates read quality, mate pair orientation and insert length (for paired-end reads), sequencing coverage, read alignment and k-mer frequency. ALE pinpoints synthetic errors in both single and metagenomic assemblies, including single-base errors, insertions/deletions, genome rearrangements and chimeric assemblies presented in metagenomes. At the genome level with real-world data, ALE identifies three large misassemblies from the Spirochaeta smaragdinae finished genome, which were all independently validated by Pacific Biosciences sequencing. At the single-base level with Illumina data, ALE recovers 215 of 222 (97%) single nucleotide variants in a training set from a GC-rich Rhodobacter sphaeroides genome. Using real Pacific Biosciences data, ALE identifies 12 of 12 synthetic errors in a Lambda Phage genome, surpassing even Pacific Biosciences’ own variant caller, EviCons. In summary, the ALE framework provides a comprehensive, reference-independent and statistically rigorous measure of single genome and metagenome assembly accuracy, which can be used to identify misassemblies or to optimize the assembly process.",AssemblyEvaluation,"ale  generic assembly likelihood evaluation framework  assess  accuracy  genome  metagenome assemblies
researchers need general purpose methods  objectively evaluate  accuracy  single  metagenome assemblies   automatically detect  errors  may contain current methods   fully meet  need   require  reference  consider one   many aspects  assembly quality  lack statistical justification  none  design  evaluate metagenome assemblies   article  present  assembly likelihood evaluation ale framework  overcome  limitations systematically evaluate  accuracy   assembly   referenceindependent manner use rigorous statistical methods  framework  comprehensive  integrate read quality mate pair orientation  insert length  pairedend read sequence coverage read alignment  kmer frequency ale pinpoint synthetic errors   single  metagenomic assemblies include singlebase errors insertionsdeletions genome rearrangements  chimeric assemblies present  metagenomes   genome level  realworld data ale identify three large misassemblies   spirochaeta smaragdinae finish genome    independently validate  pacific biosciences sequence   singlebase level  illumina data ale recover     single nucleotide variants   train set   gcrich rhodobacter sphaeroides genome use real pacific biosciences data ale identify    synthetic errors   lambda phage genome surpass even pacific biosciences  variant caller evicons  summary  ale framework provide  comprehensive referenceindependent  statistically rigorous measure  single genome  metagenome assembly accuracy    use  identify misassemblies   optimize  assembly process",12
217,CGAL,"CGAL: computing genome assembly likelihoods
Assembly algorithms have been extensively benchmarked using simulated data so that results can be compared to ground truth. However, in de novo assembly, only crude metrics such as contig number and size are typically used to evaluate assembly quality. We present CGAL, a novel likelihood-based approach to assembly assessment in the absence of a ground truth. We show that likelihood is more accurate than other metrics currently used for evaluating assemblies, and describe its application to the optimization and comparison of assembly algorithms.",AssemblyEvaluation,"cgal compute genome assembly likelihoods
assembly algorithms   extensively benchmarked use simulate data   result   compare  grind truth however   novo assembly  crude metrics   contig number  size  typically use  evaluate assembly quality  present cgal  novel likelihoodbased approach  assembly assessment   absence   grind truth  show  likelihood   accurate   metrics currently use  evaluate assemblies  describe  application   optimization  comparison  assembly algorithms",12
218,Baa. pl,"Baa.pl: A tool to evaluate de novo genome assemblies with RNA transcripts
Assessing the correctness of genome assemblies is an important step in any genome project. Several methods exist, but most are computationally intensive and, in some cases, inappropriate. Here I present baa.pl, a fast and easy-to-use program that uses transcript data to evaluate genomic assemblies. Through simulations using human chromosome 22, I show that baa.pl excels at detecting levels of missing sequence and contiguity.",AssemblyEvaluation,"baapl  tool  evaluate  novo genome assemblies  rna transcripts
assessing  correctness  genome assemblies   important step   genome project several methods exist    computationally intensive    case inappropriate   present baapl  fast  easytouse program  use transcript data  evaluate genomic assemblies  simulations use human chromosome   show  baapl excel  detect level  miss sequence  contiguity",12
219,QUAST,"QUAST: quality assessment tool for genome assemblies
Limitations of genome sequencing techniques have led to dozens of assembly algorithms, none of which is perfect. A number of methods for comparing assemblers have been developed, but none is yet a recognized benchmark. Further, most existing methods for comparing assemblies are only applicable to new assemblies of finished genomes; the problem of evaluating assemblies of previously unsequenced species has not been adequately considered. Here, we present QUAST—a quality assessment tool for evaluating and comparing genome assemblies. This tool improves on leading assembly comparison software with new ideas and quality metrics. QUAST can evaluate assemblies both with a reference genome, as well as without a reference. QUAST produces many reports, summary tables and plots to help scientists in their research and in their publications. In this study, we used QUAST to compare several genome assemblers on three datasets. ",AssemblyEvaluation,"quast quality assessment tool  genome assemblies
limitations  genome sequence techniques  lead  dozens  assembly algorithms none    perfect  number  methods  compare assemblers   develop  none  yet  recognize benchmark   exist methods  compare assemblies   applicable  new assemblies  finish genomes  problem  evaluate assemblies  previously unsequenced species    adequately consider   present quast— quality assessment tool  evaluate  compare genome assemblies  tool improve  lead assembly comparison software  new ideas  quality metrics quast  evaluate assemblies    reference genome  well  without  reference quast produce many report summary table  plot  help scientists   research    publications   study  use quast  compare several genome assemblers  three datasets ",12
220,QUAST-LG,"Versatile genome assembly evaluation with QUAST-LG
The emergence of high-throughput sequencing technologies revolutionized genomics in early 2000s. The next revolution came with the era of long-read sequencing. These technological advances along with novel computational approaches became the next step towards the automatic pipelines capable to assemble nearly complete mammalian-size genomes. In this manuscript, we demonstrate performance of the state-of-the-art genome assembly software on six eukaryotic datasets sequenced using different technologies. To evaluate the results, we developed QUAST-LG a tool that compares large genomic de novo assemblies against reference sequences and computes relevant quality metrics. Since genomes generally cannot be reconstructed completely due to complex repeat patterns and low coverage regions, we introduce a concept of upper bound assembly for a given genome and set of reads, and compute theoretical limits on assembly correctness and completeness. Using QUAST-LG, we show how close the assemblies are to the theoretical optimum, and how far this optimum is from the finished reference.",AssemblyEvaluation,"versatile genome assembly evaluation  quastlg
 emergence  highthroughput sequence technologies revolutionize genomics  early   next revolution come   era  longread sequence  technological advance along  novel computational approach become  next step towards  automatic pipelines capable  assemble nearly complete mammaliansize genomes   manuscript  demonstrate performance   stateoftheart genome assembly software  six eukaryotic datasets sequence use different technologies  evaluate  result  develop quastlg  tool  compare large genomic  novo assemblies  reference sequence  compute relevant quality metrics since genomes generally cannot  reconstruct completely due  complex repeat pattern  low coverage regions  introduce  concept  upper bind assembly   give genome  set  read  compute theoretical limit  assembly correctness  completeness use quastlg  show  close  assemblies    theoretical optimum   far  optimum    finish reference",12
221,BUSCO,"BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs
Genomics has revolutionized biological research, but quality assessment of the resulting assembled sequences is complicated and remains mostly limited to technical measures like N50. We propose a measure for quantitative assessment of genome assembly and annotation completeness based on evolutionarily informed expectations of gene content. We implemented the assessment procedure in open-source software, with sets of Benchmarking Universal Single-Copy Orthologs, named BUSCO.",AssemblyEvaluation,"busco assess genome assembly  annotation completeness  singlecopy orthologs
genomics  revolutionize biological research  quality assessment   result assemble sequence  complicate  remain mostly limit  technical measure like n50  propose  measure  quantitative assessment  genome assembly  annotation completeness base  evolutionarily inform expectations  gene content  implement  assessment procedure  opensource software  set  benchmarking universal singlecopy orthologs name busco",12
222,dnAQET,"dnAQET: a framework to compute a consolidated metric for benchmarking quality of de novo assemblies
Accurate de novo genome assembly has become reality with the advancements in sequencing technology. With the ever-increasing number of de novo genome assembly tools, assessing the quality of assemblies has become of great importance in genome research. Although many quality metrics have been proposed and software tools for calculating those metrics have been developed, the existing tools do not produce a unified measure to reflect the overall quality of an assembly. To address this issue, we developed the de novo Assembly Quality Evaluation Tool (dnAQET) that generates a unified metric for benchmarking the quality assessment of assemblies. Our framework first calculates individual quality scores for the scaffolds/contigs of an assembly by aligning them to a reference genome. Next, it computes a quality score for the assembly using its overall reference genome coverage, the quality score distribution of its scaffolds and the redundancy identified in it. Using synthetic assemblies randomly generated from the latest human genome build, various builds of the reference genomes for five organisms and six de novo assemblies for sample NA24385, we tested dnAQET to assess its capability for benchmarking quality evaluation of genome assemblies. For synthetic data, our quality score increased with decreasing number of misassemblies and redundancy and increasing average contig length and coverage, as expected. For genome builds, dnAQET quality score calculated for a more recent reference genome was better than the score for an older version. To compare with some of the most frequently used measures, 13 other quality measures were calculated. The quality score from dnAQET was found to be better than all other measures in terms of consistency with the known quality of the reference genomes, indicating that dnAQET is reliable for benchmarking quality assessment of de novo genome assemblies. The dnAQET is a scalable framework designed to evaluate a de novo genome assembly based on the aggregated quality of its scaffolds (or contigs). Our results demonstrated that dnAQET quality score is reliable for benchmarking quality assessment of genome assemblies. The dnQAET can help researchers to identify the most suitable assembly tools and to select high quality assemblies generated.",AssemblyEvaluation,"dnaqet  framework  compute  consolidate metric  benchmarking quality   novo assemblies
accurate  novo genome assembly  become reality   advancements  sequence technology   everincreasing number   novo genome assembly tool assess  quality  assemblies  become  great importance  genome research although many quality metrics   propose  software tool  calculate  metrics   develop  exist tool   produce  unify measure  reflect  overall quality   assembly  address  issue  develop   novo assembly quality evaluation tool dnaqet  generate  unify metric  benchmarking  quality assessment  assemblies  framework first calculate individual quality score   scaffoldscontigs   assembly  align    reference genome next  compute  quality score   assembly use  overall reference genome coverage  quality score distribution   scaffold   redundancy identify   use synthetic assemblies randomly generate   latest human genome build various build   reference genomes  five organisms  six  novo assemblies  sample na24385  test dnaqet  assess  capability  benchmarking quality evaluation  genome assemblies  synthetic data  quality score increase  decrease number  misassemblies  redundancy  increase average contig length  coverage  expect  genome build dnaqet quality score calculate    recent reference genome  better   score   older version  compare      frequently use measure   quality measure  calculate  quality score  dnaqet  find   better    measure  term  consistency   know quality   reference genomes indicate  dnaqet  reliable  benchmarking quality assessment   novo genome assemblies  dnaqet   scalable framework design  evaluate   novo genome assembly base   aggregate quality   scaffold  contigs  result demonstrate  dnaqet quality score  reliable  benchmarking quality assessment  genome assemblies  dnqaet  help researchers  identify   suitable assembly tool   select high quality assemblies generate",12
223,Laser,"LASER: Large genome ASsembly EvaluatoR
Genome assembly is a fundamental problem with multiple applications. Current technological limitations do not allow assembling of entire genomes and many programs have been designed to produce longer and more reliable contigs. Assessing the quality of these assemblies and comparing those produced by different tools is essential in choosing the best ones. The QUAST program has become the current state-of-the-art in quality assessment of genome assemblies. The only drawback of QUAST is high time and memory usage for large genomes, e.g., over 4 days and 120 GB of RAM for a single human genome assembly. We introduce LASER, a new tool for assembly evaluation that improves greatly the speed and memory requirements of QUAST. For a human genome assembly, LASER is 5.6 times faster than QUAST while using only half the memory; one human genome assembly is evaluated in 17 hours instead of 4 days. The code of LASER is based on that of QUAST and therefore inherits all its features. Genome assembly evaluation is an essential step in assessing the quality of an assembly that is too often done improperly, in part due to significant resource consumption. With the introduction of LASER, proper evaluation can be performed efficiently.",AssemblyEvaluation,"laser large genome assembly evaluator
genome assembly   fundamental problem  multiple applications current technological limitations   allow assemble  entire genomes  many program   design  produce longer   reliable contigs assess  quality   assemblies  compare  produce  different tool  essential  choose  best ones  quast program  become  current stateoftheart  quality assessment  genome assemblies   drawback  quast  high time  memory usage  large genomes    days     ram   single human genome assembly  introduce laser  new tool  assembly evaluation  improve greatly  speed  memory requirements  quast   human genome assembly laser   time faster  quast  use  half  memory one human genome assembly  evaluate   hours instead   days  code  laser  base    quast  therefore inherit   feature genome assembly evaluation   essential step  assess  quality   assembly    often  improperly  part due  significant resource consumption   introduction  laser proper evaluation   perform efficiently",12
