,File_Name,Content,Category,Content_Parsed,Category_Code
0,Gapped Blast,"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
The central idea of the BLAST algorithm is that a statistically significant alignment is likely to contain a high-scoring pair of aligned words. BLAST first scans the database for words (typically of length three for proteins) that score at least T when aligned with some word within the query sequence. Any aligned word pair satisfying this condition is called a hit. The second step of the algorithm checks whether each hit lies within an alignment with score sufficient to be reported. This is done by extending a hit in both directions, until the running alignment’s score has dropped more than X below the maximum score yet attained. This extension step is computationally quite costly; with the T and X parameters necessary to attain reasonable sensitivity to weak alignments, the extension step typically accounts for >90% of BLAST’s execution time. It is therefore desirable to reduce the number of extensions performed. Our refined algorithm is based upon the observation that an HSP of interest is much longer than a single word pair, and may therefore entail multiple hits on the same diagonal and within a relatively short distance of one another. (The diagonal of a hit involving words starting at positions (x1, x2) of the database and query sequences may be defined as x1 – x2. The distance between two hits on the same diagonal is the difference between their first coordinates.) This signature may be used to locate HSPs more efficiently. Specifically, we choose a window length A, and invoke an extension only when two non-overlapping hits are found within distance A of one another on the same diagonal. Any hit that overlaps the most recent one is ignored. Efficient execution requires an array to record, for each diagonal, the first coordinate of the most recent hit found. Since database sequences are scanned sequentially, this coordinate always increases for successive hits. The idea of seeking multiple hits on the same diagonal was first used in the context of biological database searches by Wilbur and Lipman (19). Because we require two hits rather than one to invoke an extension, the threshold parameter T must be lowered to retain comparable sensitivity. The effect is that many more single hits are found, but only a small fraction have an associated second hit on the same diagonal that triggers an extension. The great majority of hits may be dismissed after the minor calculation of looking up, for the appropriate diagonal, the coordinate of the most recent hit, checking whether it is within distance A of the current hit’s coordinate, and finally replacing the old with the new coordinate. Empirically, the computation saved by requiring fewer extensions more than offsets the extra computation required to process the larger number of hits. To study the relative abilities of the one-hit and two-hit methods to detect HSPs of varying score, we model proteins using the background amino acid frequencies of Robinson and Robinson (20), and use the BLOSUM-62 substitution matrix (18) for sequence comparison. Given these Pi and sij, the statistical parameters for ungapped local alignments are calculated to be λu = 0.3176 and Ku = 0.134. Using equation 3 above, we may calculate the qij for which the scoring system is optimized, and employ these target frequencies to generate model HSPs. Finally, we evaluate the sensitivity of the one-hit and two-hit BLAST heuristics using these HSPs. The one-hit method will detect an HSP if it somewhere contains a length-W word of score at least T. For W = 3 and T = 13, Figure 1 shows the empirically estimated probability that an HSP is missed by this method, as a function of its normalized score. The two-hit method will detect an HSP if it contains two nonoverlapping length-W words of score at least T, with starting positions that differ by no more than A residues. For W = 3, T = 11 and A = 40, Figure 1 shows the estimated probability that an HSP is missed by this method, as a function of its normalized score. For HSPs with score at least 33 bits, the two-hit heuristic is more sensitive. To analyze the relative speeds of the one-hit and two-hit methods, using the parameters studied above, we note that the two-hit method generates on average ∼3.2 times as many hits, but only ∼0.14 times as many hit extensions (Fig. 2). Because it takes approximately one ninth as long to decide whether a hit need be extended as actually to extend it, the hit-processing component of the two-hit method is approximately twice as rapid as the same component of the one-hit method. Figure 1 shows that even when using the original one-hit method with threshold parameter T = 13, there is generally no greater than a 4% chance of missing an HSP with score >38 bits. While this would appear sufficient for most purposes, the one-hit default T parameter has typically been set as low as 11, yielding an execution time nearly three times that for T = 13. Why pay this price for what appears at best marginal gains in sensitivity? The reason is that the original BLAST program treats gapped alignments implicitly by locating, in many cases, several distinct HSPs involving the same database sequence, and calculating a statistical assessment of the combined result (21,22). This means that two or more HSPs with scores well below 38 bits can, in combination, rise to statistical significance. If any one of these HSPs is missed, so may be the combined result. The approach taken here allows BLAST to simultaneously produce gapped alignments and run significantly faster than previously. The central idea is to trigger a gapped extension for any HSP that exceeds a moderate score Sg, chosen so that no more than about one extension is invoked per 50 database sequences. (By equation 2, for a typical-length protein query, Sg should be set at ∼22 bits.) A gapped extension takes much longer to execute than an ungapped extension, but by performing very few of them the fraction of the total running time they consume can be kept relatively low. By seeking a single gapped alignment, rather than a collection of ungapped ones, only one of the constituent HSPs need be located for the combined result to be generated successfully. This means that we may tolerate a much higher chance of missing any single moderately scoring HSP. For example, consider a result involving two HSPs, each with the same probability P of being missed at the hit-stage of the BLAST algorithm, and suppose that we desire to find the combined result with probability at least 0.95. The original algorithm, needing to find both HSPs, requires 2P – P2 ≤ 0.05, or P less than ∼0.025. In contrast, the new algorithm requires only that P2 ≤ 0.05, and thus can tolerate P as high as 0.22. This permits the T parameter for the hit-stage of the algorithm to be raised substantially while retaining comparable sensitivity—from T = 11 to T = 13 for the one-hit heuristic. (The two-hit heuristic described above lowers T back to 11.) As will be discussed below, the resulting increase in speed more than compensates for the extra time required for the rare gapped extension. In summary, the new gapped BLAST algorithm requires two non-overlapping hits of score at least T, within a distance A of one another, to invoke an ungapped extension of the second hit. If the HSP generated has normalized score at least Sg bits, then a gapped extension is triggered. The resulting gapped alignment is reported only if it has an E-value low enough to be of interest. For example, in the pairwise comparison of Figure 2, the ungapped extension invoked by the hit pair on the left produces an HSP with score 23.6 bits (calculated using λu and Ku). This is sufficient to trigger a gapped extension, which generates an alignment with score 32.4 bits (calculated using λg and Kg) and E-value of 0.5 (Fig. 3). The original BLAST program locates only the first and last ungapped segments of this alignment (Fig. 3c), and assigns them a combined E-value >50 times greater The standard dynamic programming algorithms for pairwise sequence alignment perform a fixed amount of computation per cell of a path graph, whose dimensions are the lengths of the two sequences being compared (23–25). In order to gain speed, database search algorithms such as Fasta (2) and an earlier gapped version of BLAST (3) sacrifice rigor by confining the dynamic programming to a banded section of the full path graph (4), chosen to include a region of already identified similarity. One problem with this approach is that the optimal gapped alignment may stray beyond the confines of the band explored. As the width of the band is increased to reduce this possibility, the speed advantage of the algorithm is vitiated. We have accordingly taken a different heuristic approach to constructing gapped local alignments, which is a simple generalization of BLAST’s method for constructing HSPs. The central idea is to consider only cells for which the optimal local alignment score falls no more than Xg below the best alignment score yet found. Starting from a single aligned pair of residues, called the seed, the dynamic programming proceeds both forward and backward through the path graph (Zheng Zhang et al., manuscript in preparation) (Figs 3a and 4). The advantage of this approach is that the region of the path graph explored adapts to the alignment being constructed. The alignment can wander arbitrarily many diagonals away from the seed, but the number of cells expanded on each row tends to remain limited, and may even shrink to zero before a boundary of the path graph is encountered (Fig. 4). The Xg parameter serves a similar function to the band-width parameter of the earlier heuristic, but the region of the path graph it implicitly specifies be explored is in general more productively chosen. An important element for this heuristic is the intelligent choice of a seed. Given an HSP whose score is sufficiently high that it triggers a gapped extension, how does one choose a residue pair to force into alignment? While more sophisticated approaches are possible, the simple procedure we have implemented is to locate, along the HSP, the length-11 segment with highest alignment score, and use its central residue pair as the seed. If the HSP itself is shorter than 11, a central residue pair is chosen. For example, the first ungapped region in the alignment of Figure 3c constitutes the HSP that triggered the alignment. The highest-scoring length-11 segment of this HSP aligns leghemoglobin residues 55–65 with β-globin residues 57–67. Thus the alanine residues at respective positions 60 and 62 are used as the seed for the gapped extension illustrated in Figure 3a. As discussed in the performance evaluation section below, this procedure is extremely good at selecting seeds that in fact participate in an optimal alignment. Most gapped extensions are triggered by chance similarities, and are therefore likely to be of limited extent, as illustrated in Figure 4. The reverse extension in this example explores ∼2000 path graph cells, so that a typical two-way gapped extension that does not encounter the end of either sequence is expected to involve ∼4000 cells. Because Sg is set so that a gapped extension is invoked less than once per 50 database sequences, fewer than 80 cells need be explored per database sequence. The execution time required for a gapped extension is ∼500 times that for an ungapped extension. However, by triggering gapped extensions in the manner described, while simultaneously raising T for the single-hit version of BLAST from 11 to 13, approximately one gapped extension is invoked for every 4000 ungapped extensions avoided. Because the number of ungapped extensions is reduced by about two thirds, the total time spent on the extension stage of BLAST is cut by well over half. Of course, the two-hit strategy described above reduces the time needed for the ungapped extensions still further. Once program overhead is accounted for, the net speedup is a factor of about three. For any alignment actually reported, a gapped extension that records ‘traceback’ information (25) needs to be executed. To increase BLAST’s accuracy in producing optimal local alignments, these gapped extensions use by default a substantially larger Xg parameter than employed during the program’s search stage The times required by various steps of the BLAST algorithm vary substantially from one query and one database to another. Table 1 shows typical relative times spent by the original and the gapped BLAST programs on various algorithmic stages. The ‘original BLAST’ program is represented, here and below, by a variant form of blastp version 1.4.9, modified so that it uses the same edge-effect correction (22) and background amino acid frequencies as the ‘gapped BLAST’. The times represent the average for three different queries, with the time for the original BLAST program normalized in each instance to 100 units. More concretely, to search SWISS-PROT (26), release 34 (59 576 sequences; 21 219 450 residues), with the length-567 influenza A virus hemagglutinin precursor (27) as query, the original BLAST program requires 45.8 s, and the gapped BLAST program 15.8 s. This timing experiment, and others referred to below, was run on one 200 MHz R10000 cpu processor of a lightly loaded SGI Power Challenge XL computer with 2.5 Gbytes of RAM. This machine runs the operating system IRIX, version 6.2, which is an implementation of UNIX. We used the standard SGI C compiler, with the -O flag for optimization, to compile all versions of the programs. The times reported are the user times given by the time command, and are for the better of two identical runs. A closely related type of gapped extension routine to that used here was developed by G. Myers during the evaluation of the original BLAST algorithm. It was not included in the publicly distributed code primarily because the then current strategy of extending every hit decreased the algorithm’s speed unduly for the relatively small gain in sensitivity realized (1). As discussed above, the statistical significance of gapped alignments may be evaluated using the two statistical parameters λg and Kg. The current version of the Fasta program (2) estimates these parameters on each run, by analyzing the distribution of alignment scores produced by all the sequences in the database. BLAST gains speed by producing alignments for only the few database sequences likely to be related to the query, and therefore does not have the option of estimating λg and Kg on the fly. Instead, it uses estimates of these parameters produced beforehand by random simulation (3). A drawback of this approach is that the program may not accept an arbitrary scoring system, for which no simulation has been performed, and still produce accurate estimates of statistical significance. The original BLAST programs, in contrast, because they dealt only with ungapped local alignments, could derive λu and Ku from theory for any scoring matrix (8,9). ",Alignment,"gap blast  psiblast  new generation  protein database search programs
 central idea   blast algorithm    statistically significant alignment  likely  contain  highscoring pair  align word blast first scan  database  word typically  length three  proteins  score  least   align   word within  query sequence  align word pair satisfy  condition  call  hit  second step   algorithm check whether  hit lie within  alignment  score sufficient   report     extend  hit   directions   run alignments score  drop      maximum score yet attain  extension step  computationally quite costly      parameters necessary  attain reasonable sensitivity  weak alignments  extension step typically account    blast execution time   therefore desirable  reduce  number  extensions perform  refine algorithm  base upon  observation   hsp  interest  much longer   single word pair  may therefore entail multiple hit    diagonal  within  relatively short distance  one another  diagonal   hit involve word start  position     database  query sequence may  define      distance  two hit    diagonal   difference   first coordinate  signature may  use  locate hsps  efficiently specifically  choose  window length   invoke  extension   two nonoverlapping hit  find within distance   one another    diagonal  hit  overlap   recent one  ignore efficient execution require  array  record   diagonal  first coordinate    recent hit find since database sequence  scan sequentially  coordinate always increase  successive hit  idea  seek multiple hit    diagonal  first use   context  biological database search  wilbur  lipman    require two hit rather  one  invoke  extension  threshold parameter  must  lower  retain comparable sensitivity  effect   many  single hit  find    small fraction   associate second hit    diagonal  trigger  extension  great majority  hit may  dismiss   minor calculation  look    appropriate diagonal  coordinate    recent hit check whether   within distance    current hit coordinate  finally replace  old   new coordinate empirically  computation save  require fewer extensions   offset  extra computation require  process  larger number  hit  study  relative abilities   onehit  twohit methods  detect hsps  vary score  model proteins use  background amino acid frequencies  robinson  robinson   use  blosum substitution matrix   sequence comparison give    sij  statistical parameters  ungapped local alignments  calculate          use equation    may calculate  qij    score system  optimize  employ  target frequencies  generate model hsps finally  evaluate  sensitivity   onehit  twohit blast heuristics use  hsps  onehit method  detect  hsp   somewhere contain  lengthw word  score  least          figure  show  empirically estimate probability   hsp  miss   method   function   normalize score  twohit method  detect  hsp   contain two nonoverlapping lengthw word  score  least   start position  differ      residues            figure  show  estimate probability   hsp  miss   method   function   normalize score  hsps  score  least  bits  twohit heuristic   sensitive  analyze  relative speed   onehit  twohit methods use  parameters study   note   twohit method generate  average  time  many hit    time  many hit extensions fig    take approximately one ninth  long  decide whether  hit need  extend  actually  extend   hitprocessing component   twohit method  approximately twice  rapid    component   onehit method figure  show  even  use  original onehit method  threshold parameter      generally  greater    chance  miss  hsp  score  bits   would appear sufficient   purpose  onehit default  parameter  typically  set  low   yield  execution time nearly three time       pay  price   appear  best marginal gain  sensitivity  reason    original blast program treat gap alignments implicitly  locate  many case several distinct hsps involve   database sequence  calculate  statistical assessment   combine result   mean  two   hsps  score well   bits   combination rise  statistical significance   one   hsps  miss  may   combine result  approach take  allow blast  simultaneously produce gap alignments  run significantly faster  previously  central idea   trigger  gap extension   hsp  exceed  moderate score  choose       one extension  invoke per  database sequence  equation    typicallength protein query    set   bits  gap extension take much longer  execute   ungapped extension   perform      fraction   total run time  consume   keep relatively low  seek  single gap alignment rather   collection  ungapped ones  one   constituent hsps need  locate   combine result   generate successfully  mean   may tolerate  much higher chance  miss  single moderately score hsp  example consider  result involve two hsps     probability    miss   hitstage   blast algorithm  suppose   desire  find  combine result  probability  least   original algorithm need  find  hsps require    ≤    less    contrast  new algorithm require    ≤   thus  tolerate   high    permit   parameter   hitstage   algorithm   raise substantially  retain comparable sensitivity—          onehit heuristic  twohit heuristic describe  lower  back      discuss   result increase  speed   compensate   extra time require   rare gap extension  summary  new gap blast algorithm require two nonoverlapping hit  score  least  within  distance   one another  invoke  ungapped extension   second hit   hsp generate  normalize score  least  bits   gap extension  trigger  result gap alignment  report      evalue low enough    interest  example   pairwise comparison  figure   ungapped extension invoke   hit pair   leave produce  hsp  score  bits calculate use      sufficient  trigger  gap extension  generate  alignment  score  bits calculate use     evalue   fig   original blast program locate   first  last ungapped segment   alignment fig   assign   combine evalue  time greater  standard dynamic program algorithms  pairwise sequence alignment perform  fix amount  computation per cell   path graph whose dimension   lengths   two sequence  compare   order  gain speed database search algorithms   fasta    earlier gap version  blast  sacrifice rigor  confine  dynamic program   band section   full path graph  choose  include  region  already identify similarity one problem   approach    optimal gap alignment may stray beyond  confine   band explore   width   band  increase  reduce  possibility  speed advantage   algorithm  vitiate   accordingly take  different heuristic approach  construct gap local alignments    simple generalization  blast method  construct hsps  central idea   consider  cells    optimal local alignment score fall       best alignment score yet find start   single align pair  residues call  seed  dynamic program proceed  forward  backward   path graph zheng zhang   manuscript  preparation figs     advantage   approach    region   path graph explore adapt   alignment  construct  alignment  wander arbitrarily many diagonals away   seed   number  cells expand   row tend  remain limit  may even shrink  zero   boundary   path graph  encounter fig    parameter serve  similar function   bandwidth parameter   earlier heuristic   region   path graph  implicitly specify  explore   general  productively choose  important element   heuristic   intelligent choice   seed give  hsp whose score  sufficiently high   trigger  gap extension   one choose  residue pair  force  alignment   sophisticate approach  possible  simple procedure   implement   locate along  hsp  length segment  highest alignment score  use  central residue pair   seed   hsp   shorter    central residue pair  choose  example  first ungapped region   alignment  figure  constitute  hsp  trigger  alignment  highestscoring length segment   hsp align leghemoglobin residues   βglobin residues  thus  alanine residues  respective position     use   seed   gap extension illustrate  figure   discuss   performance evaluation section   procedure  extremely good  select seed   fact participate   optimal alignment  gap extensions  trigger  chance similarities   therefore likely    limit extent  illustrate  figure   reverse extension   example explore  path graph cells    typical twoway gap extension    encounter  end  either sequence  expect  involve  cells    set    gap extension  invoke less   per  database sequence fewer   cells need  explore per database sequence  execution time require   gap extension   time    ungapped extension however  trigger gap extensions   manner describe  simultaneously raise    singlehit version  blast     approximately one gap extension  invoke  every  ungapped extensions avoid   number  ungapped extensions  reduce   two thirds  total time spend   extension stage  blast  cut  well  half  course  twohit strategy describe  reduce  time need   ungapped extensions still   program overhead  account   net speedup   factor   three   alignment actually report  gap extension  record traceback information  need   execute  increase blast accuracy  produce optimal local alignments  gap extensions use  default  substantially larger  parameter  employ   program search stage  time require  various step   blast algorithm vary substantially  one query  one database  another table  show typical relative time spend   original   gap blast program  various algorithmic stag  original blast program  represent      variant form  blastp version  modify    use   edgeeffect correction   background amino acid frequencies   gap blast  time represent  average  three different query   time   original blast program normalize   instance   units  concretely  search swissprot  release    sequence    residues   length influenza  virus hemagglutinin precursor   query  original blast program require     gap blast program    time experiment  others refer    run  one  mhz r10000 cpu processor   lightly load sgi power challenge  computer   gbytes  ram  machine run  operate system irix version     implementation  unix  use  standard sgi  compiler    flag  optimization  compile  versions   program  time report   user time give   time command     better  two identical run  closely relate type  gap extension routine   use   develop   myers   evaluation   original blast algorithm    include   publicly distribute code primarily    current strategy  extend every hit decrease  algorithms speed unduly   relatively small gain  sensitivity realize   discuss   statistical significance  gap alignments may  evaluate use  two statistical parameters     current version   fasta program  estimate  parameters   run  analyze  distribution  alignment score produce    sequence   database blast gain speed  produce alignments     database sequence likely   relate   query  therefore     option  estimate      fly instead  use estimate   parameters produce beforehand  random simulation   drawback   approach    program may  accept  arbitrary score system    simulation   perform  still produce accurate estimate  statistical significance  original blast program  contrast   deal   ungapped local alignments could derive     theory   score matrix  ",0
1,PSI BLAST,"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
The central idea of the BLAST algorithm is that a statistically significant alignment is likely to contain a high-scoring pair of aligned words. BLAST first scans the database for words (typically of length three for proteins) that score at least T when aligned with some word within the query sequence. Any aligned word pair satisfying this condition is called a hit. The second step of the algorithm checks whether each hit lies within an alignment with score sufficient to be reported. This is done by extending a hit in both directions, until the running alignment’s score has dropped more than X below the maximum score yet attained. This extension step is computationally quite costly; with the T and X parameters necessary to attain reasonable sensitivity to weak alignments, the extension step typically accounts for >90% of BLAST’s execution time. It is therefore desirable to reduce the number of extensions performed. Our refined algorithm is based upon the observation that an HSP of interest is much longer than a single word pair, and may therefore entail multiple hits on the same diagonal and within a relatively short distance of one another. (The diagonal of a hit involving words starting at positions (x1, x2) of the database and query sequences may be defined as x1 – x2. The distance between two hits on the same diagonal is the difference between their first coordinates.) This signature may be used to locate HSPs more efficiently. Specifically, we choose a window length A, and invoke an extension only when two non-overlapping hits are found within distance A of one another on the same diagonal. Any hit that overlaps the most recent one is ignored. Efficient execution requires an array to record, for each diagonal, the first coordinate of the most recent hit found. Since database sequences are scanned sequentially, this coordinate always increases for successive hits. The idea of seeking multiple hits on the same diagonal was first used in the context of biological database searches by Wilbur and Lipman (19). Because we require two hits rather than one to invoke an extension, the threshold parameter T must be lowered to retain comparable sensitivity. The effect is that many more single hits are found, but only a small fraction have an associated second hit on the same diagonal that triggers an extension. The great majority of hits may be dismissed after the minor calculation of looking up, for the appropriate diagonal, the coordinate of the most recent hit, checking whether it is within distance A of the current hit’s coordinate, and finally replacing the old with the new coordinate. Empirically, the computation saved by requiring fewer extensions more than offsets the extra computation required to process the larger number of hits. To study the relative abilities of the one-hit and two-hit methods to detect HSPs of varying score, we model proteins using the background amino acid frequencies of Robinson and Robinson (20), and use the BLOSUM-62 substitution matrix (18) for sequence comparison. Given these Pi and sij, the statistical parameters for ungapped local alignments are calculated to be λu = 0.3176 and Ku = 0.134. Using equation 3 above, we may calculate the qij for which the scoring system is optimized, and employ these target frequencies to generate model HSPs. Finally, we evaluate the sensitivity of the one-hit and two-hit BLAST heuristics using these HSPs. The one-hit method will detect an HSP if it somewhere contains a length-W word of score at least T. For W = 3 and T = 13, Figure 1 shows the empirically estimated probability that an HSP is missed by this method, as a function of its normalized score. The two-hit method will detect an HSP if it contains two nonoverlapping length-W words of score at least T, with starting positions that differ by no more than A residues. For W = 3, T = 11 and A = 40, Figure 1 shows the estimated probability that an HSP is missed by this method, as a function of its normalized score. For HSPs with score at least 33 bits, the two-hit heuristic is more sensitive. To analyze the relative speeds of the one-hit and two-hit methods, using the parameters studied above, we note that the two-hit method generates on average ∼3.2 times as many hits, but only ∼0.14 times as many hit extensions (Fig. 2). Because it takes approximately one ninth as long to decide whether a hit need be extended as actually to extend it, the hit-processing component of the two-hit method is approximately twice as rapid as the same component of the one-hit method. Figure 1 shows that even when using the original one-hit method with threshold parameter T = 13, there is generally no greater than a 4% chance of missing an HSP with score >38 bits. While this would appear sufficient for most purposes, the one-hit default T parameter has typically been set as low as 11, yielding an execution time nearly three times that for T = 13. Why pay this price for what appears at best marginal gains in sensitivity? The reason is that the original BLAST program treats gapped alignments implicitly by locating, in many cases, several distinct HSPs involving the same database sequence, and calculating a statistical assessment of the combined result (21,22). This means that two or more HSPs with scores well below 38 bits can, in combination, rise to statistical significance. If any one of these HSPs is missed, so may be the combined result. The approach taken here allows BLAST to simultaneously produce gapped alignments and run significantly faster than previously. The central idea is to trigger a gapped extension for any HSP that exceeds a moderate score Sg, chosen so that no more than about one extension is invoked per 50 database sequences. (By equation 2, for a typical-length protein query, Sg should be set at ∼22 bits.) A gapped extension takes much longer to execute than an ungapped extension, but by performing very few of them the fraction of the total running time they consume can be kept relatively low. By seeking a single gapped alignment, rather than a collection of ungapped ones, only one of the constituent HSPs need be located for the combined result to be generated successfully. This means that we may tolerate a much higher chance of missing any single moderately scoring HSP. For example, consider a result involving two HSPs, each with the same probability P of being missed at the hit-stage of the BLAST algorithm, and suppose that we desire to find the combined result with probability at least 0.95. The original algorithm, needing to find both HSPs, requires 2P – P2 ≤ 0.05, or P less than ∼0.025. In contrast, the new algorithm requires only that P2 ≤ 0.05, and thus can tolerate P as high as 0.22. This permits the T parameter for the hit-stage of the algorithm to be raised substantially while retaining comparable sensitivity—from T = 11 to T = 13 for the one-hit heuristic. (The two-hit heuristic described above lowers T back to 11.) As will be discussed below, the resulting increase in speed more than compensates for the extra time required for the rare gapped extension. In summary, the new gapped BLAST algorithm requires two non-overlapping hits of score at least T, within a distance A of one another, to invoke an ungapped extension of the second hit. If the HSP generated has normalized score at least Sg bits, then a gapped extension is triggered. The resulting gapped alignment is reported only if it has an E-value low enough to be of interest. For example, in the pairwise comparison of Figure 2, the ungapped extension invoked by the hit pair on the left produces an HSP with score 23.6 bits (calculated using λu and Ku). This is sufficient to trigger a gapped extension, which generates an alignment with score 32.4 bits (calculated using λg and Kg) and E-value of 0.5 (Fig. 3). The original BLAST program locates only the first and last ungapped segments of this alignment (Fig. 3c), and assigns them a combined E-value >50 times greater The standard dynamic programming algorithms for pairwise sequence alignment perform a fixed amount of computation per cell of a path graph, whose dimensions are the lengths of the two sequences being compared (23–25). In order to gain speed, database search algorithms such as Fasta (2) and an earlier gapped version of BLAST (3) sacrifice rigor by confining the dynamic programming to a banded section of the full path graph (4), chosen to include a region of already identified similarity. One problem with this approach is that the optimal gapped alignment may stray beyond the confines of the band explored. As the width of the band is increased to reduce this possibility, the speed advantage of the algorithm is vitiated. We have accordingly taken a different heuristic approach to constructing gapped local alignments, which is a simple generalization of BLAST’s method for constructing HSPs. The central idea is to consider only cells for which the optimal local alignment score falls no more than Xg below the best alignment score yet found. Starting from a single aligned pair of residues, called the seed, the dynamic programming proceeds both forward and backward through the path graph (Zheng Zhang et al., manuscript in preparation) (Figs 3a and 4). The advantage of this approach is that the region of the path graph explored adapts to the alignment being constructed. The alignment can wander arbitrarily many diagonals away from the seed, but the number of cells expanded on each row tends to remain limited, and may even shrink to zero before a boundary of the path graph is encountered (Fig. 4). The Xg parameter serves a similar function to the band-width parameter of the earlier heuristic, but the region of the path graph it implicitly specifies be explored is in general more productively chosen. An important element for this heuristic is the intelligent choice of a seed. Given an HSP whose score is sufficiently high that it triggers a gapped extension, how does one choose a residue pair to force into alignment? While more sophisticated approaches are possible, the simple procedure we have implemented is to locate, along the HSP, the length-11 segment with highest alignment score, and use its central residue pair as the seed. If the HSP itself is shorter than 11, a central residue pair is chosen. For example, the first ungapped region in the alignment of Figure 3c constitutes the HSP that triggered the alignment. The highest-scoring length-11 segment of this HSP aligns leghemoglobin residues 55–65 with β-globin residues 57–67. Thus the alanine residues at respective positions 60 and 62 are used as the seed for the gapped extension illustrated in Figure 3a. As discussed in the performance evaluation section below, this procedure is extremely good at selecting seeds that in fact participate in an optimal alignment. Most gapped extensions are triggered by chance similarities, and are therefore likely to be of limited extent, as illustrated in Figure 4. The reverse extension in this example explores ∼2000 path graph cells, so that a typical two-way gapped extension that does not encounter the end of either sequence is expected to involve ∼4000 cells. Because Sg is set so that a gapped extension is invoked less than once per 50 database sequences, fewer than 80 cells need be explored per database sequence. The execution time required for a gapped extension is ∼500 times that for an ungapped extension. However, by triggering gapped extensions in the manner described, while simultaneously raising T for the single-hit version of BLAST from 11 to 13, approximately one gapped extension is invoked for every 4000 ungapped extensions avoided. Because the number of ungapped extensions is reduced by about two thirds, the total time spent on the extension stage of BLAST is cut by well over half. Of course, the two-hit strategy described above reduces the time needed for the ungapped extensions still further. Once program overhead is accounted for, the net speedup is a factor of about three. For any alignment actually reported, a gapped extension that records ‘traceback’ information (25) needs to be executed. To increase BLAST’s accuracy in producing optimal local alignments, these gapped extensions use by default a substantially larger Xg parameter than employed during the program’s search stage The times required by various steps of the BLAST algorithm vary substantially from one query and one database to another. Table 1 shows typical relative times spent by the original and the gapped BLAST programs on various algorithmic stages. The ‘original BLAST’ program is represented, here and below, by a variant form of blastp version 1.4.9, modified so that it uses the same edge-effect correction (22) and background amino acid frequencies as the ‘gapped BLAST’. The times represent the average for three different queries, with the time for the original BLAST program normalized in each instance to 100 units. More concretely, to search SWISS-PROT (26), release 34 (59 576 sequences; 21 219 450 residues), with the length-567 influenza A virus hemagglutinin precursor (27) as query, the original BLAST program requires 45.8 s, and the gapped BLAST program 15.8 s. This timing experiment, and others referred to below, was run on one 200 MHz R10000 cpu processor of a lightly loaded SGI Power Challenge XL computer with 2.5 Gbytes of RAM. This machine runs the operating system IRIX, version 6.2, which is an implementation of UNIX. We used the standard SGI C compiler, with the -O flag for optimization, to compile all versions of the programs. The times reported are the user times given by the time command, and are for the better of two identical runs. A closely related type of gapped extension routine to that used here was developed by G. Myers during the evaluation of the original BLAST algorithm. It was not included in the publicly distributed code primarily because the then current strategy of extending every hit decreased the algorithm’s speed unduly for the relatively small gain in sensitivity realized (1). As discussed above, the statistical significance of gapped alignments may be evaluated using the two statistical parameters λg and Kg. The current version of the Fasta program (2) estimates these parameters on each run, by analyzing the distribution of alignment scores produced by all the sequences in the database. BLAST gains speed by producing alignments for only the few database sequences likely to be related to the query, and therefore does not have the option of estimating λg and Kg on the fly. Instead, it uses estimates of these parameters produced beforehand by random simulation (3). A drawback of this approach is that the program may not accept an arbitrary scoring system, for which no simulation has been performed, and still produce accurate estimates of statistical significance. The original BLAST programs, in contrast, because they dealt only with ungapped local alignments, could derive λu and Ku from theory for any scoring matrix (8,9). ",Alignment,"gap blast  psiblast  new generation  protein database search programs
 central idea   blast algorithm    statistically significant alignment  likely  contain  highscoring pair  align word blast first scan  database  word typically  length three  proteins  score  least   align   word within  query sequence  align word pair satisfy  condition  call  hit  second step   algorithm check whether  hit lie within  alignment  score sufficient   report     extend  hit   directions   run alignments score  drop      maximum score yet attain  extension step  computationally quite costly      parameters necessary  attain reasonable sensitivity  weak alignments  extension step typically account    blast execution time   therefore desirable  reduce  number  extensions perform  refine algorithm  base upon  observation   hsp  interest  much longer   single word pair  may therefore entail multiple hit    diagonal  within  relatively short distance  one another  diagonal   hit involve word start  position     database  query sequence may  define      distance  two hit    diagonal   difference   first coordinate  signature may  use  locate hsps  efficiently specifically  choose  window length   invoke  extension   two nonoverlapping hit  find within distance   one another    diagonal  hit  overlap   recent one  ignore efficient execution require  array  record   diagonal  first coordinate    recent hit find since database sequence  scan sequentially  coordinate always increase  successive hit  idea  seek multiple hit    diagonal  first use   context  biological database search  wilbur  lipman    require two hit rather  one  invoke  extension  threshold parameter  must  lower  retain comparable sensitivity  effect   many  single hit  find    small fraction   associate second hit    diagonal  trigger  extension  great majority  hit may  dismiss   minor calculation  look    appropriate diagonal  coordinate    recent hit check whether   within distance    current hit coordinate  finally replace  old   new coordinate empirically  computation save  require fewer extensions   offset  extra computation require  process  larger number  hit  study  relative abilities   onehit  twohit methods  detect hsps  vary score  model proteins use  background amino acid frequencies  robinson  robinson   use  blosum substitution matrix   sequence comparison give    sij  statistical parameters  ungapped local alignments  calculate          use equation    may calculate  qij    score system  optimize  employ  target frequencies  generate model hsps finally  evaluate  sensitivity   onehit  twohit blast heuristics use  hsps  onehit method  detect  hsp   somewhere contain  lengthw word  score  least          figure  show  empirically estimate probability   hsp  miss   method   function   normalize score  twohit method  detect  hsp   contain two nonoverlapping lengthw word  score  least   start position  differ      residues            figure  show  estimate probability   hsp  miss   method   function   normalize score  hsps  score  least  bits  twohit heuristic   sensitive  analyze  relative speed   onehit  twohit methods use  parameters study   note   twohit method generate  average  time  many hit    time  many hit extensions fig    take approximately one ninth  long  decide whether  hit need  extend  actually  extend   hitprocessing component   twohit method  approximately twice  rapid    component   onehit method figure  show  even  use  original onehit method  threshold parameter      generally  greater    chance  miss  hsp  score  bits   would appear sufficient   purpose  onehit default  parameter  typically  set  low   yield  execution time nearly three time       pay  price   appear  best marginal gain  sensitivity  reason    original blast program treat gap alignments implicitly  locate  many case several distinct hsps involve   database sequence  calculate  statistical assessment   combine result   mean  two   hsps  score well   bits   combination rise  statistical significance   one   hsps  miss  may   combine result  approach take  allow blast  simultaneously produce gap alignments  run significantly faster  previously  central idea   trigger  gap extension   hsp  exceed  moderate score  choose       one extension  invoke per  database sequence  equation    typicallength protein query    set   bits  gap extension take much longer  execute   ungapped extension   perform      fraction   total run time  consume   keep relatively low  seek  single gap alignment rather   collection  ungapped ones  one   constituent hsps need  locate   combine result   generate successfully  mean   may tolerate  much higher chance  miss  single moderately score hsp  example consider  result involve two hsps     probability    miss   hitstage   blast algorithm  suppose   desire  find  combine result  probability  least   original algorithm need  find  hsps require    ≤    less    contrast  new algorithm require    ≤   thus  tolerate   high    permit   parameter   hitstage   algorithm   raise substantially  retain comparable sensitivity—          onehit heuristic  twohit heuristic describe  lower  back      discuss   result increase  speed   compensate   extra time require   rare gap extension  summary  new gap blast algorithm require two nonoverlapping hit  score  least  within  distance   one another  invoke  ungapped extension   second hit   hsp generate  normalize score  least  bits   gap extension  trigger  result gap alignment  report      evalue low enough    interest  example   pairwise comparison  figure   ungapped extension invoke   hit pair   leave produce  hsp  score  bits calculate use      sufficient  trigger  gap extension  generate  alignment  score  bits calculate use     evalue   fig   original blast program locate   first  last ungapped segment   alignment fig   assign   combine evalue  time greater  standard dynamic program algorithms  pairwise sequence alignment perform  fix amount  computation per cell   path graph whose dimension   lengths   two sequence  compare   order  gain speed database search algorithms   fasta    earlier gap version  blast  sacrifice rigor  confine  dynamic program   band section   full path graph  choose  include  region  already identify similarity one problem   approach    optimal gap alignment may stray beyond  confine   band explore   width   band  increase  reduce  possibility  speed advantage   algorithm  vitiate   accordingly take  different heuristic approach  construct gap local alignments    simple generalization  blast method  construct hsps  central idea   consider  cells    optimal local alignment score fall       best alignment score yet find start   single align pair  residues call  seed  dynamic program proceed  forward  backward   path graph zheng zhang   manuscript  preparation figs     advantage   approach    region   path graph explore adapt   alignment  construct  alignment  wander arbitrarily many diagonals away   seed   number  cells expand   row tend  remain limit  may even shrink  zero   boundary   path graph  encounter fig    parameter serve  similar function   bandwidth parameter   earlier heuristic   region   path graph  implicitly specify  explore   general  productively choose  important element   heuristic   intelligent choice   seed give  hsp whose score  sufficiently high   trigger  gap extension   one choose  residue pair  force  alignment   sophisticate approach  possible  simple procedure   implement   locate along  hsp  length segment  highest alignment score  use  central residue pair   seed   hsp   shorter    central residue pair  choose  example  first ungapped region   alignment  figure  constitute  hsp  trigger  alignment  highestscoring length segment   hsp align leghemoglobin residues   βglobin residues  thus  alanine residues  respective position     use   seed   gap extension illustrate  figure   discuss   performance evaluation section   procedure  extremely good  select seed   fact participate   optimal alignment  gap extensions  trigger  chance similarities   therefore likely    limit extent  illustrate  figure   reverse extension   example explore  path graph cells    typical twoway gap extension    encounter  end  either sequence  expect  involve  cells    set    gap extension  invoke less   per  database sequence fewer   cells need  explore per database sequence  execution time require   gap extension   time    ungapped extension however  trigger gap extensions   manner describe  simultaneously raise    singlehit version  blast     approximately one gap extension  invoke  every  ungapped extensions avoid   number  ungapped extensions  reduce   two thirds  total time spend   extension stage  blast  cut  well  half  course  twohit strategy describe  reduce  time need   ungapped extensions still   program overhead  account   net speedup   factor   three   alignment actually report  gap extension  record traceback information  need   execute  increase blast accuracy  produce optimal local alignments  gap extensions use  default  substantially larger  parameter  employ   program search stage  time require  various step   blast algorithm vary substantially  one query  one database  another table  show typical relative time spend   original   gap blast program  various algorithmic stag  original blast program  represent      variant form  blastp version  modify    use   edgeeffect correction   background amino acid frequencies   gap blast  time represent  average  three different query   time   original blast program normalize   instance   units  concretely  search swissprot  release    sequence    residues   length influenza  virus hemagglutinin precursor   query  original blast program require     gap blast program    time experiment  others refer    run  one  mhz r10000 cpu processor   lightly load sgi power challenge  computer   gbytes  ram  machine run  operate system irix version     implementation  unix  use  standard sgi  compiler    flag  optimization  compile  versions   program  time report   user time give   time command     better  two identical run  closely relate type  gap extension routine   use   develop   myers   evaluation   original blast algorithm    include   publicly distribute code primarily    current strategy  extend every hit decrease  algorithms speed unduly   relatively small gain  sensitivity realize   discuss   statistical significance  gap alignments may  evaluate use  two statistical parameters     current version   fasta program  estimate  parameters   run  analyze  distribution  alignment score produce    sequence   database blast gain speed  produce alignments     database sequence likely   relate   query  therefore     option  estimate      fly instead  use estimate   parameters produce beforehand  random simulation   drawback   approach    program may  accept  arbitrary score system    simulation   perform  still produce accurate estimate  statistical significance  original blast program  contrast   deal   ungapped local alignments could derive     theory   score matrix  ",0
2,RapSearch,"RAPSearch: a fast protein similarity search tool for short reads
RAPSearch adopts the seed-extension approach of BLAST, which identifies the seeds, the maximal exact matches (MEMs) between the reduced alphabet sequence of a query protein and the reduced alphabet sequence of all proteins in the database, followed by extending and evaluating each of these seeds. RAPSearch employs a linear time algorithm to retrieve the MEMs, which first builds a suffix array and a corresponding longest common prefix (LCP) array to index all proteins in the database [20], and then traverses the suffix array based on each query protein. All identified MEMs are subject to a heuristic extension algorithm including an ungapped extension and then gapped extension, similar to BLAST. Protein sequence seeds using a reduced amino acid alphabet The first reduced amino acid alphabet was introduced by Dill in the hydrophobic-polar (HP) model for the study of the folding of globular proteins [21]. Since then, there are more than 50 reduced alphabets of different size that have been proposed for various purposes [22]. A recent study even demonstrated that reduced alphabet is more sensitive and selective in identifying remote homologous proteins [22]. These observations suggested that homologous proteins exhibit a higher sequence identity on the reduced alphabet than that on the 20-aa alphabet, indicating that it is possible to design efficient and sensitive seeds based on a reduced amino acid alphabet. To select an appropriate reduced amino acid alphabet for RAPSearch, we carried out the following experiments. Using the BaliBase database [23]http://www-bio3d-igbmc.u-strasbg.fr/balibase/, we collected 10,000 pairs of distant homologous proteins that share ~20-40% sequence identify, and 10,000 pairs of proteins from different families (which serve as non-homologous proteins). For each alphabet and each length, we computed the coverage and efficiency of the corresponding seeds: the coverage is defined as the fraction of homolog proteins containing at least one seed match, and the efficiency is defined as the log ratio of the numbers of homologous and non-homologous proteins containing at least one seed match. Seed identification by using suffix array An essential procedure in RAPSearch (and other seed-extension tools like BLAST, which uses hash table instead of suffix array) is how to choose appropriate seeds for extension (which is more time consuming than finding the seed itself). A commonly used strategy is to define a cutoff for the minimum seed size--a small cutoff may result in a huge amount of seeds to be extended (thus slow down the similarity search), whereas a large cutoff may miss some seeds that otherwise may lead to significant alignments. BLAST uses minimum size of 3 (residues for proteins) but also requires that there are two seeds in the same diagonal that span no more than a certain length. RAPSearch will extend all single seeds that have been identified by looking up in the suffix array of target protein sequences. As RAPSearch uses reduced alphabets to present proteins, RAPSearch can use longer seed cutoff, thus achieving faster similarity search. Minimal seed selection algorithm RAPSearch generates seeds of a minimal length of 6-9 amino acids, with longer seeds for frequent words and shorter seeds for rare words. RAPSearch decides the minimum length of seeds starting at a particular query position based on the frequency of the 6-mers starting at that position. Once the minimum length of the seeds is selected for a particular position, all the seeds of at least the required length will be retrieved by looking up in the pre-computed suffix array of protein similarity search database. The minimum seed length selection algorithm is shown as follows (for a given position i in a query sequence). Seed length selection algorithm (position i): minseed ← 6 addlen ← 0 hexmerF ← the frequency of the 6-mer starting at position i medianF ← the median of the frequencies of all 6-mers in the protein database expectF ← hexmerF if (expectF > medianF): # aaComp(k) is the frequency of the corresponding amino acid at position k while expectF * aaComp(i + addlen + 1) < medianF: addlen ← addlen + 1 expectF ← expectF * aaComp(i + addlen + 1) minseed ← minseed + addlen return minseed Seeds with mismatches We further consider seeds with mismatches (these mismatches that can not be handled by reduced alphabets). Long seeds (at least 10 aa) that allow at most one mismatch either at position 3, 4, 5, or 6 as in the following patterns, OOOXOOOOOO, OOOOOOXOOO, OOOOXOOOOO and OOOOOXOOOO (where X indicates a mismatch, and Os indicate exact matches). We replace the residue at each of the positions allowing mismatches (marked with X) by one of the reduced amino acids in the reduced alphabet in turn to search for exact matches, achieving identification of seeds with mismatches using suffix array. Ungapped and gapped alignment We implemented ungapped and gapped extension procedures following the same approach used in BLAST [2]. Statistical significance evaluation We used the statistical evaluation method from BLAST, and used the same model and parameters (for BLOSUM62 substitution matrix) as BLAST to evaluate the significance of the resulting local alignment. Protein similarity search databases and other datasets We tested RAPSearch on several public metagenomic datasets with various read lengths [7, 24]. The nucleotide sequences were downloaded from the NCBI short read archive, and the MG-RAST server http://metagenomics.nmpdr.org/. The protein similarity search databases we used include a 98% non-redundant dataset (prepared by using CD-HIT [25]) of protein sequences from prokaryotic genomes, plasmid and viral genomes, collected in the IMG 3.0 http://img.jgi.doe.gov, eggNOG database (of sequences that have COG annotations) (downloaded from http://eggnog.embl.de/), and NCBI non-redundant (nr) database (downloaded from NCBI ftp site). The complete genomes (Escherichia coli K12 substr MG1655, NC_000913; Salmonella typhi, NC_003198; and Desulfococcus oleovorans Hxd3, NC_009943) and their gene annotations we used for the simulation study were downloaded from the NCBI ftp site. Other computational tools RAPSearch was compared to BLAST, BLAT and HMMER. The BLAT source codes were downloaded from http://hgwdev.cse.ucsc.edu/~kent/src/blatSrc34.zip. The default filtering option in BLAST automatically masks low complexity regions of amino acids by using the SEG approach [26] prior to similarity search. The SEG masking is also implemented in RAPSearch. For comparison purpose, SEG was also applied to the six frame translations of the short reads for (protein) BLAT similarity search.",Alignment,"rapsearch  fast protein similarity search tool  short reads
rapsearch adopt  seedextension approach  blast  identify  seed  maximal exact match mems   reduce alphabet sequence   query protein   reduce alphabet sequence   proteins   database follow  extend  evaluate    seed rapsearch employ  linear time algorithm  retrieve  mems  first build  suffix array   correspond longest common prefix lcp array  index  proteins   database    traverse  suffix array base   query protein  identify mems  subject   heuristic extension algorithm include  ungapped extension   gap extension similar  blast protein sequence seed use  reduce amino acid alphabet  first reduce amino acid alphabet  introduce  dill   hydrophobicpolar  model   study   fold  globular proteins  since       reduce alphabets  different size    propose  various purpose   recent study even demonstrate  reduce alphabet   sensitive  selective  identify remote homologous proteins   observations suggest  homologous proteins exhibit  higher sequence identity   reduce alphabet      alphabet indicate    possible  design efficient  sensitive seed base   reduce amino acid alphabet  select  appropriate reduce amino acid alphabet  rapsearch  carry   follow experiment use  balibase database   collect  pair  distant homologous proteins  share ~ sequence identify   pair  proteins  different families  serve  nonhomologous proteins   alphabet   length  compute  coverage  efficiency   correspond seed  coverage  define   fraction  homolog proteins contain  least one seed match   efficiency  define   log ratio   number  homologous  nonhomologous proteins contain  least one seed match seed identification  use suffix array  essential procedure  rapsearch   seedextension tool like blast  use hash table instead  suffix array    choose appropriate seed  extension    time consume  find  seed   commonly use strategy   define  cutoff   minimum seed sizea small cutoff may result   huge amount  seed   extend thus slow   similarity search whereas  large cutoff may miss  seed  otherwise may lead  significant alignments blast use minimum size   residues  proteins  also require    two seed    diagonal  span     certain length rapsearch  extend  single seed    identify  look    suffix array  target protein sequence  rapsearch use reduce alphabets  present proteins rapsearch  use longer seed cutoff thus achieve faster similarity search minimal seed selection algorithm rapsearch generate seed   minimal length   amino acids  longer seed  frequent word  shorter seed  rare word rapsearch decide  minimum length  seed start   particular query position base   frequency   mers start   position   minimum length   seed  select   particular position   seed   least  require length   retrieve  look    precomputed suffix array  protein similarity search database  minimum seed length selection algorithm  show  follow   give position    query sequence seed length selection algorithm position  minseed   addlen   hexmerf   frequency   mer start  position  medianf   median   frequencies   mers   protein database expectf  hexmerf  expectf  medianf # aacompk   frequency   correspond amino acid  position   expectf * aacompi  addlen    medianf addlen  addlen   expectf  expectf * aacompi  addlen   minseed  minseed  addlen return minseed seed  mismatch   consider seed  mismatch  mismatch     handle  reduce alphabets long seed  least    allow   one mismatch either  position         follow pattern oooxoooooo ooooooxooo ooooxooooo  oooooxoooo   indicate  mismatch   indicate exact match  replace  residue     position allow mismatch mark    one   reduce amino acids   reduce alphabet  turn  search  exact match achieve identification  seed  mismatch use suffix array ungapped  gap alignment  implement ungapped  gap extension procedures follow   approach use  blast  statistical significance evaluation  use  statistical evaluation method  blast  use   model  parameters  blosum62 substitution matrix  blast  evaluate  significance   result local alignment protein similarity search databases   datasets  test rapsearch  several public metagenomic datasets  various read lengths    nucleotide sequence  download   ncbi short read archive   mgrast server   protein similarity search databases  use include   nonredundant dataset prepare  use cdhit   protein sequence  prokaryotic genomes plasmid  viral genomes collect   img   eggnog database  sequence   cog annotations download    ncbi nonredundant  database download  ncbi ftp site  complete genomes escherichia coli k12 substr mg1655 nc_000913 salmonella typhi nc_003198  desulfococcus oleovorans hxd3 nc_009943   gene annotations  use   simulation study  download   ncbi ftp site  computational tool rapsearch  compare  blast blat  hmmer  blat source cod  download  ~kentsrcblatsrc34zip  default filter option  blast automatically mask low complexity regions  amino acids  use  seg approach  prior  similarity search  seg mask  also implement  rapsearch  comparison purpose seg  also apply   six frame translations   short read  protein blat similarity search",0
3,PhenoMeter,"PhenoMeter: A Metabolome Database Search Tool Using Statistical Similarity Matching of Metabolic Phenotypes for High-Confidence Detection of Functional Links
Plant Genotypes The line used as the wild-type reference for metabolomics in this study was A. thaliana ecotype Col-0. Analyzed mutant lines included: GDC-impaired mtkas-1 (Somerville and Ogren, 1982; Ewald et al., 2007), SHMT-impaired shm1-1 (Somerville and Ogren, 1981; Voll et al., 2006), Fd-GOGAT-impaired gls1-103 and gls1-113 (Somerville and Ogren, 1980a; Coschigano et al., 1998), DiT2-impaired dct (Somerville and Ogren, 1983; Renné et al., 2003), rubisco activase-impaired rca (Somerville et al., 1982; Orozco et al., 1993), GLYK-impaired glyk (Boldt et al., 2005), SGAT-impaired agt1-1 and agt1-2 (Somerville and Ogren, 1980b; Liepman and Olsen, 2001), HPR1-impaired hpr1-1 (Timm et al., 2008), peroxisomal catalase 2 (CAT2)-impaired cat2-2 (Queval et al., 2007), peroxisomal malate dehydrogenase 1 (pMDH1)-impaired pmdh1, and peroxisomal malate dehydrogenase 2 (pMDH2)-impaired pmdh2, pMDH1- and pMDH2-impaired double mutant pmdh1pmdh2 (Pracharoenwattana et al., 2007; Cousins et al., 2008). Seed for these lines were obtained through the Arabidopsis Biological Resource Center with the exception of the pmdh, cat2-2, and agt1 mutant lines that were kindly provided by Pracharoenwattana et al. (2007), Graham Noctor (Queval et al., 2007), and (Liepman and Olsen, 2001), respectively. Plant Growth and Tissue Harvest for Metabolomics Experiments Arabidopsis thaliana seeds were allowed to imbibe on wet filter paper and stratified for 5 days. Seeds were germinated and grown on a mixture of potting soil (Debco seed raising mix) and 4 g Osmocote L−1 soil (Osmocote Exact Mini; Scotts Australia). Five replicate plants of each genotype were grown under high CO2 conditions at 6 mL L−1 CO2 in a controlled environment growth cabinet at an irradiance of 140 μmol quanta m−2 s−1 and air temperature of 22°C during the day and 18°C at night, with a day length of 14 h. After 5 weeks, plants were transferred from high (CO2) to ambient (CO2) (400 μL L−1 CO2 but otherwise similar) conditions for 2 h before leaves were harvested for metabolomic analysis. Harvesting was carried out by rapidly excising and sealing 50–100 mg of leaf tissue, cut at the base of the petiole, in a 2 ml polypropylene round-bottom safe-lock Eppendorf tube (Eppendorf; Cat. No. 0030 120.094) containing a 5 mm diameter stainless steel ball (Qiagen; Cat. No. 69989) and freezing in liquid nitrogen within 15 s. Harvested samples were kept frozen at −80°C until analysis. The genotypes were analyzed over three separate experiments, each with its own set of wild-type control plants. Metabolite Extraction Frozen leaf samples were pulverized in a TissueLyser II bead mill (Qiagen; Cat. No. 85300) for 1 min at 20 Hz. Approximately 30 mg of the resulting tissue powder was transferred and accurately weighed, without thawing, to a new, cold 2 mL round-bottom safe-lock Eppendorf tube (Eppendorf; Cat. No. 0030 120.094). Tubes were kept frozen on a tube rack chilled with liquid nitrogen as five volumes (5 μL per mg fresh weight of tissue) of room temperature extraction medium [85% (v/v) HPLC grade MeOH (Sigma), 15% (v/v) untreated MilliQ H2O, 100 ng μL−1 ribitol] was added to each tube followed by brief vortexing to give thorough mixing of the solvent and tissue powder. Tubes were placed back on liquid nitrogen until all the samples had been mixed with extraction medium. The tubes were then quickly transferred to an Eppendorf Thermomixer Comfort (Eppendorf; Cat. No. 5355 000.011), rapidly heated to 60°C and shaken at 1400 RPM for 15 min with internal gas pressure being released from tubes by momentarily opening tube lids after 1 min of heating. Tubes were then centrifuged at 20000 g for 10 min to pellet insoluble material and the supernatants transferred to new 2 mL round-bottom safe-lock Eppendorf microfuge tubes (Eppendorf; Cat. No. 5355 000.011). These stock extracts were centrifuged again at 20000 g for 10 min to ensure the complete absence of insoluble material and 20 μL aliquots of the supernatants were dried in 2 mL amber crimp-cap autosampler vials (Grace Davison Discovery Sciences; Catalog Number 31811E-1232A) fitted with silanized glass 250 μL low-volume inserts (Grace Davison Discovery Sciences; Cat. No. 983228) using a Labconco CentriVap Acid-Resistant System (Labconco; Cat. No. 7983014) operated at room temperature. Metabolite Derivatization Dried metabolite extracts were chemically derivatized by methoximation and trimethylsilylation on a Gerstel MPS2XL Multipurpose Sampler (Gerstel) operating in the PrepAhead mode for automated online derivatization and sample injection. The derivatization procedure consisted of the following steps: (1) addition of 10 μL of 20 mg ml−1 methoxyamine hydrochloride (Supelco, Cat. # 33045-U) in anhydrous derivatization-grade pyridine (Sigma-Aldrich, Cat. # 270970) and incubation at 37°C for 90 min with agitation at 750 RPM; (2) addition of 15 μL of derivatization grade N-methyl-N-(trimethylsilyl)trifluoroacetamide (MSTFA; Sigma-Aldrich; Cat. No. 394866) and incubation at 37°C for 30 min with agitation at 750 RPM; 3) addition of 5 μL of alkane mix [0.029% (v/v) n-dodecane, 0.029% (v/v) n-pentadecane, 0.029% (w/v) n-non-adecane, 0.029% (w/v) n-docosane, 0.029% (w/v) n-octacosane, 0.029% (w/v) n-dotriacontane, and 0.029% (w/v) n-hexatriacontane dissolved in anhydrous pyridine] and incubation for 1 min at 37°C with agitation at 750 RPM. Samples were injected into the GC/MS instrument immediately after derivatization. Gas Chromatography/Mass Spectrometry Metabolomic Analysis Derivatized metabolite samples were analyzed on an Agilent 5975C GC/MSD system comprised of an Agilent GC 7890N gas chromatograph (Agilent Technologies, Palo Alto, CA, USA) and 5975C Inert MSD quadrupole MS detector (Agilent Technologies, Palo Alto, CA, USA). The GC was fitted with a 0.25 mm ID, 0.25 μm film thickness, 30 m Varian FactorFour VF-5 ms capillary column with 10 m integrated guard column (Varian, Inc., Palo Alto, CA, USA; Product No. CP9013). Samples were injected into the split/splitless injector operating in splitless mode with an injection volume of 1 μL, an initial septum purge flow of 3 mL min−1 increasing to 20 mL min−1 after 1 min and a constant inlet temperature of 230°C. Helium carrier gas flow rate was held constant at 1 mL min−1. The GC column oven was held at the initial temperature of 70°C for 1 min before being increased to 325°C at 15°C min−1 before being held at 325°C for 3 min. Total run time was 21 min. Transfer line temperature was 250°C. MS source temperature was 250°C. Quadrupole temperature was 150°C. Electron Impact ionization energy was 70 eV and the MS detector was operated in full scan mode in the range of 40–600 m/z with a scan rate of 3.6 Hz. The MSD was pre-tuned against perfluorotributylamine (PFTBA) mass calibrant using the “atune.u” autotune method provided with Agilent GC/MSD Productivity ChemStation Software (Revision E.02.01.1177; Agilent Technologies, Palo Alto, CA, USA; Product No. G1701EA). Metabolomics Data Processing and Statistical Analysis All GC/MS data were processed using the online MetabolomeExpress data processing pipeline4 (Carroll et al., 2010). Raw GC/MS files were exported to NetCDF format using Agilent MSD ChemStation software (Revision E.02.01.1177; Agilent Technologies, Palo Alto, CA, USA; Product No. G1701EA) and NetCDF files were uploaded to the ANU_Badger MetabolomeExpress data repository. Peak detection settings were: Slope threshold = 200; Min Peak Area = 1000; Min. Peak Height = 500; Min. Peak Purity Factor = 2; Min. Peak Width (Scans) = 5; Extract Peaks = on. Peaks were identified by MSRI library matching which used retention index and mass-spectral similarity as identification criteria. MSRI library matching parameters were as follows: RI Window = ± 2 RI Units; MST Centroid Distance = ± 1 RI Unit; Min. Peak Area (for peak import): 5000; MS Qualifier Ion Ratio Error Tolerance = 30%; Min. Number of Correct Ratio Qualifier Ions = 2; Max. Average MS Ratio Error = 70%; Remove qualifier ion not time-correlated with quantifier ion = OFF; Primary MSRI Library = “Carroll_2014_Arabidopsis_Photorespiration_Mutants.MSRI”; Add Unidentified Peaks to Custom MSRI Library = OFF; Use RI calibration file specified in metadata file = ON; Carry out per-sample fine RI calibration using internal RI standards = OFF. The Carroll_2014_Arabidopsis_Photorespiration_Mutants.MSRI primary library contains entries derived manually from analyses of authentic metabolite standards run under the same GC/MS conditions as the biological samples as well as entries for unidentified peaks that were automatically generated by MetabolomeExpress while processing the data from the reference photorespiration mutants. Library matching results were then used to construct a metabolite x sample data matrix with peak areas being normalized to internal standard (i.e., ribitol). As a quality control filter, samples were checked for the presence of a strong ribitol peak with a peak area of at least 1 × 105 and a deviation from the median internal standard peak area (for that GC/MS batch sequence) of less than 70% of the median value. Statistical normalization to tissue mass was not required because chemical normalization to tissue mass had already been carried out by adjusting extraction solvent volume proportionally to tissue mass. For determination of metabolic phenotypes, the mutant/genotype SIR of each metabolite was calculated by dividing the mean (normalized) signal intensity of each metabolite in each set of mutant plants by its mean (normalized) signal intensity in its associated set of wild-type control plants. Statistical significances were calculated by two-tailed Welch’s t-tests (n = 5) in the MetabolomeExpress Comparative Statistics tool. The full dataset has been uploaded to the MetabolomeExpress Phenotype Database (MetabolomeExpress Dataset IDs 36, 42, and 43) and will be made publicly accessible upon publication of this article. Next-Generation Genome Sequencing Genomic DNA was extracted using the Qiagen DNeasy Plant Mini Kit following the manufacturer’s instruction. Quality was checked using spectrophotometer and agarose gel electrophoresis. DNA concentrations were determined by the Qubit (Invitrogen) system. Genomic libraries were constructed using the TruSeq™ DNA Sample Preparation kit (Illumina) following the manufacturer’s Low-Throughput Protocol. Briefly, 1 μg of genomic DNA was fragmented by Covaris shearing to produce 300–400 bp fragments. After repairing the ends of the fragments to produce blunt ends, 3′ adenylation was performed, followed by ligating distinct DNA adapter indexes to distinct genotypes. The ligation products were enriched by 10 cycles of PCR. The size of the products was analyzed using the Bioanalyzer 2100 (Agilent Technology). The DNA libraries were diluted and pooled so that equal amount of DNA from each genotype was sequenced on a lane of a flow cell, with seven libraries on a single lane. DNA was sequenced using a HiSeq 2000 (Illumina) 100 bp paired-ends reads at the Biomolecular Research Facility at the Australian National University John Curtin School of Medical Research (JCSMR). Reads that had been de-multiplexed and filtered using the instrument manufacturer’s software were supplied by the facility. Alignment of reads from each mutant to the Col-0 reference genome assembly (ftp://ftp.jgi-psf.org/pub/compgen/phytozome/v9.0/Athaliana/annotation/Athaliana_167_protein.fa.gz) available at Phytozome (Goodstein et al., 2012) was performed using BWA (Li and Durbin, 2009). Single nucleotide polymorphisms (SNPs) were detected using SAMtools (Li et al., 2009). The scripts used to align reads and detect SNPs are provided in Data Sheet S1 in Supplementary Material. SNPs within or close to known photorespiratory genes were retrieved from VCF files generated by SAMtools using a custom PHP script to retrieve any SNPs lying between 1000bp upstream of the start coordinates and the end coordinates of the known photorespiratory genes provided in Table S2 in Supplementary Material. The effects of SNPs on protein sequences were predicted using the ENSEMBL Plant Variant Effect Predictor.5 The conservation of the affected amino acids across plants was assessed using PipeAlign (Plewniak et al., 2003). Protein Extraction, Electrophoresis and Immunodetection Leaf total protein was extracted with buffer containing 50 mM EPPS, 1 mM EDTA pH 7.8, 5 mM MgCl2, 1% PVPP, 1% Triton X-100, and 10 mM DTT. Protein (10 μg) was separated using 4 -12% NuPAGE® Bis-Tris Precast Gel (Inivitrogen), then transferred onto PVDF membrane. The Fd-GOGAT protein was detected with rabbit anti-Fd-GOGAT antibody (Agrisera). An AP-conjugated Goat anti-rabbit secondary antibody (Sigma) and AP-conjugate substrate kit (BioRad) were used for detection of the primary antibody.",Alignment,"phenometer  metabolome database search tool use statistical similarity match  metabolic phenotypes  highconfidence detection  functional links
plant genotypes  line use   wildtype reference  metabolomics   study   thaliana ecotype col analyze mutant line include gdcimpaired mtkas somerville  ogren  ewald    shmtimpaired shm1 somerville  ogren  voll    fdgogatimpaired gls1  gls1 somerville  ogren  coschigano    dit2impaired dct somerville  ogren  renné    rubisco activaseimpaired rca somerville    orozco    glykimpaired glyk boldt    sgatimpaired agt1  agt1 somerville  ogren  liepman  olsen  hpr1impaired hpr1 timm    peroxisomal catalase  cat2impaired cat2 queval    peroxisomal malate dehydrogenase  pmdh1impaired pmdh1  peroxisomal malate dehydrogenase  pmdh2impaired pmdh2 pmdh1  pmdh2impaired double mutant pmdh1pmdh2 pracharoenwattana    cousins    seed   line  obtain   arabidopsis biological resource center   exception   pmdh cat2  agt1 mutant line   kindly provide  pracharoenwattana    graham noctor queval     liepman  olsen  respectively plant growth  tissue harvest  metabolomics experiment arabidopsis thaliana seed  allow  imbibe  wet filter paper  stratify   days seed  germinate  grow   mixture  pot soil debco seed raise mix    osmocote  soil osmocote exact mini scotts australia five replicate plant   genotype  grow  high co2 condition     co2   control environment growth cabinet   irradiance   μmol quanta    air temperature  °   day  °  night   day length      weeks plant  transfer  high co2  ambient co2    co2  otherwise similar condition     leave  harvest  metabolomic analysis harvest  carry   rapidly excise  seal    leaf tissue cut   base   petiole     polypropylene roundbottom safelock eppendorf tube eppendorf cat    contain    diameter stainless steel ball qiagen cat    freeze  liquid nitrogen within   harvest sample  keep freeze  °  analysis  genotypes  analyze  three separate experiment     set  wildtype control plant metabolite extraction freeze leaf sample  pulverize   tissuelyser  bead mill qiagen cat     min    approximately     result tissue powder  transfer  accurately weigh without thaw   new cold   roundbottom safelock eppendorf tube eppendorf cat    tube  keep freeze   tube rack chill  liquid nitrogen  five volumes   per  fresh weight  tissue  room temperature extraction medium   hplc grade meoh sigma   untreated milliq h2o    ribitol  add   tube follow  brief vortexing  give thorough mix   solvent  tissue powder tube  place back  liquid nitrogen    sample   mix  extraction medium  tube   quickly transfer   eppendorf thermomixer comfort eppendorf cat    rapidly heat  °  shake   rpm   min  internal gas pressure  release  tube  momentarily open tube lids   min  heat tube   centrifuge      min  pellet insoluble material   supernatants transfer  new   roundbottom safelock eppendorf microfuge tube eppendorf cat     stock extract  centrifuge       min  ensure  complete absence  insoluble material    aliquots   supernatants  dry    amber crimpcap autosampler vials grace davison discovery sciences catalog number 31811e1232a fit  silanized glass   lowvolume insert grace davison discovery sciences cat   use  labconco centrivap acidresistant system labconco cat   operate  room temperature metabolite derivatization dry metabolite extract  chemically derivatized  methoximation  trimethylsilylation   gerstel mps2xl multipurpose sampler gerstel operate   prepahead mode  automate online derivatization  sample injection  derivatization procedure consist   follow step  addition        methoxyamine hydrochloride supelco cat #   anhydrous derivatizationgrade pyridine sigmaaldrich cat #   incubation  °   min  agitation   rpm  addition     derivatization grade nmethylntrimethylsilyltrifluoroacetamide mstfa sigmaaldrich cat    incubation  °   min  agitation   rpm  addition     alkane mix   ndodecane   npentadecane   nnonadecane   ndocosane   noctacosane   ndotriacontane    nhexatriacontane dissolve  anhydrous pyridine  incubation   min  °  agitation   rpm sample  inject   gcms instrument immediately  derivatization gas chromatographymass spectrometry metabolomic analysis derivatized metabolite sample  analyze   agilent  gcmsd system comprise   agilent   gas chromatograph agilent technologies palo alto  usa   inert msd quadrupole  detector agilent technologies palo alto  usa    fit        film thickness   varian factorfour   capillary column    integrate guard column varian inc palo alto  usa product  cp9013 sample  inject   splitsplitless injector operate  splitless mode   injection volume     initial septum purge flow    min increase    min   min   constant inlet temperature  ° helium carrier gas flow rate  hold constant    min   column oven  hold   initial temperature  °   min   increase  °  ° min   hold  °   min total run time   min transfer line temperature  °  source temperature  ° quadrupole temperature  ° electron impact ionization energy       detector  operate  full scan mode   range      scan rate     msd  pretuned  perfluorotributylamine pftba mass calibrant use  “atuneu” autotune method provide  agilent gcmsd productivity chemstation software revision  agilent technologies palo alto  usa product  g1701ea metabolomics data process  statistical analysis  gcms data  process use  online metabolomeexpress data process pipeline4 carroll    raw gcms file  export  netcdf format use agilent msd chemstation software revision  agilent technologies palo alto  usa product  g1701ea  netcdf file  upload   anu_badger metabolomeexpress data repository peak detection settings  slope threshold   min peak area   min peak height   min peak purity factor   min peak width scan   extract peak   peak  identify  msri library match  use retention index  massspectral similarity  identification criteria msri library match parameters   follow  window  ±   units mst centroid distance  ±   unit min peak area  peak import   qualifier ion ratio error tolerance   min number  correct ratio qualifier ions   max average  ratio error   remove qualifier ion  timecorrelated  quantifier ion   primary msri library  “carroll_2014_arabidopsis_photorespiration_mutantsmsri” add unidentified peak  custom msri library   use  calibration file specify  metadata file   carry  persample fine  calibration use internal  standards    carroll_2014_arabidopsis_photorespiration_mutantsmsri primary library contain entries derive manually  analyse  authentic metabolite standards run    gcms condition   biological sample  well  entries  unidentified peak   automatically generate  metabolomeexpress  process  data   reference photorespiration mutants library match result   use  construct  metabolite  sample data matrix  peak areas  normalize  internal standard  ribitol   quality control filter sample  check   presence   strong ribitol peak   peak area   least      deviation   median internal standard peak area   gcms batch sequence  less     median value statistical normalization  tissue mass   require  chemical normalization  tissue mass  already  carry   adjust extraction solvent volume proportionally  tissue mass  determination  metabolic phenotypes  mutantgenotype sir   metabolite  calculate  divide  mean normalize signal intensity   metabolite   set  mutant plant   mean normalize signal intensity   associate set  wildtype control plant statistical significances  calculate  twotailed welch ttests      metabolomeexpress comparative statistics tool  full dataset   upload   metabolomeexpress phenotype database metabolomeexpress dataset ids        make publicly accessible upon publication   article nextgeneration genome sequence genomic dna  extract use  qiagen dneasy plant mini kit follow  manufacturers instruction quality  check use spectrophotometer  agarose gel electrophoresis dna concentrations  determine   qubit invitrogen system genomic libraries  construct use  truseq™ dna sample preparation kit illumina follow  manufacturers lowthroughput protocol briefly    genomic dna  fragment  covaris shear  produce   fragment  repair  end   fragment  produce blunt end ′ adenylation  perform follow  ligate distinct dna adapter index  distinct genotypes  ligation products  enrich   cycle  pcr  size   products  analyze use  bioanalyzer  agilent technology  dna libraries  dilute  pool   equal amount  dna   genotype  sequence   lane   flow cell  seven libraries   single lane dna  sequence use  hiseq  illumina   pairedends read   biomolecular research facility   australian national university john curtin school  medical research jcsmr read    demultiplexed  filter use  instrument manufacturers software  supply   facility alignment  read   mutant   col reference genome assembly ftpftpjgipsforgpubcompgenphytozomev9athalianaannotationathaliana_167_proteinfagz available  phytozome goodstein     perform use bwa   durbin  single nucleotide polymorphisms snps  detect use samtools      script use  align read  detect snps  provide  data sheet   supplementary material snps within  close  know photorespiratory genes  retrieve  vcf file generate  samtools use  custom php script  retrieve  snps lie  1000bp upstream   start coordinate   end coordinate   know photorespiratory genes provide  table   supplementary material  effect  snps  protein sequence  predict use  ensembl plant variant effect predictor  conservation   affect amino acids across plant  assess use pipealign plewniak    protein extraction electrophoresis  immunodetection leaf total protein  extract  buffer contain   epps   edta     mgcl2  pvpp  triton     dtt protein    separate use   nupage® bistris precast gel inivitrogen  transfer onto pvdf membrane  fdgogat protein  detect  rabbit antifdgogat antibody agrisera  apconjugated goat antirabbit secondary antibody sigma  apconjugate substrate kit biorad  use  detection   primary antibody",0
4,cuBlASTp,"cuBLASTP: Fine-Grained Parallelization of Protein Sequence Search on a GPU
Challenges of Mapping BLASTP to GPUs Figure 3 illustrates the hit detection and ungapped extension phases in the BLASTP algorithm. In the hit detection, each subject sequence in the database will be scanned from left to right, and each word will be compared with all words in the query sequence. All similar words will be tagged as hits. The hit detection is in the column-major order in nature, that means all hits in one column will be tagged at the same time. The ungapped extension is in the diagonal-major order, where two or more hits in one diagonal will be checked to trigger the extension along the diagonal until the gap is encountered or the diagonal is ended. Algorithm 1 illustrates the algorithm used in existing BLASTP research on CPU and GPU. When one hit is detected, the corresponding diagonal number will be calculated as the difference of hit.sub pos and hit.query pos as shown in Line 5. The previous hit in this diagonal will be obtained from the lasthit arr array. If the distance between current hit and previous hit is shorter than a threshold, the ungapped extension is triggered. After finish the extension in current column, the algorithm will move to the next word in the subject sequence. This algorithm illustrates the interleaving execution of the hit detection and ungapped extension. Due to the heuristic nature, there exists irregular execution paths for different words in the subject sequence. Since the number of hits being able to trigger ungapped extension in different columns cannot be obtained in advance, it is hard to avoid the divergence branch when threads in a same warp are mapped to handle different sequence alignment. Another issue is the random memory access mode in one thread, since the current hit and the previous hit could be in any place of the diagonal. When threads of a warp are used for different sequence alignment, it is hard to organize the coalesced memory access, since each thread has its own previous hit array. A “fine-grained” multithreaded mode that uses multiple threads unfolding the “for” loop could also lead to severe branch divergence on GPU, considering the uncertain hit number for different words and the uncertain distance with previous hits in diagonals. Furthermore, since any element in the previous hit array may be accessed in any iteration, the “fine-grained” mode may also lead to significant memory access conflict in the irregular mode. Due to these challenges, it is not straightforward to design a fine-grained BLASTP to fully utilize the capability of GPU. We decouple the stages of the BLASTP algorithm and use different strategies to optimize each of them. Hit Detection with Binning In the fine-grained hit detection, we use multiple threads to detect hit positions in current subject sequence. We issue multiple threads in the column-major order, meaning that the successive threads will handle the consecutive words in the subject sequence. In this way, the memory access for words is efficient in the coalesced pattern. Since the ungapped extension has to be executed along diagonals, the output results of hit detection need to be reorganized by the diagonals. As a result, we introduce the bin data structure and bin-based algorithms for hit detection and ungapped extension. We first allocate the consecutive buffer in the global memory, and organize the buffer as bins to hold the hit positions. Although one bin can be allocated for one diagonal, considering the increasing length of sequence in databases, we allocate one bin for multiple diagonals to reduce memory usage. illustrates the process of fine-grained hit detection, where one word comparison between the subject sequence and the query sequence will be scheduled to one thread. A thread will get the word from the corresponding position (column number) in the subject sequence, search the word in the DFA to get hit positions (row numbers), and immediately calculate the corresponding diagonal numbers as the difference of row numbers and column number. For example, the thread 3 should get the word “ABC” from the column 3 of the subject sequence, search “ABC” in the DFA to get the hit positions 1, 7, and 11, and calculate the diagonal numbers as 2, -4, and -8. Since different threads can write hit positions into a same bin simultaneously, we have to use atomic operations to write hit positions. Algorithm 2 describes the fine-grained hit detection algorithm. The variable num bins represents the number of bins, which is a configurable parameter in our fine-grained BLASTP algorithm. We set the number of bins to 128 in our experimental evaluation. This algorithm will schedule a warp of threads for a specific sequence. Each word in current thread seq[i][j] is handled by the thread with laneId j. For each hit of the word, the diagonal number will be calculated and mapped to the bin at Line 13. An array named top in shared memory is allocated. Each element of this array is used to store current available position in corresponding bin. Using atomic operation on top, we can avoid the heavy overhead of atomic operation directly on bins that are allocated on global memory. A warp of threads will be scheduled to next sequence until all words in current sequence are detected. C. Hit Reorganization with Sorting and Filtering After the hit detection, hits are grouped into bins by diagonal numbers. Since multiple threads could write hits for different diagonals into a same bin simultaneously, the sequence of hits in each bin are out of order. Because the ungapped extension will determine whether continue the extension based on the distance of two or more neighboring The hit reorganization is to sort the hits by diagonal number with a top-left to bottom-right order. Since one hit is related with row number, column number, diagonal number, and sequence number, we design the bin data structure for the hit to unify the information. As shown in Figure 5, we pack sequence number, diagonal number, and subject position (column number) into a 64 bit length integer. Because more than 99.95% sequences in the most recent NCBI NR database are shorted than 4K letters and the longest sequence contains 36,805 letters, it is enough to use the 16 bit length for the subject position, which can represent 128K positions. Using this packed data structure, we can sort hits in each bin once rather than sort hits twice by diagonal number and subject position, respectively. Another benefit to use this data structure is that when we do the ungapped extension, which needs sequence number, query position (row number) and subject position (column number), the query position can be easily calculated as subject position − diagonal number, and the sequence number can be obtained with the shift operation. With the sorted hits using the specific data structure, the irregular memory access in ungapped extension can be reduced significantly. After we finish sorting the hits in bins, we add the filtering step to eliminate hits whose distances with neighbors are longer than a threshold, that means these hits can not be used to trigger the ungapped extension (based on two or more hits in each diagonal). A warp of threads are used to eliminate hits for each sequence in one bin. A thread scheduled for one hit compares the threshold with the distance to the neighbor on left and then the distance to the neighbor on right. Only when the distances to two neighbors are longer than the threshold, the hits will be eliminated. The overall performance with the additional filtering step is determined by the ratio of overhead of hit filtering and the overhead of the branch divergence. For the datasets used in our experimental evaluation, we have observed only 5% to 11% hits from the hit detection stage can be used to trigger the ungapped extension. As a result, the overall performance is improved with the hit filtering. D. Fine-grained Ungapped Extension After the hit reorganization with sorting and filtering, the hits in each bin are arranged in ascending order by diagonals and the hits whose distances with neighbors are longer than the threshold are eliminated. Based on the ordered hits, we design a diagonal-based ungapped extension algorithm that is illustrated in Algorithm 3, where one diagonal will be scheduled to one thread for the ungapped extension. As shown from Line 6 to Line 8, different thread warps are scheduled to different bins and threads in a warp are scheduled to different diagonals. We get the sequence number seq id, the column number sub pos, and the row number query pos from the bin data structure and call ungapped ext function to extend the diagonal until a gap is encountered or the diagonal is ended. The variable ext represents the extension result. Since an extension could cover other hits along the diagonal, Line 17 is used to determine whether a hit is covered by the previous extension. Only if a hit is not covered by the previous extension, we trigger the extension from this hit. Since there are still divergence branches in the diagonal based extension algorithm, we design a hit-based ungapped extension to eliminate the divergence further. Algorithm 4 illustrates the hit-based ungapped extension. We schedule one thread to one hit and start the extension per hit independently. Since the extension results from different hits could be the same, the hit-based extension may write duplication at Line 15 with the redundant computation. We leave the result de-duplication in the following stag running on CPU. The performance comparison between the hit-based ungapped extension and the diagonal-based ungapped extension depends on the characters of the sequence. If there are too many hits that can be covered by the extension from other hits in diagonals, the diagonal-based ungapped extension should performs better. As a result, we use a configurable parameter to allow the user to select the ungapped extension algorithms at runtime compares the parallelism mode of different ungapped extension algorithms. Figure 6(a) illustrates the coarse-grained ungapped extension in existing research. Since the hit detection and ungapped extension are interleaved, the coarse-grained ungapped extension extends hits from different diagonals in a sequence sequentially. Our warp-based algorithms can extend hits in a sequence in parallel. E. Hierarchical Buffering To fully utilize memory bandwidth, we propose a hierarchical buffering for the core data structure DFA used in the hit detection. As shown in Figure 2(a), DFA consists of the states in the finite state machine and the query positions for the states. Both the states and query positions are highly reused in hit detection for each word. Loading DFA into shared memory can improve the data access bandwidth. However, because the number of query positions depend on the query length, prefetching all positions into the shared memory may affect the occupancy of GPU kernels and offset the improvement from higher data access bandwidth, especially for the long sequences. Thus, we load the states having relatively fixed and small sizes into the shared memory, and store query positions on the constant memory. On the latest NVIDIA Kepler GPU, a 48KB read-only cache with relaxed memory coalescing rules is introduced for the reusable but randomly accessed data. We allocate the query positions in the global memory and tag them with the keyword “const restrict” for loading them into the readonly cache automatically Figure 7 illustrates the hierarchical buffering architecture for DFA on Kepler GPU. We put the DFA states, e.g., “ABB” and “ABC”, into the shared memory. For the first access of “ABC” from thread 3, the positions are set into bins and loaded into the read-only cache. For the following access of “ABC” from thread 13, the positions will be obtained from the cache. PSS matrix (or scoring matrix) is core data structure highly reused in the ungapped extension. The number of column in PSS matrix is equal to the length of the query sequence. Since each column contains 64 Bytes (32 rows with 2 Bytes for each), the size of PSS matrix increase quickly with the query length. The 48KB shared memory cannot hold the PSS matrix for the query sequence longer than 768. Furthermore, too many shared memory usage will degrade the performance due to the degraded GPU occupancy. On the other hand, if the scoring matrix is used to substitute the PSS matrix, the scoring matrix with the fixed 2KB size can be always fit into the shared memory, while more memory accesses using scoring matrix could decrease the performance compared with using PSS matrix for short sequences. Thus, we provide a configurable parameter to select PSS matrix or scoring matrix. For the PSS matrix, we put it into the shared memory until a threshold and then we put it into the global memory. For the scoring matrix, we always put it into the shared memory",Alignment,"cublastp finegrained parallelization  protein sequence search   gpu
challenges  map blastp  gpus figure  illustrate  hit detection  ungapped extension phase   blastp algorithm   hit detection  subject sequence   database   scan  leave  right   word   compare   word   query sequence  similar word   tag  hit  hit detection    columnmajor order  nature  mean  hit  one column   tag    time  ungapped extension    diagonalmajor order  two   hit  one diagonal   check  trigger  extension along  diagonal   gap  encounter   diagonal  end algorithm  illustrate  algorithm use  exist blastp research  cpu  gpu  one hit  detect  correspond diagonal number   calculate   difference  hitsub pos  hitquery pos  show  line   previous hit   diagonal   obtain   lasthit arr array   distance  current hit  previous hit  shorter   threshold  ungapped extension  trigger  finish  extension  current column  algorithm  move   next word   subject sequence  algorithm illustrate  interleave execution   hit detection  ungapped extension due   heuristic nature  exist irregular execution paths  different word   subject sequence since  number  hit  able  trigger ungapped extension  different columns cannot  obtain  advance   hard  avoid  divergence branch  thread    warp  map  handle different sequence alignment another issue   random memory access mode  one thread since  current hit   previous hit could    place   diagonal  thread   warp  use  different sequence alignment   hard  organize  coalesce memory access since  thread    previous hit array  “finegrained” multithreaded mode  use multiple thread unfold  “” loop could also lead  severe branch divergence  gpu consider  uncertain hit number  different word   uncertain distance  previous hit  diagonals furthermore since  element   previous hit array may  access   iteration  “finegrained” mode may also lead  significant memory access conflict   irregular mode due   challenge    straightforward  design  finegrained blastp  fully utilize  capability  gpu  decouple  stag   blastp algorithm  use different strategies  optimize    hit detection  bin   finegrained hit detection  use multiple thread  detect hit position  current subject sequence  issue multiple thread   columnmajor order mean   successive thread  handle  consecutive word   subject sequence   way  memory access  word  efficient   coalesce pattern since  ungapped extension    execute along diagonals  output result  hit detection need   reorganize   diagonals   result  introduce  bin data structure  binbased algorithms  hit detection  ungapped extension  first allocate  consecutive buffer   global memory  organize  buffer  bin  hold  hit position although one bin   allocate  one diagonal consider  increase length  sequence  databases  allocate one bin  multiple diagonals  reduce memory usage illustrate  process  finegrained hit detection  one word comparison   subject sequence   query sequence   schedule  one thread  thread  get  word   correspond position column number   subject sequence search  word   dfa  get hit position row number  immediately calculate  correspond diagonal number   difference  row number  column number  example  thread   get  word “abc”   column    subject sequence search “abc”   dfa  get  hit position      calculate  diagonal number      since different thread  write hit position    bin simultaneously    use atomic operations  write hit position algorithm  describe  finegrained hit detection algorithm  variable num bin represent  number  bin    configurable parameter   finegrained blastp algorithm  set  number  bin     experimental evaluation  algorithm  schedule  warp  thread   specific sequence  word  current thread seqij  handle   thread  laneid    hit   word  diagonal number   calculate  map   bin  line   array name top  share memory  allocate  element   array  use  store current available position  correspond bin use atomic operation  top   avoid  heavy overhead  atomic operation directly  bin   allocate  global memory  warp  thread   schedule  next sequence   word  current sequence  detect  hit reorganization  sort  filter   hit detection hit  group  bin  diagonal number since multiple thread could write hit  different diagonals    bin simultaneously  sequence  hit   bin    order   ungapped extension  determine whether continue  extension base   distance  two   neighbor  hit reorganization   sort  hit  diagonal number   topleft  bottomright order since one hit  relate  row number column number diagonal number  sequence number  design  bin data structure   hit  unify  information  show  figure   pack sequence number diagonal number  subject position column number    bite length integer     sequence    recent ncbi  database  short   letter   longest sequence contain  letter   enough  use   bite length   subject position   represent  position use  pack data structure   sort hit   bin  rather  sort hit twice  diagonal number  subject position respectively another benefit  use  data structure       ungapped extension  need sequence number query position row number  subject position column number  query position   easily calculate  subject position  diagonal number   sequence number   obtain   shift operation   sort hit use  specific data structure  irregular memory access  ungapped extension   reduce significantly   finish sort  hit  bin  add  filter step  eliminate hit whose distance  neighbor  longer   threshold  mean  hit    use  trigger  ungapped extension base  two   hit   diagonal  warp  thread  use  eliminate hit   sequence  one bin  thread schedule  one hit compare  threshold   distance   neighbor  leave    distance   neighbor  right    distance  two neighbor  longer   threshold  hit   eliminate  overall performance   additional filter step  determine   ratio  overhead  hit filter   overhead   branch divergence   datasets use   experimental evaluation   observe     hit   hit detection stage   use  trigger  ungapped extension   result  overall performance  improve   hit filter  finegrained ungapped extension   hit reorganization  sort  filter  hit   bin  arrange  ascend order  diagonals   hit whose distance  neighbor  longer   threshold  eliminate base   order hit  design  diagonalbased ungapped extension algorithm   illustrate  algorithm   one diagonal   schedule  one thread   ungapped extension  show  line   line  different thread warp  schedule  different bin  thread   warp  schedule  different diagonals  get  sequence number seq   column number sub pos   row number query pos   bin data structure  call ungapped ext function  extend  diagonal   gap  encounter   diagonal  end  variable ext represent  extension result since  extension could cover  hit along  diagonal line   use  determine whether  hit  cover   previous extension    hit   cover   previous extension  trigger  extension   hit since   still divergence branch   diagonal base extension algorithm  design  hitbased ungapped extension  eliminate  divergence  algorithm  illustrate  hitbased ungapped extension  schedule one thread  one hit  start  extension per hit independently since  extension result  different hit could     hitbased extension may write duplication  line    redundant computation  leave  result deduplication   follow stag run  cpu  performance comparison   hitbased ungapped extension   diagonalbased ungapped extension depend   character   sequence     many hit    cover   extension   hit  diagonals  diagonalbased ungapped extension  perform better   result  use  configurable parameter  allow  user  select  ungapped extension algorithms  runtime compare  parallelism mode  different ungapped extension algorithms figure  illustrate  coarsegrained ungapped extension  exist research since  hit detection  ungapped extension  interleave  coarsegrained ungapped extension extend hit  different diagonals   sequence sequentially  warpbased algorithms  extend hit   sequence  parallel  hierarchical buffer  fully utilize memory bandwidth  propose  hierarchical buffer   core data structure dfa use   hit detection  show  figure  dfa consist   state   finite state machine   query position   state   state  query position  highly reuse  hit detection   word load dfa  share memory  improve  data access bandwidth however   number  query position depend   query length prefetching  position   share memory may affect  occupancy  gpu kernels  offset  improvement  higher data access bandwidth especially   long sequence thus  load  state  relatively fix  small size   share memory  store query position   constant memory   latest nvidia kepler gpu  48kb readonly cache  relax memory coalesce rule  introduce   reusable  randomly access data  allocate  query position   global memory  tag    keyword “const restrict”  load    readonly cache automatically figure  illustrate  hierarchical buffer architecture  dfa  kepler gpu  put  dfa state  “abb”  “abc”   share memory   first access  “abc”  thread   position  set  bin  load   readonly cache   follow access  “abc”  thread   position   obtain   cache pss matrix  score matrix  core data structure highly reuse   ungapped extension  number  column  pss matrix  equal   length   query sequence since  column contain  bytes  row   bytes    size  pss matrix increase quickly   query length  48kb share memory cannot hold  pss matrix   query sequence longer   furthermore  many share memory usage  degrade  performance due   degrade gpu occupancy    hand   score matrix  use  substitute  pss matrix  score matrix   fix 2kb size   always fit   share memory   memory access use score matrix could decrease  performance compare  use pss matrix  short sequence thus  provide  configurable parameter  select pss matrix  score matrix   pss matrix  put    share memory   threshold    put    global memory   score matrix  always put    share memory",0
5,muBLASTP,"muBLASTP: database-indexed protein sequence search on multicore CPUs
Database index The most challenging component of muBLASTP is the design of the database index. The index should include the positions of overlapping words from all subject sequences of the database. Thus, each position contains the information for the sequence id and the offset in the subject sequence, i.e., subject offset. For the protein sequence search, the BLASTP algorithm uses the small word size (W=3), large alphabet size (22 letters), and neighboring word comparisons. Because these factors may make the database index very large, we design our database index with the following techniques: index blocking, sorting, and compression. Index blocking Figure 1 a illustrates the design of index blocking. We first sort the database by the sequence length; partition the database into small blocks, where each block has the same number of letters; and then build the index for each block separately. In this way, the search algorithm can go through the index blocks one by one and merge the high-scoring results of each block in the final stage. Index blocking can enable the database index to fit into main memory, especially for large databases whose total index size exceeds the size of main memory. By configuring the size of the index block, we can achieve better performance. For example, if the index block is small enough to fit into the CPU cache, the hit detection and gapped and ungapped extension may achieve better data locality. Another benefit of using index blocking is to reduce the index size. Without index blocking and assuming a total of M sequences in the database, we need log2M bits to store sequence ids. After dividing the database into N blocks, each block contains 𝑀𝑁 sequences on average. Thus, we only need log2⌈𝑀𝑁⌉ bits to store sequence ids. For example, if there are 220 sequences in a database, we need 20 bits to represent the sequence ids. With 28 blocks, if each block contains 212 sequences, then we only need a maximum of 12 bits to store the sequence ids. In addition, because the number of bits for storing subject offsets is determined by the longest sequences in each block, after sorting the database by the sequence length, we can use fewer bits for subject offsets in the blocks having short and medium sequences, and more bits only for the blocks having extremely long sequences. (This is the reason why we sort the database by the sequence length). Furthermore, index blocking allows us to parallelize the BLASTP algorithm via the mapping of one block to a thread on a modern multicore processor. For this block-wise parallel method to achieve the ideal load balance, we partition index blocks equally to make each block have a similar number of letters, instead of an identical number of sequences. To avoid cutting a sequence in the middle, if this sequence reaches the cap of the block size, we put it into the next block. After the database is partitioned into blocks, each block is indexed individually. As shown in Fig. 1 b, the index consists of two parts: the lookup table and the position array. The lookup table contains a w entries, where a is the alphabet size of amino acids and w is the length of the words. Each entry contains an offset to the starting position of the corresponding word. In the position array, a position of the word consists of the sequence id and the subject offset. For protein sequence search, the BLASTP algorithm not only searches the hits of exactly matched words, but it also searches the neighboring words, which are similar words. The query index used in existing BLAST tools, e.g., NCBI BLAST, includes the positions of neighboring words in the lookup table. However, for the database index in muBLASTP, if we store the positions for the neighboring words, the total size of the index becomes extraordinarily large. To address this problem, instead of storing positions of the neighboring words in the index, we put the offsets, which point to the neighboring words of every word, into the lookup table. The hit detection stage then goes through the positions of neighbors via the offsets after visiting the current word. In this way, we use additional stride memory accesses to reduce the total memory footprint for the index. Index compression As shown in Fig. 1 b, a specific subject offset for a word may be repeated in multiple sequences. For example, the word “ABC” appears in position 0 of sequence 1 and 3. In light of this repetition, it is possible to compress the index by optimizing the storage of subject offsets. Next, we sort the position array by the subject offset to group the same subject offsets together, as shown in Fig. 1 c. After that, we reduce the index size via merging the repeated subject offsets: for each word, we store the subject offset and the number of positions once and store the corresponding sequence ids sequentially, as shown in Fig. 1 d. After the index merging, we only need a small array for the sorted subject offsets. Furthermore, because the index is sorted by subject offsets, instead of storing the absolute value of subject offsets, we store the incremental subject offsets, as noted in Fig. 1 e, and only use eight (8) bits for the incremental subject offsets. Because the number of positions for a specific subject offset in one block is generally less than 256, we can also use eight (8) bits for the number of positions. Thus, in total, we only need a 16-bit integer to store a subject offset and its number of positions. However, this compressed method presents a challenge. When we use eight (8) bits each for the incremental subject offset and the number of repeated positions, there still exist a few cases that the increment subject offsets or the number of repeated positions is larger than 255. When such situations are encountered, we split one position entry into multiple entries to make the value less than 255. For example, as shown in Fig. 2 a, if the increment subject offset is 300 with 25 positions, then we split the subject offset into two entries, where the first entry has the incremental subject offset 255 and the number of repeated position 0, and the second entry has the incremental subject offset 45 for the 25 positions. Similarly, as shown in Fig. 2 b, for 300 repeated number of positions, the subject offset is split into two entries, where the first entry has the incremental subject offset 2 for 255 positions, but the second has the incremental subject offset 0 for an additional 45 positions. Optimized BLASTP algorithm with database index Because the BLASTP search algorithm introduces a more irregular memory access pattern when using a database index (rather than a query index), we propose and realize hit reordering with two-level binning in order to mitigate the irregular memory access pattern and irregular control flow, especially for the two-hit ungapped extension. Hit reordering with two-level binning The two-hit ungapped extension in protein sequence search requires searching for two-hit pairs, where two hits are on the same diagonal and close together, to trigger ungapped extensions. The traditional method, namely the last-hit array-based method, is commonly used in query-indexed BLAST. The last-hit array method uses an array to record the last hit of each diagonal. When a new hit is detected, the algorithm checks the distance between the newly found hit and the last hit in the same diagonal of the last-hit array and updates the last hit with the new hit. Although the algorithm scans the subject sequence from the beginning to the end, the diagonal access for a new hit can be random. The random memory accesses on last-hit arrays is a critical problem for database-indexed BLAST, which aligns a query to thousands of subject sequences at once (rather than aligning a subject sequence to a single query, as is done in query-indexed BLAST). Therefore, to improve the performance of finding two-hit pairs, we propose a new method that reorders hits with two-level binning. As shown in Fig. 3, each bin is mapped to a diagonal in the first level of binning, and the hits are grouped into bins by diagonal ids, which are calculated by subject offsets minus query offsets. Because query offsets can be calculated by subject offsets minus diagonal ids, we only store the sequence ids and subject offsets directly from the index in order to to minimize memory usage. After the first-level binning, hits having the same diagonal ids are placed into the same bins. However, in each bin, the hits from different sequences are interleaved. Thus, we design a second level of binning to reorder the hits by sequence ids. In contrast to first-level binning, where the bin id is equal to the diagonal id, second-level binning sets the bin id to the sequence id. Because we scan the bins of the first-level binning one by one, the hits in a second-level bin are sorted naturally by the diagonal id. As shown in Fig. 4, a hit in the second-level bin contains the subject offset and the diagonal id. With the second-level binning, hits from different sequences are put into different bins and sorted by diagonal ids. After that, we can quickly detect two-hit pairs by scanning every second-level bin. To improve the performance of the two-hit ungapped extension further, we filter out the hits that cannot be used to trigger the ungapped extension (instead of directly putting all the hits into the second-level bins). This optimization, as captured in Fig. 4, can dramatically reduce processing overhead by reducing memory usage, and in turn, improving performance. Specifically, before writing a hit into a second-level bin, we check its distance to the last hit in last-hit array. Only if the distance of the current hit to the last hit satisfies the distance thresholds, i.e., less than threshold_A and greater than or equal to overlap, the hit can be put into the second-level bins. As the number of sequences in a index block can be adjusted by configuring the size of the index block, the size of the last-hit array may be small enough to fit in the cache: not only in the last-level cache (LLC) on the Haswell CPU in our evaluation but also in the L2 cache. As a result, this optimization to ungapped extension exhibits excellent data locality when accessing the reordered hits, thus improving performance. Moreover, because our optimization filters out the majority of hits, we also significantly reduce the time spent on memory-write operations, and in turn, improve performance further. If the subject offsets are unsorted in the database index, as shown in Fig. 5 a, the binning method can introduce random memory accesses, which would adversely impact performance. However, sorting the subject offsets in the database index, as shown in in Fig. 1 c, can resolve this problem. Once the index sorting is complete, as shown in Fig. 5 b, both the reads on the database index and the writes on the first-level binning are contiguous, thus improving the binning performance via better data locality. Optimizations via multithreading In BLAST algorithm, the query sequence is aligned to each subject sequence in the database independently and iteratively. Thus, we can parallelize the BLAST algorithm with OpenMP multithreading on the multicore processors in a compute node, e.g., our pair of 12-core Intel Haswell CPUs or 24 cores in total. However, achieving robust scalability on such multicore processors is non-trivial, particularly for a data-/memory-intensive program like BLAST, which also introduces irregular memory access patterns as well as irregular control flow. At a high level, two major challenges exist for parallelizing BLAST within a compute node: (1) cache and memory contention among threads on different cores and (2) load balancing among these threads. Because the alignment on each query is independent, a straightforward approach to parallelization maps the alignment of each query to a thread. However, this approach results in different threads potentially accessing different index blocks at the same time. In light of the limited cache size, this approach results in severe cache contention between threads. To mitigate this cache contention and maximize cache-sharing across threads, we exchange execution order, as shown in Algorithm 1. That is, the first two stages, i.e., hit detection and ungapped extension, which share the same database index, access the same database block for all batch query sequences (from Line 6 to 10). So, we apply the OpenMP pragma on the inner loop to make different threads process different input query sequences but on the same index block. Then, threads on different cores may share the database index that is loaded into memory and even cache. The aligned results for each index block are then merged together for the final alignment with traceback, For better load balancing, and in turn, better performance, we leverage the fact that we already have a sorted database with respect to sequence lengths. We then partition this database into blocks of equal size and leverage OpenMP dynamic scheduling. Discussion In muBLASTP, we use the composition-based statistics presented in [27], which is also the default method used in NCBI BLAST. For other composition-based statistics methods in NCBI BLAST, such as [28], our current code base does not support it. We leave this work for the future versions. Moreover, the current version of muBLASTP can only produce the identical results to NCBI BLAST when both use the default output format (i.e., “pairwise” format) and the default composition-based statistics method. As a result, our software can only generate the similar results to NCBI BLAST if any other parameter is set. In the future updates of this software, we will add the supports for different formats, making muBLASTP to be a comprehensive tool as NCBI BLAST.",Alignment,"mublastp databaseindexed protein sequence search  multicore cpus
database index   challenge component  mublastp   design   database index  index  include  position  overlap word   subject sequence   database thus  position contain  information   sequence    offset   subject sequence  subject offset   protein sequence search  blastp algorithm use  small word size  large alphabet size  letter  neighbor word comparisons   factor may make  database index  large  design  database index   follow techniques index block sort  compression index block figure   illustrate  design  index block  first sort  database   sequence length partition  database  small block   block    number  letter   build  index   block separately   way  search algorithm     index block one  one  merge  highscoring result   block   final stage index block  enable  database index  fit  main memory especially  large databases whose total index size exceed  size  main memory  configure  size   index block   achieve better performance  example   index block  small enough  fit   cpu cache  hit detection  gap  ungapped extension may achieve better data locality another benefit  use index block   reduce  index size without index block  assume  total   sequence   database  need log2m bits  store sequence ids  divide  database   block  block contain  sequence  average thus   need log2⌈⌉ bits  store sequence ids  example     sequence   database  need  bits  represent  sequence ids   block   block contain  sequence    need  maximum   bits  store  sequence ids  addition   number  bits  store subject offset  determine   longest sequence   block  sort  database   sequence length   use fewer bits  subject offset   block  short  medium sequence   bits    block  extremely long sequence    reason   sort  database   sequence length furthermore index block allow   parallelize  blastp algorithm via  map  one block   thread   modern multicore processor   blockwise parallel method  achieve  ideal load balance  partition index block equally  make  block   similar number  letter instead   identical number  sequence  avoid cut  sequence   middle   sequence reach  cap   block size  put    next block   database  partition  block  block  index individually  show  fig    index consist  two part  lookup table   position array  lookup table contain   entries     alphabet size  amino acids     length   word  entry contain  offset   start position   correspond word   position array  position   word consist   sequence    subject offset  protein sequence search  blastp algorithm   search  hit  exactly match word   also search  neighbor word   similar word  query index use  exist blast tool  ncbi blast include  position  neighbor word   lookup table however   database index  mublastp   store  position   neighbor word  total size   index become extraordinarily large  address  problem instead  store position   neighbor word   index  put  offset  point   neighbor word  every word   lookup table  hit detection stage  go   position  neighbor via  offset  visit  current word   way  use additional stride memory access  reduce  total memory footprint   index index compression  show  fig    specific subject offset   word may  repeat  multiple sequence  example  word “abc” appear  position   sequence     light   repetition   possible  compress  index  optimize  storage  subject offset next  sort  position array   subject offset  group   subject offset together  show  fig      reduce  index size via merge  repeat subject offset   word  store  subject offset   number  position   store  correspond sequence ids sequentially  show  fig     index merge   need  small array   sort subject offset furthermore   index  sort  subject offset instead  store  absolute value  subject offset  store  incremental subject offset  note  fig     use eight  bits   incremental subject offset   number  position   specific subject offset  one block  generally less     also use eight  bits   number  position thus  total   need  bite integer  store  subject offset   number  position however  compress method present  challenge   use eight  bits    incremental subject offset   number  repeat position  still exist   case   increment subject offset   number  repeat position  larger     situations  encounter  split one position entry  multiple entries  make  value less    example  show  fig     increment subject offset     position   split  subject offset  two entries   first entry   incremental subject offset    number  repeat position    second entry   incremental subject offset     position similarly  show  fig     repeat number  position  subject offset  split  two entries   first entry   incremental subject offset    position   second   incremental subject offset    additional  position optimize blastp algorithm  database index   blastp search algorithm introduce   irregular memory access pattern  use  database index rather   query index  propose  realize hit reorder  twolevel bin  order  mitigate  irregular memory access pattern  irregular control flow especially   twohit ungapped extension hit reorder  twolevel bin  twohit ungapped extension  protein sequence search require search  twohit pair  two hit     diagonal  close together  trigger ungapped extensions  traditional method namely  lasthit arraybased method  commonly use  queryindexed blast  lasthit array method use  array  record  last hit   diagonal   new hit  detect  algorithm check  distance   newly find hit   last hit    diagonal   lasthit array  update  last hit   new hit although  algorithm scan  subject sequence   begin   end  diagonal access   new hit   random  random memory access  lasthit array   critical problem  databaseindexed blast  align  query  thousands  subject sequence   rather  align  subject sequence   single query     queryindexed blast therefore  improve  performance  find twohit pair  propose  new method  reorder hit  twolevel bin  show  fig   bin  map   diagonal   first level  bin   hit  group  bin  diagonal ids   calculate  subject offset minus query offset  query offset   calculate  subject offset minus diagonal ids   store  sequence ids  subject offset directly   index  order   minimize memory usage   firstlevel bin hit    diagonal ids  place    bin however   bin  hit  different sequence  interleave thus  design  second level  bin  reorder  hit  sequence ids  contrast  firstlevel bin   bin   equal   diagonal  secondlevel bin set  bin    sequence    scan  bin   firstlevel bin one  one  hit   secondlevel bin  sort naturally   diagonal   show  fig   hit   secondlevel bin contain  subject offset   diagonal    secondlevel bin hit  different sequence  put  different bin  sort  diagonal ids     quickly detect twohit pair  scan every secondlevel bin  improve  performance   twohit ungapped extension   filter   hit  cannot  use  trigger  ungapped extension instead  directly put   hit   secondlevel bin  optimization  capture  fig   dramatically reduce process overhead  reduce memory usage   turn improve performance specifically  write  hit   secondlevel bin  check  distance   last hit  lasthit array    distance   current hit   last hit satisfy  distance thresholds  less  threshold_a  greater   equal  overlap  hit   put   secondlevel bin   number  sequence   index block   adjust  configure  size   index block  size   lasthit array may  small enough  fit   cache     lastlevel cache llc   haswell cpu   evaluation  also    cache   result  optimization  ungapped extension exhibit excellent data locality  access  reorder hit thus improve performance moreover   optimization filter   majority  hit  also significantly reduce  time spend  memorywrite operations   turn improve performance    subject offset  unsorted   database index  show  fig    bin method  introduce random memory access  would adversely impact performance however sort  subject offset   database index  show   fig    resolve  problem   index sort  complete  show  fig     read   database index   write   firstlevel bin  contiguous thus improve  bin performance via better data locality optimizations via multithreading  blast algorithm  query sequence  align   subject sequence   database independently  iteratively thus   parallelize  blast algorithm  openmp multithreading   multicore processors   compute node   pair  core intel haswell cpus   core  total however achieve robust scalability   multicore processors  nontrivial particularly   datamemoryintensive program like blast  also introduce irregular memory access pattern  well  irregular control flow   high level two major challenge exist  parallelize blast within  compute node  cache  memory contention among thread  different core   load balance among  thread   alignment   query  independent  straightforward approach  parallelization map  alignment   query   thread however  approach result  different thread potentially access different index block    time  light   limit cache size  approach result  severe cache contention  thread  mitigate  cache contention  maximize cachesharing across thread  exchange execution order  show  algorithm     first two stag  hit detection  ungapped extension  share   database index access   database block   batch query sequence  line      apply  openmp pragma   inner loop  make different thread process different input query sequence     index block  thread  different core may share  database index   load  memory  even cache  align result   index block   merge together   final alignment  traceback  better load balance   turn better performance  leverage  fact   already   sort database  respect  sequence lengths   partition  database  block  equal size  leverage openmp dynamic schedule discussion  mublastp  use  compositionbased statistics present     also  default method use  ncbi blast   compositionbased statistics methods  ncbi blast     current code base   support   leave  work   future versions moreover  current version  mublastp   produce  identical result  ncbi blast   use  default output format  “pairwise” format   default compositionbased statistics method   result  software   generate  similar result  ncbi blast    parameter  set   future update   software   add  support  different format make mublastp    comprehensive tool  ncbi blast",0
6,Pauda,"A poor man’s BLASTX—high-throughput metagenomic protein database search using PAUDA
In metagenomics studies, millions of DNA or cDNA reads are sequenced from environmental samples, and these are then analyzed in an attempt to determine the functional or taxonomic content of the samples (Handelsman et al., 1998). An important computational step is to determine the genes or coding sequences present, which is usually done by aligning the sequences against a reference database of protein sequences. In most projects, BLASTX (Altschul et al., 1990) has been the method of choice, despite the fact that running BLASTX requires thousands of CPU hours per million reads. In the related area of read mapping, numerous methods have been developed to solve the problem of aligning sequencing reads against DNA reference sequences in a high-throughput manner (for example, Langmead and Salzberg, 2012). Using read mapping tools directly for analyzing complex metagenomes is problematic because environmental reads usually do not match existing genome reference sequences. Moreover, the underlying algorithms cannot easily be extended to protein sequences. In this article, we present a new paradigm for the alignment of environmental sequencing reads called PAUDA, an acronym for ‘protein alignment using a DNA aligner’. It allows one to harness the high efficiency of DNA read aligners to compute BLASTXlike alignments. The key idea is to convert all protein sequences into ‘pseudo DNA’, or ‘pDNA’ for short, by mapping the amino acid alphabet onto a four-lettered alphabet that reflects which amino acids are likely to replace each other in significant BLASTX alignments. A high-throughput sequencing read aligner, such as Bowtie2, is then used to compare pDNA reads with a pDNA database. For any match found, the participating pDNA sequences are translated back into protein sequences, and the corresponding protein alignment is calculated so as to determine statistical significance. The final output is a file of statically significant protein alignments in BLASTX format. We have implemented this approach in a new software package called PAUDA. The package provides two scripts, paudabuild and pauda-run. The first script is run on the protein reference database and builds an appropriate index. The second script is run on a file of DNA reads and produces a BLASTX file as output. The two scripts use the Bowtie2 suite and a number of new Java programs that we have written. Bowtie2 can easily be replaced by some other method, if desired. An overview of the package is given in Figure 1. Using Bowtie2 as the comparison engine, PAUDA runs 10 000 times faster than BLASTX, while assigning about one-third as many reads to KO groups. Because of the huge computational burden of running BLASTX on a large dataset, BLASTX is rarely run to completion; therefore, the key question is how many reads can be assigned per hour. PAUDA assigns 3000 as many reads as BLASTX does, per hour. Mackelprang et al. (2011) present a taxonomic and functional analysis of 12 permafrost datasets. Reanalysis of their data, a comparison of 246 million Illumina reads with the KEGG database (Kanehisa and Goto, 2000), takes 2 h on a single workstation (64 cores, 512 GB of main memory) using PAUDA, reproducing the main result of the article. In addition, we applied an early version of PAUDA to an unpublished dataset consisting of all 2.9 billion reads of a whole HiSeq2000 run on a waste-waster sample, requiring 2 days on 150 cores, whereas RAPSearch2 (Zhao et al., 2012) required 115 days. We produced a benchmark dataset for comparing the performance of PAUDA, BLASTX and RAPSearch2 by taking the first 600 000 good quality reads from each of the 12 samples published in Mackelprang et al. (2011). We then ran all three programs on each of the 12 samples benchmark samples, comparing with the KEGG database. Running all samples in parallel on a single workstation using 48 cores, the runtime ranged from 7 min (PAUDA) to 7 days (BLASTX), Table 1. We used the metagenome analysis program MEGAN (Huson et al., 2011) to assign reads to KEGG orthology (KO) groups based on their alignments. Using PAUDA, the rate of assignment is 33% of that of BLASTX. In more detail, for alignments with a protein identity of 60, 70, 80, 90 and 100% the sensitivity is 35.1, 48.4, 61.6 and 78.5%, respectively. For alignments with identity 550%, the sensitivity is58.1%. For those reads for which both BLASTX and PAUDA are able to assign a KO group, the assignment differs in 2% of all cases. Assuming a false-positive error rate of 1% for the assignment of reads to KO groups, BLASTX identifies 1735 ‘true’ KO groups for this dataset that account for 99% of all reads with BLASTX hits. PAUDA identifies 99% of these. The number of reads assigned to individual KO groups by PAUDA and BLASTX is highly correlated, as shown in Figure 2. The Pearson correlation is 0.977 for linear read counts and 0.949 for log-transformed counts. Using the LCA assignment algorithm as implemented in MEGAN, we also performed a taxonomic analysis of these datasets at a number of different taxonomic ranks. The results based on PAUDA and BLASTX are highly correlated, with a Pearson’s correlation coefficient r that ranges from 0.993 for the taxonomic rank of class to 0.953 for species. The corresponding range for log-transformed counts is 0.982–0.914. To further illustrate the accuracy of PAUDA, we applied the program to all 12 permafrost samples in their entirety, in total comparing 246 million reads with the KEGG database. A key result of (Mackelprang et al., 2011) is that, on the one hand, two different frozen samples taken from the active layer of the permafrost have similar functional profiles, and that these change only little after thawing for 2 or 7 days. Although, on the other hand, two frozen samples obtained from the permafrost layer initially exhibit distinctive profiles that gradually become more similar during thawing. A PCoA analysis of Bray–Curtis distances (Mitra et al., 2010) based on a PAUDA comparison of the data with the KEGG database delivers the same result in a small fraction of the computational time.",Alignment," poor man blastx—highthroughput metagenomic protein database search use pauda
 metagenomics study millions  dna  cdna read  sequence  environmental sample     analyze   attempt  determine  functional  taxonomic content   sample handelsman     important computational step   determine  genes  cod sequence present   usually   align  sequence   reference database  protein sequence   project blastx altschul       method  choice despite  fact  run blastx require thousands  cpu hours per million read   relate area  read map numerous methods   develop  solve  problem  align sequence read  dna reference sequence   highthroughput manner  example langmead  salzberg  use read map tool directly  analyze complex metagenomes  problematic  environmental read usually   match exist genome reference sequence moreover  underlie algorithms cannot easily  extend  protein sequence   article  present  new paradigm   alignment  environmental sequence read call pauda  acronym  protein alignment use  dna aligner  allow one  harness  high efficiency  dna read aligners  compute blastxlike alignments  key idea   convert  protein sequence  pseudo dna  pdna  short  map  amino acid alphabet onto  fourlettered alphabet  reflect  amino acids  likely  replace    significant blastx alignments  highthroughput sequence read aligner   bowtie2   use  compare pdna read   pdna database   match find  participate pdna sequence  translate back  protein sequence   correspond protein alignment  calculate    determine statistical significance  final output   file  statically significant protein alignments  blastx format   implement  approach   new software package call pauda  package provide two script paudabuild  paudarun  first script  run   protein reference database  build  appropriate index  second script  run   file  dna read  produce  blastx file  output  two script use  bowtie2 suite   number  new java program    write bowtie2  easily  replace    method  desire  overview   package  give  figure  use bowtie2   comparison engine pauda run   time faster  blastx  assign  onethird  many read   group    huge computational burden  run blastx   large dataset blastx  rarely run  completion therefore  key question   many read   assign per hour pauda assign   many read  blastx  per hour mackelprang    present  taxonomic  functional analysis   permafrost datasets reanalysis   data  comparison   million illumina read   kegg database kanehisa  goto  take     single workstation  core    main memory use pauda reproduce  main result   article  addition  apply  early version  pauda   unpublished dataset consist    billion read   whole hiseq2000 run   wastewaster sample require  days   core whereas rapsearch2 zhao    require  days  produce  benchmark dataset  compare  performance  pauda blastx  rapsearch2  take  first   good quality read      sample publish  mackelprang      run  three program      sample benchmark sample compare   kegg database run  sample  parallel   single workstation use  core  runtime range   min pauda   days blastx table   use  metagenome analysis program megan huson     assign read  kegg orthology  group base   alignments use pauda  rate  assignment      blastx   detail  alignments   protein identity         sensitivity       respectively  alignments  identity   sensitivity is58   read    blastx  pauda  able  assign   group  assignment differ     case assume  falsepositive error rate     assignment  read   group blastx identify  true  group   dataset  account     read  blastx hit pauda identify     number  read assign  individual  group  pauda  blastx  highly correlate  show  figure   pearson correlation    linear read count    logtransformed count use  lca assignment algorithm  implement  megan  also perform  taxonomic analysis   datasets   number  different taxonomic rank  result base  pauda  blastx  highly correlate   pearsons correlation coefficient   range     taxonomic rank  class    species  correspond range  logtransformed count     illustrate  accuracy  pauda  apply  program    permafrost sample   entirety  total compare  million read   kegg database  key result  mackelprang        one hand two different freeze sample take   active layer   permafrost  similar functional profile    change  little  thaw     days although    hand two freeze sample obtain   permafrost layer initially exhibit distinctive profile  gradually become  similar  thaw  pcoa analysis  braycurtis distance mitra    base   pauda comparison   data   kegg database deliver   result   small fraction   computational time",0
7,Diamond,"Fast and sensitive protein alignment using DIAMOND
Overview of DIAMOND. DIAMOND is a high-throughput alignment program that compares a file of DNA sequencing reads against a file of protein reference sequences, such as NCBI-nr19 or KEGG3. It is implemented in C++ and is designed to run on multicore servers. The software can be obtained at http://ab.inf. uni-tuebingen.de/software/diamond. DIAMOND is 4 orders of magnitude faster than BLASTX4 at comparing short DNA reads against the NCBI-nr database and maintains a comparable level of sensitivity on alignments with an e-value <10−3. The program is explicitly designed to make use of modern computer architectures that have large memory capacity and many cores. It follows the seed-and-extend approach. Additional algorithmic ingredients are the use of a reduced alphabet, spaced seeds and double indexing. Seed and extend. The program is based on the traditional seed-and-extend paradigm for sequence comparison, in which exact occurrences of seeds (that is, short words of a given fixed length) contained in query sequences are located in the reference sequences, and these seed matches are then extended, if possible, to full alignments between the queries and references. The seed length used by an alignment program has a substantial impact on performance; shorter seeds increase sensitivity, whereas longer seeds increase speed. Reduced alphabet. To increase speed without losing sensitivity, one approach is to use a reduced alphabet when comparing seeds. Using this, RAPSearch2 (ref. 7) is 40–100 times faster than BLASTX with minimal loss of sensitivity. For DIAMOND, we investigated the use of published reductions to four, eight and ten letters12. By analyzing a large number of BLASTX alignments, we developed a new reduction to an alphabet of size 11 that achieves slightly better sensitivity (brackets indicate one letter): [KREDQN] [C] [G] [H] [ILV] [M] [F] [Y] [W] [P] [STA]. Spaced seeds. A second improvement of the seed step is to use spaced seeds—that is, longer seeds in which only a subset of positions are used. The number and exact layout of those positions are called the weight and shape of the spaced seed, respectively. Theoretical analysis shows that a single spaced seed can perform better than a contiguous seed of the same weight, if its shape is suitably chosen10. Moreover, sensitivity can be increased further by using additional seed shapes, each resulting only in a sublinear increase in running time10. By default, DIAMOND uses a set of four shapes11 of length 15–24 and weight 12 (Supplementary Fig. 1). Seed index. The main bottleneck in aligning a large number of reads against a large reference database is not CPU performance but rather memory latency and bandwidth. The limiting factor is the amount of time required to load seed locations from main memory for comparison. Moving data from main memory into the cache takes hundreds of CPU clock cycles. Thus, a fast algorithm should take the cache hierarchy of computers into account so as to maximize data locality and minimize the number of main memory accesses. This can be done by decomposing the problem into small subproblems that fit into the cache. In most seed-and-extend programs, an index structure (such as hash table or FM index) is built on the reference sequences to facilitate the location of seeds in the references. Queries are processed in the order that they occur in the input file. For a given query, all seeds are determined and the index is then used to look up all matching locations in the reference sequences. The reference locations are loaded from main memory into the cache. When another read is later encountered that contains some of the same seeds, the corresponding index and sequence data will usually have been evicted from the cache, so this data will have to be loaded from main memory again. Hence, using a single index in this way does not make good use of the cache. The problem is compounded when using a full-text index, such as a suffix array or a compressed FM index, as these require multiple individual memory accesses per index lookup. Double indexing. DIAMOND uses a double-indexing approach in which both the queries and the references are indexed. A DIAMOND index is a list of seed-location pairs that is lexicographically ordered on a compressed representation of the seed. By traversing the two index lists linearly in parallel, all matching seeds between the queries and the references are identified, allowing the local alignment computation at the corresponding seed locations. The index memory access pattern of this approach is linear and can be efficiently handled by the hardware prefetcher, which will fill the cache with the indexing information before it is needed. The double-indexed approach also improves data locality with respect to accessing the sequences. To see this, let Sq and Sr denote the set of seeds contained in the set of queries and references, respectively. For a given seed s, let ms and ns be the number of occurrences of the seed in the queries and the references. Using the standard index approach, the number of memory access operations is approximately K = Σs∈Sq∩Sr msns, as for each occurrence of a reference seed in the queries, all corresponding reference locations will be loaded into memory. Using the double-indexing approach, the number of memory access operations is approximately K′ = Σs∈Sq∩Sr(ms + ns), assuming that the combined size of query and reference locations for one seed is small enough to fit into the cache. This number is much smaller than K unless the sum is dominated by singleton seeds. To demonstrate this effect on real data, we plot the ratio of memory accesses for the two approaches depending on the length of the query sequence in letters as observed on our benchmark data (Supplementary Fig. 2). The double-indexing algorithm used by DIAMOND is based on the well-known database sort-merge join algorithm, applied to the two seed sets of the queries and references. The main computation, the compilation and sorting of the two lists of seeds, can be efficiently addressed in parallel using a radix clustering step20 in combination with a fast sorting algorithm. The total amount of time required to sort all the seeds in the given set of queries is smaller than what is required to access all seeds in a hash-table approach. Further, sorting all seeds in the reference sequences ‘on the fly’ takes much less time than loading a precomputed index. For example, the complete seed index for the current version of the NCBI-nr database is about 100 GB in size. This takes 100 s to generate in memory and about six times as long to read from disk at a typical read rate of 150 MB per second. The alignment program mrsFast uses sorted lists as an index structure21. The authors of that tool spent a lot of effort on making their algorithm cache oblivious. Much of the challenge discussed there stems from the fact that short nonoverlapping seeds were used, which causes the set of all occurrences of a seed to exceed the cache capacity. Owing to the more elaborate seed strategy used by DIAMOND, in our program the amount npg © 2015 Nature America, Inc. All rights reserved. nature methods doi:10.1038/nmeth.3176 of data associated with a given seed will always be small enough to fit into the cache. Memory efficiency. A drawback of using multiple spaced seeds is that this uses a lot of memory, which is a main reason why this approach has not been widely used, despite its proven advantages. The naïve implementation of a multiple spaced seed index builds a hash-table index for each of the seed shapes. With one hash table index for the NCBI-nr database being about 100 GB in size, four seed shapes would consume 400 GB, and 16 shapes would consume 1.6 TB of memory. DIAMOND constructs and processes its indexes for one shape at a time, freeing up the memory used by one shape before moving on to the next. Thus, DIAMOND can perform alignment tasks with its sensitive 16-shape configuration using only as much memory as one shape index requires, which is an additional advantage of our approach. Moreover, using the radix cluster technique, the seed space is decomposed into 1,024 disjoint partitions. By building and processing indexes for only a subset of these partitions at the same time, the memory usage will be limited to the size of the subset index. Seed extension. For each seed match found, DIAMOND determines whether it can be extended to an ungapped alignment of ten or more amino acids. If this is the case, then the seed match triggers the extend phase of the algorithm, which involves computing a Smith-Waterman alignment13. DIAMOND uses its own streaming SIMD extension (SSE)-accelerated SmithWaterman implementation that extends previous algorithms22 to allow the computation of banded and anchored alignments. By default, the program uses the BLOSUM62 matrix23, a gap score of 11 and an extension score of 1, however, other BLOSUM matrices and scoring parameters can be used. The program determines the bit score and expected value of the computed alignment as in BLASTX. By default, alignments with a bit score <50 are not reported. Because DIAMOND proceeds seed by seed rather than read by read, a key issue is how to avoid computing the same local alignment between a read and a reference more than once at different times during the search phase. To address this, DIAMOND allows a seed match to trigger an extension only if it is the left-most seed match in the corresponding ungapped alignment. Experimental study. We downloaded the NCBI-nr database, which consists of 25.9 million sequences and 8.9 billion letters, in May 2013 to use as reference database. We downloaded ten files of Illumina reads from the Human Microbiome Project website (http://www.hmpdacc.org/) covering samples from a range of different human-associated microbiomes (SRA SRS011134, SRS013687, SRS013951, SRS015578, SRS042628, SRS011239, SRS013800, SRS015369, SRS016753 and SRS053335). We extracted 500,000 random reads from each file so as to obtain a total set of 5 million reads, of average length 101. We refer to this as the Illumina (HMP) data set. We downloaded 12 Illumina data sets associated with permafrost cores14 from the US Department of Energy Joint Genome Institute website. We extracted 500,000 random reads from each file so as to obtain a total set of 6 million reads, of average length 114, to use as our benchmark set of read sequences. We refer to this as our Illumina (permafrost) data set. We downloaded a single data set of Ion Torrent reads from a study entitled ‘Metagenome from artisanal cheeses from Tucuman’ from NCBI (ERP004234). We downloaded two data sets of 454 Titanium reads from a study entitled ‘Microbial community gene content and expression in the Central North Pacific Gyre, Station ALOHA, HOT186’ from NCBI, with accession numbers SRR1298978 and SRR1298979. We downloaded a single data set of Sanger reads from the Sargasso Sea project17. We download a set of contigs from a microbial assembly18 and used MetaGeneMark24 to predict a total of 30,000 open reading frames (ORFs). These data sets were used to compare the performance of DIAMOND (version 0.4.7), RAPSearch2 (version 2.18) and BLASTX (version 2.2.28+). All three programs were run on the same computer using 48 cores of a 64 core AMD Opteron server with 512 GB of main memory, running Ubuntu 12.04. The following parameter settings were used for the programs. BLASTX: blastx _num_threads 48 -evalue 0.1 -max_target_seqs 250 -comp_based_stats 0; DIAMOND-fast: diamond blastx -t 48 -k 250 -min-score 40; DIAMOND-sensitive: diamond blastx -t 48 -k 250 -sensitive -min-score 40; RAPSearch2-fast: rapsearch -a T -z 48 -v 250 -e -1 -b 1; RAPSearch2-default: rapsearch -z 48 -v 250 -e -1 -b 1. To measure run times, BLASTX and RAPSearch2 were each run on three random subsets of each data set, and the run times were then extrapolated to the full data sets and averaged. DIAMOND was run on the full data sets (Fig. 1 and Supplementary Table 1). For Figure 1, we considered only alignments with a BLASTX e-value below 10−3. For Supplementary Table 1 we also considered only the subset of all alignments with a BLASTX e-value below 10−5. Alignments of poorer quality are usually not considered in metagenome analysis. The reported run times are the total wall-clock time required on 48 cores, minus the program’s overhead time. The overhead time of a program is measured by the wall clock time that the program requires to process an input file that contains only one read. This is a constant contribution to the run time that we ignore in our analysis because DIAMOND is intended to run on data sets that are much larger than our test data sets. For DIAMOND-fast, this is 5 min, and for DIAMOND-sensitive, this is 20 min. The reason we tested data sets that are small by practical standards was to allow us to run a full BLASTX analysis of each data set in a reasonable amount of time. Sensitivity on the level of queries (percentage of queries mapped) is based on the number of reads for which BLASTX and the test method both find at least one common alignment against some reference sequence, divided by the total number of reads for which BLASTX finds at least one alignment. Sensitivity on the level of matches (percentage of matches recovered) is based on the number of alignments found both by BLASTX and the test method, divided by the total number of alignments found by BLASTX. Principal coordinates (PCoA) analysis. To illustrate that minor differences between the output of BLASTX and DIAMOND do not affect the results of higher-level analyses, we used 12 random subsamples, each containing 200,000 reads, from 12 published permafrost samples14. Each subsample was aligned against NCBI-nr using both BLASTX and DIAMOND-fast, npg © 2015 Nature America, Inc. All rights reserved. doi:10.1038/nmeth.3176 nature methods keeping alignments with a minimum bit score of 50. Reads were mapped to KEGG Orthology (KO) numbers3, for each read using the best alignment for which a KO number is known. We then computed Bray-Curtis distances from the resulting profiles and used PCoA to generate diagrams (Supplementary Fig. 3). Memory usage and compatibility. The memory management of DIAMOND is designed to allow for an adaptable memory footprint that does not depend on the total size of the input. DIAMOND breaks down the input query and reference data into fixed size blocks of B sequence letters to be compared against each other at a time. With a seed index entry being 8 bytes long and the index being processed in C chunks, the total memory usage is then given by 2(B + 8B/C + const), where const represents a constant amount of overhead memory. With a default value of C = 4, the memory usage of the program is bounded by 6B + const irrespective of the total size of the database and queries. The block size B can be arbitrarily chosen by the user to fit the target machine. The command line options for B and C are -b and -c, respectively. To explore the effect of the block size parameter on the performance, we aligned a query set of 35 million Illumina reads from permafrost against the NCBI-nr database containing 9 billion letters using different values of B (Supplementary Table 2). This operation requires a high memory server for maximum performance but can be efficiently handled by a machine with 16 GB of memory at about half the speed. This amount of RAM is readily available at a price of $160 on a standard desktop computer",Alignment,"fast  sensitive protein alignment use diamond
overview  diamond diamond   highthroughput alignment program  compare  file  dna sequence read   file  protein reference sequence   ncbinr19  kegg3   implement     design  run  multicore servers  software   obtain   unituebingendesoftwarediamond diamond   order  magnitude faster  blastx4  compare short dna read   ncbinr database  maintain  comparable level  sensitivity  alignments   evalue   program  explicitly design  make use  modern computer architectures   large memory capacity  many core  follow  seedandextend approach additional algorithmic ingredients   use   reduce alphabet space seed  double index seed  extend  program  base   traditional seedandextend paradigm  sequence comparison   exact occurrences  seed   short word   give fix length contain  query sequence  locate   reference sequence   seed match   extend  possible  full alignments   query  reference  seed length use   alignment program   substantial impact  performance shorter seed increase sensitivity whereas longer seed increase speed reduce alphabet  increase speed without lose sensitivity one approach   use  reduce alphabet  compare seed use  rapsearch2 ref    time faster  blastx  minimal loss  sensitivity  diamond  investigate  use  publish reductions  four eight  ten letters12  analyze  large number  blastx alignments  develop  new reduction   alphabet  size   achieve slightly better sensitivity bracket indicate one letter kredqn    ilv      sta space seed  second improvement   seed step   use space seeds—  longer seed     subset  position  use  number  exact layout   position  call  weight  shape   space seed respectively theoretical analysis show   single space seed  perform better   contiguous seed    weight   shape  suitably chosen10 moreover sensitivity   increase   use additional seed shape  result    sublinear increase  run time10  default diamond use  set  four shapes11  length   weight  supplementary fig  seed index  main bottleneck  align  large number  read   large reference database   cpu performance  rather memory latency  bandwidth  limit factor   amount  time require  load seed locations  main memory  comparison move data  main memory   cache take hundreds  cpu clock cycle thus  fast algorithm  take  cache hierarchy  computers  account    maximize data locality  minimize  number  main memory access      decompose  problem  small subproblems  fit   cache   seedandextend program  index structure   hash table   index  build   reference sequence  facilitate  location  seed   reference query  process   order   occur   input file   give query  seed  determine   index   use  look   match locations   reference sequence  reference locations  load  main memory   cache  another read  later encounter  contain     seed  correspond index  sequence data  usually   evict   cache   data     load  main memory  hence use  single index   way   make good use   cache  problem  compound  use  fulltext index    suffix array   compress  index   require multiple individual memory access per index lookup double index diamond use  doubleindexing approach     query   reference  index  diamond index   list  seedlocation pair   lexicographically order   compress representation   seed  traverse  two index list linearly  parallel  match seed   query   reference  identify allow  local alignment computation   correspond seed locations  index memory access pattern   approach  linear    efficiently handle   hardware prefetcher   fill  cache   index information    need  doubleindexed approach also improve data locality  respect  access  sequence  see  let    denote  set  seed contain   set  query  reference respectively   give seed  let      number  occurrences   seed   query   reference use  standard index approach  number  memory access operations  approximately   ∈∩ msns    occurrence   reference seed   query  correspond reference locations   load  memory use  doubleindexing approach  number  memory access operations  approximately ′  ∈∩srms   assume   combine size  query  reference locations  one seed  small enough  fit   cache  number  much smaller   unless  sum  dominate  singleton seed  demonstrate  effect  real data  plot  ratio  memory access   two approach depend   length   query sequence  letter  observe   benchmark data supplementary fig   doubleindexing algorithm use  diamond  base   wellknown database sortmerge join algorithm apply   two seed set   query  reference  main computation  compilation  sort   two list  seed   efficiently address  parallel use  radix cluster step20  combination   fast sort algorithm  total amount  time require  sort   seed   give set  query  smaller    require  access  seed   hashtable approach  sort  seed   reference sequence   fly take much less time  load  precomputed index  example  complete seed index   current version   ncbinr database      size  take    generate  memory   six time  long  read  disk   typical read rate    per second  alignment program mrsfast use sort list   index structure21  author   tool spend  lot  effort  make  algorithm cache oblivious much   challenge discuss  stem   fact  short nonoverlapping seed  use  cause  set   occurrences   seed  exceed  cache capacity owe    elaborate seed strategy use  diamond   program  amount npg ©  nature america inc  right reserve nature methods doinmeth  data associate   give seed  always  small enough  fit   cache memory efficiency  drawback  use multiple space seed    use  lot  memory    main reason   approach    widely use despite  prove advantage  naïve implementation   multiple space seed index build  hashtable index     seed shape  one hash table index   ncbinr database      size four seed shape would consume     shape would consume    memory diamond construct  process  index  one shape   time free   memory use  one shape  move    next thus diamond  perform alignment task   sensitive shape configuration use   much memory  one shape index require    additional advantage   approach moreover use  radix cluster technique  seed space  decompose   disjoint partition  build  process index    subset   partition    time  memory usage   limit   size   subset index seed extension   seed match find diamond determine whether    extend   ungapped alignment  ten   amino acids     case   seed match trigger  extend phase   algorithm  involve compute  smithwaterman alignment13 diamond use   stream simd extension sseaccelerated smithwaterman implementation  extend previous algorithms22  allow  computation  band  anchor alignments  default  program use  blosum62 matrix23  gap score     extension score   however  blosum matrices  score parameters   use  program determine  bite score  expect value   compute alignment   blastx  default alignments   bite score    report  diamond proceed seed  seed rather  read  read  key issue    avoid compute   local alignment   read   reference     different time   search phase  address  diamond allow  seed match  trigger  extension      leftmost seed match   correspond ungapped alignment experimental study  download  ncbinr database  consist   million sequence   billion letter  may   use  reference database  download ten file  illumina read   human microbiome project website  cover sample   range  different humanassociated microbiomes sra srs011134 srs013687 srs013951 srs015578 srs042628 srs011239 srs013800 srs015369 srs016753  srs053335  extract  random read   file    obtain  total set   million read  average length   refer     illumina hmp data set  download  illumina data set associate  permafrost cores14    department  energy joint genome institute website  extract  random read   file    obtain  total set   million read  average length   use   benchmark set  read sequence  refer     illumina permafrost data set  download  single data set  ion torrent read   study entitle metagenome  artisanal cheese  tucuman  ncbi erp004234  download two data set   titanium read   study entitle microbial community gene content  expression   central north pacific gyre station aloha hot186  ncbi  accession number srr1298978  srr1298979  download  single data set  sanger read   sargasso sea project17  download  set  contigs   microbial assembly18  use metagenemark24  predict  total   open read frame orfs  data set  use  compare  performance  diamond version  rapsearch2 version   blastx version   three program  run    computer use  core    core amd opteron server     main memory run ubuntu   follow parameter settings  use   program blastx blastx _num_threads  evalue  max_target_seqs  comp_based_stats  diamondfast diamond blastx     minscore  diamondsensitive diamond blastx     sensitive minscore  rapsearch2fast rapsearch           rapsearch2default rapsearch          measure run time blastx  rapsearch2   run  three random subsets   data set   run time   extrapolate   full data set  average diamond  run   full data set fig   supplementary table   figure   consider  alignments   blastx evalue    supplementary table   also consider   subset   alignments   blastx evalue   alignments  poorer quality  usually  consider  metagenome analysis  report run time   total wallclock time require   core minus  program overhead time  overhead time   program  measure   wall clock time   program require  process  input file  contain  one read    constant contribution   run time   ignore   analysis  diamond  intend  run  data set   much larger   test data set  diamondfast    min   diamondsensitive    min  reason  test data set   small  practical standards   allow   run  full blastx analysis   data set   reasonable amount  time sensitivity   level  query percentage  query map  base   number  read   blastx   test method  find  least one common alignment   reference sequence divide   total number  read   blastx find  least one alignment sensitivity   level  match percentage  match recover  base   number  alignments find   blastx   test method divide   total number  alignments find  blastx principal coordinate pcoa analysis  illustrate  minor differences   output  blastx  diamond   affect  result  higherlevel analyse  use  random subsamples  contain  read   publish permafrost samples14  subsample  align  ncbinr use  blastx  diamondfast npg ©  nature america inc  right reserve doinmeth nature methods keep alignments   minimum bite score   read  map  kegg orthology  numbers3   read use  best alignment     number  know   compute braycurtis distance   result profile  use pcoa  generate diagram supplementary fig  memory usage  compatibility  memory management  diamond  design  allow   adaptable memory footprint    depend   total size   input diamond break   input query  reference data  fix size block   sequence letter   compare      time   seed index entry   bytes long   index  process   chunk  total memory usage   give    8bc  const  const represent  constant amount  overhead memory   default value      memory usage   program  bound    const irrespective   total size   database  query  block size    arbitrarily choose   user  fit  target machine  command line options         respectively  explore  effect   block size parameter   performance  align  query set   million illumina read  permafrost   ncbinr database contain  billion letter use different value   supplementary table   operation require  high memory server  maximum performance    efficiently handle   machine     memory   half  speed  amount  ram  readily available   price     standard desktop computer",0
8,Minimap2,"Minimap2: pairwise alignment for nucleotide sequences
Minimap2 follows a typical seed-chain-align procedure as is used by most full-genome aligners. It collects minimizers (Roberts et al., 2004) of the reference sequences and indexes them in a hash table, with the key being the hash of a minimizer and the value being a list of locations of the minimizer copies. Then for each query sequence, minimap2 takes query minimizers as seeds, finds exact matches (i.e. anchors) to the reference, and identifies sets of colinear anchors as chains. If base-level alignment is requested, minimap2 applies dynamic programming (DP) to extend from the ends of chains and to close regions between adjacent anchors in chains. Minimap2 uses indexing and seeding algorithms similar to minimap (Li, 2016), and furthers the predecessor with more accurate chaining, the ability to produce base-level alignment and the support of spliced alignment. 2.1 Chaining 2.1.1 Chaining An anchor is a 3-tuple (x, y, w), indicating interval [x − w + 1, x] on the reference matching interval [y − w + 1, y] on the query. Given a list of anchors sorted by ending reference position x, let f(i) be the maximal chaining score up to the i-th anchor in the list. f(i) can be calculated with dynamic programming: f(i) = max  max i>j≥1 {f(j) + α(j, i) − β(j, i)}, wi (1) where α(j, i) = min  min{yi − yj , xi − xj}, wi is the number of matching bases between the two anchors. β(j, i) > 0 is the gap cost. It equals ∞ if yj ≥ yi or max{yi − yj , xi − xj} > G (i.e. the distance between two anchors is too large); otherwise β(j, i) = γc (yi − yj ) − (xi − xj )  (2) In implementation, a gap of length l costs γc(l) =  0.01 · w¯ · |l| + 0.5 log2 |l| (l 6= 0) 0 (l = 0) where w¯ is the average seed length. For N anchors, directly computing all f(·) with Eq. (1) takes O(N2 ) time. Although theoretically faster chaining algorithms exist (Abouelhoda and Ohlebusch, 2005), they are inapplicable to generic gap cost, complex to implement and usually associated with a large constant. We introduced a simple heuristic to accelerate chaining. We note that if anchor i is chained to j, chaining i to a predecessor of j is likely to yield a lower score. When evaluating Eq. (1), we start from anchor i−1 and stop the process if we cannot find a better score after up to h iterations. This approach reduces the average time to O(hN). In practice, we can almost always find the optimal chain with h = 50; even if the heuristic fails, the optimal chain is often close. 2.1.2 Backtracking Let P(i) be the index of the best predecessor of anchor i. It equals 0 if f(i) = wi or argmaxj{f(j) + α(j, i) − β(j, i)} otherwise. For each anchor i in the descending order of f(i), we apply P(·) repeatedly to find its predecessor and mark each visited i as ‘used’, until P(i) = 0 or we reach 1 arXiv:1708.01492v5 [q-bio.GN] 16 Mar 201 Li an already ‘used’ i. This way we find all chains with no anchors used in more than one chains. 2.1.3 Identifying primary chains In the absence of copy number changes, each query segment should not be mapped to two places in the reference. However, chains found at the previous step may have significant or complete overlaps due to repeats in the reference (Li and Durbin, 2010). Minimap2 used the following procedure to identify primary chains that do not greatly overlap on the query. Let Q be an empty set initially. For each chain from the best to the worst according to their chaining scores: if on the query, the chain overlaps with a chain in Q by 50% or higher percentage of the shorter chain, mark the chain as secondary to the chain in Q; otherwise, add the chain to Q. In the end, Q contains all the primary chains. We did not choose a more sophisticated data structure (e.g. range tree or k-d tree) because this step is not the performance bottleneck. For each primary chain, minimap2 estimates its mapping quality with an empirical formula: mapQ = 40 · (1 − f2/f1) · min{1, m/10} · log f1 where log denotes natural logarithm, m is the number of anchors on the primary chain, f1 is the chaining score, and f2 ≤ f1 is the score of the best chain that is secondary to the primary chain. Intuitively, a chain is assigned to a higher mapping quality if it is long and its best secondary chain is weak. 2.1.4 Estimating per-base sequence divergence Suppose a query sequence harbors n seeds of length k, m of which are present in a chain. We want to estimate the sequence divergence  between the query and the reference sequences in the chain. This is useful when baselevel alignment is too expensive to perform. If we model substitutions with a homogeneous Poisson process along the query sequence, the probablity of seeing k consecutive bases without substitutions is e−k. On the assumption that all k-mers are independent of each other, the likelihood function of  is L(|n, m, k) = e −m·k(1 − e −k) n−m The maximum likelihood estimate of  is ˆ = 1 k log n m In reality, sequencing errors are sometimes clustered and k-mers are not independent of each other, especially when we take minimizers as seeds. These violate the assumptions in the derivation above. As a result, ˆ is only approximate and can be biased. It also ignores long deletions from the reference sequence. In practice, fortunately, ˆ is often close to and strongly correlated with the sequence divergence estimated from base-level alignments. On the several datasets used in Section 3.1, the Spearman correlation coefficient is around 0.9. 2.1.5 Indexing with homopolymer compressed k-mers SmartDenovo (https://github.com/ruanjue/smartdenovo; J. Ruan, personal communication) indexes reads with homopolymer-compressed (HPC) kmers and finds the strategy improves overlap sensitivity for SMRT reads. Minimap2 adopts the same heuristic. The HPC string of a string s, denoted by HPC(s), is constructed by contracting homopolymers in s to a single base. An HPC k-mer of s is a k-long substring of HPC(s). For example, suppose s = GGATTTTCCA, HPC(s) = GATCA and the first HPC 4-mer is GATC. To demonstrate the effectiveness of HPC k-mers, we performed read overlapping for the example E. coli SMRT reads from PBcR (Berlin et al., 2015), using different types of k-mers. With normal 15bp minimizers per 5bp window, minimap2 finds 90.9% of ≥2kb overlaps inferred from the read-to-reference alignment. With HPC 19-mers per 5bp window, minimap2 finds 97.4% of overlaps. It achieves this higher sensitivity by indexing 1/3 fewer minimizers, which further helps performance. HPC-based indexing reduces the sensitivity for current ONT reads, though. 2.2 Aligning genomic DNA 2.2.1 Alignment with 2-piece affine gap cost Minimap2 performs DP-based global alignment between adjacent anchors in a chain. It uses a 2-piece affine gap cost (Gotoh, 1990): γa(l) = min{q + |l| · e, q˜+ |l| · e˜} (3) Without losing generality, we always assume q+e < q˜+˜e. On the condition that e > e˜, it applies cost q + |l| · e to gaps shorter than d(˜q − q)/(e − e˜)e and applies q˜ + |l| · e˜ to longer gaps. This scheme helps to recover longer insertions and deletions (INDELs). The equation to compute the optimal alignment under γa(·) is    Hij = max{Hi−1,j−1 + s(i, j), Eij , Fij , E˜ ij , F˜ ij} Ei+1,j = max{Hij − q, Eij} − e Fi,j+1 = max{Hij − q, Fij} − e E˜ i+1,j = max{Hij − q, ˜ E˜ ij} − e˜ F˜ i,j+1 = max{Hij − q, ˜ F˜ ij} − e˜ (4) where s(i, j) is the score between the i-th reference base and j-th query base. Eq. (4) is a natural extension to the equation under affine gap cost (Gotoh, 1982; Altschul and Erickson, 1986). 2.2.2 The Suzuki-Kasahara formulation When we allow gaps longer than several hundred base pairs, nucleotide-level alignment is much slower than chaining. SSE acceleration is critical to the performance of minimap2. Traditional SSE implementations (Farrar, 2007) based on Eq. (4) can achieve 16-way parallelization for short sequences, but only 4-way parallelization when the peak alignment score reaches 32767. Long sequence alignment may exceed this threshold. Inspired by Wu et al. (1996) and the following work, Suzuki and Kasahara (2018) proposed a difference-based formulation that lifted this limitation. In case of 2-piece gap cost, define    uij , Hij − Hi−1,j vij , Hij − Hi,j−1 xij , Ei+1,j − Hij x˜ij , E˜ i+1,j − Hij yij , Fi,j+1 − Hij y˜ij , F˜ i,j+1 − Hij We can transform Eq. (4) to    zij = max{s(i, j), xi−1,j + vi−1,j , yi,j−1 + ui,j−1, x˜i−1,j + vi−1,j , y˜i,j−1 + ui,j−1} uij = zij − vi−1,j vij = zij − ui,j−1 xij = max{0, xi−1,j + vi−1,j − zij + q} − q − e yij = max{0, yi,j−1 + ui,j−1 − zij + q} − q − e x˜ij = max{0, x˜i−1,j + vi−1,j − zij + ˜q} − q˜− e˜ y˜ij = max{0, y˜i,j−1 + ui,j−1 − zij + ˜q} − q˜− e˜ (5) where zij is a temporary variable that does not need to be stored. An important property of Eq. (5) is that all values are bounded by scoring parameters. To see that, xij = Ei+1,j − Hij = max{−q, Eij − Hij} − e With Eij ≤ Hij , we have −q − e ≤ xij ≤ max{−q, 0} − e = −e and similar inequations for yij , x˜ij and y˜ij . In addition, uij = zij − vi−1,j ≥ max{xi−1,j , x˜i−1,j} ≥ −q − e As the maximum value of zij = Hij − Hi−1,j−1 is M, the maximal matching score, we can derive uij ≤ M − vi−1,j ≤ M + q + e In conclusion, in Eq. (5), x and y are bounded by [−q − e, −e], x˜ and y˜ by [−q˜ − e, ˜ −e˜], and u and v by [−q − e, M + q + e]. When −128 ≤ −q − e < M + q + e ≤ 127, each of them can be stored as a 8-bit integer. 2 Aligning nucleotide sequences with minimap2 This enables 16-way SSE vectorization regardless of the peak score of the alignment. For a more efficient SSE implementation, we transform the row-column coordinate to the diagonal-antidiagonal coordinate by letting r ← i + j and t ← i. Eq. (5) becomes:    zrt = max{s(t, r − t), xr−1,t−1 + vr−1,t−1, yr−1,t +ur−1,t, x˜r−1,t−1 + vr−1,t−1, y˜r−1,t + ur−1,t} urt = zrt − vr−1,t−1 vrt = zrt − ur−1,t xrt = max{0, xr−1,t−1 + vr−1,t−1 − zrt + q} − q − e yrt = max{0, yr−1,t + ur−1,t − zrt + q} − q − e x˜rt = max{0, x˜r−1,t−1 + vr−1,t−1 − zrt + ˜q} − q˜− e˜ y˜rt = max{0, y˜r−1,t + ur−1,t − zrt + ˜q} − q˜− e˜ In this formulation, cells with the same diagonal index r are independent of each other. This allows us to fully vectorize the computation of all cells on the same anti-diagonal in one inner loop. It also simplifies banded alignment (500bp band width by default), which would be difficult with striped vectorization (Farrar, 2007). On the condition that q + e < q˜ + ˜e and e > e˜, the initial values in the diagonal-antidiagonal formuation are    xr−1,−1 = yr−1,r = −q − e x˜r−1,−1 = ˜yr−1,r = −q˜− e˜ ur−1,r = vr−1,−1 = η(r) where η(r) =    −q − e (r = 0) −e (r < d q˜−q e−e˜ − 1e) r · (e − e˜) − (˜q − q) − e˜ (r = d q˜−q e−e˜ − 1e) −e˜ (r > d q˜−q e−e˜ − 1e) These can be derived from the initial values for Eq. (4). When performing global alignment, we do not need to compute Hrt in each cell. We use 16-way vectorization throughout the alignment process. When extending alignments from ends of chains, we need to find the cell (r, t) where Hrt reaches the maximum. We resort to 4-way vectorization to compute Hrt = Hr−1,t +urt. Because this computation is simple, Eq. (5) is still the dominant performance bottleneck. In practice, our 16-way vectorized implementation of global alignment is three times as fast as Parasail’s 4-way vectorization (Daily, 2016). Without banding, our implementation is slower than Edlib (Soˇ siˇ c and ´ Sikic, 2017), ˇ but with a 1000bp band, it is considerably faster. When performing global alignment between anchors, we expect the alignment to stay close to the diagonal of the DP matrix. Banding is applicable most of the time. 2.2.3 The Z-drop heuristic With global alignment, minimap2 may force to align unrelated sequences between two adjacent anchors. To avoid such an artifact, we compute accumulative alignment score along the alignment path and break the alignment where the score drops too fast in the diagonal direction. More precisely, let S(i, j) be the alignment score along the alignment path ending at cell (i, j) in the DP matrix. We break the alignment if there exist (i 0 , j0 ) and (i, j), i 0 < i and j 0 < j, such that S(i 0 , j0 ) − S(i, j) > Z + e · |(i − i 0 ) − (j − j 0 )| where e is the gap extension cost and Z is an arbitrary threshold. This strategy is first used in BWA-MEM. It is similar to X-drop employed in BLAST (Altschul et al., 1997), but unlike X-drop, it would not break the alignment in the presence of a single long gap. When minimap2 breaks a global alignment between two anchors, it performs local alignment between the two subsequences involved in the global alignment, but this time with the one subsequence reverse complemented. This additional alignment step may identify short inversions that are missed during chaining. 2.2.4 Filtering out misplaced anchors Due to sequencing errors and local homology, some anchors in a chain may be wrong. If we blindly align regions between two misplaced anchors, we will produce a suboptimal alignment. To reduce this artifact, we filter out anchors that lead to a >10bp insertion and a >10bp deletion at the same time, and filter out terminal anchors that lead to a long gap towards the ends of a chain. These heuristics greatly alleviate the issues with misplaced anchors, but they are unable to fix all such errors. Local misalignment is a limitation of minimap2 which we hope to address in future. 2.3 Aligning spliced sequences The algorithm described above can be adapted to spliced alignment. In this mode, the chaining gap cost distinguishes insertions to and deletions from the reference: γc(l) in Eq. (2) takes the form of γc(l) =  0.01 · w¯ · l + 0.5 log2 l (l > 0) min{0.01 · w¯ · |l|, log2 |l|} (l < 0) Similarly, the gap cost function used for DP-based alignment is changed to γa(l) =  q + l · e (l > 0) min{q + |l| · e, q˜} (l < 0) In alignment, a deletion no shorter than d(˜q −q)/ee is regarded as an intron, which pays no cost to gap extensions. To pinpoint precise splicing junctions, minimap2 introduces referencedependent cost to penalize non-canonical splicing:    Hij = max{Hi−1,j−1 + s(i, j), Eij , Fij , E˜ ij − a(i)} Ei+1,j = max{Hij − q, Eij} − e Fi,j+1 = max{Hij − q, Fij} − e E˜ i+1,j = max{Hij − d(i) − q, ˜ E˜ ij} (6) Let T be the reference sequence. d(i) is computed as d(i) =    0 if T[i + 1, i + 3] is GTA or GTG p/2 if T[i + 1, i + 3] is GTC or GTT p otherwise where T[i, j] extracts a substring of T between i and j inclusively. d(i) penalizes non-canonical donor sites with p and less frequent Eukaryotic splicing signal GT[C/T] with p/2 (Irimia and Roy, 2008). Similarly, a(i) =    0 if T[i − 2, i] is CAG or TAG p/2 if T[i − 2, i] is AAG or GAG p otherwise models the acceptor signal. Eq. (6) is close to an equation in Zhang and Gish (2006) except that we allow insertions immediately followed by deletions and vice versa; in addition, we use the Suzuki-Kasahara diagonal formulation in actual implementation. If RNA-seq reads are not sequenced from stranded libraries, the read strand relative to the underlying transcript is unknown. By default, minimap2 aligns each chain twice, first assuming GT–AG as the splicing signal and then assuming CT–AC, the reverse complement of GT–AG, as the splicing signal. The alignment with a higher score is taken as the final alignment. This procedure also infers the relative strand of reads that span canonical splicing sites. In the spliced alignment mode, minimap2 further increases the density of minimizers and disables banded alignment. Together with the two-round DP-based alignment, spliced alignment is several times slower than genomic DNA alignment. 2.4 Aligning short paired-end reads During chaining, minimap2 takes a pair of reads as one fragment with a gap of unknown length in the middle. It applies a normal gap cost between seeds on the same read but is a more permissive gap cost between seeds on different reads. More precisely, the gap cost during chaining is (l 6= 0): γc(l) =  0.01 · w¯ · |l| + 0.5 log2 |l| if two seeds on the same read min{0.01 · w¯ · |l|, log2 |l|} otherwise",Alignment,"minimap2 pairwise alignment  nucleotide sequences
minimap2 follow  typical seedchainalign procedure   use   fullgenome aligners  collect minimizers roberts      reference sequence  index    hash table   key   hash   minimizer   value   list  locations   minimizer copy    query sequence minimap2 take query minimizers  seed find exact match  anchor   reference  identify set  colinear anchor  chain  baselevel alignment  request minimap2 apply dynamic program   extend   end  chain   close regions  adjacent anchor  chain minimap2 use index  seed algorithms similar  minimap      predecessor   accurate chain  ability  produce baselevel alignment   support  splice alignment  chain  chain  anchor   tuple    indicate interval         reference match interval         query give  list  anchor sort  end reference position  let    maximal chain score    ith anchor   list    calculate  dynamic program   max  max ≥ {      }       min  min{      }    number  match base   two anchor       gap cost  equal ∞   ≥   max{      }     distance  two anchor   large otherwise                 implementation  gap  length  cost γcl     ¯     log2          ¯   average seed length   anchor directly compute      take on2  time although theoretically faster chain algorithms exist abouelhoda  ohlebusch    inapplicable  generic gap cost complex  implement  usually associate   large constant  introduce  simple heuristic  accelerate chain  note   anchor   chain   chain    predecessor    likely  yield  lower score  evaluate    start  anchor   stop  process   cannot find  better score     iterations  approach reduce  average time  ohn  practice   almost always find  optimal chain     even   heuristic fail  optimal chain  often close  backtrack let    index   best predecessor  anchor   equal       argmaxj{      } otherwise   anchor    descend order    apply  repeatedly  find  predecessor  mark  visit   use       reach  arxiv01492v5 qbiogn  mar    already use   way  find  chain   anchor use    one chain  identify primary chain   absence  copy number change  query segment    map  two place   reference however chain find   previous step may  significant  complete overlap due  repeat   reference   durbin  minimap2 use  follow procedure  identify primary chain    greatly overlap   query let    empty set initially   chain   best   worst accord   chain score    query  chain overlap   chain      higher percentage   shorter chain mark  chain  secondary   chain   otherwise add  chain     end  contain   primary chain    choose   sophisticate data structure  range tree   tree   step    performance bottleneck   primary chain minimap2 estimate  map quality   empirical formula mapq      f2f1  min{ }  log   log denote natural logarithm    number  anchor   primary chain    chain score   ≤    score   best chain   secondary   primary chain intuitively  chain  assign   higher map quality    long   best secondary chain  weak  estimate perbase sequence divergence suppose  query sequence harbor  seed  length      present   chain  want  estimate  sequence divergence    query   reference sequence   chain   useful  baselevel alignment   expensive  perform   model substitutions   homogeneous poisson process along  query sequence  probablity  see  consecutive base without substitutions     assumption   kmers  independent     likelihood function               maximum likelihood estimate        log    reality sequence errors  sometimes cluster  kmers   independent    especially   take minimizers  seed  violate  assumptions   derivation    result    approximate    bias  also ignore long deletions   reference sequence  practice fortunately   often close   strongly correlate   sequence divergence estimate  baselevel alignments   several datasets use  section   spearman correlation coefficient  around   index  homopolymer compress kmers smartdenovo   ruan personal communication index read  homopolymercompressed hpc kmers  find  strategy improve overlap sensitivity  smrt read minimap2 adopt   heuristic  hpc string   string  denote  hpcs  construct  contract homopolymers     single base  hpc kmer     klong substring  hpcs  example suppose   ggattttcca hpcs  gatca   first hpc mer  gatc  demonstrate  effectiveness  hpc kmers  perform read overlap   example  coli smrt read  pbcr berlin    use different type  kmers  normal 15bp minimizers per 5bp window minimap2 find   ≥2kb overlap infer   readtoreference alignment  hpc mers per 5bp window minimap2 find   overlap  achieve  higher sensitivity  index  fewer minimizers   help performance hpcbased index reduce  sensitivity  current ont read though  align genomic dna  alignment  piece affine gap cost minimap2 perform dpbased global alignment  adjacent anchor   chain  use  piece affine gap cost gotoh  γal  min{     ˜   ˜}  without lose generality  always assume   ˜˜   condition    ˜  apply cost       gap shorter  ˜    ˜  apply ˜    ˜  longer gap  scheme help  recover longer insertions  deletions indels  equation  compute  optimal alignment       hij  max{hij    eij  fij  ˜   ˜ } eij  max{hij   eij}   fij  max{hij   fij}   ˜   max{hij   ˜ ˜ }  ˜ ˜   max{hij   ˜ ˜ }  ˜       score   ith reference base  jth query base     natural extension   equation  affine gap cost gotoh  altschul  erickson    suzukikasahara formulation   allow gap longer  several hundred base pair nucleotidelevel alignment  much slower  chain sse acceleration  critical   performance  minimap2 traditional sse implementations farrar  base     achieve way parallelization  short sequence   way parallelization   peak alignment score reach  long sequence alignment may exceed  threshold inspire        follow work suzuki  kasahara  propose  differencebased formulation  lift  limitation  case  piece gap cost define    uij  hij  hij vij  hij  hij xij  eij  hij ˜  ˜   hij yij  fij  hij ˜  ˜   hij   transform       zij  max{  xij  vij  yij  uij ˜  vij  ˜  uij} uij  zij  vij vij  zij  uij xij  max{ xij  vij  zij  }     yij  max{ yij  uij  zij  }     ˜  max{ ˜  vij  zij  ˜}  ˜ ˜ ˜  max{ ˜  uij  zij  ˜}  ˜ ˜   zij   temporary variable    need   store  important property       value  bound  score parameters  see  xij  eij  hij  max{ eij  hij}    eij ≤ hij       ≤ xij ≤ max{ }      similar inequations  yij  ˜  ˜   addition uij  zij  vij ≥ max{xij  ˜} ≥      maximum value  zij  hij  hij    maximal match score   derive uij ≤   vij ≤       conclusion        bound      ˜  ˜  ˜   ˜ ˜                ≤          ≤       store   bite integer  align nucleotide sequence  minimap2  enable way sse vectorization regardless   peak score   alignment    efficient sse implementation  transform  rowcolumn coordinate   diagonalantidiagonal coordinate  let            become    zrt  max{    xrt  vrt yrt urt ˜  vrt ˜  urt} urt  zrt  vrt vrt  zrt  urt xrt  max{ xrt  vrt  zrt  }     yrt  max{ yrt  urt  zrt  }     ˜  max{ ˜  vrt  zrt  ˜}  ˜ ˜ ˜  max{ ˜  urt  zrt  ˜}  ˜ ˜   formulation cells    diagonal index   independent     allow   fully vectorize  computation   cells    antidiagonal  one inner loop  also simplify band alignment 500bp band width  default  would  difficult  strip vectorization farrar    condition      ˜  ˜    ˜  initial value   diagonalantidiagonal formuation       yrr     ˜  ˜yrr  ˜ ˜ urr                     ˜ ˜       ˜  ˜    ˜    ˜ ˜   ˜    ˜ ˜      derive   initial value     perform global alignment    need  compute hrt   cell  use way vectorization throughout  alignment process  extend alignments  end  chain  need  find  cell    hrt reach  maximum  resort  way vectorization  compute hrt  hrt urt   computation  simple    still  dominant performance bottleneck  practice  way vectorized implementation  global alignment  three time  fast  parasails way vectorization daily  without band  implementation  slower  edlib soˇ siˇ   ´ sikic      1000bp band   considerably faster  perform global alignment  anchor  expect  alignment  stay close   diagonal    matrix band  applicable    time   zdrop heuristic  global alignment minimap2 may force  align unrelated sequence  two adjacent anchor  avoid   artifact  compute accumulative alignment score along  alignment path  break  alignment   score drop  fast   diagonal direction  precisely let     alignment score along  alignment path end  cell      matrix  break  alignment   exist                                                gap extension cost     arbitrary threshold  strategy  first use  bwamem   similar  xdrop employ  blast altschul     unlike xdrop  would  break  alignment   presence   single long gap  minimap2 break  global alignment  two anchor  perform local alignment   two subsequences involve   global alignment   time   one subsequence reverse complement  additional alignment step may identify short inversions   miss  chain  filter  misplace anchor due  sequence errors  local homology  anchor   chain may  wrong   blindly align regions  two misplace anchor   produce  suboptimal alignment  reduce  artifact  filter  anchor  lead   10bp insertion   10bp deletion    time  filter  terminal anchor  lead   long gap towards  end   chain  heuristics greatly alleviate  issue  misplace anchor    unable  fix   errors local misalignment   limitation  minimap2   hope  address  future  align splice sequence  algorithm describe    adapt  splice alignment   mode  chain gap cost distinguish insertions   deletions   reference γcl    take  form  γcl     ¯     log2     min{  ¯   log2 }    similarly  gap cost function use  dpbased alignment  change  γal           min{     ˜}     alignment  deletion  shorter  ˜ qee  regard   intron  pay  cost  gap extensions  pinpoint precise splice junctions minimap2 introduce referencedependent cost  penalize noncanonical splice    hij  max{hij    eij  fij  ˜   } eij  max{hij   eij}   fij  max{hij   fij}   ˜   max{hij     ˜ ˜ }  let    reference sequence   compute                gta  gtg          gtc  gtt  otherwise    extract  substring       inclusively  penalize noncanonical donor sit    less frequent eukaryotic splice signal gtct   irimia  roy  similarly             cag  tag        aag  gag  otherwise model  acceptor signal    close   equation  zhang  gish  except   allow insertions immediately follow  deletions  vice versa  addition  use  suzukikasahara diagonal formulation  actual implementation  rnaseq read   sequence  strand libraries  read strand relative   underlie transcript  unknown  default minimap2 align  chain twice first assume gtag   splice signal   assume ctac  reverse complement  gtag   splice signal  alignment   higher score  take   final alignment  procedure also infer  relative strand  read  span canonical splice sit   splice alignment mode minimap2  increase  density  minimizers  disable band alignment together   tworound dpbased alignment splice alignment  several time slower  genomic dna alignment  align short pairedend read  chain minimap2 take  pair  read  one fragment   gap  unknown length   middle  apply  normal gap cost  seed    read     permissive gap cost  seed  different read  precisely  gap cost  chain     γcl     ¯     log2   two seed    read min{  ¯   log2 } otherwise",0
9,Bowtie,"Ultrafast and memory-efficient alignment of short DNA sequences to the human genome
Bowtie indexes the reference genome using a scheme based on the Burrows-Wheeler transform (BWT) [17] and the FM index [18, 19]. A Bowtie index for the human genome fits in 2.2 GB on disk and has a memory footprint of as little as 1.3 GB at alignment time, allowing it to be queried on a workstation with under 2 GB of RAM. The common method for searching in an FM index is the exact-matching algorithm of Ferragina and Manzini [18]. Bowtie does not simply adopt this algorithm because exact matching does not allow for sequencing errors or genetic variations. We introduce two novel extensions that make the technique applicable to short read alignment: a quality-aware backtracking algorithm that allows mismatches and favors high-quality alignments; and 'double indexing', a strategy to avoid excessive backtracking. The Bowtie aligner follows a policy similar to Maq's, in that it allows a small number of mismatches within the high-quality end of each read, and it places an upper limit on the sum of the quality values at mismatched alignment positions. Burrows-Wheeler indexing The BWT is a reversible permutation of the characters in a text. Although originally developed within the context of data compression, BWT-based indexing allows large texts to be searched efficiently in a small memory footprint. It has been applied to bioinformatics applications, including oligomer counting [20], whole-genome alignment [21], tiling microarray probe design [22], and Smith-Waterman alignment to a human-sized reference [23]. The Burrows-Wheeler transformation of a text T, BWT(T), is constructed as follows. The character $ is appended to T, where $ is not in T and is lexicographically less than all characters in T. The Burrows-Wheeler matrix of T is constructed as the matrix whose rows comprise all cyclic rotations of T$. The rows are then sorted lexicographically. BWT(T) is the sequence of characters in the rightmost column of the Burrows-Wheeler matrix (Figure 1a). BWT(T) has the same length as the original text T This matrix has a property called 'last first (LF) mapping'. The ith occurrence of character X in the last column corresponds to the same text character as the ith occurrence of X in the first column. This property underlies algorithms that use the BWT index to navigate or search the text. Figure 1b illustrates UNPERMUTE, an algorithm that applies the LF mapping repeatedly to re-create T from BWT(T). The LF mapping is also used in exact matching. Because the matrix is sorted lexicographically, rows beginning with a given sequence appear consecutively. In a series of steps, the EXACTMATCH algorithm (Figure 1c) calculates the range of matrix rows beginning with successively longer suffixes of the query. At each step, the size of the range either shrinks or remains the same. When the algorithm completes, rows beginning with S0 (the entire query) correspond to exact occurrences of the query in the text. If the range is empty, the text does not contain the query. UNPERMUTE is attributable to Burrows and Wheeler [17] and EXACTMATCH to Ferragina and Manzini [18]. See Additional data file 1 (Supplementary Discussion 1) for details. Searching for inexact alignments EXACTMATCH is insufficient for short read alignment because alignments may contain mismatches, which may be due to sequencing errors, genuine differences between reference and query organisms, or both. We introduce an alignment algorithm that conducts a backtracking search to quickly find alignments that satisfy a specified alignment policy. Each character in a read has a numeric quality value, with lower values indicating a higher likelihood of a sequencing error. Our alignment policy allows a limited number of mismatches and prefers alignments where the sum of the quality values at all mismatched positions is low. The search proceeds similarly to EXACTMATCH, calculating matrix ranges for successively longer query suffixes. If the range becomes empty (a suffix does not occur in the text), then the algorithm may select an already-matched query position and substitute a different base there, introducing a mismatch into the alignment. The EXACTMATCH search resumes from just after the substituted position. The algorithm selects only those substitutions that are consistent with the alignment policy and which yield a modified suffix that occurs at least once in the text. If there are multiple candidate substitution positions, then the algorithm greedily selects a position with a minimal quality value. Backtracking scenarios play out within the context of a stack structure that grows when a new substitution is introduced and shrinks when the aligner rejects all candidate alignments for the substitutions currently on the stack. See Figure 2 for an illustration of how the search might proceed. In short, Bowtie conducts a quality-aware, greedy, randomized, depth-first search through the space of possible alignments. If a valid alignment exists, then Bowtie will find it (subject to the backtrack ceiling discussed in the following section). Because the search is greedy, the first valid alignment encountered by Bowtie will not necessarily be the 'best' in terms of number of mismatches or in terms of quality. The user may instruct Bowtie to continue searching until it can prove that any alignment it reports is 'best' in terms of number of mismatches (using the option --best). In our experience, this mode is two to three times slower than the default mode. We expect that the faster default mode will be preferred for large re-sequencing projects. The user may also opt for Bowtie to report all alignments up to a specified number (option -k) or all alignments with no limit on the number (option -a) for a given read. If in the course of its search Bowtie finds N possible alignments for a given set of substitutions, but the user has requested only K alignments where K < N, Bowtie will report K of the N alignments selected at random. Note that these modes can be much slower than the default. In our experience, for example, -k 1 is more than twice as fast as -k 2. Excessive backtracking The aligner as described so far can, in some cases, encounter sequences that cause excessive backtracking. This occurs when the aligner spends most of its effort fruitlessly backtracking to positions close to the 3' end of the query. Bowtie mitigates excessive backtracking with the novel technique of 'double indexing'. Two indices of the genome are created: one containing the BWT of the genome, called the 'forward' index, and a second containing the BWT of the genome with its character sequence reversed (not reverse complemented) called the 'mirror' index. To see how this helps, consider a matching policy that allows one mismatch in the alignment. A valid alignment with one mismatch falls into one of two cases according to which half of the read contains the mismatch. Bowtie proceeds in two phases corresponding to those two cases. Phase 1 loads the forward index into memory and invokes the aligner with the constraint that it may not substitute at positions in the query's right half. Phase 2 uses the mirror index and invokes the aligner on the reversed query, with the constraint that the aligner may not substitute at positions in the reversed query's right half (the original query's left half). The constraints on backtracking into the right half prevent excessive backtracking, whereas the use of two phases and two indices maintains full sensitivity. Unfortunately, it is not possible to avoid excessive backtracking fully when alignments are permitted to have two or more mismatches. In our experiments, we have observed that excessive backtracking is significant only when a read has many low-quality positions and does not align or aligns poorly to the reference. These cases can trigger in excess of 200 backtracks per read because there are many legal combinations of low-quality positions to be explored before all possibilities are exhausted. We mitigate this cost by enforcing a limit on the number of backtracks allowed before a search is terminated (default: 125). The limit prevents some legitimate, low-quality alignments from being reported, but we expect that this is a desirable trade-off for most applications. Phased Maq-like search Bowtie allows the user to select the number of mismatches permitted (default: two) in the high-quality end of a read (default: the first 28 bases) as well as the maximum acceptable quality distance of the overall alignment (default: 70). Quality values are assumed to follow the definition in PHRED [24], where p is the probability of error and Q = -10log p. Both the read and its reverse complement are candidates for alignment to the reference. For clarity, this discussion considers only the forward orientation. See Additional data file 1 (Supplementary Discussion 2) for an explanation of how the reverse complement is incorporated. The first 28 bases on the high-quality end of the read are termed the 'seed'. The seed consists of two halves: the 14 bp on the high-quality end (usually the 5' end) and the 14 bp on the low-quality end, termed the 'hi-half' and the 'lo-half', respectively. Assuming the default policy (two mismatches permitted in the seed), a reportable alignment will fall into one of four cases: no mismatches in seed (case 1); no mismatches in hi-half, one or two mismatches in lo-half (case 2); no mismatches in lo-half, one or two mismatches in hi-half (case 3); and one mismatch in hi-half, one mismatch in lo-half (case 4). All cases allow any number of mismatches in the nonseed part of the read and all cases are also subject to the quality distance constraint. The Bowtie algorithm consists of three phases that alternate between using the forward and mirror indices, as illustrated in Figure 3. Phase 1 uses the mirror index and invokes the aligner to find alignments for cases 1 and 2. Phases 2 and 3 cooperate to find alignments for case 3: Phase 2 finds partial alignments with mismatches only in the hi-half and phase 3 attempts to extend those partial alignments into full alignments. Finally, phase 3 invokes the aligner to find alignments for case 4. Performance results We evaluated the performance of Bowtie using reads from the 1,000 Genomes project pilot (National Center for Biotechnology Information [NCBI] Short Read Archive:SRR001115). A total of 8.84 million reads, about one lane of data from an Illumina instrument, were trimmed to 35 bp and aligned to the human reference genome [NCBI build 36.3]. Unless specified otherwise, read data are not filtered or modified (besides trimming) from how they appear in the archive. This leads to about 70% to 75% of reads aligning somewhere to the genome. In our experience, this is typical for raw data from the archive. More aggressive filtering leads to higher alignment rates and faster alignment. All runs were performed on a single CPU. Bowtie speedups were calculated as a ratio of wall-clock alignment times. Both wall-clock and CPU times are given to demonstrate that input/output load and CPU contention are not significant factors. The time required to build the Bowtie index was not included in the Bowtie running times. Unlike competing tools, Bowtie can reuse a pre-computed index for the reference genome across many alignment runs. We anticipate most users will simply download such indices from a public repository. The Bowtie site [25] provides indices for current builds of the human, chimp, mouse, dog, rat, and Arabidopsis thaliana genomes, as well as many others. Results were obtained on two hardware platforms: a desktop workstation with 2.4 GHz Intel Core 2 processor and 2 GB of RAM; and a large-memory server with a four-core 2.4 GHz AMD Opteron processor and 32 GB of RAM. These are denoted 'PC' and 'server', respectively. Both PC and server run Red Hat Enterprise Linux AS release 4. Comparison to SOAP and Maq Maq is a popular aligner [1, 4, 5, 26, 27] that is among the fastest competing open source tools for aligning millions of Illumina reads to the human genome. SOAP is another open source tool that has been reported and used in short-read projects [6, 28]. Table 1 presents the performance and sensitivity of Bowtie v0.9.6, SOAP v1.10, and Maq v0.6.6. SOAP could not be run on the PC because SOAP's memory footprint exceeds the PC's physical memory. The 'soap.contig' version of the SOAP binary was used. For comparison with SOAP, Bowtie was invoked with '-v 2' to mimic SOAP's default matching policy (which allows up to two mismatches in the alignment and disregards quality values), and with '--maxns 5' to simulate SOAP's default policy of filtering out reads with five or more no-confidence bases. For the Maq comparison Bowtie is run with its default policy, which mimics Maq's default policy of allowing up to two mismatches in the first 28 bases and enforcing an overall limit of 70 on the sum of the quality values at all mismatched read positions. To make Bowtie's memory footprint more comparable to Maq's, Bowtie is invoked with the '-z' option in all experiments to ensure that only the forward or mirror index is resident in memory at one time. The number of reads aligned indicates that SOAP (67.3%) and Bowtie -v 2 (67.4%) have comparable sensitivity. Of the reads aligned by either SOAP or Bowtie, 99.7% were aligned by both, 0.2% were aligned by Bowtie but not SOAP, and 0.1% were aligned by SOAP but not Bowtie. Maq (74.7%) and Bowtie (71.9%) also have roughly comparable sensitivity, although Bowtie lags by 2.8%. Of the reads aligned by either Maq or Bowtie, 96.0% were aligned by both, 0.1% were aligned by Bowtie but not Maq, and 3.9% were aligned by Maq but not Bowtie. Of the reads mapped by Maq but not Bowtie, almost all are due to a flexibility in Maq's alignment algorithm that allows some alignments to have three mismatches in the seed. The remainder of the reads mapped by Maq but not Bowtie are due to Bowtie's backtracking ceiling. Maq's documentation mentions that reads containing 'poly-A artifacts' can impair Maq's performance. Table 2 presents performance and sensitivity of Bowtie and Maq when the read set is filtered using Maq's 'catfilter' command to eliminate poly-A artifacts. The filter eliminates 438,145 out of 8,839,010 reads. Other experimental parameters are identical to those of the experiments in Table 1, and the same observations about the relative sensitivity of Bowtie and Maq apply here. Read length and performance As sequencing technology improves, read lengths are growing beyond the 30-bp to 50-bp commonly seen in public databases today. Bowtie, Maq, and SOAP support reads of lengths up to 1,024, 63, and 60 bp, respectively, and Maq versions 0.7.0 and later support read lengths up to 127 bp. Table 3 shows performance results when the three tools are each used to align three sets of 2 M untrimmed reads, a 36-bp set, a 50-bp set and a 76-bp set, to the human genome on the server platform. Each set of 2 M is randomly sampled from a larger set (NCBI Short Read Archive: SRR003084 for 36-bp, SRR003092 for 50-bp, SRR003196 for 76-bp). Reads were sampled such that the three sets of 2 M have uniform per-base error rate, as calculated from per-base Phred qualities. All reads pass through Maq's 'catfilter'. Bowtie is run both in its Maq-like default mode and in its SOAP-like '-v 2' mode. Bowtie is also given the '-z' option to ensure that only the forward or mirror index is resident in memory at one time. Maq v0.7.1 was used instead of Maq v0.6.6 for the 76-bp set because v0.6.6 cannot align reads longer than 63 bp. SOAP was not run on the 76-bp set because it does not support reads longer than 60 bp. The results show that Maq's algorithm scales better overall to longer read lengths than Bowtie or SOAP. However, Bowtie in SOAP-like '-v 2' mode also scales very well. Bowtie in its default Maq-like mode scales well from 36-bp to 50-bp reads but is substantially slower for 76-bp reads, although it is still more than an order of magnitude faster than Maq. Parallel performance Alignment can be parallelized by distributing reads across concurrent search threads. Bowtie allows the user to specify a desired number of threads (option -p); Bowtie then launches the specified number of threads using the pthreads library. Bowtie threads synchronize with each other when fetching reads, outputting results, switching between indices, and performing various forms of global bookkeeping, such as marking a read as 'done'. Otherwise, threads are free to operate in parallel, substantially speeding up alignment on computers with multiple processor cores. The memory image of the index is shared by all threads, and so the footprint does not increase substantially when multiple threads are used. Table 4 shows performance results for running Bowtie v0.9.6 on the four-core server with one, two, and four threads. Index building Bowtie uses a flexible indexing algorithm [29] that can be configured to trade off between memory usage and running time. Table 5 illustrates this trade-off when indexing the entire human reference genome (NCBI build 36.3, contigs). Runs were performed on the server platform. The indexer was run four times with different upper limits on memory usage. The reported times compare favorably with alignment times of competing tools that perform indexing during alignment. Less than 5 hours is required for Bowtie to both build and query a whole-human index with 8.84 million reads from the 1,000 Genome project (NCBI Short Read Archive:SRR001115) on a server, more than sixfold faster than the equivalent Maq run. The bottom-most row illustrates that the Bowtie indexer, with appropriate arguments, is memory-efficient enough to run on a typical workstation with 2 GB of RAM. Additional data file 1 (Supplementary discussions 3 and 4) explains the algorithm and the contents of the resulting index.",Alignment,"ultrafast  memoryefficient alignment  short dna sequence   human genome
bowtie index  reference genome use  scheme base   burrowswheeler transform bwt     index    bowtie index   human genome fit     disk    memory footprint   little     alignment time allow    query   workstation      ram  common method  search    index   exactmatching algorithm  ferragina  manzini  bowtie   simply adopt  algorithm  exact match   allow  sequence errors  genetic variations  introduce two novel extensions  make  technique applicable  short read alignment  qualityaware backtrack algorithm  allow mismatch  favor highquality alignments  'double indexing'  strategy  avoid excessive backtrack  bowtie aligner follow  policy similar  maq'    allow  small number  mismatch within  highquality end   read   place  upper limit   sum   quality value  mismatch alignment position burrowswheeler index  bwt   reversible permutation   character   text although originally develop within  context  data compression bwtbased index allow large texts   search efficiently   small memory footprint    apply  bioinformatics applications include oligomer count  wholegenome alignment  tile microarray probe design   smithwaterman alignment   humansized reference   burrowswheeler transformation   text  bwtt  construct  follow  character   append           lexicographically less   character    burrowswheeler matrix    construct   matrix whose row comprise  cyclic rotations    row   sort lexicographically bwtt   sequence  character   rightmost column   burrowswheeler matrix figure  bwtt    length   original text   matrix   property call 'last first  mapping'  ith occurrence  character    last column correspond    text character   ith occurrence     first column  property underlie algorithms  use  bwt index  navigate  search  text figure  illustrates unpermute  algorithm  apply   map repeatedly  recreate   bwtt   map  also use  exact match   matrix  sort lexicographically row begin   give sequence appear consecutively   series  step  exactmatch algorithm figure  calculate  range  matrix row begin  successively longer suffix   query   step  size   range either shrink  remain     algorithm complete row begin    entire query correspond  exact occurrences   query   text   range  empty  text   contain  query unpermute  attributable  burrow  wheeler   exactmatch  ferragina  manzini  see additional data file  supplementary discussion   detail search  inexact alignments exactmatch  insufficient  short read alignment  alignments may contain mismatch  may  due  sequence errors genuine differences  reference  query organisms    introduce  alignment algorithm  conduct  backtrack search  quickly find alignments  satisfy  specify alignment policy  character   read   numeric quality value  lower value indicate  higher likelihood   sequence error  alignment policy allow  limit number  mismatch  prefer alignments   sum   quality value   mismatch position  low  search proceed similarly  exactmatch calculate matrix range  successively longer query suffix   range become empty  suffix   occur   text   algorithm may select  alreadymatched query position  substitute  different base  introduce  mismatch   alignment  exactmatch search resume     substitute position  algorithm select   substitutions   consistent   alignment policy   yield  modify suffix  occur  least    text    multiple candidate substitution position   algorithm greedily select  position   minimal quality value backtrack scenarios play  within  context   stack structure  grow   new substitution  introduce  shrink   aligner reject  candidate alignments   substitutions currently   stack see figure    illustration    search might proceed  short bowtie conduct  qualityaware greedy randomize depthfirst search   space  possible alignments   valid alignment exist  bowtie  find  subject   backtrack ceiling discuss   follow section   search  greedy  first valid alignment encounter  bowtie   necessarily   'best'  term  number  mismatch   term  quality  user may instruct bowtie  continue search    prove   alignment  report  'best'  term  number  mismatch use  option best   experience  mode  two  three time slower   default mode  expect   faster default mode   prefer  large resequencing project  user may also opt  bowtie  report  alignments    specify number option    alignments   limit   number option    give read    course   search bowtie find  possible alignments   give set  substitutions   user  request   alignments     bowtie  report     alignments select  random note   modes   much slower   default   experience  example      twice  fast    excessive backtrack  aligner  describe  far    case encounter sequence  cause excessive backtrack  occur   aligner spend    effort fruitlessly backtrack  position close   ' end   query bowtie mitigate excessive backtrack   novel technique  'double indexing' two indices   genome  create one contain  bwt   genome call  'forward' index   second contain  bwt   genome   character sequence reverse  reverse complement call  'mirror' index  see   help consider  match policy  allow one mismatch   alignment  valid alignment  one mismatch fall  one  two case accord   half   read contain  mismatch bowtie proceed  two phase correspond   two case phase  load  forward index  memory  invoke  aligner   constraint   may  substitute  position   query' right half phase  use  mirror index  invoke  aligner   reverse query   constraint   aligner may  substitute  position   reverse query' right half  original query' leave half  constraints  backtrack   right half prevent excessive backtrack whereas  use  two phase  two indices maintain full sensitivity unfortunately    possible  avoid excessive backtrack fully  alignments  permit   two   mismatch   experiment   observe  excessive backtrack  significant    read  many lowquality position    align  align poorly   reference  case  trigger  excess   backtrack per read    many legal combinations  lowquality position   explore   possibilities  exhaust  mitigate  cost  enforce  limit   number  backtrack allow   search  terminate default   limit prevent  legitimate lowquality alignments   report   expect     desirable tradeoff   applications phase maqlike search bowtie allow  user  select  number  mismatch permit default two   highquality end   read default  first  base  well   maximum acceptable quality distance   overall alignment default  quality value  assume  follow  definition  phred      probability  error    10log    read   reverse complement  candidates  alignment   reference  clarity  discussion consider   forward orientation see additional data file  supplementary discussion    explanation    reverse complement  incorporate  first  base   highquality end   read  term  eed'  seed consist  two halve      highquality end usually  ' end       lowquality end term  'hihalf'   'lohalf' respectively assume  default policy two mismatch permit   seed  reportable alignment  fall  one  four case  mismatch  seed case   mismatch  hihalf one  two mismatch  lohalf case   mismatch  lohalf one  two mismatch  hihalf case   one mismatch  hihalf one mismatch  lohalf case   case allow  number  mismatch   nonseed part   read   case  also subject   quality distance constraint  bowtie algorithm consist  three phase  alternate  use  forward  mirror indices  illustrate  figure  phase  use  mirror index  invoke  aligner  find alignments  case    phase    cooperate  find alignments  case  phase  find partial alignments  mismatch    hihalf  phase  attempt  extend  partial alignments  full alignments finally phase  invoke  aligner  find alignments  case  performance result  evaluate  performance  bowtie use read    genomes project pilot national center  biotechnology information ncbi short read archivesrr001115  total   million read  one lane  data   illumina instrument  trim     align   human reference genome ncbi build  unless specify otherwise read data   filter  modify besides trim    appear   archive  lead       read align somewhere   genome   experience   typical  raw data   archive  aggressive filter lead  higher alignment rat  faster alignment  run  perform   single cpu bowtie speedups  calculate   ratio  wallclock alignment time  wallclock  cpu time  give  demonstrate  inputoutput load  cpu contention   significant factor  time require  build  bowtie index   include   bowtie run time unlike compete tool bowtie  reuse  precomputed index   reference genome across many alignment run  anticipate  users  simply download  indices   public repository  bowtie site  provide indices  current build   human chimp mouse dog rat  arabidopsis thaliana genomes  well  many others result  obtain  two hardware platforms  desktop workstation   ghz intel core  processor     ram   largememory server   fourcore  ghz amd opteron processor     ram   denote ''  erver' respectively    server run red hat enterprise linux  release  comparison  soap  maq maq   popular aligner        among  fastest compete open source tool  align millions  illumina read   human genome soap  another open source tool    report  use  shortread project   table  presents  performance  sensitivity  bowtie  soap   maq  soap could   run     soap' memory footprint exceed  ' physical memory  oapcontig' version   soap binary  use  comparison  soap bowtie  invoke  ' '  mimic soap' default match policy  allow   two mismatch   alignment  disregard quality value   'maxns '  simulate soap' default policy  filter  read  five   noconfidence base   maq comparison bowtie  run   default policy  mimic maq' default policy  allow   two mismatch   first  base  enforce  overall limit     sum   quality value   mismatch read position  make bowtie' memory footprint  comparable  maq' bowtie  invoke   '' option   experiment  ensure    forward  mirror index  resident  memory  one time  number  read align indicate  soap   bowtie     comparable sensitivity   read align  either soap  bowtie   align     align  bowtie   soap    align  soap   bowtie maq   bowtie  also  roughly comparable sensitivity although bowtie lag     read align  either maq  bowtie   align     align  bowtie   maq    align  maq   bowtie   read map  maq   bowtie almost   due   flexibility  maq' alignment algorithm  allow  alignments   three mismatch   seed  remainder   read map  maq   bowtie  due  bowtie' backtrack ceiling maq' documentation mention  read contain 'polya artifacts'  impair maq' performance table  presents performance  sensitivity  bowtie  maq   read set  filter use maq' 'catfilter' command  eliminate polya artifacts  filter eliminate     read  experimental parameters  identical     experiment  table     observations   relative sensitivity  bowtie  maq apply  read length  performance  sequence technology improve read lengths  grow beyond     commonly see  public databases today bowtie maq  soap support read  lengths        respectively  maq versions   later support read lengths     table  shows performance result   three tool   use  align three set    untrimmed read   set   set    set   human genome   server platform  set     randomly sample   larger set ncbi short read archive srr003084   srr003092   srr003196   read  sample    three set     uniform perbase error rate  calculate  perbase phred qualities  read pass  maq' 'catfilter' bowtie  run    maqlike default mode    soaplike ' ' mode bowtie  also give  '' option  ensure    forward  mirror index  resident  memory  one time maq   use instead  maq     set   cannot align read longer    soap   run    set     support read longer     result show  maq' algorithm scale better overall  longer read lengths  bowtie  soap however bowtie  soaplike ' ' mode also scale  well bowtie   default maqlike mode scale well     read   substantially slower   read although   still    order  magnitude faster  maq parallel performance alignment   parallelize  distribute read across concurrent search thread bowtie allow  user  specify  desire number  thread option  bowtie  launch  specify number  thread use  pthreads library bowtie thread synchronize     fetch read output result switch  indices  perform various form  global bookkeeping   mark  read  'done' otherwise thread  free  operate  parallel substantially speed  alignment  computers  multiple processor core  memory image   index  share   thread    footprint   increase substantially  multiple thread  use table  shows performance result  run bowtie    fourcore server  one two  four thread index build bowtie use  flexible index algorithm     configure  trade   memory usage  run time table  illustrates  tradeoff  index  entire human reference genome ncbi build  contigs run  perform   server platform  indexer  run four time  different upper limit  memory usage  report time compare favorably  alignment time  compete tool  perform index  alignment less   hours  require  bowtie   build  query  wholehuman index   million read    genome project ncbi short read archivesrr001115   server   sixfold faster   equivalent maq run  bottommost row illustrate   bowtie indexer  appropriate arguments  memoryefficient enough  run   typical workstation     ram additional data file  supplementary discussions    explain  algorithm   content   result index",0
10,HISAT2,"Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype
GFM index and sequence search through the index To perform the LF mapping, the number of times that a ‘last’ column label of a given row r occurs up to and including r needs to be identified, which involves counting occurrences from the top of the table down to row r. This counting would be prohibitively time-consuming for the 3-Gb human genome. To accelerate the process, the table is partitioned into small blocks of only a few hundred rows each. Additional numbers are stored within each block recording the number of occurrences of a specific base that appear up to that block. We also optimized the local counting process, where we counted the number of times a specific base appeared within that block. This overall indexing scheme is called a GFM index (Supplementary Fig. 5). Supplementary Figure 6 illustrates how a read that contains a known one-base insertion was aligned to the genome using a GFM. Indexing repeat sequences (HISAT2) Given a read length R (for example, 100 bp), we first build a k-mer table from the reference genome sequence and its reverse complement together, where k was set to R and each k-mer must appear at least C times (for example five times) to be included. Note that we use both strands of the genome as a read is mapped to the reference and/or its reverse complement. Although we can directly use this k-mer table for aligning reads of length R, it would require a large amount of memory to store the sequences of all k-mers and their corresponding genomic coordinates. To reduce the memory use, we combined k-mers that originated from the same regions when possible. For example, suppose that there are 1,000 identical regions 200 bp in length in a reference genome. Each region has 101 100-mers with each 100-mer present in the 1,000 regions. If we were to store all coordinates of each k-mer, the number of all coordinates would be 101,000. However, if we can combine k-mers occurring in the same region into one sequence, we simply need to store one coordinate per region; thus, the number of coordinates would drop to 1,000. In practice, real genomes have identical sequences of varying length. Supplementary Figure 7 illustrates how to merge k-mers into repeat sequences, where we can use any k as the initial value. This approach substantially reduces the number of coordinates to store. For example, the number of 100-mers that occur more than five times in the human reference genome is 4,000,527, with the average number of coordinates corresponding to each 100-mer as 19.1. This amounts to a total of 76,446,383 coordinates that we would store using the naive approach. If we allow k-mers to be extended to up to a certain length (for example, 300 bp), we reduce the number of coordinates to 2,825,142. We refer to both k-mers and extended k-mers as repeat sequences. When k-mers are extended up to 300 bp, the number of repeat sequences is reduced from 4,000,527 to 121,793. This strategy guarantees that a read whose sequence is present ≥C times in the genome is mapped to all those locations. Similarly, a read pair in which both of its sequences are present ≥C times in the genome is mapped. More specifically, a read whose sequence is present n times (n ≥ C) is mapped to only one repeat sequence. The portion of the repeat sequence matching the read exactly includes n coordinates. This approach works perfectly for a fixed read length, R, which is typical of experiments using Illumina sequencers, although reads of a length close to R can also be handled with slightly decreased alignment sensitivity. HISAT2 also allows for building indexes of various read length and using only one (or a few) of them on an actual run so that it requires only a small amount of additional memory. We built a BWT/FM index and a minimizer-based k-mer table32 with a window size of five and k = 31 on these repeat sequences to enable rapid alignment of 100-bp reads with up to three mismatches. HISAT-genotype’s typing algorithm Because allele sequences may only be partially available (for example, exons only), HISAT-genotype first identifies two alleles on the basis of the sequences commonly available for all alleles, for example exons. For example, the IMGT/HLA database includes many sequences for some key exons of HLA genes, but it contains far fewer complete sequences comprising all exons, introns, and UTRs of the genes. So far 3,644 alleles have been classified for HLA-A. Although all alleles of HLA-A have known sequences for exons 2 and 3, only 383 alleles have full-length sequences available. The sequences for the remaining 3,261 alleles include either all eight exons or a subset of them. HLA-B has 4,454 alleles, of which 416 have full sequences available. HLA-C has 3,290 alleles, with only 590 fully sequenced, HLA-DQA1 has 76 alleles with 53 fully sequenced, HLA-DQB1 has 978 alleles with 69 fully sequenced, and HLA-DRB1 has 1,972 alleles, with only 43 fully sequenced. During this step, HISAT-genotype first chooses representative alleles from groups of alleles that have the same exon sequences. Next it identifies alleles in the representative alleles that are highly likely to be present in a sequenced sample. Then the other alleles from the groups with the same exons as the representatives are selected for assessment during the next step. Second, HISAT-genotype further identifies candidate alleles on the basis of both exons and introns. HISAT-genotype applies the following statistical model in each of the two steps to find maximum likelihood estimates of abundance through an EM algorithm33. We previously implemented an EM solution in our centrifuge system34, and we used a similar algorithm in HISAT-genotype, with modifications to the variable definitions as follows. The likelihood of a particular composition of allele abundance α: where R is the number of reads, A is the number of alleles, αj is the abundance of allele j, with a sum of 1 for all A alleles, lj is the length of allele j, and Cij is 1 if read i is aligned to allele j and 0 otherwise. Expectation (E-step): where nj is the estimated number of reads assigned to allele j. Maximization (M-step): where 𝛼 ′ 𝑗 α j ′ is the updated estimate of the abundance of allele j. α′ is then used in the next iteration. HISAT-genotype finds the abundances α that best reflect the given read alignments, that is, the abundances that maximize the likelihood function L(α|C) above by repeating the EM procedure no more than 1,000 times or until the difference between the previous and current estimates of abundances,",Alignment,"graphbased genome alignment  genotyping  hisat2  hisatgenotype
gfm index  sequence search   index  perform   map  number  time   last column label   give row  occurs    including  needs   identify  involve count occurrences   top   table   row   count would  prohibitively timeconsuming    human genome  accelerate  process  table  partition  small block     hundred row  additional number  store within  block record  number  occurrences   specific base  appear    block  also optimize  local count process   count  number  time  specific base appear within  block  overall index scheme  call  gfm index supplementary fig  supplementary figure  illustrates   read  contain  know onebase insertion  align   genome use  gfm index repeat sequence hisat2 give  read length   example    first build  kmer table   reference genome sequence   reverse complement together    set     kmer must appear  least  time  example five time   include note   use  strand   genome   read  map   reference andor  reverse complement although   directly use  kmer table  align read  length   would require  large amount  memory  store  sequence   kmers   correspond genomic coordinate  reduce  memory use  combined kmers  originate    regions  possible  example suppose     identical regions    length   reference genome  region   mers   mer present    regions     store  coordinate   kmer  number   coordinate would   however    combine kmers occur    region  one sequence  simply need  store one coordinate per region thus  number  coordinate would drop    practice real genomes  identical sequence  vary length supplementary figure  illustrates   merge kmers  repeat sequence    use     initial value  approach substantially reduce  number  coordinate  store  example  number  mers  occur   five time   human reference genome     average number  coordinate correspond   mer    amount   total   coordinate   would store use  naive approach   allow kmers   extend     certain length  example    reduce  number  coordinate    refer   kmers  extended kmers  repeat sequence  kmers  extend      number  repeat sequence  reduce      strategy guarantee   read whose sequence  present ≥ times   genome  map    locations similarly  read pair      sequence  present ≥ times   genome  map  specifically  read whose sequence  present  times  ≥   map   one repeat sequence  portion   repeat sequence match  read exactly includes  coordinates  approach work perfectly   fix read length    typical  experiment use illumina sequencers although read   length close    also  handle  slightly decrease alignment sensitivity hisat2 also allow  build index  various read length  use  one        actual run    require   small amount  additional memory  build  bwtfm index   minimizerbased kmer table32   window size  five       repeat sequence  enable rapid alignment   read    three mismatch hisatgenotypes type algorithm  allele sequence may   partially available  example exons  hisatgenotype first identify two alleles   basis   sequence commonly available   alleles  example exons  example  imgthla database include many sequence   key exons  hla genes   contain far fewer complete sequence comprise  exons introns  utrs   genes  far  alleles   classify  hlaa although  alleles  hlaa  know sequence  exons      alleles  fulllength sequence available  sequence   remain  alleles include either  eight exons   subset   hlab   alleles     full sequence available hlac   alleles    fully sequenced hladqa1   alleles   fully sequenced hladqb1   alleles   fully sequence  hladrb1   alleles    fully sequence   step hisatgenotype first choose representative alleles  group  alleles     exon sequence next  identify alleles   representative alleles   highly likely   present   sequence sample    alleles   group    exons   representatives  select  assessment   next step second hisatgenotype  identify candidate alleles   basis   exons  introns hisatgenotype apply  follow statistical model     two step  find maximum likelihood estimate  abundance    algorithm33  previously implement   solution   centrifuge system34   use  similar algorithm  hisatgenotype  modifications   variable definitions  follow  likelihood   particular composition  allele abundance      number  reads    number  alleles    abundance  allele    sum      alleles    length  allele   cij    read   align  allele    otherwise expectation estep     estimate number  read assign  allele  maximization mstep   ′    ′   update estimate   abundance  allele  ′   use   next iteration hisatgenotype find  abundances   best reflect  give read alignments    abundances  maximize  likelihood function lαc   repeat   procedure     time    difference   previous  current estimate  abundances",0
11,STAR,"STAR: ultrafast universal RNA-seq aligner.
Many previously described RNA-seq aligners were developed as extensions of contiguous (DNA) short read mappers, which were used to either align short reads to a database of splice junctions or align split-read portions contiguously to a reference genome, or a combination thereof. In contrast to these approaches, STAR was designed to align the non-contiguous sequences directly to the reference genome. STAR algorithm consists of two major steps: seed searching step and clustering/stitching/scoring step. 2.1 Seed search The central idea of the STAR seed finding phase is the sequential search for a Maximal Mappable Prefix (MMP). MMP is similar to the Maximal Exact (Unique) Match concept used by the large-scale genome alignment tools Mummer (Delcher et al., 1999, 2002; Kurtz et al.) and MAUVE (Darling et al., 2004, 2010). Given a read sequence R, read location i and a reference genome sequence G, the MMP(R,i,G) is defined as the longest substring (Ri, Ri+ 1, … , Ri+MML− 1) that matches exactly one or more substrings of G, where MML is the maximum mappable length. We will explain this concept using a simple example of a read that contains a single splice junction and no mismatches (Fig. 1a). In the first step, the algorithm finds the MMP starting from the first base of the read. Because the read in this example comprises a splice junction, it cannot be mapped contiguously to the genome, and thus the first seed will be mapped to a donor splice site. Next, the MMP search is repeated for the unmapped portion of the read, which, in this case, will be mapped to an acceptor splice site. Note that this sequential application of MMP search only to the unmapped portions of the read makes the STAR algorithm extremely fast and distinguishes it from Mummer and MAUVE, which find all possible Maximal Exact Matches. This approach represents a natural way of finding precise locations of splice junctions in a read sequence and is advantageous over an arbitrary splitting of read sequences used in the split-read methods. The splice junctions are detected in a single alignment pass without any a priori knowledge of splice junctions’ loci or properties, and without a preliminary contiguous alignment pass needed by the junction database approaches. The MMP in STAR search is implemented through uncompressed suffix arrays (SAs) (Manber and Myers, 1993). Notably, finding MMP is an inherent outcome of the standard binary string search in uncompressed SAs, and does not require any additional computational effort compared with the full-length exact match searches. The binary nature of the SA search results in a favorable logarithmic scaling of the search time with the reference genome length, allowing fast searching even against large genomes. Advantageously, for each MMP the SA search can find all distinct exact genomic matches with little computational overhead, which facilitates an accurate alignment of the reads that map to multiple genomic loci (“multimapping” reads). In addition to detecting splice junctions, the MMP search, implemented in STAR, enables finding multiple mismatches and indels, as illustrated in Figure 1b. If the MMP search does not reach the end of a read because of the presence of one or more mismatches, the MMPs will serve as anchors in the genome that can be extended, allowing for alignments with mismatches. In some cases, the extension procedure does not yield a good genomic alignment, which allows identification of poly-A tails, library adapter sequences or poor sequencing quality tails (Fig. 1c). The MMP search is performed in both forward and reverse directions of the read sequence and can be started from user-defined search start points throughout the read sequence, which facilitates finding anchors for reads with errors near the ends and improves mapping sensitivity for high sequencing error rate conditions. Besides the efficient MMP search algorithm, uncompressed SAs also demonstrate a significant speed advantage over the compressed SAs implemented in many popular short read aligners (Supplementary Section 1.8). This speed advantage is traded off against the increased memory usage by uncompressed arrays, which is assessed further in Section 3.3. 2.2 Clustering, stitching and scoring In the second phase of the algorithm, STAR builds alignments of the entire read sequence by stitching together all the seeds that were aligned to the genome in the first phase. First, the seeds are clustered together by proximity to a selected set of ‘anchor’ seeds. We found that an optimal procedure for anchor selection is through limiting the number of genomic loci the anchors align to. All the seeds that map within user-defined genomic windows around the anchors are stitched together assuming a local linear transcription model. The size of the genomic windows determines the maximum intron size for the spliced alignments. A frugal dynamic programming algorithm (see Supplementary Section 1.5 for details) is used to stitch each pair of seeds, allowing for any number of mismatches but only one insertion or deletion (gap). Importantly, the seeds from the mates of paired-end RNA-seq reads are clustered and stitched concurrently, with each paired-end read represented as a single sequence, allowing for a possible genomic gap or overlap between the inner ends of the mates. This is a principled way to use the paired-end information, as it reflects better the nature of the paired-end reads, namely, the fact that the mates are pieces (ends) of the same sequence. This approach increases the sensitivity of the algorithm, as only one correct anchor from one of the mates is sufficient to accurately align the entire read. If an alignment within one genomic window does not cover the entire read sequence, STAR will try to find two or more windows that cover the entire read, resulting in a chimeric alignment, with different parts of the read mapping to distal genomic loci, or different chromosomes, or different strands (Supplementary Fig. S1). STAR can find chimeric alignments in which the mates are chimeric to each other, with a chimeric junction located in the unsequenced portion of the RNA molecule between two mates. STAR can also find chimeric alignments in which one or both mates are internally chimerically aligned, thus pinpointing the precise location of the chimeric junction in the genome. An example of the BCR-ABL fusion transcript detection from the K562 erythroleukemia cell line is given in the Supplementary Section 1.7 (Supplementary Fig. S2). The stitching is guided by a local alignment scoring scheme, with user-defined scores (penalties) for matches, mismatches, insertions, deletions and splice junction gaps, allowing for a quantitative assessment of the alignment qualities and ranks (see Supplementary Section 1.4 for details). The stitched combination with the highest score is chosen as the best alignment of a read. For multimapping reads, all alignments with scores within a certain user-defined range below the highest score are reported. Although the sequential MMP search only finds the seeds exactly matching the genome, the subsequent stitching procedure is capable of aligning reads with a large number of mismatches, indels and splice junctions, scalable with the read length. This characteristic has become ever more important with the emergence of the third-generation sequencing technologies (such as Pacific Biosciences or Ion Torrent) that produce longer reads with elevated error rates.",Alignment,"star ultrafast universal rnaseq aligner
many previously describe rnaseq aligners  develop  extensions  contiguous dna short read mappers   use  either align short read   database  splice junctions  align splitread portion contiguously   reference genome   combination thereof  contrast   approach star  design  align  noncontiguous sequence directly   reference genome star algorithm consist  two major step seed search step  clusteringstitchingscoring step  seed search  central idea   star seed find phase   sequential search   maximal mappable prefix mmp mmp  similar   maximal exact unique match concept use   largescale genome alignment tool mummer delcher     kurtz    mauve darling     given  read sequence  read location    reference genome sequence   mmprig  define   longest substring    …  rimml   match exactly one   substrings    mml   maximum mappable length   explain  concept use  simple example   read  contain  single splice junction   mismatch fig    first step  algorithm find  mmp starting   first base   read   read   example comprise  splice junction  cannot  map contiguously   genome  thus  first seed   map   donor splice site next  mmp search  repeat   unmapped portion   read    case   map   acceptor splice site note   sequential application  mmp search    unmapped portion   read make  star algorithm extremely fast  distinguish   mummer  mauve  find  possible maximal exact match  approach represent  natural way  find precise locations  splice junctions   read sequence   advantageous   arbitrary split  read sequence use   splitread methods  splice junctions  detect   single alignment pass without   priori knowledge  splice junctions loci  properties  without  preliminary contiguous alignment pass need   junction database approach  mmp  star search  implement  uncompress suffix array sas manber  myers  notably finding mmp   inherent outcome   standard binary string search  uncompress sas    require  additional computational effort compare   fulllength exact match search  binary nature    search result   favorable logarithmic scale   search time   reference genome length allow fast search even  large genomes advantageously   mmp   search  find  distinct exact genomic match  little computational overhead  facilitate  accurate alignment   read  map  multiple genomic loci “multimapping” read  addition  detect splice junctions  mmp search implement  star enable find multiple mismatch  indels  illustrate  figure    mmp search   reach  end   read    presence  one   mismatch  mmps  serve  anchor   genome    extend allow  alignments  mismatch   case  extension procedure   yield  good genomic alignment  allow identification  polya tail library adapter sequence  poor sequence quality tail fig   mmp search  perform   forward  reverse directions   read sequence    start  userdefined search start point throughout  read sequence  facilitate find anchor  read  errors near  end  improve map sensitivity  high sequence error rate condition besides  efficient mmp search algorithm uncompress sas also demonstrate  significant speed advantage   compress sas implement  many popular short read aligners supplementary section   speed advantage  trade    increase memory usage  uncompress array   assess   section   cluster stitch  score   second phase   algorithm star build alignments   entire read sequence  stitch together   seed   align   genome   first phase first  seed  cluster together  proximity   select set  anchor seed  find   optimal procedure  anchor selection   limit  number  genomic loci  anchor align    seed  map within userdefined genomic windows around  anchor  stitch together assume  local linear transcription model  size   genomic windows determine  maximum intron size   splice alignments  frugal dynamic program algorithm see supplementary section   detail  use  stitch  pair  seed allow   number  mismatch   one insertion  deletion gap importantly  seed   mat  pairedend rnaseq read  cluster  stitch concurrently   pairedend read represent   single sequence allow   possible genomic gap  overlap   inner end   mat    principled way  use  pairedend information   reflect better  nature   pairedend read namely  fact   mat  piece end    sequence  approach increase  sensitivity   algorithm   one correct anchor  one   mat  sufficient  accurately align  entire read   alignment within one genomic window   cover  entire read sequence star  try  find two   windows  cover  entire read result   chimeric alignment  different part   read map  distal genomic loci  different chromosomes  different strand supplementary fig  star  find chimeric alignments    mat  chimeric      chimeric junction locate   unsequenced portion   rna molecule  two mat star  also find chimeric alignments   one   mat  internally chimerically align thus pinpoint  precise location   chimeric junction   genome  example   bcrabl fusion transcript detection   k562 erythroleukemia cell line  give   supplementary section  supplementary fig   stitch  guide   local alignment score scheme  userdefined score penalties  match mismatch insertions deletions  splice junction gap allow   quantitative assessment   alignment qualities  rank see supplementary section   detail  stitch combination   highest score  choose   best alignment   read  multimapping read  alignments  score within  certain userdefined range   highest score  report although  sequential mmp search  find  seed exactly match  genome  subsequent stitch procedure  capable  align read   large number  mismatch indels  splice junctions scalable   read length  characteristic  become ever  important   emergence   thirdgeneration sequence technologies   pacific biosciences  ion torrent  produce longer read  elevate error rat",0
12,TopHat2,"TopHat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions
Given RNA-seq reads as input, TopHat2 begins by mapping reads against the known transcriptome, if an annotation file is provided. This transcriptome mapping improves the overall sensitivity and accuracy of the mapping. It also gives the whole pipeline a significant speed increase, owing to the much smaller size of the transcriptome compared with that of the genome After the transcriptome-mapping step, some reads remain unmapped because they are derived from unknown transcripts not present in the annotation, or because they contain many miscalled bases. In addition, there may be poorly aligned reads that have been mapped to the wrong location. TopHat2 aligns these unmapped or potentially misaligned reads against the genome (Figure 6, step 2). Any reads contained entirely within exons will be mapped, whereas other spanning introns may not be. Using unmapped reads from step 2, TopHat2 tries to find novel splice sites that are based on known junction signals (GT-AG, GC-AG, and AT-AC). TopHat2 also provides an option to allow users to remap some of the mapped reads, depending on the edit distance values of these reads; that is, those reads whose edit distance is greater than or equal to a user-provided threshold will be treated as unmapped reads. To accomplish this, the unmapped reads (and previously mapped reads with low alignment scores) are split into smaller non-overlapping segments (25 bp each by default) which are then aligned against the genome (Figure 6, step 3). Tophat2 examines any cases in which the left and right segments of the same read are mapped within a user-defined maximum intron size (usually between 50 and 100,000 bp). When this pattern is detected, TopHat2 re-aligns the entire read sequence to that genomic region in order to identify the most likely locations of the splice sites (Figure 6). Using a similar approach, indels and fusion breakpoints are also detected in this step. The genomic sequences flanking these splice sites are concatenated, and the resulting spliced sequences are collected as a set of potential transcript fragments. Any reads not mapped in the previous stages (or mapped very poorly) are then re-aligned with Bowtie2 [15] against this novel transcriptome. After these steps, some of the reads may have been aligned incorrectly by extending an exonic alignment a few bases into the adjacent intron (Figure 1; Figure 6, steps 3 to 5). TopHat2 checks if such alignments extend into the introns identified in the split-alignment phase; if so, it can realign these reads to the adjacent exons instead. In the final stage, TopHat2 divides the reads into those with unique alignments and those with multiple alignments. For the multi-mapped reads, TopHat2 gathers statistical information (for example, the number of supporting reads) about the relevant splice junctions, insertions, and deletions, which it uses to recalculate the alignment score for each read. Based on these new alignment scores, TopHat2 reports the most likely alignment locations for such multi-mapped reads. For paired-end reads, TopHat2 processes the two reads separately through the same mapping stages described above. In the final stage, the independently aligned reads are analyzed together to produce paired alignments, taking into consideration additional factors including fragment length and orientation. For the experiments described in this study, the program version numbers were: TopHat2 2.0.8, TopHat1 1.1.4, GSNAP 2013-01-23, RUM 1.12_01, MapSplice 1.15.2, and STAR 2.3.0e. For the specific parameters for each program, see Additional file 1, Table S9, and for the source code of TopHat 2.0.8,",Alignment,"tophat2 accurate alignment  transcriptomes   presence  insertions deletions  gene fusions
given rnaseq read  input tophat2 begin  map read   know transcriptome   annotation file  provide  transcriptome map improve  overall sensitivity  accuracy   map  also give  whole pipeline  significant speed increase owe   much smaller size   transcriptome compare     genome   transcriptomemapping step  read remain unmapped    derive  unknown transcripts  present   annotation    contain many miscall base  addition  may  poorly align read    map   wrong location tophat2 align  unmapped  potentially misalign read   genome figure  step   read contain entirely within exons   map whereas  span introns may   use unmapped read  step  tophat2 try  find novel splice sit   base  know junction signal gtag gcag  atac tophat2 also provide  option  allow users  remap    map read depend   edit distance value   read    read whose edit distance  greater   equal   userprovided threshold   treat  unmapped read  accomplish   unmapped read  previously map read  low alignment score  split  smaller nonoverlapping segment     default    align   genome figure  step  tophat2 examine  case    leave  right segment    read  map within  userdefined maximum intron size usually        pattern  detect tophat2 realign  entire read sequence   genomic region  order  identify   likely locations   splice sit figure  use  similar approach indels  fusion breakpoints  also detect   step  genomic sequence flank  splice sit  concatenate   result splice sequence  collect   set  potential transcript fragment  read  map   previous stag  map  poorly   realign  bowtie2    novel transcriptome   step    read may   align incorrectly  extend  exonic alignment   base   adjacent intron figure  figure  step    tophat2 check   alignments extend   introns identify   splitalignment phase     realign  read   adjacent exons instead   final stage tophat2 divide  read    unique alignments    multiple alignments   multimapped read tophat2 gather statistical information  example  number  support read   relevant splice junctions insertions  deletions   use  recalculate  alignment score   read base   new alignment score tophat2 report   likely alignment locations   multimapped read  pairedend read tophat2 process  two read separately    map stag describe    final stage  independently align read  analyze together  produce pair alignments take  consideration additional factor include fragment length  orientation   experiment describe   study  program version number  tophat2  tophat1  gsnap  rum 12_01 mapsplice   star    specific parameters   program see additional file  table     source code  tophat ",0
13,LordFast,"lordFAST: sensitive and Fast Alignment Search Tool for LOng noisy Read sequencing Data
Overview lordFAST is a heuristic anchor-based aligner for long reads generated by third generation sequencing technologies. lordFAST aims to find a set of candidate locations (ideally, only one) per read before the costly step of base-to-base alignment to the reference genome. lordFAST works in two main stages. In stage one, it builds an index from the reference genome, which is used to find short exact matches. The index is a combination of a lookup table and an (uncompressed) FM index. In stage two, it maps the long reads to the reference genome in four steps: (i) on each read, it identifies a fixed number of evenly spaced k-mers (k = 14 in the default settings), which are matched to the reference genome through the use of the index. For each such match, it obtains the longest exact matching (prefix) extension. Among these extended matches of each k-mer identified in each read, it finally chooses the longest (there could be more than one) which acts as anchor matches; (ii) for each read, it then splits the reference genome into overlapping windows (of length twice that of the read) and identifies each such window as a candidate region if the number of anchor matches in that window is above a threshold value; (iii) for each candidate region, it identifies the longest chain of ‘concordant’ anchor matches (i.e. chain of anchor matches which have equal respective spacing in the read and the reference genome); (iv) it obtains the base-to-base alignment by performing dynamic programing between consecutive anchor matches in the selected chain. We provide a more detailed description for each step below. 2.2 Stage one: reference genome indexing In order to build a (substring) index for the reference genome, we use a combination of a simple lookup table for initial short matches, and an (uncompressed) FM index for extending such initial matches. This combined index benefits from the speed of lookup table and the compactness of the Burrows–Wheeler transform (BWT) representation for the reference genome. The lookup table (with 4h entries for all possible h-mers) provides a constant time search capability for each h-mer’s position in the uncompressed FM index (Ferragina and Manzini, 2000) (in the default setting h = 12, but the user is given the option to pick any value). As is well known, the FM index provides a compact representation of a suffix array (Manber and Myers, 1993) which we use to find (exact matching) extensions of initial h-mer matches. Note that in order to be able to perform efficient search on both strands of the reference genome, we use an extension to the FM index implemented in fermi (Li, 2012). Our combined index provides a 29% speed up over the standard uncompressed FM index (see Supplementary Figs S2 and S3 for speed gain and extra memory usage). 2.3 Stage two: read mapping Given a set of long reads, lordFAST aligns one read at a time as follows: 2.3.1 Step 1: sparse extraction of anchor matches For a given read with length ℓ, lordFAST identifies C (user defined, default 1000) evenly spaced anchoring positions on the read. For each anchoring position, it finds the longest prefix match(es) (of length at least k = 14) to the genome as follows. First, it extracts the first h-mer starting from the anchoring position and uses the lookup table of the genome index to obtain the interval that represents the initial set of matching locations on the FM index. It then uses the LF-mapping operation of the FM index to extend the initial set of matches and identify the longest match(es). Note that using longest matches reduces the total number of anchor matches significantly. The longest matches are then added to the set of anchors, 𝕄 , as triplets (r, g, s) where r is the anchoring position on the read, g is the starting location of the longest match on the genome and s is the length of the match. At the end of this step, 𝕄 is partitioned into 𝕄 + and 𝕄 − based on the strand of the matching location on the genome. (Note that for reads that are ‘too short’, i.e. ℓ<C + k – 1, we use ℓ – k + 1 anchoring positions instead of C anchoring positions.) 2.3.2 Step 2: candidate region selection In order to select the candidate regions for alignment, lordFAST splits the reference genome into overlapping windows of size 2ℓ (as illustrated in Fig. 1a). For each window, it calculates two scores for the forward and reverse strands from anchor matches of the respective strands ( 𝕄 + and 𝕄 − ). For each anchor match falling in a window, it adds s – k + 1 to the score of that window. lordFAST keeps all the windows with score >scoremax∕f where f is the factor defining the significance of the window score (default 4) and scoremax is the maximum window score. In other words, lordFAST keeps those windows whose score is not significantly worse than the maximum window score. In cases where two overlapping windows both meet the minimum window score requirement, lordFAST will keep the one with higher window score in the final list (ties are broken by choosing the window with smaller reference coordinate). Figure 1b depicts an example of the selection process. Step 3: chaining and anchor selection Among all the anchor matches in a candidate region, lordFAST chooses a set of ‘concordant’ anchors using local chaining. The best local chain is a set of co-linear, non-overlapping anchors on the reference genome that has the highest score among all such sets (Ohlebusch and Abouelhoda, 2005). To calculate the best local chain, lordFAST assigns a weight to each anchor match equal to the length of the match. lordFAST supports two chaining algorithms. By default, it obtains the best chain using the dynamic programing based chaining algorithm (Ohlebusch and Abouelhoda, 2005). Note that the time complexity of this chaining algorithm is quadratic, but in practice, it is fast due to our small number of anchor matches per read. It is also possible for the user to select the alternative chaining algorithm based on clasp (Otto et al., 2011). The anchor matches in the best local chain form the basis of the alignment in that region. 2.3.4 Step 4: alignment lordFAST prioritizes the candidate regions based on their best chaining score and performs the final alignment for the top N regions (default value for N is 10). In order to generate the base-to-base alignment of a region, it uses anchor matches from the top scoring chain and performs banded global alignment for gaps between pairs of consecutive anchor matches. Furthermore, the alignment between the prefix of the read and the reference prior to the first anchor can be performed by the use of an anchored global-to-local alignment and the alignment between the suffix of the read and the reference following the last anchor can be computed in an identical fashion. This strategy is a widely used technique to avoid computing the full alignment between long sequences as that needs huge memory and computational time. lordFAST uses Edlib (Šošić and Šikić, 2017) for computing the global alignments and ksw library (https://github.com/attractivechaos/klib) for computing the global-to-local alignments. Edlib is a library implementing a fast bit-vector algorithm devised by Myers (1999). ksw, on the other hand, provides alignment extension based on affine gap-cost model. It is worth mentioning that lordFAST supports clipping as follows: if the prefix of the read before the first anchor (or, respectively, suffix of the read after the last anchor) has an alignment score/similarity which is lower than a threshold (thclip), lordFAST performs clipping of that prefix (or, respectively, suffix). This is done by using ksw library to extend the alignment as long as a significant drop in the alignment score/similarity is not observed (using a parameter similar to BLAST’s X-dropoff). In addition, lordFAST supports split alignment as follows: Let Si,j denote the substring of S that starts at position i and ends at position j. Suppose we are mapping a long read R to the reference genome G. Consider two consecutive anchors A = (rA, gA, sA) and B = (rB, gB, sB), as per the definition above, in the best chain chosen for a candidate window. If the alignment between R r A , r B and G g A , g B has a score lower than a threshold (thsplit) we split the alignment and report one alignment as primary and another as supplementary (as the definition in the SAM format specification). One alignment corresponds to the substring before anchor A and the other alignment corresponds to the substring after anchor B. Furthermore, since the drop in alignment score/similarity could be due to the presence of an inversion, we check if the alignment between the reverse complement of R r A , r B and G g A , g B has a score higher than thsplit. In that case, such an alignment will be also reported as another supplementary alignment.",Alignment,"lordfast sensitive  fast alignment search tool  long noisy read sequence data
overview lordfast   heuristic anchorbased aligner  long read generate  third generation sequence technologies lordfast aim  find  set  candidate locations ideally  one per read   costly step  basetobase alignment   reference genome lordfast work  two main stag  stage one  build  index   reference genome   use  find short exact match  index   combination   lookup table   uncompress  index  stage two  map  long read   reference genome  four step    read  identify  fix number  evenly spaced kmers      default settings   match   reference genome   use   index    match  obtain  longest exact match prefix extension among  extend match   kmer identify   read  finally choose  longest  could    one  act  anchor match    read   split  reference genome  overlap windows  length twice    read  identify   window   candidate region   number  anchor match   window    threshold value iii   candidate region  identify  longest chain  concordant anchor match  chain  anchor match   equal respective space   read   reference genome   obtain  basetobase alignment  perform dynamic program  consecutive anchor match   select chain  provide   detail description   step   stage one reference genome index  order  build  substring index   reference genome  use  combination   simple lookup table  initial short match   uncompress  index  extend  initial match  combine index benefit   speed  lookup table   compactness   burrowswheeler transform bwt representation   reference genome  lookup table   entries   possible hmers provide  constant time search capability   hmers position   uncompress  index ferragina  manzini    default setting      user  give  option  pick  value   well know   index provide  compact representation   suffix array manber  myers    use  find exact match extensions  initial hmer match note   order   able  perform efficient search   strand   reference genome  use  extension    index implement  fermi    combine index provide   speed    standard uncompress  index see supplementary figs     speed gain  extra memory usage  stage two read map give  set  long read lordfast align one read   time  follow  step  sparse extraction  anchor match   give read  length  lordfast identifies  user define default  evenly space anchor position   read   anchor position  find  longest prefix match  length  least      genome  follow first  extract  first hmer start   anchor position  use  lookup table   genome index  obtain  interval  represent  initial set  match locations    index   use  lfmapping operation    index  extend  initial set  match  identify  longest match note  use longest match reduce  total number  anchor match significantly  longest match   add   set  anchor    triplets        anchor position   read    start location   longest match   genome     length   match   end   step   partition       base   strand   match location   genome note   read    short        use      anchor position instead   anchoring position  step  candidate region selection  order  select  candidate regions  alignment lordfast split  reference genome  overlap windows  size   illustrate  fig    window  calculate two score   forward  reverse strand  anchor match   respective strand          anchor match fall   window  adds        score   window lordfast keep   windows  score scoremax∕     factor define  significance   window score default   scoremax   maximum window score   word lordfast keep  windows whose score   significantly worse   maximum window score  case  two overlap windows  meet  minimum window score requirement lordfast  keep  one  higher window score   final list tie  break  choose  window  smaller reference coordinate figure  depicts  example   selection process step  chain  anchor selection among   anchor match   candidate region lordfast choose  set  concordant anchor use local chain  best local chain   set  colinear nonoverlapping anchor   reference genome    highest score among   set ohlebusch  abouelhoda   calculate  best local chain lordfast assign  weight   anchor match equal   length   match lordfast support two chain algorithms  default  obtain  best chain use  dynamic program base chain algorithm ohlebusch  abouelhoda  note   time complexity   chain algorithm  quadratic   practice   fast due   small number  anchor match per read   also possible   user  select  alternative chain algorithm base  clasp otto     anchor match   best local chain form  basis   alignment   region  step  alignment lordfast prioritize  candidate regions base   best chain score  perform  final alignment   top  regions default value      order  generate  basetobase alignment   region  use anchor match   top score chain  perform band global alignment  gap  pair  consecutive anchor match furthermore  alignment   prefix   read   reference prior   first anchor   perform   use   anchor globaltolocal alignment   alignment   suffix   read   reference follow  last anchor   compute   identical fashion  strategy   widely use technique  avoid compute  full alignment  long sequence   need huge memory  computational time lordfast uses edlib šošić  šikić   compute  global alignments  ksw library   compute  globaltolocal alignments edlib   library implement  fast bitvector algorithm devise  myers  ksw    hand provide alignment extension base  affine gapcost model   worth mention  lordfast support clip  follow   prefix   read   first anchor  respectively suffix   read   last anchor   alignment scoresimilarity   lower   threshold thclip lordfast perform clip   prefix  respectively suffix     use ksw library  extend  alignment  long   significant drop   alignment scoresimilarity   observe use  parameter similar  blast xdropoff  addition lordfast support split alignment  follow let sij denote  substring    start  position   end  position  suppose   map  long read    reference genome  consider two consecutive anchors             per  definition    best chain choose   candidate window   alignment                 score lower   threshold thsplit  split  alignment  report one alignment  primary  another  supplementary   definition   sam format specification one alignment correspond   substring  anchor     alignment correspond   substring  anchor  furthermore since  drop  alignment scoresimilarity could  due   presence   inversion  check   alignment   reverse complement                 score higher  thsplit   case   alignment   also report  another supplementary alignment",0
14,BWAMEM,"Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM
Aligning a single query sequence 2.1.1 Seeding and re-seeding BWA-MEM follows the canonical seed-and-extend paradigm. It initially seeds an alignment with supermaximal exact matches (SMEMs) using an algorithm we found previously (Li, 2012, Algorithm 5), which essentially finds at each query position the longest exact match covering the position. However, occasionally the true alignment may not contain any SMEMs. To reduce mismappings caused by missing seeds, we introduce re-seeding. Suppose we have a SMEM of length l with k occurrences in the reference genome. If l is too large (over 28bp by default), we re-seed with the longest exact matches that cover the middle base of the SMEM and occur at least k + 1 times in the genome. Such seeds can be found by requiring a minimum occurrence in the original SMEM algorithm. 2.1.2 Chaining and chain filtering We call a group of seeds that are colinear and close to each other as a chain. We greedily chain the seeds while seeding and then filter out short chains that are largely contained in a long chain and are much worse than the long chain (by default, both 50% and 38bp shorter than the long chain). Chain filtering aims to reduce unsuccessful seed extension at a later step. Each chain may not always correspond to a final hit. Chains detected here do not need to be accurate. 2.1.3 Seed extension We rank a seed by the length of the chain it belongs to and then by the seed length. For each seed in the ranked list, from the best to the worst, we drop the seed if it is already contained in an alignment found before, or extend the seed with a banded affine-gap-penalty dynamic programming (DP) if it potentially leads to a new alignment. BWA-MEM’s seed extension differs from the standard seed extension in two aspects. Firstly, suppose at a certain extension step we come to reference position x with the best extension score achieved at query position y. BWAMEM will stop extension if the difference between the best extension score and the score at (x, y) is larger than Z+|x−y|×pgapExt, where pgapExt is the gap extension penalty and Z is an arbitrary cutoff. This heuristic avoids extension through a poorly aligned region with good flanking alignment. It is similar to the X-dropoff heuristic in BLAST (Altschul et al., 1997), but does not penalize long gaps in one of the reference or the query sequences. Secondly, while extending a seed, BWA-MEM tries to keep track of the best extension score reaching the end of the query sequence. If the difference between the best score reaching the end and the best local alignment score is below a threshold, the local alignment will be rejected even if it has a higher score. BWA-MEM uses this strategy to automatically choose between local and end-to-end alignments. It may align through true variations towards the end of a read and thus reduces reference bias, while avoids introducing excessive mismatches and gaps which may happen when we force a chimeric read into an end-to-end alignment. Percent mapped reads as a function of the false alignment rate under different mapping quality cutoff. Alignments with mapping quality 3 or lower are excluded. An alignment is wrong if after correcting clipping, its start position is within 20bp from the simulated position. 106 pairs of 101bp reads are simulated from the human reference genome using wgsim (http://bit.ly/wgsim2) with 1.5% substitution errors and 0.2% indel variants. The insert size follows a normal distribution N(500, 502 ). The reads are aligned back to the genome either as single end (SE; top panel) or as paired end (PE; bottom panel). GEM is configured to allow up to 5 gaps and to output suboptimal alignments (option ‘–e5 –m5 –s1’ for SE and ‘– e5 –m5 –s1 –pb’ for PE). GEM does not compute mapping quality. Its mapping quality is estimated with a BWA-like algorithm with suboptimal alignments available. Other mappers are run with the default setting except for specifying the insert size distribution. The run time in seconds on a single CPU core is shown in the parentheses. 2.2 Paired-end mapping 2.2.1 Rescuing missing hits Like BWA (Li and Durbin, 2009), BWAMEM processes a batch of reads at a time. For each batch, it estimates the mean µ and the variance σ 2 of the insert size distribution from reliable single-end hits. For the top 100 hits (by default) of either end, if the mate is unmapped in a window [µ − 4σ, µ + 4σ] from each hit, BWA-MEM performs SSE2-based Smith-Waterman alignment (SW; Farrar 2007) for the mate within the window. The second best SW score is recorded to detect potential mismapping in a long tandem repeat. Hits found from both the single-sequence alignment and SW rescuing will be used for pairing. 2.2.2 Pairing Given the i-th hit for the first read and j-th hit for the second, BWA-MEM computes their distance dij if the two hits are in the right orientation, or sets dij to infinity otherwise. It scores the pair (i, j) with Sij = Si + Sj − min{−a log4 P(dij ), U}, where Si and Sj are the SW score of the two hits, respectively, a is the matching score and P(d) gives the probability of observing an insert size larger than d assuming a normal distribution. ‘log4 ’ arises when we interpret SW score as odds ratio (Durbin et al., 1998). U is a threshold that controls pairing: if dij is small enough such that −a log4 P(dij ) < U, BWA-MEM prefers to pair the two ends; otherwise it prefers the unpaired alignments. BWA-MEM chooses the pair (i, j) that maximizes Sij as the final alignments for both ends. This pairing strategy jointly considers the alignment scores, the insert size and the possibility of chimeric read pairs.",Alignment,"align sequence read clone sequence  assembly contigs  bwamem
aligning  single query sequence  seed  reseed bwamem follow  canonical seedandextend paradigm  initially seed  alignment  supermaximal exact match smems use  algorithm  find previously   algorithm   essentially find   query position  longest exact match cover  position however occasionally  true alignment may  contain  smems  reduce mismappings cause  miss seed  introduce reseed suppose    smem  length    occurrences   reference genome     large  28bp  default  reseed   longest exact match  cover  middle base   smem  occur  least    time   genome  seed   find  require  minimum occurrence   original smem algorithm  chain  chain filter  call  group  seed   colinear  close      chain  greedily chain  seed  seed   filter  short chain   largely contain   long chain   much worse   long chain  default    38bp shorter   long chain chain filter aim  reduce unsuccessful seed extension   later step  chain may  always correspond   final hit chain detect    need   accurate  seed extension  rank  seed   length   chain  belong      seed length   seed   rank list   best   worst  drop  seed    already contain   alignment find   extend  seed   band affinegappenalty dynamic program    potentially lead   new alignment bwamems seed extension differ   standard seed extension  two aspects firstly suppose   certain extension step  come  reference position    best extension score achieve  query position  bwamem  stop extension   difference   best extension score   score     larger  zxypgapext  pgapext   gap extension penalty     arbitrary cutoff  heuristic avoid extension   poorly align region  good flank alignment   similar   xdropoff heuristic  blast altschul       penalize long gap  one   reference   query sequence secondly  extend  seed bwamem try  keep track   best extension score reach  end   query sequence   difference   best score reach  end   best local alignment score    threshold  local alignment   reject even     higher score bwamem use  strategy  automatically choose  local  endtoend alignments  may align  true variations towards  end   read  thus reduce reference bias  avoid introduce excessive mismatch  gap  may happen   force  chimeric read   endtoend alignment percent map read   function   false alignment rate  different map quality cutoff alignments  map quality   lower  exclude  alignment  wrong   correct clip  start position  within 20bp   simulate position  pair  101bp read  simulate   human reference genome use wgsim    substitution errors   indel variants  insert size follow  normal distribution     read  align back   genome either  single end  top panel   pair end  bottom panel gem  configure  allow    gap   output suboptimal alignments option              gem   compute map quality  map quality  estimate   bwalike algorithm  suboptimal alignments available  mappers  run   default set except  specify  insert size distribution  run time  second   single cpu core  show   parentheses  pairedend map  rescue miss hit like bwa   durbin  bwamem process  batch  read   time   batch  estimate  mean    variance     insert size distribution  reliable singleend hit   top  hit  default  either end   mate  unmapped   window         hit bwamem perform sse2based smithwaterman alignment  farrar    mate within  window  second best  score  record  detect potential mismapping   long tandem repeat hit find    singlesequence alignment   rescue   use  pair  pair give  ith hit   first read  jth hit   second bwamem compute  distance dij   two hit    right orientation  set dij  infinity otherwise  score  pair    sij      min{ log4 pdij  }        score   two hit respectively    match score   give  probability  observe  insert size larger   assume  normal distribution log4  arise   interpret  score  odds ratio durbin       threshold  control pair  dij  small enough    log4 pdij    bwamem prefer  pair  two end otherwise  prefer  unpaired alignments bwamem choose  pair    maximize sij   final alignments   end  pair strategy jointly consider  alignment score  insert size   possibility  chimeric read pair",0
15,Kart,"Kart: a divide-and-conquer algorithm for NGS read alignment 
Overview of our algorithms Most suffix/BWT array based aligners, which follow the canonical seed-and-extend methodology, initiate an alignment with an MEM (seed) and extend the alignment with different dynamic programming strategies. Therefore, the performance of an aligner is greatly affected by the algorithms for seed exploration and the strategies for handling inexact matches. These aligners are sequential in nature. We adopted a divide-and-conquer strategy to reduce the time-consuming gapped alignment step using dynamic programming, which is suitable for mapping highly similar fragment sequences (each read is essentially a copy of a specific genome fragment except for a small percentage of sequencing errors). 2.2 Simple pairs and normal pairs Since un-gapped (without indels) alignment is much faster than gapped alignment, for each mapped candidate region in the reference genome, we separate the given read sequence and their candidate regions into two groups: simple region pairs (abbreviated as simple pairs) and normal region pairs (normal pairs), where all simple pairs have perfect alignment (exact matches), and normal pairs require un-gapped/gapped alignment. Once the simple and normal pairs are identified, they can be processed and aligned independently and the final mapping result for a candidate region is simply the concatenation of the alignment of each simple and normal pair. Consider a read sequence R, the reference genome G, and the BWT array constructed from G and its reverse sequence G’. For simplicity and without losing generality, we assume G is the concatenation of G and G’ in the remainder of the paper. Let R[i1] be the i1-th nucleotide of R, and R[i1, i2] be the subsequence between R[i1] and R[i2]. Similarly, let G[j1] be the j1-th nucleotide of G, and G[j1, j2] be the subsequence between G[j1] and G[j2]. A locally maximal exact matches (LMEMs) on a given BWT array of length l is defined as a maximal exact match between R[i1, i2] (called the read block) and G[j1, j2] (called the genome block), where i2 – i1 = j2 - j1 = l - 1 and is denoted by a 4-tuple (i1, i2, j1, j2). We use ΔPos = (j1- i1) to represent the position difference between the read and genome block. 2.3 Finding all LMEMs for the given read sequence Since an LMEM represents an identical fragment pair between R and G, it is considered as a simple pair in this study. Kart finds all LMEMs via traversing a BWT array. An LMEM exploration starts from R[i1] and stops at R[i2] if the exact match extension reaches a mismatch at R[i2+1]. In such cases, the next LMEM exploration will skip R[i2+1] and starts from R[i2+2] because R[i2+1] is very likely a sequencing error or sequence variation. Kart only considers an LMEM (i1, i2, j1, j2) as a qualified simple pair if its length is no less than a predefined threshold k and its occurrence is less than 50. The value k is typically between 10 and 16, and is determined based on the size of the reference genome. Intuitively, a short LMEM (< k bp) might contain erroneous bases and would less likely include the true coordinate. A larger genome needs a larger k for a compromise between specificity and sensitivity. Figure 1 shows the algorithm of the LMEM exploration procedure. The function BWT_search is a general BWT traversal method which takes a read sequence as the input and returns desirable LMEMs as well as their occurrences in the reference genome. If there are no sequencing errors or variations, there should be only one LMEM which covers the whole read sequence (i.e. LMEM.len = |R|). However, in reality, sequencing errors and variations happen a lot and they break a read into several LMEMs with variable lengths. Kart considers all qualified LMEMs as simple pairs, and identify normal pairs according to the distribution of simple pairs to create one or more candidate alignments. The algorithm to explore all LMEMs with length ≥ k. BWT_search is the function to search for the occurrences of the maximal exact match for R[start, stop] on the given BWT array. It returns desirable LMEMs as well as their occurrences on the reference genome 2.4 Identifying simple pairs and normal pairs Since the same read can be mapped to multiple genome regions if it originates from a repetitive sequence, these simple pairs can also spread all over the genome. A candidate alignment is defined as an alignment between the read and a specific region on the reference genome. To identify all candidate alignments for the read, we cluster all adjacent simple pairs whose ΔPos differences are smaller than a predefined threshold g (the default value is 5). Therefore, neighboring simple pairs whose ΔPos differences < g are clustered together to form a candidate alignment. Note that two simple pairs in a candidate alignment could overlap due to tandem repeats, sequence variations or overlapping LMEMs (when doing 8-LMEMs, as explained in Section 2.5). If two simple pairs overlap in a candidate alignment, say, in the genome portion (the same goes for the read portion), we chop off the overlapped portion in the genome and its corresponding read portion from the shorter pair to ensure that all simple pairs are non-overlapping. Figure 2 illustrates an example of two overlapped simple pairs, in which simple pair A overlaps with simple pair B due to sequence variation at that corresponding region. Kart removes the overlap by shrinking the shorter simple pair, say A. In this way, any two simple pairs in the same candidate alignment will not share any nucleotide. We then create normal pairs filling the gaps between adjacent simple pairs to make a complete alignment as follows. Suppose two neighboring simple pairs are (i2q-1, i2q, j2q-1, j2q) and (i2q+1, i2q+2, j2q+1, j2q+2) respectively, and they must have i2q+1 − i2q> 1 or j2q+1 − j2q> 1, Kart inserts a normal pair to fill the gaps between the two regions. In such cases, a normal pair (ir, ir+1, jr, jr+1) is inserted where ir– i2q= i2q+1 – ir+1 = 1 if i2q+1 − i2q > 1, otherwise let ir= ir+1 = -1 (i.e., a null base). Likewise, jr– j2q= j2q+1 – jr+1 = 1 if j2q+1 − j2q> 1, otherwise let jr= jr+1 = -1. On the other hand, if the first (or the last) simple region in a candidate alignment does not cover the first (or the last) nucleotide of the read sequence, a normal pair would also be created to fill the gap. Figure 3 gives an example illustrating the concept of simple and normal pairs. In this example, the read sequence contains three substitution errors as well as an insertion error of size two. After the LMEM exploration, we can identify four simple pairs (labelled A, B, C and D). However, these four simple pairs do not cover the whole read sequence. Thus, we check every adjacent simple pairs and generate the corresponding normal pair according to the gaps in between: we generate the normal pair (11, 11, 321, 321) between simple pairs A and B; likewise, we generate the normal pair (23, 24, -1, -1) between simple pairs B and C, and the normal pair (49, 51, 357, 359) between simple pairs C and D. Thus, all of these simple pairs and normal pairs together form a complete alignment candidate. Four types of normal pairs Simple pairs are formed from LMEMs with perfect matches. They partition the read sequence into interlaced simple and normal region pairs, which can be independently aligned and the final alignment is simply the concatenation of the alignment of each simple and normal pair. A closer look at the normal pairs indicates that a substantial portion of normal pairs do not require gapped alignment either. When both the read block and the genome block of a normal pair are more than 30 bp long, we perform a second round of sequence partition operation to further divide it and reduce the portion that needs gapped alignment. This time, we look for LMEMs of size ≥ 8 bp within such normal pair. Since the size of such normal pair is much smaller than the whole genome, 8-mer index (from a hash table constructed on the fly) is used to identify matched 8-mer seeds between the read block and the genome block. These seeds were extended to LMEMs, referred to as 8-LMEMs. These 8-LMEMs could further separate a longer normal pair into shorter ones. When processing PacBio reads, if a normal pair (without 8-LMEMs) at the end of a read has length more than 5000, then its corresponding read block is simply clipped from further consideration, and we only perform a local alignment for that read to avoid making an alignment in a poor quality region. When processing Illumina reads, we found that if the read block and genome block of a normal pair have equal size, then it is very likely the normal pair only contains substitution errors and the un-gapped alignment gives rise to the best alignment for such pair; however, if a normal pair contains indel errors, the un-gapped alignment will result in low sequence identity. So, by checking the percentage of mismatches with a linear scan, we can determine whether a normal pair requires gapped alignment or not. Moreover, Illumina reads could contain adaptor sequences or become chimera with a tiny probability. We check the sequence identity of the normal block at both ends of a read and remove the one whose sequence identity is < 50%. In such cases, Kart clips the corresponding read block and report a local alignment instead. Summarizing the above discussion, we divided the normal pairs into the following four types: A normal pair is a NP-clip if (1) it is at the ends of a PacBio read and its length is more than 5000 or (2) it is at the ends of an Illumina read and its sequence identity is < 50%. A normal pair is a NP-gap free if its read block and genome block have equal size, and its number of mismatches in a linear scan is less than 20% of the block size. A normal pair is a NP-indel if one of the fragment (either its read block or genome block) is an empty string and the other contains at least one nucleotide. The remaining normal pairs are referred to as NP-NWs, which require Needleman-Wunsch algorithm for gapped alignment. Disregarding those NP-clips, one can see from Table 3 that those NP-NWs requiring gapped alignment were sufficiently separated so that their average lengths are around 20 for different datasets, which can be processed much faster than the original read. Paired-end reads mapping Paired-end reads are two reads that are physically associated within a certain distance from each other. They can help reduce mapping ambiguity and increase mapping reliability. Kart supports paired-end reads mapping as follows. To align paired-end reads, Kart finds the candidate alignments for each read separately and then compare the two groups of mapping results to see if there is a pair of alignments one from each group that satisfies the paired-end distance constraints. If there is no such pair, it implies that the paired-end reads contain more sequencing errors such that at least one of them is not mapped properly. In such cases, Kart will initiate a rescue procedure trying to find a proper pair of alignments based on the candidate alignments of the opposite read. The implementation of the rescue procedure is described in detail below. Suppose G1 and G2 are the two groups of candidate alignments corresponding to paired-end reads R1 and R2. Let G1 = {m1, m2, …, mp} and G2 = {n1, n2, …, nq} where m1, m2, …, and mp represent candidate alignments of R1; n1, n2, … and nq represent candidate alignments of R2. For each alignment m in G1 with the coordinate c, Kart tries to map R2 onto the downstream region of c according to the mapping procedure described above, but the LMEM length threshold is set to 10 to increase the sensitivity of the mapping. In doing so, Kart is able to compromise the noises of R2 and identify the best alignment n’ in the downstream region of R1’s alignment. Kart repeats the same procedure using each alignment n in G2 to find the best alignment m’ for R1 in the upstream region of n. At this moment, both the pair of mi and n’ and m’ and nj meet the paired-end constraint, and we select the pair with the highest sequence identity of the alignments. 2.7 Mapping quality score MAQ (Li et al., 2008a,b) introduced the idea of mapping quality to estimate the reliability of a read alignment. It can be converted into the probability of a query sequence being aligned incorrectly. The mapping quality is estimated based on the optimal and sub-optimal alignments. We adopted a scoring scheme to estimate the mapping quality. The mapping quality score MAPQ is estimated with the formula: MAPQ=30×[1−( S 1 − S 2 ) / S 1 ]*log( S 1 ) M A P Q = 30 × [ 1 - ( S 1 - S 2 ) / S 1 ] * l o g ( S 1 ) , where S1 is the optimal alignment score and S2 is the sub-optimal alignment score. MAPQ is limited to be between 0 and 60. If there are multiple alignments with the optimal score (i.e., repeats), MAPQ is 0 and Kart reports the first best alignment. 2.8 Summary of our algorithms Given a read sequence R, Kart identifies all LMEMs with variable lengths along the read sequence by the BWT search against the reference sequences. Each LMEM is turned into a simple pair or more if it appears multiple times in the reference. Adjacent simple pairs are then clustered according to their ΔPos. After removing overlaps between adjacent simple pairs, Kart fills in the gaps between simple pairs with normal pairs to make each cluster a complete candidate alignment. When both the read block and the genome block of a normal pair are more than 30 bp long, we perform a second round of sequence partitioning to further divide it and reduce the portion that needs gapped alignment. The final read alignment is the concatenation of simple/normal pairs in the same candidate alignment. Finally, Kart reports the alignment with the highest alignment score or paired alignments for paired-end reads.",Alignment,"kart  divideandconquer algorithm  ngs read alignment 
overview   algorithms  suffixbwt array base aligners  follow  canonical seedandextend methodology initiate  alignment   mem seed  extend  alignment  different dynamic program strategies therefore  performance   aligner  greatly affect   algorithms  seed exploration   strategies  handle inexact match  aligners  sequential  nature  adopt  divideandconquer strategy  reduce  timeconsuming gap alignment step use dynamic program   suitable  map highly similar fragment sequence  read  essentially  copy   specific genome fragment except   small percentage  sequence errors  simple pair  normal pair since ungapped without indels alignment  much faster  gap alignment   map candidate region   reference genome  separate  give read sequence   candidate regions  two group simple region pair abbreviate  simple pair  normal region pair normal pair   simple pair  perfect alignment exact match  normal pair require ungappedgapped alignment   simple  normal pair  identify    process  align independently   final map result   candidate region  simply  concatenation   alignment   simple  normal pair consider  read sequence   reference genome    bwt array construct     reverse sequence   simplicity  without lose generality  assume    concatenation       remainder   paper let ri1   i1th nucleotide    ri1    subsequence  ri1  ri2 similarly let gj1   j1th nucleotide    gj1    subsequence  gj1  gj2  locally maximal exact matches lmems   give bwt array  length   define   maximal exact match  ri1  call  read block  gj1  call  genome block               denote   tuple      use δpos     represent  position difference   read  genome block  find  lmems   give read sequence since  lmem represent  identical fragment pair       consider   simple pair   study kart find  lmems via traverse  bwt array  lmem exploration start  ri1  stop  ri2   exact match extension reach  mismatch  ri2   case  next lmem exploration  skip ri2  start  ri2  ri2   likely  sequence error  sequence variation kart  consider  lmem       qualify simple pair   length   less   predefined threshold    occurrence  less    value   typically       determine base   size   reference genome intuitively  short lmem    might contain erroneous base  would less likely include  true coordinate  larger genome need  larger    compromise  specificity  sensitivity figure  shows  algorithm   lmem exploration procedure  function bwt_search   general bwt traversal method  take  read sequence   input  return desirable lmems  well   occurrences   reference genome     sequence errors  variations     one lmem  cover  whole read sequence  lmemlen   however  reality sequence errors  variations happen  lot   break  read  several lmems  variable lengths kart consider  qualify lmems  simple pair  identify normal pair accord   distribution  simple pair  create one   candidate alignments  algorithm  explore  lmems  length ≥  bwt_search   function  search   occurrences   maximal exact match  rstart stop   give bwt array  return desirable lmems  well   occurrences   reference genome  identify simple pair  normal pair since   read   map  multiple genome regions   originate   repetitive sequence  simple pair  also spread    genome  candidate alignment  define   alignment   read   specific region   reference genome  identify  candidate alignments   read  cluster  adjacent simple pair whose δpos differences  smaller   predefined threshold   default value   therefore neighbor simple pair whose δpos differences    cluster together  form  candidate alignment note  two simple pair   candidate alignment could overlap due  tandem repeat sequence variations  overlap lmems   lmems  explain  section   two simple pair overlap   candidate alignment say   genome portion   go   read portion  chop   overlap portion   genome   correspond read portion   shorter pair  ensure   simple pair  nonoverlapping figure  illustrates  example  two overlap simple pair   simple pair  overlap  simple pair  due  sequence variation   correspond region kart remove  overlap  shrink  shorter simple pair say    way  two simple pair    candidate alignment   share  nucleotide   create normal pair fill  gap  adjacent simple pair  make  complete alignment  follow suppose two neighbor simple pair  i2q i2q j2q j2q  i2q i2q j2q j2q respectively   must  i2q  i2q   j2q  j2q  kart insert  normal pair  fill  gap   two regions   case  normal pair      insert   i2q i2q      i2q  i2q   otherwise let       null base likewise  j2q j2q      j2q  j2q  otherwise let        hand   first   last simple region   candidate alignment   cover  first   last nucleotide   read sequence  normal pair would also  create  fill  gap figure  gives  example illustrate  concept  simple  normal pair   example  read sequence contain three substitution errors  well   insertion error  size two   lmem exploration   identify four simple pair labelled      however  four simple pair   cover  whole read sequence thus  check every adjacent simple pair  generate  correspond normal pair accord   gap    generate  normal pair      simple pairs    likewise  generate  normal pair      simple pairs      normal pair      simple pairs    thus    simple pair  normal pair together form  complete alignment candidate four type  normal pair simple pair  form  lmems  perfect match  partition  read sequence  interlace simple  normal region pair    independently align   final alignment  simply  concatenation   alignment   simple  normal pair  closer look   normal pair indicate   substantial portion  normal pair   require gap alignment either    read block   genome block   normal pair      long  perform  second round  sequence partition operation   divide   reduce  portion  need gap alignment  time  look  lmems  size ≥   within  normal pair since  size   normal pair  much smaller   whole genome mer index   hash table construct   fly  use  identify match mer seed   read block   genome block  seed  extend  lmems refer   lmems  lmems could  separate  longer normal pair  shorter ones  process pacbio read   normal pair without lmems   end   read  length      correspond read block  simply clip   consideration    perform  local alignment   read  avoid make  alignment   poor quality region  process illumina read  find    read block  genome block   normal pair  equal size     likely  normal pair  contain substitution errors   ungapped alignment give rise   best alignment   pair however   normal pair contain indel errors  ungapped alignment  result  low sequence identity   check  percentage  mismatch   linear scan   determine whether  normal pair require gap alignment   moreover illumina read could contain adaptor sequence  become chimera   tiny probability  check  sequence identity   normal block   end   read  remove  one whose sequence identity      case kart clip  correspond read block  report  local alignment instead summarize   discussion  divide  normal pair   follow four type  normal pair   npclip       end   pacbio read   length           end   illumina read   sequence identity     normal pair   npgap free   read block  genome block  equal size   number  mismatch   linear scan  less     block size  normal pair   npindel  one   fragment either  read block  genome block   empty string    contain  least one nucleotide  remain normal pair  refer   npnws  require needlemanwunsch algorithm  gap alignment disregard  npclips one  see  table    npnws require gap alignment  sufficiently separate    average lengths  around   different datasets    process much faster   original read pairedend read map pairedend read  two read   physically associate within  certain distance      help reduce map ambiguity  increase map reliability kart support pairedend read map  follow  align pairedend read kart find  candidate alignments   read separately   compare  two group  map result  see     pair  alignments one   group  satisfy  pairedend distance constraints      pair  imply   pairedend read contain  sequence errors    least one     map properly   case kart  initiate  rescue procedure try  find  proper pair  alignments base   candidate alignments   opposite read  implementation   rescue procedure  describe  detail  suppose      two group  candidate alignments correspond  pairedend reads    let   {  … }    {  … }    …   represent candidate alignments     …   represent candidate alignments     alignment      coordinate  kart try  map  onto  downstream region   according   map procedure describe    lmem length threshold  set    increase  sensitivity   map    kart  able  compromise  noise    identify  best alignment    downstream region  r1s alignment kart repeat   procedure use  alignment     find  best alignment      upstream region     moment   pair         meet  pairedend constraint   select  pair   highest sequence identity   alignments  map quality score maq    2008ab introduce  idea  map quality  estimate  reliability   read alignment    convert   probability   query sequence  align incorrectly  map quality  estimate base   optimal  suboptimal alignments  adopt  score scheme  estimate  map quality  map quality score mapq  estimate   formula mapq          *log                         *             optimal alignment score     suboptimal alignment score mapq  limit          multiple alignments   optimal score  repeat mapq    kart report  first best alignment  summary   algorithms give  read sequence  kart identify  lmems  variable lengths along  read sequence   bwt search   reference sequence  lmem  turn   simple pair     appear multiple time   reference adjacent simple pair   cluster accord   δpos  remove overlap  adjacent simple pair kart fill   gap  simple pair  normal pair  make  cluster  complete candidate alignment    read block   genome block   normal pair      long  perform  second round  sequence partition   divide   reduce  portion  need gap alignment  final read alignment   concatenation  simplenormal pair    candidate alignment finally kart report  alignment   highest alignment score  pair alignments  pairedend read",0
16,NGMLR,"Accurate detection of complex structural variations using single molecule sequencing
NGMLR: Fast, Accurate Mapping of Long Single Molecule Reads NGMLR is designed to accurately map long single molecule sequencing reads from either Pacific Biosciences or Oxford Nanopore to a reference genome with the goal of enabling precise structural variation calls. We follow the terminology used by the SAM specification47 where a read mapping consists either of one linear alignment covering the full read length or multiple linear alignments covering non-overlapping segments of the read (i.e. split reads). The main challenge when mapping high error long-reads is to evaluate whether a read should be mapped to the reference genome with one linear alignment, or must be split. For example, the correct mapping for a read that spans an inversion can only be found when splitting the read into three segments. Conversely, reads that do not span a structural variation should always be mapped with a single linear alignment. However, error rates are high, and are not always uniform with some regions having an error rate of over 30%. These segments can cause read mappers to falsely split a read. Furthermore, the high insertion and deletion sequencing error of long-read technologies cause current read aligners to falsely split up large SVs into several smaller ones and make it difficult to detect exact break points. To address these challenges, NGMLR implements the following workflow (Figure 1a): NGMLR identifies sub-segments of the read and of the reference genome that show high similarity and can be aligned with a single linear alignment. These segments can contain small insertions and deletions, but must not span a larger structural variation breakpoint. In reference to BLAST’s High-scoring Segment Pairs (HSPs), we call those segment linear mapping pairs (LMPs). For each LMP, NGMLR extracts the read sequence and the reference sequence and uses the Smith-Waterman algorithm to compute a pairwise sequence alignment using a convex gap cost model that accounts for sequencing error and SVs at the same time. NGMLR scans the sequence alignments for regions of low sequence identity to identify small SVs that were missed in step (1) and (3). Finally, NGMLR selects the set of linear alignments with the highest joint score, computes a mapping quality for each alignment and reports them as the final read mapping in a SAM/BAM file. Convex scoring model When aligning high error long-reads it is crucial to choose an appropriate gap model as there are two sources of insertions and deletions (indels). Sequencing error predominantly causes very short randomly distributed indels (1–5bp) while longer indels (20bp+) are caused by genomic structural variations. A linear gap model appropriately models indels originating from sequencing error, but cannot account for longer indels from genomic variation as these large blocks occur as a single unit, not as the combination of multiple single base insertions or deletions. With affine gap models the gap-open penalty falsely causes short indels from sequencing error to cluster together for noisy long-reads, and has only little effect on longer indels, especially in repetitive regions of the genome. With the convex scoring model of NGMLR, extending an indel is penalized proportionally less the longer the indel is. Therefore, the convex scoring model encourages large alignment gaps, such as those occurring from a structural variation, to be grouped together into contiguous stretches (extending a large indel has relatively low cost), while small indels, which commonly occur as sequencing errors, remain separate (extending a 1 bp gap has almost the same cost as opening a new gap). Using a convex gap model to compute optimal alignments increases computation time substantially as each cell in the alignment matrix not only depends on three other cells, but on the full row and column it is located in 36. This would make it infeasible to use convex gap costs for aligning large long-read datasets, so we adapted a heuristic implementation of the convex gap cost algorithm found in the swalign library (https://github.com/mbreese/swalign). Instead of scanning the full cell and row while filling the alignment matrix, we use two additional matrixes to store indel length estimations for each cell. Furthermore, we use the initial sub-segment alignments to identify the part of the alignment matrix that is most likely to contain the correct alignment and skip all other cells of the matrix during alignment computation. (Supplementary Note 1). Sniffles: Robust Detection of Structural Variations from Long-read Alignments Sniffles operates within and between the long-read alignments to infer SVs. It applies five major steps (Figure 1b). Sniffles first estimates the parameters to adapt itself to the underlying data set, such as the distribution in alignment scores and distances between indels and mismatches on the read, as well as the ratios of the best and second best alignments scores. Sniffles then scans the read alignments and segments to determine if they potentially represent SVs. Putative SVs are clustered and scored based on the number of supporting reads, the type and length of the SV, consistency of the SV composition, and other features. Sniffles optionally genotypes the variant calls to identify homozygous or heterozygous SVs. Sniffles optionally provides a clustering of SVs based on the overlap with the same reads, especially to detect nested variants. For details on each step see Supplementary Note 2. In the following, we focus on the methods that are unique to Sniffles, which are the detection and analysis of alignment artifacts to reduce falsely called variants and the clustering of variants. Putative Variant Scoring The high error rate of the long-reads induces many alignments that falsely appear as SVs. Sniffles addresses these by scoring each putative variant using several characteristics that we have determined to be the most relevant to detecting SVs. The two main user thresholds are the number of high quality reads supporting the variant (set using the –s parameter) as well as the standard deviation of the coordinates in the start and stop breakpoint across all supporting reads. The minimum variant size reported defaults to 50bp, but can also be adjusted using the –l parameter. To account for additional noise in the data and imprecision of the breakpoints we use a quantile filtering to ignore outliers given a coverage of more than 8 reads. The computed standard deviations for both breakpoints are compared to the standard deviation of a uniform distribution representing spurious SV breakpoints reported in the region. SVs are only reported if both breakpoints are below this threshold. If the standard deviation for both breakpoints is < 5bp, the coordinates are marked as PRECISE in the VCF file. See Supplementary Note 2. Variant Scoring and Genotyping At the start of the program the user may specify that the VCF output should be genotyped. In this case, Sniffles stores summary information (coordinates and orientation) about all high quality reads that do not include a SVs in a binary file. This includes those reads that support the reference sequence that pass the thresholds for MQ and alignment score ratio. After the detection of SVs, the VCF file is read in, and Sniffles constructs a self-balancing tree of the variants. With this information, Sniffles then computes the fraction of reads that support each variant versus those that support the reference. Variants below the minimum allele frequency (default: below 30%) are considered unreliable; variants with high allele frequency (default: above 80%) are considered homozygous; and variants with an intermediate allele frequency are considered heterozygous. Note Sniffles does not currently consider higher ploidy, however this will be the focus of future work. See Supplementary Note 2. Clustering and Nested SVs To enable the study of closely positioned or nested SVs, Sniffles optionally clusters SVs that are supported by the same set of reads. Note that Sniffles does not fully phase the haplotypes, as it does not consider SNPs or small indels, but rather identifies SVs that occur together. If this option is enabled, Sniffles stores the names of each read that supports a SVs in a hash table keyed by the read name, with the list of SVs associated with that read name as the value. The hash table is used to find reads that span more than one event, and later to cluster reads that span the one or more of the same variants. In this way Sniffles can cluster two or more events, even if the distance between the events is larger than the read length. Future work will include a full phasing of hapolotypes including SVs, SNPs and other small variants. See Supplementary Note 2. Mapping and SV Evaluation Simulation of SV and reads As described above, SVs were randomly simulated on chromosome 21 and 22 of the human genome (GRCh37). Data sets were generated with 20 variants for each type of SV (tandem duplication, indel, inversion, translocation and nested) and sizes of these events (100, 250, 500, 1kb, 5kb, 10kb and 50kb). Illumina reads were simulated as 100bp paired end reads using the default parameters of dwgsim. For Pacbio and Oxford Nanopore we scanned the alignments of HG002 (GiaB) and NA12878, respectively, and measured their error profile using SURIVOR (option 2). The measured error profiles and read lengths were then used to simulate the reads for each simulated SV data set (Supplementary Note 3). Modified reference analysis To allow for a more realistic scenario, we also modified the human reference (GRCh37) and analyzed real reads to assess the introduced SVs. Here we could only simulate a subset of SV types to be insertions, deletions, inversions and translocations. We simulated 140 variants of each type on the human genome (GRCh37) using SURVIVOR (option 1) (Supplementary Note 5). Evaluation of long-read mappings All simulated reads were mapped to the human reference genome (GRCh37) using BWA-mem26, BLASR25, GraphMap27, MECAT28, LAST24, Minimap229, and NGMLR. Reads that overlap or map in close proximity to a simulated SV were extracted from the BAM files and used for evaluation. For the genuine datasets, we first mapped the reads to the unmodified reference genome (without SV) using BWA-MEM and extracted all reads that would span our simulated SV by at least 500 bp. Only these reads were then mapped to the modified reference genome using the four read mappers and used for evaluation. Both simulated and genuine reads were then divided into six categories (Supplementary Figure 3.5): Read mappings are considered “precise” if they fully identify the SV they cover. To fall into this category, read mappings have to cover all parts of the SV that are required for identification, e.g. a read mapping to an inversion has to cover the inverted part of the genome as well as the non-inverted sequences flanking the inversion. Furthermore, correct mappings have to be split at the simulated breakpoints (+/− 10bp) of the SV. Read mappings are considered “indicated” if they show the presence of the correct SV but as the wrong type, e.g. a duplication that is represented as an insertion, or show the correct SV but do not show the exact borders (>10bp away). Read mappings are considered “forced” if they indicated the wrong number of SVs (e.g. several small instead of a single long insertion) or contain a significant portion of mapping artifacts (eg. not simulated mismatches) over > 10% of the SV length. These include mappings such as a read that is aligned through a deletion or inversion (Figure 2, top). Read mappings are considered “trimmed” if they have been soft-clipped or otherwise trimmed so that they cannot indicate the SV and do not contain randomly aligned base pairs (ie. noisy regions) Read mappings that are split into more parts than required to cover the underlying SV are classified as “fragmented”. Read mappings that are supposed to map across the SV but are not mapped are considered “unaligned”. For all simulated SV types and sizes and all mappers, we count how many reads fall into the above categories, normalize by the number of simulated reads and visualize the result as barplots. Evaluation of SV callers After the SV calling each VCF file was evaluated using SURVIVOR 48 with appropriate parameter sets to compare the variants to the truth set. A SV is considered precise if its start and stop coordinate is within 10bp of the simulated start and stop coordinate and the caller predicted the correct type. A SV is considered indicated if the start and stop coordinate of the SV is within +-1kb of the simulated event regardless of the inferred type of SV. A simulated SV is considered not detected if there is no call that fulfill the two previous criteria. A SV is considered false-positive if the event was not simulated.",Alignment,"accurate detection  complex structural variations use single molecule sequencing
ngmlr fast accurate map  long single molecule read ngmlr  design  accurately map long single molecule sequence read  either pacific biosciences  oxford nanopore   reference genome   goal  enable precise structural variation call  follow  terminology use   sam specification47   read map consist either  one linear alignment cover  full read length  multiple linear alignments cover nonoverlapping segment   read  split read  main challenge  map high error longreads   evaluate whether  read   map   reference genome  one linear alignment  must  split  example  correct map   read  span  inversion    find  split  read  three segment conversely read    span  structural variation  always  map   single linear alignment however error rat  high    always uniform   regions   error rate     segment  cause read mappers  falsely split  read furthermore  high insertion  deletion sequence error  longread technologies cause current read aligners  falsely split  large svs  several smaller ones  make  difficult  detect exact break point  address  challenge ngmlr implement  follow workflow figure  ngmlr identify subsegments   read    reference genome  show high similarity    align   single linear alignment  segment  contain small insertions  deletions  must  span  larger structural variation breakpoint  reference  blast highscoring segment pair hsps  call  segment linear map pair lmps   lmp ngmlr extract  read sequence   reference sequence  use  smithwaterman algorithm  compute  pairwise sequence alignment use  convex gap cost model  account  sequence error  svs    time ngmlr scan  sequence alignments  regions  low sequence identity  identify small svs   miss  step    finally ngmlr select  set  linear alignments   highest joint score compute  map quality   alignment  report    final read map   sambam file convex score model  align high error longreads   crucial  choose  appropriate gap model    two source  insertions  deletions indels sequence error predominantly cause  short randomly distribute indels 5bp  longer indels 20bp  cause  genomic structural variations  linear gap model appropriately model indels originate  sequence error  cannot account  longer indels  genomic variation   large block occur   single unit    combination  multiple single base insertions  deletions  affine gap model  gapopen penalty falsely cause short indels  sequence error  cluster together  noisy longreads    little effect  longer indels especially  repetitive regions   genome   convex score model  ngmlr extend  indel  penalize proportionally less  longer  indel  therefore  convex score model encourage large alignment gap    occur   structural variation   group together  contiguous stretch extend  large indel  relatively low cost  small indels  commonly occur  sequence errors remain separate extend    gap  almost   cost  open  new gap use  convex gap model  compute optimal alignments increase computation time substantially   cell   alignment matrix   depend  three  cells    full row  column   locate    would make  infeasible  use convex gap cost  align large longread datasets   adapt  heuristic implementation   convex gap cost algorithm find   swalign library  instead  scan  full cell  row  fill  alignment matrix  use two additional matrixes  store indel length estimations   cell furthermore  use  initial subsegment alignments  identify  part   alignment matrix    likely  contain  correct alignment  skip   cells   matrix  alignment computation supplementary note  sniffle robust detection  structural variations  longread alignments sniffle operate within    longread alignments  infer svs  apply five major step figure  sniffle first estimate  parameters  adapt    underlie data set    distribution  alignment score  distance  indels  mismatch   read  well   ratios   best  second best alignments score sniffle  scan  read alignments  segment  determine   potentially represent svs putative svs  cluster  score base   number  support read  type  length    consistency    composition   feature sniffle optionally genotypes  variant call  identify homozygous  heterozygous svs sniffle optionally provide  cluster  svs base   overlap    read especially  detect nest variants  detail   step see supplementary note    follow  focus   methods   unique  sniffle    detection  analysis  alignment artifacts  reduce falsely call variants   cluster  variants putative variant score  high error rate   longreads induce many alignments  falsely appear  svs sniffle address   score  putative variant use several characteristics    determine     relevant  detect svs  two main user thresholds   number  high quality read support  variant set use   parameter  well   standard deviation   coordinate   start  stop breakpoint across  support read  minimum variant size report default  50bp   also  adjust use   parameter  account  additional noise   data  imprecision   breakpoints  use  quantile filter  ignore outliers give  coverage     read  compute standard deviations   breakpoints  compare   standard deviation   uniform distribution represent spurious  breakpoints report   region svs   report   breakpoints    threshold   standard deviation   breakpoints   5bp  coordinate  mark  precise   vcf file see supplementary note  variant score  genotyping   start   program  user may specify   vcf output   genotyped   case sniffle store summary information coordinate  orientation   high quality read    include  svs   binary file  include  read  support  reference sequence  pass  thresholds    alignment score ratio   detection  svs  vcf file  read   sniffle construct  selfbalancing tree   variants   information sniffle  compute  fraction  read  support  variant versus   support  reference variants   minimum allele frequency default    consider unreliable variants  high allele frequency default    consider homozygous  variants   intermediate allele frequency  consider heterozygous note sniffle   currently consider higher ploidy however     focus  future work see supplementary note  cluster  nest svs  enable  study  closely position  nest svs sniffle optionally cluster svs   support    set  read note  sniffle   fully phase  haplotypes     consider snps  small indels  rather identify svs  occur together   option  enable sniffle store  name   read  support  svs   hash table key   read name   list  svs associate   read name   value  hash table  use  find read  span   one event  later  cluster read  span  one      variants   way sniffle  cluster two   events even   distance   events  larger   read length future work  include  full phase  hapolotypes include svs snps   small variants see supplementary note  map   evaluation simulation    read  describe  svs  randomly simulate  chromosome      human genome grch37 data set  generate   variants   type   tandem duplication indel inversion translocation  nest  size   events    1kb 5kb 10kb  50kb illumina read  simulate  100bp pair end read use  default parameters  dwgsim  pacbio  oxford nanopore  scan  alignments  hg002 giab  na12878 respectively  measure  error profile use surivor option   measure error profile  read lengths   use  simulate  read   simulate  data set supplementary note  modify reference analysis  allow    realistic scenario  also modify  human reference grch37  analyze real read  assess  introduce svs   could  simulate  subset   type   insertions deletions inversions  translocations  simulate  variants   type   human genome grch37 use survivor option  supplementary note  evaluation  longread mappings  simulate read  map   human reference genome grch37 use bwamem26 blasr25 graphmap27 mecat28 last24 minimap229  ngmlr read  overlap  map  close proximity   simulate   extract   bam file  use  evaluation   genuine datasets  first map  read   unmodified reference genome without  use bwamem  extract  read  would span  simulate    least     read   map   modify reference genome use  four read mappers  use  evaluation  simulate  genuine read   divide  six categories supplementary figure  read mappings  consider “precise”   fully identify    cover  fall   category read mappings   cover  part      require  identification   read map   inversion   cover  invert part   genome  well   noninverted sequence flank  inversion furthermore correct mappings    split   simulate breakpoints  10bp    read mappings  consider “indicated”   show  presence   correct     wrong type   duplication   represent   insertion  show  correct     show  exact border 10bp away read mappings  consider “forced”   indicate  wrong number  svs  several small instead   single long insertion  contain  significant portion  map artifacts   simulate mismatch       length  include mappings    read   align   deletion  inversion figure  top read mappings  consider “trimmed”     softclipped  otherwise trim    cannot indicate      contain randomly align base pair  noisy regions read mappings   split   part  require  cover  underlie   classify  “fragmented” read mappings   suppose  map across      map  consider “unaligned”   simulate  type  size   mappers  count  many read fall    categories normalize   number  simulate read  visualize  result  barplots evaluation   callers    call  vcf file  evaluate use survivor   appropriate parameter set  compare  variants   truth set    consider precise   start  stop coordinate  within 10bp   simulate start  stop coordinate   caller predict  correct type    consider indicate   start  stop coordinate     within 1kb   simulate event regardless   infer type    simulate   consider  detect     call  fulfill  two previous criteria    consider falsepositive   event   simulate",0
17,BFAST,"BFAST: An Alignment Tool for Large Scale Genome Resequencing
Simulated Variant Classes Simulation strategy To better reflect the real alignment problems of interest, simulated reads are derived from the human genome (NCBI Build 36.1), rather than constructing an artificial random genome to permit the assessment of the sensitivity and accuracy of alignment of short reads that contain variants including errors, single base mismatches, insertions, and deletions as well as combinations. As single base mismatches are an error mode common to all technologies, we investigate high mismatch rates, as well as mismatches in combination with insertions or deletions, as might occur in reads that contain both a real variant and errors, which occurs frequently in practice. We evaluate the different variant states separately in order to obtain accuracy measures for each type of event, as they differ in the degree of alignment difficulty. We examine both the true positive rate, or the sensitivity, to assess what fraction of reads can be located back to their appropriate location, and we assess the mismapping rate, which is determined as the fraction of all reads that are mapped to the genome that are mapped to an incorrect location. Ideally, a method will maximize true positives and minimize mismapping. For these comparisons, we do not require that the exact edits (mismatches, insertions, and deletions) introduced in a simulated read be observed, but rather that the read be placed approximately in the correct location since some methods can align a read with an indel to the approximate location, but never call or specify the indel since they perform ungapped alignments. We tally the results in this manner so as to perform a more reasonable comparison between the core alignment aspects in the context of the whole human genome of other methods. We simulated reads from the human genome by creating sets of reads with a fixed number of variants. To produce a synthetic dataset, we randomly choose an L letter long substring from the human genome. Each selected string was randomly altered to contain a specified number of mismatches, insertions or deletions, to produce a final read for the variant class of reads. We generated 10,000 reads for each variant class. This number of reads was sufficient to obtain robust performance statistics. In total, 187 different variant-specific nucleotide datasets were created: Reads with exactly x mismatches (0≤x≤10). Reads with one contiguous x letter insertion (1≤x≤10) and y mismatches (0≤y≤5). Reads with one contiguous x letter deletion (1≤x≤10) and y mismatches (0≤y≤5). Similarly, we also generated 10,000 50 bp variant classes in ABI SOLiD color space: Reads with one SNP and x color errors (0≤x≤5). Reads with one SNP, a 10 bp deletion, and x color errors (0≤x≤5). Reads with one SNP, a 10 bp insertion, and x color errors (0≤x≤5). The mismatches, and insertion or deletion break points, and color errors (ABI SOLiD data only) were uniformly distributed within the reads. For the nucleotide data, the high number of mismatches, and for the color space data, the high number of color errors, are meant to represent reads confounded by the impact of sequencing errors on both SNP and indel detection. This is especially important for ABI SOLiD color space data due to its higher error rate, which is corrected only after successful genomic alignment. Additionally, the high number of mismatches (for nucleotide data) considered might correspond to a read from a variant dense region, where several single base variants are further confounded by several read errors. Since BWA and MAQ rely on the color qualities for ABI SOLiD data to detect errors, we give color errors a color quality of 20, and 30 otherwise. The simulated datasets are available at http://bfast.sourceforge.net. We call a read correctly aligned if the read was aligned uniquely within 10 bases of the original location. If two alignments were found with the same best score, the read was not called correctly aligned. In our simulations we do not require that the errors, SNPs, or indels be placed correctly or even found by other methods, but instead require that the read is placed within 10 bases of the true location, thus assessing global alignment rather than local alignment. We evaluated each algorithm using one compute node, with two dual-core AMD64 processors at 2.0GHz and with 32GB of RAM. Each algorithm was run as a single process, and thus does not take advantage of any multi-threading or parallel processing capabilities of the algorithms, including those of BFAST. This comparison is done only to evaluate the relative speeds of the various algorithms under comparable hardware architecture, since many processors typically would be used in practice, which is the practical solution implemented with BFAST in practice. The precise settings for each algorithm, including BFAST, are described in the Supplemental Materials S1: Section Algorithm Settings for Simulations. Illumina Datasets For demonstration purposes, we used a 10.9 million 36 base read data set from the human genome. In total, 33 different regions with known mutations across 5 genes were PCR amplified individually and pooled. Amplicon sizes ranged from 191 bp to 762 bp. After purifying each amplicon with QIAGEN PCR Purification Kit, the amplicons were pooled in one tube to create an equal mixture of all products. The sequencing library from the genomic fragments was prepared using manufacturer provided genomic library preparation protocol version 2.3 (Illumina, La Jolla, CA). Specifically, this dataset consisted of sequence from PCR products known to contain 13 mismatches, as well as 6 small insertions and 6 small deletions, and sequenced at a depth of coverage generally exceeding 1000-fold. We also analyze 3.5 million 55 base paired-end reads of human genomic sequence from our Illumina GAII sequencer. Libraries were generated from genomic DNA. We then selected only the first end of the pair, giving us 3.5 million 55 base pair single-end reads for alignment. ABI SOLiD Datasets One million reads from two different runs of in house generated ABI SOLiD sequencer data was used for all comparisons, as this is a sufficient dataset to offer reasonable comparison. Both datasets consisted of sequences from human genomic DNA, generated by using standard 25 base and 50 base manufacturer supplied protocols. Support for Paired-End Data BFAST supports paired-end data by finding CALs for each end separately. Before local alignment, the user has the option to mirror CALs for one end using the other by specifying an estimated paired-end insert length. The paired-end insert length can be inferred by examining paired-end reads for which each end has only one CAL. The option to mirror (or rescue) one end of the read can help either to improve accuracy or to use one end of the pair as anchor for the other. Each CAL for each end is then locally aligned independently. The criteria to choose the best pair of alignments for the ends are then dictated by the user, and can be based on best-combined score, uniqueness, as well as other post-alignment filtering criteria. Further details on paired-end support can be found in the Supplemental Materials S1. Support for ABI SOLiD Color Space To support ABI SOLiD color space reads[21], [22], we first convert the reference sequence to color space such that each genomic read offset is artificially started with an A base to mimic the process of decoding within the SOLiD system which always generates an A terminal base in the ligated oligo in library creation. CALs are identified in color space, under the assumption that errors are more common than variants, and therefore more color errors will occur than variants encoded in color space. After finding CALs for each read, we use a modified local alignment algorithm previously described for color space reads by Homer et al.[17] and Rumble et al. [23]. This local alignment algorithm searches the space of all possible color errors, nucleotide mismatches, insertions and deletions. In this process, BFAST is able to use the entire color string for alignment.",Alignment,"bfast  alignment tool  large scale genome resequencing
simulated variant class simulation strategy  better reflect  real alignment problems  interest simulate read  derive   human genome ncbi build  rather  construct  artificial random genome  permit  assessment   sensitivity  accuracy  alignment  short read  contain variants include errors single base mismatch insertions  deletions  well  combinations  single base mismatch   error mode common   technologies  investigate high mismatch rat  well  mismatch  combination  insertions  deletions  might occur  read  contain   real variant  errors  occur frequently  practice  evaluate  different variant state separately  order  obtain accuracy measure   type  event   differ   degree  alignment difficulty  examine   true positive rate   sensitivity  assess  fraction  read   locate back   appropriate location   assess  mismapping rate   determine   fraction   read   map   genome   map   incorrect location ideally  method  maximize true positives  minimize mismapping   comparisons    require   exact edit mismatch insertions  deletions introduce   simulate read  observe  rather   read  place approximately   correct location since  methods  align  read   indel   approximate location  never call  specify  indel since  perform ungapped alignments  tally  result   manner    perform   reasonable comparison   core alignment aspects   context   whole human genome   methods  simulate read   human genome  create set  read   fix number  variants  produce  synthetic dataset  randomly choose   letter long substring   human genome  select string  randomly alter  contain  specify number  mismatch insertions  deletions  produce  final read   variant class  read  generate  read   variant class  number  read  sufficient  obtain robust performance statistics  total  different variantspecific nucleotide datasets  create read  exactly  mismatch ≤≤ read  one contiguous  letter insertion ≤≤   mismatch ≤≤ read  one contiguous  letter deletion ≤≤   mismatch ≤≤ similarly  also generate    variant class  abi solid color space read  one snp   color errors ≤≤ read  one snp    deletion   color errors ≤≤ read  one snp    insertion   color errors ≤≤  mismatch  insertion  deletion break point  color errors abi solid data   uniformly distribute within  read   nucleotide data  high number  mismatch    color space data  high number  color errors  mean  represent read confound   impact  sequence errors   snp  indel detection   especially important  abi solid color space data due   higher error rate   correct   successful genomic alignment additionally  high number  mismatch  nucleotide data consider might correspond   read   variant dense region  several single base variants   confound  several read errors since bwa  maq rely   color qualities  abi solid data  detect errors  give color errors  color quality     otherwise  simulate datasets  available    call  read correctly align   read  align uniquely within  base   original location  two alignments  find    best score  read   call correctly align   simulations    require   errors snps  indels  place correctly  even find   methods  instead require   read  place within  base   true location thus assess global alignment rather  local alignment  evaluate  algorithm use one compute node  two dualcore amd64 processors  0ghz   32gb  ram  algorithm  run   single process  thus   take advantage   multithreading  parallel process capabilities   algorithms include   bfast  comparison     evaluate  relative speed   various algorithms  comparable hardware architecture since many processors typically would  use  practice    practical solution implement  bfast  practice  precise settings   algorithm include bfast  describe   supplemental materials  section algorithm settings  simulations illumina datasets  demonstration purpose  use   million  base read data set   human genome  total  different regions  know mutations across  genes  pcr amplify individually  pool amplicon size range        purify  amplicon  qiagen pcr purification kit  amplicons  pool  one tube  create  equal mixture   products  sequence library   genomic fragment  prepare use manufacturer provide genomic library preparation protocol version  illumina  jolla  specifically  dataset consist  sequence  pcr products know  contain  mismatch  well   small insertions   small deletions  sequence   depth  coverage generally exceed fold  also analyze  million  base pairedend read  human genomic sequence   illumina gaii sequencer libraries  generate  genomic dna   select   first end   pair give   million  base pair singleend read  alignment abi solid datasets one million read  two different run   house generate abi solid sequencer data  use   comparisons     sufficient dataset  offer reasonable comparison  datasets consist  sequence  human genomic dna generate  use standard  base   base manufacturer supply protocols support  pairedend data bfast support pairedend data  find cals   end separately  local alignment  user   option  mirror cals  one end use    specify  estimate pairedend insert length  pairedend insert length   infer  examine pairedend read    end   one cal  option  mirror  rescue one end   read  help either  improve accuracy   use one end   pair  anchor     cal   end   locally align independently  criteria  choose  best pair  alignments   end   dictate   user    base  bestcombined score uniqueness  well   postalignment filter criteria  detail  pairedend support   find   supplemental materials  support  abi solid color space  support abi solid color space read   first convert  reference sequence  color space    genomic read offset  artificially start    base  mimic  process  decode within  solid system  always generate   terminal base   ligate oligo  library creation cals  identify  color space   assumption  errors   common  variants  therefore  color errors  occur  variants encode  color space  find cals   read  use  modify local alignment algorithm previously describe  color space read  homer    rumble     local alignment algorithm search  space   possible color errors nucleotide mismatch insertions  deletions   process bfast  able  use  entire color string  alignment",0
18,SeqAlto,"Fast and accurate read alignment for resequencing
SeqAlto searches for the global alignment of sequencer reads to a reference using the commonly known seed and extend approach. There are three phases to the alignment process: index construction, ungapped alignment and Needleman–Wunsch extension. Define a k-mer (seed) to be a sequence of nucleotides of length k. Each nucleotide must come from the set {A, C, T, G}. Hence, each k-mer is encoded as an unsigned integer with 2 bits per nucleotide. On a 64-bit system, this allows for k-mers of size of up to 32 bp. If the a k-mer has a nucleotide other than {A, C, T, G}, such as N, it is replaced with a uniform random nucleotide since there is only a 2-bit alphabet available. This encoding is the same approach used in BWA and many other tools. Let the index be defined as a list of tuples recording the k-mer and the location in the reference of that k-mer. The index is sorted by the numeric value of each k-mer for fast searching. SeqAlto achieves its high speed first by using much larger k-mer sizes compared to existing approaches, examining less repetitive k-mers first and by adaptively stopping the k-mer search. Large contiguous k-mers greatly reduce the number of locations of each k-mer in the reference to mostly unique hits. In general, large k-mers would reduce the sensitivity of the alignment. However, for longer reads this choice of k-mer size does not reduce sensitivity as shown in the simulation results. Furthermore, a novel sub-sampling approach is used to reduce the amount of memory required for the index to under 8 GB. Compared to BWT-based approaches for large k-mer search, our approach requires less random access to memory per index lookup resulting in significantly improved performance. 5.1 Index construction A genome index ideally contains the location of all overlapping k-mers in the genome. Naively storing all k-mers of size between 17 bp and 32 bp in one strand of the human genome requires about 36 GB of memory. Using a combination, of two strategies, we can reduce the size of the index to under 8 GB with only a minor penalty on the sensitivity. This enables SeqAlto to be run on almost all desktop computers. These strategies can be tuned for more powerful computers as they become available. At the first stage of the index construction, each overlapping k-mer together with its location is extracted from the genome and stored in an array W. This array of tuples is then sorted by the k-mer value. Then the leading m bits are removed from each k-mer such that (2k – m) is less than 32. These m bits are stored in a prefix array P that records the start and end locations in W of k-mers tuples prefixed with the same m bits. Since W is sorted, k-mers with the same prefix exist next to each other. Now, only the (2k – m) remaining bits of each k-mer needs to be stored in W. This array of tuples W together with the prefix array P form the genome index. Considering only genomes with less than 4 billion nucleotides, this two-level index reduces the size of each (k-mer, location) tuple in the index to 8 Bytes instead of 12 Bytes. Hence, for small prefix table sizes the entire human genome index will only take about 24 G to store. This index construction also greatly improves the lookup speed over a naive binary search. SeqAlto has the option to not store every k-mer in the reference. In order to reduce memory consumption without greatly impacting alignment, quality and speed of alignment we only store a k-mer if it satisfies a hash function f(x), where x is the numeric value of the k-mer. The hash function f(x) is chosen so that there is approximately uniform coverage of the genome. In SeqAlto, (1) is used. was selected so that memory usage for the human genome is under 8 GB and for Chromosome 1, the median gap size is 3 bp and the largest gap apart from the centromere is 131 bp. Empirically, there is little loss in sensitivity from using this approach as seen in the Section 3. In principle, this expression can also be easily tuned for machines with more memory, as any expression of similar form is adequate. On machines with 32 GB or more memory, this second step is not required and a larger index can be constructed for improved sensitivity and accuracy. The index is essentially a large sorted list. Hence, it is easy to add arbitrary additional k-mers to the index with negligible cost to both the construction and search. k-mers overlapping known SNPs and indels could easily be added to the index to improve the sensitivity of alignment. This change would only moderately increase the size of the index. 5.2 Ungapped alignment The first stage of the alignment process is the ungapped alignment stage. This stage is separate from the gapped stage since we expect that most gapped reads will not align anywhere in the genome as high-quality ungapped alignments. This separation also allows for easy interfacing with a hardware accelerator for the gapped alignment. Each read is treated separately in the alignment process. All overlapping k-mers are extracted from each read. k-mers satisfying the function f(x) as defined in (1) are retained and the other k-mers are discarded. Highly repetitive k-mers are also discarded. Each k-mer is then searched for in the index according to the following order. Starting with the non-overlapping k-mers, the number of locations the k-mer exists in the genome is computed from the index. If this number is less than a predefined variable MAX_LOC, all of the locations are examined for an ungapped alignment of the read. Once all k-mers with number of locations less than MAX_LOC have been examined, the other k-mers are examined. Due to the large k-mer size, many k-mers either align uniquely to the genome or do not align at all. Hence, the ungapped alignment stage proceeds very quickly. Similar to the idea in Baeza-yates and Perleberg (1992), it is not necessary to examine all k-mers in a read. For instance, reads with no mismatches should be unambiguously identified after examining just one non-overlapping k-mer. By the pigeon hole principle, reads with m mismatches only require examination of (m + 1) non-overlapping k-mers. Additional k-mers are examined in order to determine if a read is from a repetitive region (see Section 5.6). Hence, SeqAlto decides to stop examining k-mers according to the following boolean function , where m is the number of mismatches and gaps of the best alignment so far and l is the number of k-mers examined. For reads with high number of mismatches, this early stopping does not guarantee finding the correct hit. This procedure is described in Algorithms 1 and 2. Overall, this procedure results in SeqAlto spending less time on high-quality reads. If the ungapped alignment outputs a best alignment with penalty score greater than a single gap open, the read is then examined for a single gap alignment. At this stage, the location of each k-mer is normalized by subtracting the location of the k-mer in the read. For each pair of normalized locations with distance smaller than the maximum gap size, the locations are extended to find single gap alignments with at most one mismatch on each side. This is able to resolve some indels in repetitive regions. 5.3 Needleman–Wunsch extension All reads that do not align in the ungapped alignment stage are passed to the Needleman–Wunsch extension stage. The order of examining k-mers is the same as for the ungapped alignment. Each k-mer location is searched with a banded Needleman–Wunsch with affine gap penalty to find gapped alignments. The width of the band can be chosen by the user and by default it finds up to at least 50 bp indels. The same function g(l, m) is used to determine if the program should continue examining k-mers. Single instruction multiple data (SIMD) acceleration through SSE2 instructions is used to improve the performance of this stage. 5.4 Paired-end alignment Paired-end information is both used to help search for alignments and also resolve discordant read pairs. If one read in a pair aligns and the other read does not, SeqAlto searches for the remaining read within the maximum insert size of the pair with a combination of Smith–Waterman and Needleman–Wunsch. These alignments are flagged in the output giving the user additional flexibility in post-processing. If both pairs align with one or more equally good hits, all possible combinations are checked for a concordant pair. If all possible combinations are discordant, SeqAlto searches for a concordant alignment within a region around each alignment defined by the maximum insert size. 5.5 Hybrid mode For reads much less than 100 bp, SeqAlto has some trouble aligning reads in repeat regions and also reads with high numbers of mismatches due to an inability to locate enough seeds on the reads. In hybrid mode, SeqAlto outputs reads with few valid seeds as a FASTQ file that can be then aligned with BWA or any other alignment tool. The two resulting SAM files can then be merged to produce the final alignment. 5.6 Mapping quality score Mapping quality (MAPQ) scores provide a way for users to judge the reliability of each alignment. They were first introduced in MAQ (Li et al., 2008) as an estimation of the probability a read was aligned incorrectly. Accurate calculation of mapping quality considerably slows down the alignment process since all k-mers should be visited. Despite not visiting all k-mers, SeqAlto also provides an estimate of mapping quality. The mapping quality reported by SeqAlto is not as specific as the mapping quality reported by other tools. However, in practice there is little difference. As discussed in the Mapping and Assembly with Quality (MAQ) article (Li et al., 2008), there are mainly two sources of alignment error. The first is when the best alignment fails to be reported by the alignment algorithm. The second is when a read aligns optimally to an incorrect location due to genetic variation or read errors. Similar to BWA, SeqAlto ignores the first source of error and assumes either the best alignment will be reported or no alignment will be reported at all. SeqAlto estimates the second source of error by recording all alignments that differ from the best by at most one mismatch and also the percentage of the read covered by highly repetitive k-mers. The formula used is similar to that of BWA for compatibility with downstream tools. 5.7 Choice of k-mer size The size of the k-mer seed needs to be selected prior to index construction. A single size is clearly not optimal for all read lengths. Smaller k-mers will be too repetitive and larger k-mers will reduce sensitivity. Table 6 displays the performance with various k-mer sizes. 5.8 Multi-threading SeqAlto is able to take advantage of modern multi-core computers by distributing the alignment of each read to multiple threads. There is little communication between the threads except when reading and writing files. SeqAlto is run in default mode on 1 million 101-bp paired-end reads from the real Illumina dataset and the alignment time is recorded. Table 7 shows that SeqAlto scales well up to at least eight threads while BWA is limited by its non-parallelized pairing step. An Illumina HiSeq2000 generates about 600 gigabases of data per run. This translates to about 3 billion 100-bp pairs. On a modern octa-core workstation computer, SeqAlto is able to align the 3 billion pairs in about 6 days compared to about 15 days with BWA on the same computer. Considering one Illumina HiSeq 2000 takes 11 days to generate the sequence data, this allows one inexpensive desktop workstation to service several large sequencers. As read lengths and the quality of sequencer output improves, these results will become even more significant.",Alignment,"fast  accurate read alignment  resequencing
seqalto search   global alignment  sequencer read   reference use  commonly know seed  extend approach   three phase   alignment process index construction ungapped alignment  needlemanwunsch extension define  kmer seed    sequence  nucleotides  length   nucleotide must come   set {   } hence  kmer  encode   unsigned integer   bits per nucleotide   bite system  allow  kmers  size         kmer   nucleotide   {   }      replace   uniform random nucleotide since     bite alphabet available  encode    approach use  bwa  many  tool let  index  define   list  tuples record  kmer   location   reference   kmer  index  sort   numeric value   kmer  fast search seqalto achieve  high speed first  use much larger kmer size compare  exist approach examine less repetitive kmers first   adaptively stop  kmer search large contiguous kmers greatly reduce  number  locations   kmer   reference  mostly unique hit  general large kmers would reduce  sensitivity   alignment however  longer read  choice  kmer size   reduce sensitivity  show   simulation result furthermore  novel subsampling approach  use  reduce  amount  memory require   index     compare  bwtbased approach  large kmer search  approach require less random access  memory per index lookup result  significantly improve performance  index construction  genome index ideally contain  location   overlapping kmers   genome naively store  kmers  size        one strand   human genome require     memory use  combination  two strategies   reduce  size   index        minor penalty   sensitivity  enable seqalto   run  almost  desktop computers  strategies   tune   powerful computers   become available   first stage   index construction  overlapping kmer together   location  extract   genome  store   array   array  tuples   sort   kmer value   leading  bits  remove   kmer       less     bits  store   prefix array   record  start  end locations    kmers tuples prefix     bits since   sorted kmers    prefix exist next          remain bits   kmer need   store    array  tuples  together   prefix array  form  genome index consider  genomes  less   billion nucleotides  twolevel index reduce  size   kmer location tuple   index   bytes instead   bytes hence  small prefix table size  entire human genome index   take     store  index construction also greatly improve  lookup speed   naive binary search seqalto   option   store every kmer   reference  order  reduce memory consumption without greatly impact alignment quality  speed  alignment   store  kmer   satisfy  hash function      numeric value   kmer  hash function   choose     approximately uniform coverage   genome  seqalto   use  select   memory usage   human genome       chromosome   median gap size      largest gap apart   centromere    empirically   little loss  sensitivity  use  approach  see   section   principle  expression  also  easily tune  machine   memory   expression  similar form  adequate  machine      memory  second step   require   larger index   construct  improve sensitivity  accuracy  index  essentially  large sort list hence   easy  add arbitrary additional kmers   index  negligible cost    construction  search kmers overlap know snps  indels could easily  add   index  improve  sensitivity  alignment  change would  moderately increase  size   index  ungapped alignment  first stage   alignment process   ungapped alignment stage  stage  separate   gap stage since  expect   gap read   align anywhere   genome  highquality ungapped alignments  separation also allow  easy interfacing   hardware accelerator   gap alignment  read  treat separately   alignment process  overlapping kmers  extract   read kmers satisfy  function   define    retain    kmers  discard highly repetitive kmers  also discard  kmer   search    index accord   follow order start   nonoverlapping kmers  number  locations  kmer exist   genome  compute   index   number  less   predefined variable max_loc    locations  examine   ungapped alignment   read   kmers  number  locations less  max_loc   examine   kmers  examine due   large kmer size many kmers either align uniquely   genome    align   hence  ungapped alignment stage proceed  quickly similar   idea  baezayates  perleberg     necessary  examine  kmers   read  instance read   mismatch   unambiguously identify  examine  one nonoverlapping kmer   pigeon hole principle read   mismatches  require examination     nonoverlapping kmers additional kmers  examine  order  determine   read    repetitive region see section  hence seqalto decide  stop examining kmers accord   follow boolean function      number  mismatch  gap   best alignment  far     number  kmers examine  read  high number  mismatch  early stop   guarantee find  correct hit  procedure  describe  algorithms    overall  procedure result  seqalto spend less time  highquality read   ungapped alignment output  best alignment  penalty score greater   single gap open  read   examine   single gap alignment   stage  location   kmer  normalize  subtract  location   kmer   read   pair  normalize locations  distance smaller   maximum gap size  locations  extend  find single gap alignments    one mismatch   side   able  resolve  indels  repetitive regions  needlemanwunsch extension  read    align   ungapped alignment stage  pass   needlemanwunsch extension stage  order  examining kmers       ungapped alignment  kmer location  search   band needlemanwunsch  affine gap penalty  find gap alignments  width   band   choose   user   default  find    least   indels   function    use  determine   program  continue examining kmers single instruction multiple data simd acceleration  sse2 instructions  use  improve  performance   stage  pairedend alignment pairedend information   use  help search  alignments  also resolve discordant read pair  one read   pair align    read   seqalto search   remain read within  maximum insert size   pair   combination  smithwaterman  needlemanwunsch  alignments  flag   output give  user additional flexibility  postprocessing   pair align  one   equally good hit  possible combinations  check   concordant pair   possible combinations  discordant seqalto search   concordant alignment within  region around  alignment define   maximum insert size  hybrid mode  read much less    seqalto   trouble align read  repeat regions  also read  high number  mismatch due   inability  locate enough seed   read  hybrid mode seqalto output read   valid seed   fastq file     align  bwa    alignment tool  two result sam file    merge  produce  final alignment  map quality score map quality mapq score provide  way  users  judge  reliability   alignment   first introduce  maq       estimation   probability  read  align incorrectly accurate calculation  map quality considerably slow   alignment process since  kmers   visit despite  visit  kmers seqalto also provide  estimate  map quality  map quality report  seqalto    specific   map quality report   tool however  practice   little difference  discuss   map  assembly  quality maq article       mainly two source  alignment error  first    best alignment fail   report   alignment algorithm  second    read align optimally   incorrect location due  genetic variation  read errors similar  bwa seqalto ignore  first source  error  assume either  best alignment   report   alignment   report   seqalto estimate  second source  error  record  alignments  differ   best    one mismatch  also  percentage   read cover  highly repetitive kmers  formula use  similar    bwa  compatibility  downstream tool  choice  kmer size  size   kmer seed need   select prior  index construction  single size  clearly  optimal   read lengths smaller kmers    repetitive  larger kmers  reduce sensitivity table  displays  performance  various kmer size  multithreading seqalto  able  take advantage  modern multicore computers  distribute  alignment   read  multiple thread   little communication   thread except  read  write file seqalto  run  default mode   million  pairedend read   real illumina dataset   alignment time  record table  shows  seqalto scale well    least eight thread  bwa  limit   nonparallelized pair step  illumina hiseq2000 generate   gigabases  data per run  translate    billion  pair   modern octacore workstation computer seqalto  able  align   billion pair    days compare    days  bwa    computer consider one illumina hiseq  take  days  generate  sequence data  allow one inexpensive desktop workstation  service several large sequencers  read lengths   quality  sequencer output improve  result  become even  significant",0
19,Blast,"Basic local alignment search tool
The maximal segment pair measure Sequence similarity measures generally can be classified as either global or local. Global similarity algorithms optimize the overall alignment of two sequences, which may include large stretches of low similarity (Needleman & Wunsch, 1970). Local similarity algorithms seek only relatively conserved subsequences, and a single comparison may yield several distinct subsequence alignments; uneonserved regions do not contribute to the measure of similarity (Smith & Waterman, 1981; Goad-& Kanehisa, 1982; Sellers, 1984). Local similarity measures are generally preferred for database searches, where eDNAs may be compared with partially sequenced genes, and where distantly related proteins may share only isolated regions of similarity, e.g. in the vicinity of an active site. Many similarity measures, including the one we employ, begin with a matrix of similarity scores for all possible pairs of residues. Identities and conservative replacements have positive scores, while unlikely replacements have negative scores. For amino acid sequence comparisons we generally use the PAM-120 matrix (a variation of that of Dayhoff el al., 1978), while for DNA sequence comparisons we score identities +5, and mismatches --4; other scores are of course possible. A sequence segment is a contiguous stretch of residues of any length, and the similarity score for two aligned segments of the same length is the sum of tim similarity values for each pair of aligned residues. Given these rules, we define a maximal segment pair (MSP) to be the highest scoring pair of identical length segments chosen from2 sequences. The boundaries of an MSP are chosen to maximize its score, so an MSP may be of any length. The MSP score, which BLAST heuristically attempts to calculate, provides a measure of local similarity for any pair of sequences. A molecular biologist, however, may be interested in all conserved regions shared by 2 proteins, not only in their highest scoring pair. We therefore define a segment pair to be locally maximal if its score cannot be improved either by extending or by shortening both segments (Sellers, 1984). BLAST can seek all locally maximal segment pairs with scores above some cutoff. L. ike many other similarity measures, tile MSP score for 2 sequences may be computed in time proportional to the product of their lengths using a simple dynamic programruing algorithm. An important advantage of the MSP measure is that recent mathematical results allow the statistical significance of MSP scores to be estimated under an appropriate random sequence model (Karlin & Altsehul, 1990; Karlin et al., 1990). Furthermore, for any particular scoring matrix (e.g. PAM-120) one can estimate the frequencies of paired residues in maximal segments. This tractability to mathematical analysis is a crucial feature of the BLAST algorithm. (b) Rapid approximation of MSP scores In searching a database of thousands of sequences, generally only a handful, if any, will be homologous to the query sequence. The scientist is therefore interested in identifying only those sequence entries with MSP scores over some cutoff score S. These sequences include those sharing highly significant similarity with the query as well as some sequences with borderline scores. This latter set of sequences may include high scoring random matches as well as sequences distantly related to the query. The biological significance of the high scoring sequences may be inferred almost solely on the basis of the similarity score, while the biological context of the borderline sequences may be helpful in distinguishing biologically interesting relationships. Recent results (Karlin & Altschul, 1990; Karlin et al., 1990) allow us to estimate the highest MSP score ,S at which clmnce similarities are likely to appear. To accelerate database searches, BLAST minimizes the time spent on sequence regions whose similarity with the query has little chance of exceeding this score. Let a word pair be a segment pair of fixed length w. The main strategy of BLAST is to seek only segment pairs that contain a word pair with a score of at least T. Scanning through a sequence, one can determine quickly whether it contains a word of length w that can pair with Jhe query sequence to produce a word pair with a score greater than or equal to tile threshold T. Any such hit is extended to determine if it is contained within a segment pair whose score is greater than or equal to S. The lower the threshold T, the greater the chance that a segment pair with a score of at least S will contain a word pair with a score of at least T. A small value for T, however, increases tile number of lilts and therefore tile execution time of the algorithm. Random simulation permits us to select a threshold T that balances these considerations. (e) Implementation In our implementations of this approach, details of the 3 algorithmic steps (namely compiling a list of highscoring words, scanning the database for hits, and extending hits) vary somewhat depending on whether the database contains proteins or DNA sequences. For proteins, the list consists of all words (w-reefs) that score at least T when compared to some word in the query sequence. Thus, a query word may be represented by no words in the list (e.g. for common w-mers using PAM-120 scores) or by many. (One may, of course, insist that every w-mer in the query sequence be included in the word list, irrespective of whether tmiring the word with itself yields a score of at least 7'.) For values ofw and T tlmt we have found most usefnl (see below), there are typically of the order of S0 words in the list for every residue in the query sequence, e.g. 12,500 words for a sequence of length 250. If a little care is taken in programming, the list of words can be generated in time essentially proportional to the length of the list. The scanning phase raised a classic algorithmic problem, i.e. search a long sequence for all occurrences of certain short sequences. We investigated 2 approaches. Simplified, the first works as follows. Suppose that w = 4 and maI) each word to an integer between 1 and 204, so a word can be used as an index into an array of size 204= 160,000. Let the ith entry of such an array point to the list of all occurrences in the query sequence of the ith word. Thus, as we scan the database, each database word leads us immediately to the corresponding hits. Typically, only a few thousand of the 204 possible words will be in this table, and it is easy to modify the approach to use far fewer than 204 pointers. The second approach we explored for the scanning phase was the use of a deterministic finite automaton or finite state machine (Mealy, 1955; Hopcroft & Ullman, 1979). An important feature of our construction was to signal acceptance on transitions (Mealy paradigm) as opposed to on states (Moore paradigm). In the automaton's construction, this saved a factor in space and time roughly proportional to the size of the underlying alphabet. This method yielded a program that ran faster and we prefer this approach for general use. With typical query lengths and parameter settings, this version of BLAST scans a protein database at approximately 500,000 residues/s. Extending a hit to find a locally maximal segment pair containing that hit is straightforward. To economize time, we terminate the process of extending in one direction when we reach a segment pair whose score falls a certain distance below the best score found for shorter extensions. This introduces a further departure from the ideal of finding guaranteed MSPs, but the added inaccuracy is negligible, as can be demonstrated by both experiment and analysis (e.g. for protein comparisons the default distance is 20, and the probability of missing a higher scoring ext.ension is about 0""001). For DNA, we use a simpler word list, i.e. the list of all contiguous w-mers in the query sequence, often with w = 12. Thus, a query sequence of length n yields a list of n-w+l words, and again there are commonly a few thousand words in the list. It is advantageous to compress the database by packing 4 nucleotides into a single byte, using an auxiliary table to delimit the boundaries between adjacent sequences. Assuming w > 11, each hit must contain an 8-mer hit that lies on a byte boundary. This observation allows us to scan the database byte-wise and thereby increase speed 4-fold. For each 8-mer hit, we check for an enclosing u,-mer hit; if found, we extend as before. Running on a SUN4, with a query of typical length (e.g. several thousand bases), BLAST scans at ai)t)roxinmtely 2x 10 6 bases/s. At facilities which run many such searches a day, loading the compressed database into menmry once in a shared menmry sehenm affords a substantial saving in subsequent search times. It should be noted that DNA sequences are highly nonrandom, with locally biased base composition (e.g. A+T-rich regions), and repeated sequence elements (e.g. Alu sequences) and this has important consequences for the design of a DNA database search tool. If a given query sequence has, for example, an A+T-rich subsequence, or a commonly occurring repetitive element, then a database search will produce a copious output of matchcs with little interest. We have designed a somewhat ad hoc but effective means of dealing with these 2 problems. The program that produces the compressed version of the DNA database tabulates the frequencies of all 8-tuples. Those occurring much more frequently than expected by chance (controllable by parameter) are stored and used to filter ""uninformative"" words from the query word list. Also, preceding full database searches, a search of a sublibrary of repetitive elements is perforfimd, and the locations in the query of significant matches are stored. Words generated by these regions are renmved from the query word list for the full search. Matches to the sublibrary, however, are reported in the final output. These 2 filters allow alignments to regions with biased composition, or to regions containing repetitive elements to be reported, as long as adjacent regions not containing such features share significant similarity to the query sequence. The BLAST strategy admits numerous variations. We implemented a version of BLAST that uses dynamic programming to extend hits so as to allow gaps in the resulting alignments. Needless to say, this greatly slows the extension process. While the sensitivity of amino acid searches was improved in some cases, the selectivity was reduced as well. Given the trade-off of speed and selectivity for sensitivity, it is questionable whether the gap version of BLAST constitutes an improvement. We also implemented the alternative of making a table of all occurrences of the w-mers in the database, then scanning the query sequence and processing hits. The disk space requirements are considerable, approximately 2 computer words for ever)"" residue in the database. More damaging was that for query sequences of typical length, the need for random access into the database (as opposed to sequential access) made the approach slower, on the computer systems we used, titan scanning the entire database.",Alignment,"basic local alignment search tool
 maximal segment pair measure sequence similarity measure generally   classify  either global  local global similarity algorithms optimize  overall alignment  two sequence  may include large stretch  low similarity needleman  wunsch  local similarity algorithms seek  relatively conserve subsequences   single comparison may yield several distinct subsequence alignments uneonserved regions   contribute   measure  similarity smith  waterman  goad kanehisa  sellers  local similarity measure  generally prefer  database search  ednas may  compare  partially sequence genes   distantly relate proteins may share  isolate regions  similarity    vicinity   active site many similarity measure include  one  employ begin   matrix  similarity score   possible pair  residues identities  conservative replacements  positive score  unlikely replacements  negative score  amino acid sequence comparisons  generally use  pam matrix  variation    dayhoff      dna sequence comparisons  score identities   mismatch   score   course possible  sequence segment   contiguous stretch  residues   length   similarity score  two align segment    length   sum  tim similarity value   pair  align residues give  rule  define  maximal segment pair msp    highest score pair  identical length segment choose from2 sequence  boundaries   msp  choose  maximize  score   msp may    length  msp score  blast heuristically attempt  calculate provide  measure  local similarity   pair  sequence  molecular biologist however may  interest   conserve regions share   proteins     highest score pair  therefore define  segment pair   locally maximal   score cannot  improve either  extend   shorten  segment sellers  blast  seek  locally maximal segment pair  score   cutoff  ike many  similarity measure tile msp score   sequence may  compute  time proportional   product   lengths use  simple dynamic programruing algorithm  important advantage   msp measure   recent mathematical result allow  statistical significance  msp score   estimate   appropriate random sequence model karlin  altsehul  karlin    furthermore   particular score matrix  pam one  estimate  frequencies  pair residues  maximal segment  tractability  mathematical analysis   crucial feature   blast algorithm  rapid approximation  msp score  search  database  thousands  sequence generally   handful     homologous   query sequence  scientist  therefore interest  identify   sequence entries  msp score   cutoff score   sequence include  share highly significant similarity   query  well   sequence  borderline score  latter set  sequence may include high score random match  well  sequence distantly relate   query  biological significance   high score sequence may  infer almost solely   basis   similarity score   biological context   borderline sequence may  helpful  distinguish biologically interest relationships recent result karlin  altschul  karlin    allow   estimate  highest msp score    clmnce similarities  likely  appear  accelerate database search blast minimize  time spend  sequence regions whose similarity   query  little chance  exceed  score let  word pair   segment pair  fix length   main strategy  blast   seek  segment pair  contain  word pair   score   least  scan   sequence one  determine quickly whether  contain  word  length    pair  jhe query sequence  produce  word pair   score greater   equal  tile threshold    hit  extend  determine    contain within  segment pair whose score  greater   equal    lower  threshold   greater  chance   segment pair   score   least   contain  word pair   score   least   small value   however increase tile number  lilt  therefore tile execution time   algorithm random simulation permit   select  threshold   balance  considerations  implementation   implementations   approach detail    algorithmic step namely compile  list  highscoring word scan  database  hit  extend hit vary somewhat depend  whether  database contain proteins  dna sequence  proteins  list consist   word wreefs  score  least   compare   word   query sequence thus  query word may  represent   word   list   common wmers use pam score   many one may  course insist  every wmer   query sequence  include   word list irrespective  whether tmiring  word   yield  score   least '  value ofw   tlmt   find  usefnl see    typically   order   word   list  every residue   query sequence   word   sequence  length    little care  take  program  list  word   generate  time essentially proportional   length   list  scan phase raise  classic algorithmic problem  search  long sequence   occurrences  certain short sequence  investigate  approach simplify  first work  follow suppose      mai  word   integer       word   use   index   array  size   let  ith entry    array point   list   occurrences   query sequence   ith word thus   scan  database  database word lead  immediately   correspond hit typically    thousand    possible word     table    easy  modify  approach  use far fewer   pointers  second approach  explore   scan phase   use   deterministic finite automaton  finite state machine mealy  hopcroft  ullman   important feature   construction   signal acceptance  transition mealy paradigm  oppose   state moore paradigm   automaton' construction  save  factor  space  time roughly proportional   size   underlie alphabet  method yield  program  run faster   prefer  approach  general use  typical query lengths  parameter settings  version  blast scan  protein database  approximately  residuess extend  hit  find  locally maximal segment pair contain  hit  straightforward  economize time  terminate  process  extend  one direction   reach  segment pair whose score fall  certain distance   best score find  shorter extensions  introduce   departure   ideal  find guarantee msps   add inaccuracy  negligible    demonstrate   experiment  analysis   protein comparisons  default distance     probability  miss  higher score extension   ""  dna  use  simpler word list   list   contiguous wmers   query sequence often     thus  query sequence  length  yield  list  nwl word     commonly   thousand word   list   advantageous  compress  database  pack  nucleotides   single byte use  auxiliary table  delimit  boundaries  adjacent sequence assume     hit must contain  mer hit  lie   byte boundary  observation allow   scan  database bytewise  thereby increase speed fold   mer hit  check   enclose umer hit  find  extend   run   sun4   query  typical length  several thousand base blast scan  aitroxinmtely    base  facilities  run many  search  day load  compress database  menmry    share menmry sehenm afford  substantial save  subsequent search time    note  dna sequence  highly nonrandom  locally bias base composition  atrich regions  repeat sequence elements  alu sequence    important consequences   design   dna database search tool   give query sequence   example  atrich subsequence   commonly occur repetitive element   database search  produce  copious output  matchcs  little interest   design  somewhat  hoc  effective mean  deal    problems  program  produce  compress version   dna database tabulate  frequencies   tuples  occur much  frequently  expect  chance controllable  parameter  store  use  filter ""uninformative"" word   query word list also precede full database search  search   sublibrary  repetitive elements  perforfimd   locations   query  significant match  store word generate   regions  renmved   query word list   full search match   sublibrary however  report   final output   filter allow alignments  regions  bias composition   regions contain repetitive elements   report  long  adjacent regions  contain  feature share significant similarity   query sequence  blast strategy admit numerous variations  implement  version  blast  use dynamic program  extend hit    allow gap   result alignments needle  say  greatly slow  extension process   sensitivity  amino acid search  improve   case  selectivity  reduce  well give  tradeoff  speed  selectivity  sensitivity   questionable whether  gap version  blast constitute  improvement  also implement  alternative  make  table   occurrences   wmers   database  scan  query sequence  process hit  disk space requirements  considerable approximately  computer word  ever"" residue   database  damage    query sequence  typical length  need  random access   database  oppose  sequential access make  approach slower   computer systems  use titan scan  entire database",0
20,H-BLAST,"H-BLAST: a fast protein sequence alignment toolkit on heterogeneous computers with GPUs
Both BLASTX and BLASTP searching procedures include four stages: the setup stage, the preliminary search stage, the traceback stage and the output stage. Without loss of generality, we take the BLASTX searching procedure as an example. At the setup stage, BLASTX not only prepares searching options, query sequences and subject sequences from the given database, but also creates a lookup table indexing query sequences. At the preliminary search stage, it scans each subject, lists all matching words (called hits or seeds) by the lookup table, extends each hit linking another hit to a gapped-free alignment. Each gapped-free alignment exceeding a threshold score is saved as a high scoring pair (HSP). Then, an HSP triggers a gapped alignment. At the traceback stage, BLASTX tries to find more aggressive alignments and records the number and positions of insertions, deletions and matching letter information. Finally, at the last stage, it prints out all alignments by queries. The pseudo codes are listed in Algorithm 1. Algorithms of BLASTX and BLASTP are very similar. In fact, those used in their seeding step and the ungapped extension step are exactly the same. Their major difference is the data type of queries: nucleotide and amino-acid for BLASTX and BLASTP, respectively. Compared to BLASTP, there are extra jobs in BLASTX: translating nucleotide query sequences into amino-acid sequences in all six reading frames in the setup stage, and considering insertion and deletion events of translated nucleotide query sequences among different frames in the gapped extension step. The most promising approach to accelerate BLAST is to speed up the preliminary search stage, which is the most computation intensive stage in BLAST reported in profiling studies (Vouzis and Sahinidis, 2011; Zhao and Chu, 2014). In our profiling study of NCBI-BLASTX, seeding and ungapped extension steps are still the most time consuming and they account for 80–90% of the total computing time. Reasons why we use heterogeneous computing to speed up BLAST can be listed as follows. Heterogenous computers with GPUs offer a low cost approach to high performance. GPUs are computing power sources. A latest TitanX GPU can delivers 11Tflops peak performance with 250 W power budget, while a latest Intel 18-core CPU presents about 1.3Tflops with 145 W power budget. A computer server can be equipped with 1–8 GPUs. However, one bottleneck of GPU-only computing is the memory limitation. The main stream memory size of a GPU is 2–4 GB, the largest is 24 GB. Facing a 58 GB NCBI nonredundant (NCBI-nr in brief) protein database (released in 2016) and massive alignment results, there is not enough memory space on a GPU. Using the host memory space can ease this problem. The state-of-the-art accelerated BLAST algorithms on heterogeneous computers mainly focused on designs of fast GPU kernels and CPU/GPU cooperation pipelines. CUDA-BLASTP (Liu et al., 2011) not only used a compressed DFA for hit detetion with a sorted database for better load balancing in GPUs, but also processed gapped step in GPUs. GPU-BLAST (Vouzis and Sahinidis, 2011) and G-BLASTN (Zhao and Chu, 2014) offered GPU kernels preserving the same alignment results as NCBI-BLAST. GPU-BLAST claimed a speedup of 3–4 times over sequential NCBI-BLASTP. G-BLASTN provided a CPU/GPU cooperation pipeline scheme to speed up BLASTN. cuBlastp (Zhang et al., 2014) introduced a fine-grained GPU kernel, which replaced the embarrassing parallel approach and bundled execution tasks of the same diagonals into queues for parallel execution. It improved the memory accessing pattern of GPU kernel and claimed a speedup of 3.7 times over multi-threaded NCBI-BLASTP. However, seeding and extension branches were still mixed together that would harm the performance. The major bottleneck of accelerating BLAST on heterogenous computers is unable to fully utilize all available parallelism, especially on GPUs. The source of this bottleneck is the unpredictable execution paths of the BLAST heuristic (Zhang et al., 2014). Its many different coupled branches are not compatible to the lock step execution style of GPUs and increase difficulties on balancing workloads among CPU tasks and GPU tasks. That is a performance killer of GPU applications (Bakhoda et al., 2009). The heuristic with massive parallelism needs huge memory space to store on-the-fly results. For example, using algorithms of GPU-BLAST in BLASTX searching would consume more than 12GB memory, which cannot be easily satisfied by many GPUs. The key ideas underlying H-BLAST is to solve this bottleneck. H-BLAST tries to decouple alignment jobs and bundle branches of the same type into queues for parallel execution. Our algorithms eliminate the interrelationship of finding seeds (short stretches of identities) and ungapped seed extension in a short subject region to preserve the same alignment results as those from NCBI-BLAST. Jobs in a queue are shared with multiple GPU threads (known as fine-grained parallelism) in batch execution. It leads to much less divergence and better performance theoretically. H-BLAST also applies a linear workload prediction model for workload balancing in a CPU/GPU cooperation pipeline. Furthermore, it uses a lossless data compression strategy with a dynamic memory allocation approach in GPU algorithms for decreasing the memory need. To speed up Algorithm 1, H-BLAST presents two approaches depending on cases. When only one GPU is used, the seeding and ungapped extension steps are accelerated by multi-core CPUs and a GPU; the score only gapped extension step is speeded up by multi-core CPUs; the rest part runs in serial. To gain a better total speedup, a cooperation step pipelining tasks on CPUs and tasks on the GPU is involved with extra cost. While analyzing massive query sequences with k GPUs, H-BLAST takes advantages of the independency of alignment results of different sequences and divides queries into k parts equally. As a result, stages 1, 3, 4 and the cooperation step can be accelerated by k CPU cores. H-BLAST software consists of three modules: a GPU scanning module, a CPU scanning module and a cooperation module. The GPU scanning module is for accelerating the seeding and ungapped extension step on GPUs, its entire algorithms are the same as those in both BLASTX and BLASTP. The CPU scanning module processes not only the HSPs from the GPU module with gapped extension, but also some unprocessed database sequences with the whole BLAST algorithms, and merges the alignment results. The cooperation module is to parallelize the CPU and GPU tasks. The main framework of H-BLAST is illustrated in Figure 1. We implement the H-BLAST algorithm with C language for CPU codes and CUDA C language for GPU codes. The H-BLAST software is based on NCBI-BLAST 2.2.28+ software package and immigrates code segments from GPU-BLAST. We describe the implementation of the fine-grained GPU kernel design and the cooperation pipeline. 3.1 Accelerating the seeding and ungapped extension steps on GPUs There are three major challenges in accelerating the BLAST algorithm on GPUs, due to the unpredictable execution path and the storage demand of the heuristic. Parallelizing the algorithm as well as balancing the workload and the storage demand are primary challenges in parallel applications. Minimizing divergence of the execution paths is a specified issue for GPU applications. Especially, divergence of the execution paths is the performance killer of GPU applications. H-BLAST offers a systematic approach to settle these issues. The H-BLAST GPU kernel adopts a fine-grained parallel strategy, i.e. mapping the alignment task of a database subject to multiple GPU threads. To this end, H-BLAST takes advantages of hierarchy parallelism. Among different database subjects, the alignment tasks are independent, denoted as the external parallelism. For a given offset of one database subject, all seeding tasks among different query offsets and ungapped extension tasks can be parallelized, denoted as the internal parallelism. The execution of an H-BLAST GPU kernel is organized by groups of GPU threads with the external parallelism. The GPU threads within a group share all tasks with the internal parallelism. To minimize divergence of the execution paths and balance the workload among different GPU threads, a locally decoupled seed-extension algorithm is proposed, depicted in Algorithm 2. In our algorithm, the execution of H-BLAST seeding and ungapped extension jobs are organized in batch by a dual-job-queue pipeline. The first stage of the pipeline is PSeeding, presented in Algorithm 3. A hit-queue used in Pseeding is formed by the contents (indexed by a w-word) from the lookup table. GPU threads searching the same subject sequence filter these hits and push all hits requesting extension into extension queues for lazy processing. The second stage of the pipeline is PUngapped_extension, shown in Algorithm 4. Unlike seeding jobs executed every offset, ungapped extension jobs are triggered in batch every B offsets. The group size of GPU threads sharing one subject’s sequence alignment jobs varies from query sequences in H-BLAST adaptively. The group size permits a tradeoff between the parallel efficiency and the cooperation cost among GPU threads. A small group size leads to a low cooperation cost in job sharing, while a large group size yields high parallel efficiency due to the balanced workload. The group size of H-BLAST is determined by the total length of all query sequences, varying between 1 and 4. If the total length of amino-acid sequences is less than 1000, single thread in a group is used. Otherwise, four threads in a group is optimal. To ensure the same alignment results as NCBI-BLAST as well as independent jobs in an extension queue, the value of B in Algorithm 2 for H-BLAST is not greater than the word size w. Otherwise, jobs in extension queues are not ensured to be independent and cannot be executed concurrently. To balance the storage demand with the limited GPU memory volume, H-BLAST combines a memory dynamic allocation approach with a lossless data compression strategy. The parallel dynamic memory allocation approach for H-BLAST is named the joint memory allocation, which manages a memory pool shared by eight GPU threads within a warp for job queues and output result sets. The implementation of the joint memory allocation is based on the hardware warp voting function on GPUs, replacing the sequential atom operation used in G-BLASTN. Hence its cost can be ignored. When failed to allocate memory in a joint memory allocation with a fixed size buffer, the corresponding subject sequence is marked for realignment by CPU threads. Furthermore, the joint memory allocation is thread-safe, since there is no race contention within the same warp. The lossless data compression strategy for H-BLAST is used to reduce the memory demand for on-the-fly and alignment results. The memory needs for on-the-fly results and alignment results are as less as 1/8 and 1/2 of those in GPU-BLAST, respectively. In our BLASTX experiment, the peak memory requirement for on-the-fly results in H-BLAST is 1.5 GB, compared to 12 GB in GPU-BLAST. 3.2 Parallelize CPU tasks and GPU tasks To execute CPU tasks and GPU tasks at the same time, a cooperation module is employed for H-BLAST. There are three functionalities of the cooperation module. The first is organizing the BLAST algorithm in a pipeline and overlapping the CPU and GPU execution with different database volumes. The second is the data transport among CPUs and GPUs. The third is the workload assignment among CPUs and GPUs. The key new element of this module is the load balancer. The load balancer for H-BLAST is self-adaptive and hardware-aware, based on a workload prediction model. The model is related to the total length of the protein database sequences and the computational capacity ratio (CCR in brief) between a CPU core and a GPU. The former reflects that the workload obeys uniform distribution of lengths of sequences approximately. The latter is aware of hardware settings changing. To optimize the division of the work, we introduce a real-time greedy algorithm, which is based on minimizing the difference of the execution time between concurrent tasks on CPUs and that on GPUs. At the beginning of alignment, we give an initial load assignment ratio between CPUs and GPUs. After each database volume is searched, the load assignment rate is adjusted automatically by the difference between the execution time of the CPU task and that of the GPU task as well as a given CCR. The load balancer for H-BLAST is user-tunable. There is an interface for setting options, such as CCR and the value of an initial load assignment ratio. The details about the tuning interface will be discussed in the next section.",Alignment,"hblast  fast protein sequence alignment toolkit  heterogeneous computers  gpus
 blastx  blastp search procedures include four stag  setup stage  preliminary search stage  traceback stage   output stage without loss  generality  take  blastx search procedure   example   setup stage blastx   prepare search options query sequence  subject sequence   give database  also create  lookup table index query sequence   preliminary search stage  scan  subject list  match word call hit  seed   lookup table extend  hit link another hit   gappedfree alignment  gappedfree alignment exceed  threshold score  save   high score pair hsp   hsp trigger  gap alignment   traceback stage blastx try  find  aggressive alignments  record  number  position  insertions deletions  match letter information finally   last stage  print   alignments  query  pseudo cod  list  algorithm  algorithms  blastx  blastp   similar  fact  use   seed step   ungapped extension step  exactly    major difference   data type  query nucleotide  aminoacid  blastx  blastp respectively compare  blastp   extra job  blastx translate nucleotide query sequence  aminoacid sequence   six read frame   setup stage  consider insertion  deletion events  translate nucleotide query sequence among different frame   gap extension step   promise approach  accelerate blast   speed   preliminary search stage     computation intensive stage  blast report  profile study vouzis  sahinidis  zhao  chu    profile study  ncbiblastx seed  ungapped extension step  still   time consume   account     total compute time reason   use heterogeneous compute  speed  blast   list  follow heterogenous computers  gpus offer  low cost approach  high performance gpus  compute power source  latest titanx gpu  deliver 11tflops peak performance    power budget   latest intel core cpu present  3tflops    power budget  computer server   equip   gpus however one bottleneck  gpuonly compute   memory limitation  main stream memory size   gpu     largest    face    ncbi nonredundant ncbinr  brief protein database release    massive alignment result    enough memory space   gpu use  host memory space  ease  problem  stateoftheart accelerate blast algorithms  heterogeneous computers mainly focus  design  fast gpu kernels  cpugpu cooperation pipelines cudablastp liu      use  compress dfa  hit detetion   sort database  better load balance  gpus  also process gap step  gpus gpublast vouzis  sahinidis   gblastn zhao  chu  offer gpu kernels preserve   alignment result  ncbiblast gpublast claim  speedup   time  sequential ncbiblastp gblastn provide  cpugpu cooperation pipeline scheme  speed  blastn cublastp zhang    introduce  finegrained gpu kernel  replace  embarrass parallel approach  bundle execution task    diagonals  queue  parallel execution  improve  memory access pattern  gpu kernel  claim  speedup   time  multithreaded ncbiblastp however seed  extension branch  still mix together  would harm  performance  major bottleneck  accelerate blast  heterogenous computers  unable  fully utilize  available parallelism especially  gpus  source   bottleneck   unpredictable execution paths   blast heuristic zhang     many different couple branch   compatible   lock step execution style  gpus  increase difficulties  balance workloads among cpu task  gpu task    performance killer  gpu applications bakhoda     heuristic  massive parallelism need huge memory space  store onthefly result  example use algorithms  gpublast  blastx search would consume   12gb memory  cannot  easily satisfy  many gpus  key ideas underlie hblast   solve  bottleneck hblast try  decouple alignment job  bundle branch    type  queue  parallel execution  algorithms eliminate  interrelationship  find seed short stretch  identities  ungapped seed extension   short subject region  preserve   alignment result    ncbiblast job   queue  share  multiple gpu thread know  finegrained parallelism  batch execution  lead  much less divergence  better performance theoretically hblast also apply  linear workload prediction model  workload balance   cpugpu cooperation pipeline furthermore  use  lossless data compression strategy   dynamic memory allocation approach  gpu algorithms  decrease  memory need  speed  algorithm  hblast present two approach depend  case   one gpu  use  seed  ungapped extension step  accelerate  multicore cpus   gpu  score  gap extension step  speed   multicore cpus  rest part run  serial  gain  better total speedup  cooperation step pipelining task  cpus  task   gpu  involve  extra cost  analyze massive query sequence   gpus hblast take advantage   independency  alignment result  different sequence  divide query   parts equally   result stag      cooperation step   accelerate   cpu core hblast software consist  three modules  gpu scan module  cpu scan module   cooperation module  gpu scan module   accelerate  seed  ungapped extension step  gpus  entire algorithms        blastx  blastp  cpu scan module process    hsps   gpu module  gap extension  also  unprocessed database sequence   whole blast algorithms  merge  alignment result  cooperation module   parallelize  cpu  gpu task  main framework  hblast  illustrate  figure   implement  hblast algorithm   language  cpu cod  cuda  language  gpu cod  hblast software  base  ncbiblast  software package  immigrate code segment  gpublast  describe  implementation   finegrained gpu kernel design   cooperation pipeline  accelerate  seed  ungapped extension step  gpus   three major challenge  accelerate  blast algorithm  gpus due   unpredictable execution path   storage demand   heuristic parallelize  algorithm  well  balance  workload   storage demand  primary challenge  parallel applications minimize divergence   execution paths   specify issue  gpu applications especially divergence   execution paths   performance killer  gpu applications hblast offer  systematic approach  settle  issue  hblast gpu kernel adopt  finegrained parallel strategy  map  alignment task   database subject  multiple gpu thread   end hblast take advantage  hierarchy parallelism among different database subject  alignment task  independent denote   external parallelism   give offset  one database subject  seed task among different query offset  ungapped extension task   parallelize denote   internal parallelism  execution   hblast gpu kernel  organize  group  gpu thread   external parallelism  gpu thread within  group share  task   internal parallelism  minimize divergence   execution paths  balance  workload among different gpu thread  locally decouple seedextension algorithm  propose depict  algorithm    algorithm  execution  hblast seed  ungapped extension job  organize  batch   dualjobqueue pipeline  first stage   pipeline  pseeding present  algorithm   hitqueue use  pseeding  form   content index   wword   lookup table gpu thread search   subject sequence filter  hit  push  hit request extension  extension queue  lazy process  second stage   pipeline  pungapped_extension show  algorithm  unlike seed job execute every offset ungapped extension job  trigger  batch every  offsets  group size  gpu thread share one subject sequence alignment job vary  query sequence  hblast adaptively  group size permit  tradeoff   parallel efficiency   cooperation cost among gpu thread  small group size lead   low cooperation cost  job share   large group size yield high parallel efficiency due   balance workload  group size  hblast  determine   total length   query sequence vary       total length  aminoacid sequence  less   single thread   group  use otherwise four thread   group  optimal  ensure   alignment result  ncbiblast  well  independent job   extension queue  value    algorithm   hblast   greater   word size  otherwise job  extension queue   ensure   independent  cannot  execute concurrently  balance  storage demand   limit gpu memory volume hblast combine  memory dynamic allocation approach   lossless data compression strategy  parallel dynamic memory allocation approach  hblast  name  joint memory allocation  manage  memory pool share  eight gpu thread within  warp  job queue  output result set  implementation   joint memory allocation  base   hardware warp vote function  gpus replace  sequential atom operation use  gblastn hence  cost   ignore  fail  allocate memory   joint memory allocation   fix size buffer  correspond subject sequence  mark  realignment  cpu thread furthermore  joint memory allocation  threadsafe since    race contention within   warp  lossless data compression strategy  hblast  use  reduce  memory demand  onthefly  alignment result  memory need  onthefly result  alignment result   less        gpublast respectively   blastx experiment  peak memory requirement  onthefly result  hblast    compare     gpublast  parallelize cpu task  gpu task  execute cpu task  gpu task    time  cooperation module  employ  hblast   three functionalities   cooperation module  first  organize  blast algorithm   pipeline  overlap  cpu  gpu execution  different database volumes  second   data transport among cpus  gpus  third   workload assignment among cpus  gpus  key new element   module   load balancer  load balancer  hblast  selfadaptive  hardwareaware base   workload prediction model  model  relate   total length   protein database sequence   computational capacity ratio ccr  brief   cpu core   gpu  former reflect   workload obey uniform distribution  lengths  sequence approximately  latter  aware  hardware settings change  optimize  division   work  introduce  realtime greedy algorithm   base  minimize  difference   execution time  concurrent task  cpus    gpus   begin  alignment  give  initial load assignment ratio  cpus  gpus   database volume  search  load assignment rate  adjust automatically   difference   execution time   cpu task     gpu task  well   give ccr  load balancer  hblast  usertunable    interface  set options   ccr   value   initial load assignment ratio  detail   tune interface   discuss   next section",0
21,Rapsearch2,"RAPSearch2: a fast and memory-efficient protein similarity search tool for next-generation sequencing data
RAPSearch2 uses a collision-free hash table to index the protein sequences in a given search database. Each key of the hash table represents a 6-mer on the reduced amino acid alphabet, and all positions of 6-mers in the database are sorted according to the hash values of the 6-mers on the regular amino acid alphabet. (Note that the reduced alphabet representation is only used for seed storage and retrieval, while the seed extension and significance evaluation are based on the original sequences.) RAPSearch2 uses 4 bytes (32 bits) to encode each 6-mer in the database: the first 20 bits are used to represent the hash keys on the reduced alphabet for all 6-mers (since 220 ≈106) in the database; and the lower 12 bits are used to represent the offset of each instance of the 6-mers in the regular amino acid alphabet. Such a representation allows us to use bit shift operations to retrieve the position of each 6-mer (in the full 20 amino acid alphabet) in constant time. For each entry of the same key, the hash values are sorted according to the four amino acids following the 6-mer, allowing seeds of up to 10 residues. In the search step, RAPSearch2 first scans a query sequence and finds the entry of each 6-mer in the hash table, then uses a binary search to find all occurrences of the seed (beyond the 6-mers) from the range of actual instances in the same entry. We also implemented a multi-threading technique in RAPSearch2. Since both the searches for individual queries and the seed-extensions between a query and individual subjects are independent, we implemented an inter-query scheme to process queries in parallel to reduce overhead on the thread switch. RAPSearch2 was implemented in C++ using Boost library 1.42 (http://www.boost.org/) and threadpool 0.2.5 (http://threadpool.sourceforge.net/), and was tested extensively on Linux platforms. 3 RESULTS We used the same NGS datasets as used in Ye et al. (2011) to test RAPSearch2 (see the example datasets in the Supplementary Material). The Integrated Microbial Genome (IMG) v3.0 and the NCBI NR (as of June 2009) (98% non-redundant set) databases were used as the search databases. Table 1 compares the performance of RAPSearch2 to RAPSearch and BLAST on four query datasets on a computer with four Xeon 2.93 GHz X5570 CPUs with 48G RAM [see more detailed comparison between RAPSearch and BLAST in (Ye et al., 2011)]. Since RAPSearch2 uses the same seed-extension algorithm as RAPSearch, it gives identical results as RAPSearch, except that it runs 2–3 times faster due to its optimized data structure. Therefore, RAPSearch2 retained the high sensitivity of RAPSearch relative to BLAT [see the comparison between RAPSearch and BLAT in (Ye et al., 2011) and in Supplementary Materials]. RAPSearch2 also requires less memory than RAPSearch. Running in single thread mode requires up to 2G memory (one-fourth the ∼8G memory required by the original RAPSearch), whereas running in 4-thread mode requires up to 3.5G memory, which is typically available on regular computer clusters. Furthermore, the 4-thread mode achieved about a 3.5X acceleration compared with singlethread mode, while the 8-thread mode achieved an almost 6X acceleration, indicating that our multi-threading implementation is efficient. Note that BLAT runs only slightly faster than RAPSearch, but ∼2–3 times slower than RAPSearch2. RAPSearch2 uses the same probabilistic model to evaluate the significance of protein sequence alignments used in BLAST (Altschul et al., 1997), and reports the E-values similarly. The output of RAPSearch2 has the same format as BLAST, and can be directly adopted into any analytical pipeline to replace BLAST as the similarity search engine. Therefore, RAPSearch2 is readily used for routine protein similarity searches for NGS data.",Alignment,"rapsearch2  fast  memoryefficient protein similarity search tool  nextgeneration sequence data
rapsearch2 use  collisionfree hash table  index  protein sequence   give search database  key   hash table represent  mer   reduce amino acid alphabet   position  mers   database  sort accord   hash value   mers   regular amino acid alphabet note   reduce alphabet representation   use  seed storage  retrieval   seed extension  significance evaluation  base   original sequence rapsearch2 use  bytes  bits  encode  mer   database  first  bits  use  represent  hash key   reduce alphabet   mers since  ≈   database   lower  bits  use  represent  offset   instance   mers   regular amino acid alphabet   representation allow   use bite shift operations  retrieve  position   mer   full  amino acid alphabet  constant time   entry    key  hash value  sort accord   four amino acids follow  mer allow seed     residues   search step rapsearch2 first scan  query sequence  find  entry   mer   hash table  use  binary search  find  occurrences   seed beyond  mers   range  actual instance    entry  also implement  multithreading technique  rapsearch2 since   search  individual query   seedextensions   query  individual subject  independent  implement  interquery scheme  process query  parallel  reduce overhead   thread switch rapsearch2  implement   use boost library    threadpool     test extensively  linux platforms  result  use   ngs datasets  use       test rapsearch2 see  example datasets   supplementary material  integrate microbial genome img    ncbi    june   nonredundant set databases  use   search databases table  compare  performance  rapsearch2  rapsearch  blast  four query datasets   computer  four xeon  ghz x5570 cpus   ram see  detail comparison  rapsearch  blast      since rapsearch2 use   seedextension algorithm  rapsearch  give identical result  rapsearch except   run  time faster due   optimize data structure therefore rapsearch2 retain  high sensitivity  rapsearch relative  blat see  comparison  rapsearch  blat        supplementary materials rapsearch2 also require less memory  rapsearch run  single thread mode require    memory onefourth   memory require   original rapsearch whereas run  thread mode require    memory   typically available  regular computer cluster furthermore  thread mode achieve    acceleration compare  singlethread mode   thread mode achieve  almost  acceleration indicate   multithreading implementation  efficient note  blat run  slightly faster  rapsearch   time slower  rapsearch2 rapsearch2 use   probabilistic model  evaluate  significance  protein sequence alignments use  blast altschul     report  evalues similarly  output  rapsearch2    format  blast    directly adopt   analytical pipeline  replace blast   similarity search engine therefore rapsearch2  readily use  routine protein similarity search  ngs data",0
22,USEARCH,"Search and clustering orders of magnitude faster than BLAST.
UBLAST and USEARCH are new algorithms for sequence database search that seek high-scoring local and global alignments, respectively. UCLUST is a new clustering algorithm that employs USEARCH as a subroutine to assign sequences to clusters. High-throughput is achieved by using a fast heuristic designed to enable rapid identification of one or a few good hits rather than all homologous sequences. For a given query, database sequences are sorted in order of decreasing number of words in common to exploit the fact that similar sequences tend to have short words in common (see e.g. Edgar, 2004a). When examined in this order (i) if a hit exists in the database, it is likely to be found among the first few candidates, and (ii) the probability that a hit exists falls rapidly as the number of failed attempts increases. A search can therefore often be terminated after examining a small number of candidates without a large cost in sensitivity. Pair-wise sequence comparisons are performed using standard fast alignment techniques, including gapless high-scoring segment pair detection by extending word seeds and banded dynamic programming. More details are provided in the Supplementary Material. 3 RESULTS I compared UBLAST and USEARCH to NCBI BLAST+ v2.2.22 using Pfam-A v24.0 (Finn et al., 2008) and Rfam v9.1 (Gardner et al., 2009), two large sequence family databases containing 4.9M protein domains and 192k RNAs, respectively. In each case, 1000 sequences were extracted at random to use as a query and the remainder used as a search database (Table 1 and Supplementary Table S1). As a simple model of a classification task, only the top hit was considered, which was designated as a true (false) positive if it belonged to the same (different) family as the query, though sequences in different families may in fact be homologous. On this test, UBLAST was ∼70× faster than MEGABLAST with significantly higher sensitivity, ∼300× faster than BLASTN with similar sensitivity, and ∼350× faster than BLASTP with comparable sensitivity above the protein twilight zone (20–35% id.). To illustrate typical improvements achieved by UCLUST compared to the widely used program CD-HIT (Li and Godzik, 2006), the most efficient previous clustering method, a set of 1.1×106 pyrosequencing reads of length ∼300 nt was taken from a recent microbial ecology study (Costello et al., 2009) and clustered at representative identities (Table 2). UCLUST produced higher quality clusters, enabled clustering at lower identities, used substantially less memory for large sets and was often one or more orders of magnitude faster. CD-HIT generated clusters with larger, and thus apparently better, average size at 99% identity, but this proved to be an artifact of a bug in CD-HIT as almost half (47%) of the reported identities were 98% and therefore fell below the threshold by the CD-HIT’s own measure. Many of these assignments were verified as being below 99% by creating independent alignments using MUSCLE (Edgar, 2004b). Despite generating smaller clusters, which should enable higher average similarity to be achieved, CD-HIT produced clusters with lower average similarity than UCLUST in all cases. This shows that when a sequence can be assigned to more than one cluster, UCLUST tends to identify a better match. UBLAST and USEARCH introduce a new search paradigm that seeks one or a few good hits rather than all hits in order to improve throughput. This approach is well suited to next-generation sequence classification, where search is often a critical bottleneck in analysis. Low-ranking hits are rarely needed and may even be undesirable due to increased overhead. The user can tune the trade-off between speed and sensitivity by adjusting parameters that control how many candidate hits are examined before the search is terminated. Speed and sensitivity are thus strongly dependent on the data and program options. In typical applications, USEARCH typically achieves good sensitivity at around 40% identity and above for amino acids and 65% for nucleotides. UBLAST is sensitive to more distant relationships, with useful sensitivity extending into the twilight zone for proteins. BLASTP has significantly better sensitivity at lower identities, which is partly due to refinements of the BLAST algorithm not yet implemented in UBLAST and also the problem that some distantly related proteins have no perfectly conserved words of the required length. UBLAST and USEARCH require memory that is roughly 10× the size of the database, which is sometimes more than BLAST but in most cases is readily accommodated by currently available commodity computer hardware. UCLUST is definitively superior to CD-HIT. It is usually significantly faster, uses significantly less memory, can cluster at lower identities and is more sensitive. While CD-HIT often fails to identify the closest cluster, or overlooks that a match is possible (false negative), UCLUST rarely misses a match and in most cases finds the best possible match. UCLUST also enables rapid clustering of much larger numbers of sequences. UBLAST, USEARCH and UCLUST can dramatically reduce the resources required for classification of large sequence sets, and will therefore be of value to biologists in a wide range of applications.",Alignment,"search  cluster order  magnitude faster  blast
ublast  usearch  new algorithms  sequence database search  seek highscoring local  global alignments respectively uclust   new cluster algorithm  employ usearch   subroutine  assign sequence  cluster highthroughput  achieve  use  fast heuristic design  enable rapid identification  one    good hit rather   homologous sequence   give query database sequence  sort  order  decrease number  word  common  exploit  fact  similar sequence tend   short word  common see  edgar   examine   order    hit exist   database   likely   find among  first  candidates    probability   hit exist fall rapidly   number  fail attempt increase  search  therefore often  terminate  examine  small number  candidates without  large cost  sensitivity pairwise sequence comparisons  perform use standard fast alignment techniques include gapless highscoring segment pair detection  extend word seed  band dynamic program  detail  provide   supplementary material  result  compare ublast  usearch  ncbi blast  use pfama v24 finn     rfam  gardner    two large sequence family databases contain  protein domains   rnas respectively   case  sequence  extract  random  use   query   remainder use   search database table   supplementary table    simple model   classification task   top hit  consider   designate   true false positive   belong    different family   query though sequence  different families may  fact  homologous   test ublast   faster  megablast  significantly higher sensitivity  faster  blastn  similar sensitivity   faster  blastp  comparable sensitivity   protein twilight zone    illustrate typical improvements achieve  uclust compare   widely use program cdhit   godzik    efficient previous cluster method  set   pyrosequencing read  length    take   recent microbial ecology study costello     cluster  representative identities table  uclust produce higher quality cluster enable cluster  lower identities use substantially less memory  large set   often one   order  magnitude faster cdhit generate cluster  larger  thus apparently better average size   identity   prove    artifact   bug  cdhit  almost half    report identities    therefore fell   threshold   cdhits  measure many   assignments  verify      create independent alignments use muscle edgar  despite generate smaller cluster   enable higher average similarity   achieve cdhit produce cluster  lower average similarity  uclust   case  show    sequence   assign    one cluster uclust tend  identify  better match ublast  usearch introduce  new search paradigm  seek one    good hit rather   hit  order  improve throughput  approach  well suit  nextgeneration sequence classification  search  often  critical bottleneck  analysis lowranking hit  rarely need  may even  undesirable due  increase overhead  user  tune  tradeoff  speed  sensitivity  adjust parameters  control  many candidate hit  examine   search  terminate speed  sensitivity  thus strongly dependent   data  program options  typical applications usearch typically achieve good sensitivity  around  identity    amino acids    nucleotides ublast  sensitive   distant relationships  useful sensitivity extend   twilight zone  proteins blastp  significantly better sensitivity  lower identities   partly due  refinements   blast algorithm  yet implement  ublast  also  problem   distantly relate proteins   perfectly conserve word   require length ublast  usearch require memory   roughly   size   database   sometimes   blast    case  readily accommodate  currently available commodity computer hardware uclust  definitively superior  cdhit   usually significantly faster use significantly less memory  cluster  lower identities    sensitive  cdhit often fail  identify  closest cluster  overlook   match  possible false negative uclust rarely miss  match    case find  best possible match uclust also enable rapid cluster  much larger number  sequence ublast usearch  uclust  dramatically reduce  resources require  classification  large sequence set   therefore   value  biologists   wide range  applications",0
23,VSEARCH,"VSEARCH: a versatile open source tool for metagenomics
Algorithms and implementation Below is a brief description of the most important functions of VSEARCH and details of their implementation. VSEARCH command line options are shown in italics, and should be preceded by a single (-) or double dash (-TI:\,-) when used. Reading FASTA and FASTQ files Most VSEARCH commands read files in FASTA or FASTQ format. The parser for FASTQ files in VSEARCH is compliant with the standard as described by Cock et al. (2010) and correctly parses all their tests files. FASTA and FASTQ files are automatically detected and many commands accept both as input. Files compressed with gzip or bzip2 are automatically detected and decompressed using the zlib library by Gailly & Adler (2016) or the bzip2 library by Seward (2016), respectively. Data may also be piped into or out of VSEARCH, allowing for instance many separate FASTA files to be piped into VSEARCH for simultaneous dereplication, or allowing the creation of complex pipelines without ever having to write on slow disks. VSEARCH is a 64-bit program and allows very large datasets to be processed, essentially limited only by the amount of memory available. The free USEARCH versions are 32-bit programs that limit the available memory to somewhere less than 4GB, often seriously hampering the analysis of realistic datasets. Writing result files VSEARCH can output results in a variety of formats (FASTA, FASTQ, tables, alignments, SAM) depending on the input format and command used. When outputting FASTA files, the line width may be specified using the fasta_width option, where 0 means that line wrapping should be turned off. Similar controls are offered for pairwise or multiple sequence alignments. Searching Global pairwise sequence comparison is a core functionality of VSEARCH. Several commands compare a query sequence against a database of sequences: all-vs-all alignment (allpairs_global), clustering (cluster_fast, cluster_size, cluster_smallmem), chimera detection (uchime_denovo and uchime_ref) and searching (usearch_global). This comparison function proceeds in two phases: an initial heuristic filtering based on shared words, followed by optimal alignment of the query with the most promising candidates. The first phase is presumably quite similar to USEARCH (Edgar, 2010). Heuristics are used to identify a small set of database sequences that have many words in common with the query sequence. Words (or k-mers) consist of a certain number k of consecutive nucleotides of a sequence (8 by default, adjustable with the wordlength option). All overlapping words are included. A sequence of length n then contains at most n − k + 1 unique words. VSEARCH counts the number of shared words between the query and each database sequence. Words that appear multiple times are counted only once. To count the words in the database sequences quickly, VSEARCH creates an index of all the 4k possible distinct words and stores information about which database sequences they appear in. For extremely frequent words, the set of database sequences is represented by a bitmap; otherwise the set is stored as a list. A finer control of k-mer indexing is described for USEARCH by the pattern (binary string indicating which positions must match) and slots options. USEARCH has such options but seems to ignore them. Currently, VSEARCH ignores these two options too. The minimum number of shared words required may be specified with the minwordmatches option (10 by default), but a lower value is automatically used for short or simple query sequences with less than 10 unique words. Comparing sequences based on statistics of shared words is a common method to quickly assess the similarity between two sequences without aligning them, which is often time-consuming. The D2 statistic and related metrics for alignment-free sequence comparison have often been used for rapid and approximate sequence matching and their statistical properties have been well studied (Song et al., 2014). The approach used here has similarities to the D2 statistic, but multiple matches of the same word are ignored. In the second phase, searching proceeds by considering the database sequences in a specific order, starting with the sequence having the largest number of words in common with the query, and proceeding with a decreasing number of shared words. If two database sequences have the same number of words in common with the query, the shortest sequence is considered first. The query sequence is compared with each database sequence by computing the optimal global alignment. The alignment is performed using a multi-threaded and vectorised full dynamic programming algorithm (Needleman & Wunsch, 1970) adapted from SWIPE (Rognes, 2011). Due to the extreme memory requirements of this method when aligning two long sequences, an alternative algorithm described by Hirschberg (1975) and Myers & Miller (1988) is used when the product of the length of the sequences is greater than 25,000,000, corresponding to aligning two 5,000 bp sequences. This alternative algorithm uses only a linear amount of memory but is considerably slower. This second phase is probably where USEARCH and VSEARCH differ the most, as USEARCH by default presumably performs heuristic seed-and-extend alignment similar to BLAST (Altschul et al., 1997), and only performs optimal alignment when the option fulldp (full dynamic programming) is used. Computing the optimal pairwise alignment in each case gives more accurate results but is also computationally more demanding. The efficient and vectorised full dynamic programming implementation in VSEARCH compensates that extra cost, at least for sequences that are not too long. If the resulting alignment indicates a similarity equal to or greater than the value specified with the id option, the database sequence is accepted. If the similarity is too low, it is rejected. Several other options may also be used to determine how similarity is computed (iddef, as USEARCH used to offer up to version 6), and which sequences should be accepted and rejected, either before (e.g., self, minqsize) or after alignment (e.g., maxgaps, maxsubs). The search is terminated when either a certain number of sequences have been accepted (1 by default, adjustable with the maxaccepts option), or a certain number of sequences have been rejected (32 by default, adjustable with the maxrejects option). The accepted sequences are sorted by sequence similarity and presented as the search results. VSEARCH also includes a search_exact command that only identifies exact matches to the query. It uses a hash table in a way similar to the full-length dereplication command described below. Clustering VSEARCH includes commands to perform de novo clustering using a greedy and heuristic centroid-based algorithm with an adjustable sequence similarity threshold specified with the id option (e.g., 0.97). The input sequences are either processed in the user supplied order (cluster_smallmem) or pre-sorted based on length (cluster_fast) or abundance (the new cluster_size option). Each input sequence is then used as a query in a search against an initially empty database of centroid sequences. The query sequence is clustered with the first centroid sequence found with similarity equal to or above the threshold. The search is performed using the heuristic approach described above which generally finds the most similar sequences first. If no matches are found, the query sequence becomes the centroid of a new cluster and is added to the database. If maxaccepts is higher than 1, several centroids with sufficient sequence similarity may be found and considered. By default, the query is clustered with the centroid presenting the highest sequence similarity (distance-based greedy clustering, DGC), or, if the sizeorder option is turned on, the centroid with the highest abundance (abundance-based greedy clustering, AGC) (He et al., 2015; Westcott & Schloss, 2015; Schloss, 2016). VSEARCH performs multi-threaded clustering by searching the database of centroid sequences with several query sequences in parallel. If there are any non-matching query sequences giving rise to new centroids, the required internal comparisons between the query sequences are subsequently performed to achieve correct results. For each cluster, VSEARCH can create a simple multiple sequence alignment using the center star method (Gusfield, 1993) with the centroid as the center sequence, and then compute a consensus sequence and a sequence profile. Dereplication and rereplication Full-length dereplication (derep_fulllength) is performed using a hash table with an open addressing and linear probing strategy based on the Google CityHash hash functions (written by Geoff Pike and Jyrki Alakuijala, and available at https://github.com/google/cityhash). The hash table is initially empty. For each input sequence, the hash is computed and a lookup in the hash table is performed. If an identical sequence is found, the input sequence is clustered with the matching sequence; otherwise the input sequence is inserted into the hash table. Prefix dereplication (derep_prefix) is also implemented. As with full-length dereplication, identical sequences are clustered. In addition, sequences that are identical to prefixes of other sequences will also be clustered together. If a sequence is identical to the prefix of multiple sequences, it is generally not defined how prefix clustering should behave. VSEARCH resolves this ambiguity by clustering the sequence with the shortest of the candidate sequences. If they are equally long, priority will be given to the most abundant, the one with the lexicographically smaller identifier or the one with the earliest original position, in that order. To perform prefix dereplication, VSEARCH first creates an initially empty hash table. It then sorts the input sequences by length and identifies the length s of the shortest sequence in the dataset. Each input sequence is then processed as follows, starting with the shortest: If an exact match to the full input sequence is found in the hash table, the input sequence is clustered with the matching hash table sequence. If no match to the full input sequence is found, the prefixes of the input sequence are considered, starting with the longest prefix and proceeding with shorter prefixes in order, down to prefixes of length s. If a match is now found in the hash table, the sequences are clustered, the matching sequence is deleted from the hash table and the full input sequence is inserted into the hash table instead. If no match is found for any prefix, the full sequence is inserted into the hash table. In the end, the remaining sequences in the hash table will be output with accumulated abundances for all sequences in each cluster. In order to identify matches in the hash table during prefix dereplication, a hash is computed for each full-length input sequence and all its prefixes. The hash function used is the 64-bit Fowler–Noll–Vo 1a hash function (Fowler, Noll & Vo, 1991), which is simple and quick to compute for such a series of sequences by adding one nucleotide at a time. The sequences resulting from dereplication and many other commands may be relabeled with a given prefix followed by a sequentially increasing number. VSEARCH exclusively also offers the possibility of relabelling each sequence with the SHA-1 (Eastlake & Jones, 2001) or MD5 (Rivest, 1992) message digest (hash) of the sequence. These are strings that are highly likely to be unique for each sequence. Before the digest is computed, the sequence is normalized by converting U’s to T’s and converting all symbols to upper case. VSEARCH includes public domain code for the MD5 algorithm written by Alexander Peslyak, and for SHA1 by Steve Reid and others. VSEARCH also includes a new command (rereplicate) to perform rereplication that can be used to recreate datasets as they were before full-length dereplication, but of course original labels cannot be recreated. Chimera detection Chimeras are detected either de novo (uchime_denovo command) or with a reference database (uchime_ref command) using the UCHIME algorithm described by Edgar et al. (2011). VSEARCH will divide each query sequence into four segments and look for similarity of each segment to sequences in the set of potential parents using the heuristic search function described earlier. It will consider the four best candidates for each segment using maxaccepts 4 and maxrejects 16, and an id threshold of 0.55. VSEARCH optionally outputs borderline sequences, that is, sequences having a high enough score (as specified with the minh option) but with too small a divergence from the closest parent (as specified with the mindiv option). Multi-threading is supported for reference-based chimera detection. Low-complexity sequence masking VSEARCH includes a highly optimized and parallelized implementation of the Dust algorithm by Tatusov and Lipman for masking of simple repeats and low-complexity nucleotide sequences. It is considerably faster than the implementation of the same algorithm in USEARCH. Their code available at ftp://ftp.ncbi.nlm.nih.gov/pub/tatusov/dust/version1/src/ is in the public domain. VSEARCH uses this algorithm by default, while USEARCH by default uses an undocumented rapid masking algorithm called fastnucleo. VSEARCH performs soft-masking automatically for the pairwise alignment, search, clustering and chimera detection commands. This behaviour can be controlled with the hardmask option to replace masked symbols with N’s instead of lower-casing them, and the dbmask and qmask options, which selects the masking algorithm (none, dust or soft) used for the database and query sequences, respectively. Masking may also be performed explicitly on an input file using the fastx_mask and maskfasta commands. FASTQ file processing VSEARCH includes commands to detect the FASTQ file version and the range of quality scores used (fastq_chars), as well as two commands for computing sequence quality statistics (fastq_stats and fastq_eestats). It can also truncate and filter sequences in FASTQ files based on various criteria (fastq_filter). A new command is added to convert between different FASTQ file versions and quality encodings (fastq_convert), e.g., from the old Phred+64 encoded Illumina FASTQ files to the newer Phred+33 format. Merging of paired-end reads Merging of paired-end reads is supported by VSEARCH using the fastq_mergepairs command. The method used has some similarity to PEAR (Zhang et al., 2014) and recognises options similar to USEARCH. The algorithm computes the optimal ungapped alignment of the overlapping region of the forward sequence and the reverse-complemented reverse sequence. The alignment requires a minimum overlap length (specified with the fastq_minovlen option, default 10), a maximum number of mismatches (fastq_maxdiffs option, default 5), and a minimum and maximum length of the merged sequence (fastq_minmergelen option, default 1, and fastq_maxmergelen option, default infinite). Staggered read pairs, i.e., read pairs where the 3′ end of the reverse read has an overhang to the left of the 5′  end of the forward read, are not allowed by default, but may be turned on by the fastq_allowmergestagger option. VSEARCH uses a match score (alpha) of +4 and a mismatch score (beta) of −5 for perfect quality residues. These scores are weighted by the probability that these two residues really match or mismatch, respectively, taking quality scores into account. These probabilities are computed in a way similar to PEAR score method 2 described in ‘Algorithms and implementation’ of the PEAR paper (Zhang et al., 2014), but VSEARCH assumes all nucleotide background frequencies are 0.25. When merging sequences, VSEARCH computes posterior quality scores for the overlapping regions as described by Edgar & Flyvbjerg (2015). For speed, scores and probabilities are pre-computed for all possible quality scores. Sorting and shuffling VSEARCH can sort FASTA files by decreasing sequence length (sortbylength) or abundance (sortbysize). VSEARCH can also perform shuffling of FASTA files in random order (shuffle). A seed value for the pseudo random number generator may be provided by the randseed option to obtain replicable results. Subsampling Sequences in FASTA and FASTQ files can be subsampled (fastx_subsample) by randomly extracting a certain number (sample_size) or percentage (sample_pct) of the input sequences. Abundances may be taken into account, giving results as if the input sequences were rereplicated, subsampled and then dereplicated.",Alignment,"vsearch  versatile open source tool  metagenomics
algorithms  implementation    brief description    important function  vsearch  detail   implementation vsearch command line options  show  italics    precede   single   double dash   use read fasta  fastq file  vsearch command read file  fasta  fastq format  parser  fastq file  vsearch  compliant   standard  describe  cock     correctly parse   test file fasta  fastq file  automatically detect  many command accept   input file compress  gzip  bzip2  automatically detect  decompress use  zlib library  gailly  adler    bzip2 library  seward  respectively data may also  pip     vsearch allow  instance many separate fasta file   pip  vsearch  simultaneous dereplication  allow  creation  complex pipelines without ever   write  slow disk vsearch   bite program  allow  large datasets   process essentially limit    amount  memory available  free usearch versions  bite program  limit  available memory  somewhere less  4gb often seriously hamper  analysis  realistic datasets write result file vsearch  output result   variety  format fasta fastq table alignments sam depend   input format  command use  output fasta file  line width may  specify use  fasta_width option   mean  line wrap   turn  similar control  offer  pairwise  multiple sequence alignments search global pairwise sequence comparison   core functionality  vsearch several command compare  query sequence   database  sequence allvsall alignment allpairs_global cluster cluster_fast cluster_size cluster_smallmem chimera detection uchime_denovo  uchime_ref  search usearch_global  comparison function proceed  two phase  initial heuristic filter base  share word follow  optimal alignment   query    promise candidates  first phase  presumably quite similar  usearch edgar  heuristics  use  identify  small set  database sequence   many word  common   query sequence word  kmers consist   certain number   consecutive nucleotides   sequence   default adjustable   wordlength option  overlap word  include  sequence  length   contain        unique word vsearch count  number  share word   query   database sequence word  appear multiple time  count    count  word   database sequence quickly vsearch create  index     possible distinct word  store information   database sequence  appear   extremely frequent word  set  database sequence  represent   bitmap otherwise  set  store   list  finer control  kmer index  describe  usearch   pattern binary string indicate  position must match  slot options usearch   options  seem  ignore  currently vsearch ignore  two options   minimum number  share word require may  specify   minwordmatches option   default   lower value  automatically use  short  simple query sequence  less   unique word compare sequence base  statistics  share word   common method  quickly assess  similarity  two sequence without align    often timeconsuming   statistic  relate metrics  alignmentfree sequence comparison  often  use  rapid  approximate sequence match   statistical properties   well study song     approach use   similarities    statistic  multiple match    word  ignore   second phase search proceed  consider  database sequence   specific order start   sequence   largest number  word  common   query  proceed   decrease number  share word  two database sequence    number  word  common   query  shortest sequence  consider first  query sequence  compare   database sequence  compute  optimal global alignment  alignment  perform use  multithreaded  vectorised full dynamic program algorithm needleman  wunsch  adapt  swipe rognes  due   extreme memory requirements   method  align two long sequence  alternative algorithm describe  hirschberg   myers  miller   use   product   length   sequence  greater   correspond  align two   sequence  alternative algorithm use   linear amount  memory   considerably slower  second phase  probably  usearch  vsearch differ    usearch  default presumably perform heuristic seedandextend alignment similar  blast altschul      perform optimal alignment   option fulldp full dynamic program  use compute  optimal pairwise alignment   case give  accurate result   also computationally  demand  efficient  vectorised full dynamic program implementation  vsearch compensate  extra cost  least  sequence     long   result alignment indicate  similarity equal   greater   value specify    option  database sequence  accept   similarity   low   reject several  options may also  use  determine  similarity  compute iddef  usearch use  offer   version    sequence   accept  reject either   self minqsize   alignment  maxgaps maxsubs  search  terminate  either  certain number  sequence   accept   default adjustable   maxaccepts option   certain number  sequence   reject   default adjustable   maxrejects option  accept sequence  sort  sequence similarity  present   search result vsearch also include  search_exact command   identify exact match   query  use  hash table   way similar   fulllength dereplication command describe  cluster vsearch include command  perform  novo cluster use  greedy  heuristic centroidbased algorithm   adjustable sequence similarity threshold specify    option    input sequence  either process   user supply order cluster_smallmem  presorted base  length cluster_fast  abundance  new cluster_size option  input sequence   use   query   search   initially empty database  centroid sequence  query sequence  cluster   first centroid sequence find  similarity equal     threshold  search  perform use  heuristic approach describe   generally find   similar sequence first   match  find  query sequence become  centroid   new cluster   add   database  maxaccepts  higher   several centroids  sufficient sequence similarity may  find  consider  default  query  cluster   centroid present  highest sequence similarity distancebased greedy cluster dgc    sizeorder option  turn   centroid   highest abundance abundancebased greedy cluster agc     westcott  schloss  schloss  vsearch perform multithreaded cluster  search  database  centroid sequence  several query sequence  parallel     nonmatching query sequence give rise  new centroids  require internal comparisons   query sequence  subsequently perform  achieve correct result   cluster vsearch  create  simple multiple sequence alignment use  center star method gusfield    centroid   center sequence   compute  consensus sequence   sequence profile dereplication  rereplication fulllength dereplication derep_fulllength  perform use  hash table   open address  linear probe strategy base   google cityhash hash function write  geoff pike  jyrki alakuijala  available    hash table  initially empty   input sequence  hash  compute   lookup   hash table  perform   identical sequence  find  input sequence  cluster   match sequence otherwise  input sequence  insert   hash table prefix dereplication derep_prefix  also implement   fulllength dereplication identical sequence  cluster  addition sequence   identical  prefix   sequence  also  cluster together   sequence  identical   prefix  multiple sequence   generally  define  prefix cluster  behave vsearch resolve  ambiguity  cluster  sequence   shortest   candidate sequence    equally long priority   give    abundant  one   lexicographically smaller identifier   one   earliest original position   order  perform prefix dereplication vsearch first create  initially empty hash table   sort  input sequence  length  identify  length    shortest sequence   dataset  input sequence   process  follow start   shortest   exact match   full input sequence  find   hash table  input sequence  cluster   match hash table sequence   match   full input sequence  find  prefix   input sequence  consider start   longest prefix  proceed  shorter prefix  order   prefix  length    match   find   hash table  sequence  cluster  match sequence  delete   hash table   full input sequence  insert   hash table instead   match  find   prefix  full sequence  insert   hash table   end  remain sequence   hash table   output  accumulate abundances   sequence   cluster  order  identify match   hash table  prefix dereplication  hash  compute   fulllength input sequence    prefix  hash function use   bite fowlernollvo  hash function fowler noll      simple  quick  compute    series  sequence  add one nucleotide   time  sequence result  dereplication  many  command may  relabeled   give prefix follow   sequentially increase number vsearch exclusively also offer  possibility  relabelling  sequence   sha eastlake  jones   md5 rivest  message digest hash   sequence   string   highly likely   unique   sequence   digest  compute  sequence  normalize  convert     convert  symbols  upper case vsearch include public domain code   md5 algorithm write  alexander peslyak   sha1  steve reid  others vsearch also include  new command rereplicate  perform rereplication    use  recreate datasets     fulllength dereplication   course original label cannot  recreate chimera detection chimeras  detect either  novo uchime_denovo command    reference database uchime_ref command use  uchime algorithm describe  edgar    vsearch  divide  query sequence  four segment  look  similarity   segment  sequence   set  potential parent use  heuristic search function describe earlier   consider  four best candidates   segment use maxaccepts   maxrejects     threshold   vsearch optionally output borderline sequence   sequence   high enough score  specify   minh option    small  divergence   closest parent  specify   mindiv option multithreading  support  referencebased chimera detection lowcomplexity sequence mask vsearch include  highly optimize  parallelize implementation   dust algorithm  tatusov  lipman  mask  simple repeat  lowcomplexity nucleotide sequence   considerably faster   implementation    algorithm  usearch  code available  ftpftpncbinlmnihgovpubtatusovdustversion1src    public domain vsearch use  algorithm  default  usearch  default use  undocumented rapid mask algorithm call fastnucleo vsearch perform softmasking automatically   pairwise alignment search cluster  chimera detection command  behaviour   control   hardmask option  replace mask symbols   instead  lowercasing    dbmask  qmask options  select  mask algorithm none dust  soft use   database  query sequence respectively mask may also  perform explicitly   input file use  fastx_mask  maskfasta command fastq file process vsearch include command  detect  fastq file version   range  quality score use fastq_chars  well  two command  compute sequence quality statistics fastq_stats  fastq_eestats   also truncate  filter sequence  fastq file base  various criteria fastq_filter  new command  add  convert  different fastq file versions  quality encode fastq_convert    old phred encode illumina fastq file   newer phred format merge  pairedend read merge  pairedend read  support  vsearch use  fastq_mergepairs command  method use   similarity  pear zhang     recognise options similar  usearch  algorithm compute  optimal ungapped alignment   overlap region   forward sequence   reversecomplemented reverse sequence  alignment require  minimum overlap length specify   fastq_minovlen option default   maximum number  mismatch fastq_maxdiffs option default    minimum  maximum length   merge sequence fastq_minmergelen option default   fastq_maxmergelen option default infinite stagger read pair  read pair   ′ end   reverse read   overhang   leave   ′  end   forward read   allow  default  may  turn    fastq_allowmergestagger option vsearch use  match score alpha     mismatch score beta    perfect quality residues  score  weight   probability   two residues really match  mismatch respectively take quality score  account  probabilities  compute   way similar  pear score method  describe  algorithms  implementation   pear paper zhang     vsearch assume  nucleotide background frequencies    merge sequence vsearch compute posterior quality score   overlap regions  describe  edgar  flyvbjerg   speed score  probabilities  precomputed   possible quality score sort  shuffle vsearch  sort fasta file  decrease sequence length sortbylength  abundance sortbysize vsearch  also perform shuffle  fasta file  random order shuffle  seed value   pseudo random number generator may  provide   randseed option  obtain replicable result subsampling sequence  fasta  fastq file   subsampled fastx_subsample  randomly extract  certain number sample_size  percentage sample_pct   input sequence abundances may  take  account give result    input sequence  rereplicated subsampled   dereplicated",0
24,CLUS_GPU-BLASTP,"CLUS_GPU-BLASTP: accelerated protein sequence alignment using GPU-enabled cluster
In our implementation of protein BLAST, subject database is divided into multiple fragments by the head node of the cluster. The number of fragments is equal to the number of compute nodes in the cluster. All the compute nodes in the cluster are homogeneous. Each compute node is allocated a different fragment to be searched against the query sequences. Framework developed for the execution of CLUS_GPUBLASTP is shown in Fig. 4. As shown in Fig. 4, to execute the protein BLAST on GPU-enabled highperformance cluster, the application is divided into head node and compute node modules. The head node performs the task of work distribution and result compilation. The process of database fragmentation and allocation is also performed by the head node. Each participating compute node needs to be GPU enabled. All the remaining operations of CLUS_GPU-BLASTP are performed on the compute nodes. Parallel BLAST for multiple query processing is implemented with CUDA 5.5. OpenMPI 1.7 is used to handle inter-processor communication. Portable Batch System (PBS) scheduler is used to schedule the query sequences to different compute nodes. As the texture memory of the GPUs provides faster access to the data, in our implementation textures are defined to access the database sequences by different threads of the GPU. 3.1 Head node When execution of the parallel BLAST program is started on the head node, the very first activity performed is the node identification. This process allows the head node to discover which compute nodes are available to run the parallel BLAST application. 123 CLUS_GPU-BLASTP: accelerated protein sequence alignment ... Fig. 4 Parallel framework for running CLUS_GPU-BLATP As per the availability of the compute nodes, the target database will be processed and divided into fragments. Target database is divided into equal-sized fragments (with an almost equal number of amino acids), each node to take almost the same time for processing. Number of fragments will be equal to the number of participating compute nodes. After segmentation, each node will be allocated a fragment to search. When all the compute nodes will be allocated the different database fragments to perform search against different query sequences, the next process to be performed is transfer of query sequences to the compute nodes. As this parallel BLAST application is designed to handle multiple queries concurrently, query scheduling plays an important role. Multiple queries received from the user by the head node for alignment will be scheduled to the different compute nodes. In current application, Portable Batch System (PBS) scheduler is used for scheduling and dynamic scheduling is performed to manage multiple queries. As each compute node of the cluster is GPU enabled, PBS schedules the queries to the different nodes to reap the maximum benefit of GPU-enabled compute nodes. It is focused that none of the compute node should remain idle. When search of a query sequence against the database fragment of a compute node will be complete, the node will be allocated some other query sequence for comparison. Partial results obtained for a particular query sequence will be transmitted to the head node. The head node will schedule the query to some other compute node whose database fragment is not being searched. The pseudo-code for the head node is shown in Fig. 5. As each compute node is carrying only a fragment of the database, each compute node will have only partial alignment results for a query sequence. When a compute node will complete the search for its assigned database for a particular query sequence, it will return the partial results to the head node. The head node will combine the partial results to already received results for the same query sequence from other nodes, if 123 S. Rani, O. P. Gupta Fig. 5 Pseudo-code for the head node any. When partial results from all the compute nodes for a particular query sequence will be obtained and compiled, the head node will send the final compiled results to the user. The same process will be repeated for each query sequence of the input file.",Alignment,"clus_gpublastp accelerate protein sequence alignment use gpuenabled cluster
  implementation  protein blast subject database  divide  multiple fragment   head node   cluster  number  fragment  equal   number  compute nod   cluster   compute nod   cluster  homogeneous  compute node  allocate  different fragment   search   query sequence framework develop   execution  clus_gpublastp  show  fig   show  fig   execute  protein blast  gpuenabled highperformance cluster  application  divide  head node  compute node modules  head node perform  task  work distribution  result compilation  process  database fragmentation  allocation  also perform   head node  participate compute node need   gpu enable   remain operations  clus_gpublastp  perform   compute nod parallel blast  multiple query process  implement  cuda  openmpi   use  handle interprocessor communication portable batch system pbs scheduler  use  schedule  query sequence  different compute nod   texture memory   gpus provide faster access   data   implementation textures  define  access  database sequence  different thread   gpu  head node  execution   parallel blast program  start   head node   first activity perform   node identification  process allow  head node  discover  compute nod  available  run  parallel blast application  clus_gpublastp accelerate protein sequence alignment  fig  parallel framework  run clus_gpublatp  per  availability   compute nod  target database   process  divide  fragment target database  divide  equalsized fragment   almost equal number  amino acids  node  take almost   time  process number  fragment   equal   number  participate compute nod  segmentation  node   allocate  fragment  search    compute nod   allocate  different database fragment  perform search  different query sequence  next process   perform  transfer  query sequence   compute nod   parallel blast application  design  handle multiple query concurrently query schedule play  important role multiple query receive   user   head node  alignment   schedule   different compute nod  current application portable batch system pbs scheduler  use  schedule  dynamic schedule  perform  manage multiple query   compute node   cluster  gpu enable pbs schedule  query   different nod  reap  maximum benefit  gpuenabled compute nod   focus  none   compute node  remain idle  search   query sequence   database fragment   compute node   complete  node   allocate   query sequence  comparison partial result obtain   particular query sequence   transmit   head node  head node  schedule  query    compute node whose database fragment    search  pseudocode   head node  show  fig    compute node  carry   fragment   database  compute node    partial alignment result   query sequence   compute node  complete  search   assign database   particular query sequence   return  partial result   head node  head node  combine  partial result  already receive result    query sequence   nod    rani   gupta fig  pseudocode   head node   partial result    compute nod   particular query sequence   obtain  compile  head node  send  final compile result   user   process   repeat   query sequence   input file",0
25,Kraken,"Kraken: ultrafast metagenomic sequence classification using exact alignments
Sequence classification algorithm To classify a DNA sequence S, we collect all k-mers within that sequence into a set, denoted as K(S). We then map each k-mer in K(S), using the algorithm described below, to the LCA taxon of all genomes that contain that k-mer. These LCA taxa and their ancestors in the taxonomy tree form what we term the classification tree, a pruned subtree that is used to classify S. Each node in the classification tree is weighted with the number of k-mers in K(S) that mapped to the taxon associated with that node. Then, each root-to-leaf (RTL) path in the classification tree is scored by calculating the sum of all node weights along the path. The maximum scoring RTL path in the classification tree is the classification path, and S is assigned the label corresponding to its leaf (if there are multiple maximally scoring paths, the LCA of all those paths’ leaves is selected). This algorithm, illustrated in Figure 1, allows Kraken to consider each k-mer within a sequence as a separate piece of evidence, and then attempt to resolve any conflicting evidence if necessary. Note that for an appropriate choice of k, most k-mers will map uniquely to a single species, greatly simplifying the classification process. Sequences for which none of the k-mers in K(S) are found in any genome are left unclassified by this algorithm. The use of RTL path scoring in the classification tree is necessary in light of the inevitable differences between the sequences to be classified and the sequences present in any library of genomes. Such differences can, even for large values of k, result in a k-mer that is present in the library but associated with a species far removed from the true source species. By scoring the various RTL paths in the classification tree, we can compensate for these differences and correctly classify sequences even when a small minority of k-mers in a sequence indicate that the sequence should be assigned an incorrect taxonomic label. Database creation Efficient implementation of Kraken’s classification algorithm requires that the mapping of k-mers to taxa is performed by querying a pre-computed database. Kraken creates this database through a multi-step process, beginning with the selection of a library of genomic sequences. Kraken includes a default library, based on completed microbial genomes in the National Center for Biotechnology Information’s (NCBI) RefSeq database, but the library can be customized as needed by individual users [18]. Once the library is chosen, we use the Jellyfish multithreaded k-mer counter [19] to create a database containing every distinct 31-mer in the library. Once the database is complete, the 4-byte spaces Jellyfish used to store the k-mer counts in the database file are instead used by Kraken to store the taxonomic ID numbers of the k-mers’ LCA values. After the database has been created by Jellyfish, the genomic sequences in the library are processed one at a time. For each sequence, the taxon associated with it is used to set the stored LCA values of all k-mers in the sequence. As sequences are processed, if a k-mer from a sequence has had its LCA value previously set, then the LCA of the stored value and the current sequence’s taxon is calculated and that LCA is stored for the k-mer. Taxon information is obtained from the NCBI taxonomy database. Database structure and search algorithm Because Kraken very frequently uses a k-mer as a database query immediately after querying an adjacent k-mer, and because adjacent k-mers share a substantial amount of sequence, we utilize the minimizer concept [20] to group similar k-mers together. To explain our application of this concept, we here define the canonical representation of a DNA sequence S as the lexicographically smaller of S and the reverse complement of S. To determine a k-mer’s minimizer of length M, we consider the canonical representation of all M-mers in the k-mer, and select the lexicographically smallest of those M-mers as the k-mer’s minimizer. In practice, adjacent k-mers will often have the same minimizer. In Kraken’s database, all k-mers with the same minimizer are stored consecutively, and are sorted in lexicographical order of their canonical representations. A query for a k-mer R can then be processed by looking up in an index the positions in the database where the k-mers with R’s minimizer would be stored, and then performing a binary search within that region (Figure 5). Because adjacent k-mers often have the same minimizer, the search range is often the same between two consecutive queries, and the search in the first query can often bring data into the CPU cache that will be used in the second query. By allowing memory accesses in subsequent queries to access data in the CPU cache instead of RAM, this strategy makes subsequent queries much faster than they would otherwise be. The index containing the offsets of each group of k-mers in the database requires 8 × 4M bytes. By default Kraken uses 15-bp minimizers, but the user can modify this value; for example, in creating MiniKraken, we used 13-bp minimizers to ensure the total database size stayed under 4 GB. In implementing Kraken, we made further optimizations to the structure and search algorithm described above. First, as noted by Roberts et al. [20], a simple lexicographical ordering of M-mers can result in a skewed distribution of minimizers that over-represents low-complexity M-mers. In Kraken, such a bias would create many large search ranges, which would require more time to search. To create a more even distribution of minimizers (and thus speed up searches), we use the exclusive-or (XOR) operation to toggle half of the bits of each M-mer’s canonical representation prior to comparing the M-mers to each other using lexicographical ordering. This XOR operation effectively scrambles the standard ordering, and prevents the large bias toward low-complexity minimizers. We also take advantage of the fact that the search range is often the same between queries to make Kraken’s queries faster. Rather than compute the minimizer each time we perform a query, we first search the previous range. If our queried k-mer is found in this range, the query can return immediately. If the k-mer is not found, then the minimizer is computed; if the k-mer’s minimizer is the same as the last queried k-mer’s, then the query fails, as the minimizer’s search space has been shown not to have the k-mer. Only if the minimizer has changed does Kraken have to adjust the search range and search again for the k-mer. Constructing simulated metagenomes The HiSeq and MiSeq metagenomes were built using 20 sets of bacterial whole-genome shotgun reads. These reads were found either as part of the GAGE-B project [21] or in the NCBI Sequence Read Archive. Each metagenome contains sequences from ten genomes (Additional file 1: Table S1). For both the 10,000 and 10 million read samples of each of these metagenomes, 10% of their sequences were selected from each of the ten component genome data sets (i.e., each genome had equal sequence abundance). All sequences were trimmed to remove low quality bases and adapter sequences. The composition of these two metagenomes poses certain challenges to our classifiers. For example, Pelosinus fermentans, found in our HiSeq metagenome, cannot be correctly identified at the genus level by Kraken (or any of the other previously described classifiers), because there are no Pelosinus genomes in the RefSeq complete genomes database; however, there are seven such genomes in Kraken-GB’s database, including six strains of P. fermentans. Similarly, in our MiSeq metagenome, Proteus vulgaris is often classified incorrectly at the genus level because the only Proteus genome in Kraken’s database is a single Proteus mirabilis genome. Five more Proteus genomes are present in Kraken-GB’s database, allowing Kraken-GB to classify reads better from that genus. In addition, the MiSeq metagenome contains five genomes from the Enterobacteriaceae family (Citrobacter, Enterobacter, Klebsiella, Proteus and Salmonella). The high sequence similarity between the genera in this family can make distinguishing between genera difficult for any classifier. The simBA-5 metagenome was created by simulating reads from the set of complete bacterial and archaeal genomes in RefSeq. Replicons from those genomes were used if they were associated with a taxon that had an entry associated with the genus rank, resulting in a set of replicons from 607 genera. We then used the Mason read simulator [22] with its Illumina model to produce 10 million 100-bp reads from these genomes. First we created simulated genomes for each species, using a SNP rate of 0.1% and an indel rate of 0.1% (both default parameters), from which we generated the reads. For the simulated reads, we multiplied the default mismatch and indel rates by five, resulting in an average mismatch rate of 2% (ranging from 1% at the beginning of reads to 6% at the ends) and an indel rate of 1% (0.5% insertion probability and 0.5% deletion probability). For the simBA-5 metagenome, the 10,000 read set was generated from a random sample of the 10 million read set. Evaluation of accuracy and speed We elected to measure accuracy primarily at the genus level, which was the lowest level for which we could easily determine the taxonomy information for PhymmBL and NBC’s predictions in an automated fashion. (This is due to the manner in which PhymmBL and NBC report their results). Because some genomes do not have taxonomic entries at all seven ranks (species, genus, family, order, class, phylum and kingdom), we defined genus-level sensitivity as A/B, where A is the number of reads with an assigned genus that were correctly classified at that rank, and B is the total number of reads with any assigned genus. We defined sensitivity similarly for other taxonomic ranks. Because Kraken may classify a read at levels above the species, measuring its precision requires us to define the effect on precision of assigning the correct genus (for example) while not assigning a species at all. For this reason, we defined rank-level precision as C/(D + E), where C is the number of reads labeled at or below the correct taxon at the measured rank, D is the number of reads labeled at or below the measured rank, and E is the number of reads incorrectly labeled above the measured rank. For example, given a read R that should be labeled as Escherichia coli, a labeling of R as E. coli, E. fergusonii or Escherichia would improve genus-level precision. A label of Enterobacteriaceae (correct family) or Proteobacteria (correct phylum) would have no effect on genus-level precision. A label for R of Bacillus (incorrect genus) or Firmicutes (incorrect phylum) would decrease the genus-level precision. When evaluating PhymmBL’s accuracy, following its developers’ advice [7], we selected a genus confidence threshold for our comparisons. We selected 3,333 reads from the simulated medium complexity (simMC) [23] data set, covering 31 different genera. To simulate short reads from the Sanger sequence data in the simMC set, we selected the last 100 bp from each of the reads. We then ran PhymmBL against those 100-bp reads, and evaluated the genus-level sensitivity and precision of PhymmBL’s predictions with genus confidence thresholds from 0 to 1, in increments of 0.05. We found that a threshold of 0.65 yielded the highest F-score (the harmonic mean of sensitivity and precision), with 0.60 and 0.70 also having F-scores within 0.5 percentage points of the maximum (Additional file 1: Table S2). We therefore used the 0.65 genus confidence threshold in our comparisons. Although the selection of a threshold depends on a user’s individual needs, and so is to some extent arbitrary, a threshold selected in this manner provides a more proper comparison to a selective classifier such as Kraken than no threshold at all. The time and accuracy results when using Megablast as a classifier were obtained from the log data produced by PhymmBL, as PhymmBL uses Megablast for its alignment step. When assigning a taxonomic label to a read with Megablast, we used the taxon associated with the first reported alignment. Megablast was run with default options. Speed was evaluated using the single-threaded operation of each program (except for NBC). PhymmBL was altered so that its call to the blastn program used one thread instead of two. NBC was run with 36 concurrent processes operating on disjoint sets of genomes in its genomic library, and the total time for the classifier was determined by summing the decompression and scoring times for each genome. Wall clock times were recorded for all classifiers. In comparing Kraken to the other classifiers, we used BLAST+ 2.2.27, PhymmBL 4.0, NBC 1.1 and MetaPhlAn 1.7.6. Classifiers were all run on the same computer, with 48 AMD Opteron 6172 2.1 GHz CPUs and 252 GB of RAM, running Red Hat Enterprise Linux 5. The data sets used for speed evaluation had 10,000 reads each for all programs other than Kraken (and its variants) and MetaPhlAn, which used 10,000,000 read data sets. Higher read numbers were used with these faster programs to minimize the effect of the initial and final operations that take place during the programs’ execution. Although Kraken is the only one of the programs we examined that explicitly performs operations to ensure its data is in physical memory before classification, we wanted to be sure that all programs were evaluated in a similar manner. When evaluating speed, for each program, we read all database files (e.g. IMM files and BLAST databases for PhymmBL, k-mer frequency lists for NBC and the Bowtie index for MetaPhlAn) into memory three times before running the program, in order to place the database content in the operating system cache (which is stored in physical memory). Reduced database sizes To generate the 4-GB database for our MiniKraken results, we removed the first 18 of every block of 19 records in the standard Kraken database. A shrinking factor of 19 was selected as it was the smallest integer factor that would reduce the size to less than 4 GB, a size that can easily fit into the memory of many common personal computers. For users that have more RAM available, Kraken allows a smaller shrinking factor to be used, which will give increased sensitivity. Use of draft genomes When constructing the Kraken-GB database, we noticed there were several contigs with known adapter sequences at the ends. In subsequent tests, we also found that some sequences in samples with large amounts of human sequence were consistently misclassified by this database, leading us to conclude that contamination was likely present in the draft genomes. In an attempt to counteract this contamination, we removed from the database those k-mers from known adapter sequences, as well as the first and last 20 k-mers from each of the draft contigs. While this did improve classification, it did not eliminate the misclassification problem. For this reason, we believe that if draft genomes are used in a Kraken database, very stringent measures should be used to remove contaminant sequences from the genomic library. Clade exclusion experiments When re-analyzing the simBA-5 data set for our clade exclusion experiments, some reads were not used for certain pairs of measured and excluded ranks. If a read’s origin lacked a taxonomic entry at either of the measured or excluded ranks, it was not used for that particular experiment. In addition, a read was not used in an experiment unless at least two other taxa represented in our database (aside from the excluded clade) at the excluded rank shared the clade of origin’s taxon at the measured rank. For example, a read from genus G would not be used in an experiment measuring accuracy at the class rank and excluding the genus rank unless G’s home class had at least two other genera with genomes in Kraken’s genomic library. Without this filtering step, were a genus excluded when it was the only genus in its class, Kraken could not possibly name the correct class, as all entries in the database from that class would be excluded as well. This is the same approach taken in similar experiments that were used to evaluate PhymmBL [5]. Human microbiome data classification We classified the Human Microbiome Project data using a Kraken database made from complete RefSeq bacterial, archaeal and viral genomes, along with the GRCh37 human genome. We retrieved the sequences of three accessions (SRS019120, SRS014468 and SRS015055) from the NCBI Sequence Read Archive, and each accession had two runs submitted. All reads were trimmed to remove low quality bases and adapter sequences. Krona [24] was used to generate all taxonomic distribution plots. Because the sequences were all paired reads, we joined the reads together by concatenating the mates with a sequence of ‘NNNNN’ between them. Kraken ignores k-mers with ambiguous nucleotides, so the k-mers that span these ‘N’ characters do not affect classification. This operation allowed Kraken to classify a pair of reads as a single unit rather than having to classify the mates separately.",Classification,"kraken ultrafast metagenomic sequence classification use exact alignments
sequence classification algorithm  classify  dna sequence   collect  kmers within  sequence   set denote     map  kmer   use  algorithm describe    lca taxon   genomes  contain  kmer  lca taxa   ancestors   taxonomy tree form   term  classification tree  prune subtree   use  classify   node   classification tree  weight   number  kmers    map   taxon associate   node   roottoleaf rtl path   classification tree  score  calculate  sum   node weight along  path  maximum score rtl path   classification tree   classification path    assign  label correspond   leaf    multiple maximally score paths  lca    paths leave  select  algorithm illustrate  figure  allow kraken  consider  kmer within  sequence   separate piece  evidence   attempt  resolve  conflict evidence  necessary note    appropriate choice    kmers  map uniquely   single species greatly simplify  classification process sequence   none   kmers    find   genome  leave unclassified   algorithm  use  rtl path score   classification tree  necessary  light   inevitable differences   sequence   classify   sequence present   library  genomes  differences  even  large value   result   kmer   present   library  associate   species far remove   true source species  score  various rtl paths   classification tree   compensate   differences  correctly classify sequence even   small minority  kmers   sequence indicate   sequence   assign  incorrect taxonomic label database creation efficient implementation  krakens classification algorithm require   map  kmers  taxa  perform  query  precomputed database kraken create  database   multistep process begin   selection   library  genomic sequence kraken include  default library base  complete microbial genomes   national center  biotechnology informations ncbi refseq database   library   customize  need  individual users    library  choose  use  jellyfish multithreaded kmer counter   create  database contain every distinct mer   library   database  complete  byte space jellyfish use  store  kmer count   database file  instead use  kraken  store  taxonomic  number   kmers lca value   database   create  jellyfish  genomic sequence   library  process one   time   sequence  taxon associate    use  set  store lca value   kmers   sequence  sequence  process   kmer   sequence    lca value previously set   lca   store value   current sequence taxon  calculate   lca  store   kmer taxon information  obtain   ncbi taxonomy database database structure  search algorithm  kraken  frequently use  kmer   database query immediately  query  adjacent kmer   adjacent kmers share  substantial amount  sequence  utilize  minimizer concept   group similar kmers together  explain  application   concept   define  canonical representation   dna sequence    lexicographically smaller     reverse complement    determine  kmers minimizer  length   consider  canonical representation   mmers   kmer  select  lexicographically smallest   mmers   kmers minimizer  practice adjacent kmers  often    minimizer  krakens database  kmers    minimizer  store consecutively   sort  lexicographical order   canonical representations  query   kmer     process  look    index  position   database   kmers   minimizer would  store   perform  binary search within  region figure   adjacent kmers often    minimizer  search range  often    two consecutive query   search   first query  often bring data   cpu cache    use   second query  allow memory access  subsequent query  access data   cpu cache instead  ram  strategy make subsequent query much faster   would otherwise   index contain  offset   group  kmers   database require    bytes  default kraken use  minimizers   user  modify  value  example  create minikraken  use  minimizers  ensure  total database size stay     implement kraken  make  optimizations   structure  search algorithm describe  first  note  roberts     simple lexicographical order  mmers  result   skew distribution  minimizers  overrepresents lowcomplexity mmers  kraken   bias would create many large search range  would require  time  search  create   even distribution  minimizers  thus speed  search  use  exclusiveor xor operation  toggle half   bits   mmers canonical representation prior  compare  mmers    use lexicographical order  xor operation effectively scramble  standard order  prevent  large bias toward lowcomplexity minimizers  also take advantage   fact   search range  often    query  make krakens query faster rather  compute  minimizer  time  perform  query  first search  previous range   queried kmer  find   range  query  return immediately   kmer   find   minimizer  compute   kmers minimizer      last queried kmers   query fail   minimizers search space   show     kmer    minimizer  change  kraken   adjust  search range  search    kmer construct simulate metagenomes  hiseq  miseq metagenomes  build use  set  bacterial wholegenome shotgun read  read  find either  part   gageb project     ncbi sequence read archive  metagenome contain sequence  ten genomes additional file  table        million read sample     metagenomes    sequence  select     ten component genome data set   genome  equal sequence abundance  sequence  trim  remove low quality base  adapter sequence  composition   two metagenomes pose certain challenge   classifiers  example pelosinus fermentans find   hiseq metagenome cannot  correctly identify   genus level  kraken      previously describe classifiers     pelosinus genomes   refseq complete genomes database however   seven  genomes  krakengbs database include six strain   fermentans similarly   miseq metagenome proteus vulgaris  often classify incorrectly   genus level    proteus genome  krakens database   single proteus mirabilis genome five  proteus genomes  present  krakengbs database allow krakengb  classify read better   genus  addition  miseq metagenome contain five genomes   enterobacteriaceae family citrobacter enterobacter klebsiella proteus  salmonella  high sequence similarity   genera   family  make distinguish  genera difficult   classifier  simba metagenome  create  simulate read   set  complete bacterial  archaeal genomes  refseq replicons   genomes  use    associate   taxon    entry associate   genus rank result   set  replicons   genera   use  mason read simulator    illumina model  produce  million  read   genomes first  create simulate genomes   species use  snp rate     indel rate    default parameters    generate  read   simulate read  multiply  default mismatch  indel rat  five result   average mismatch rate   range     begin  read     end   indel rate    insertion probability   deletion probability   simba metagenome   read set  generate   random sample    million read set evaluation  accuracy  speed  elect  measure accuracy primarily   genus level    lowest level    could easily determine  taxonomy information  phymmbl  nbcs predictions   automate fashion   due   manner   phymmbl  nbc report  result   genomes    taxonomic entries   seven rank species genus family order class phylum  kingdom  define genuslevel sensitivity       number  read   assign genus   correctly classify   rank     total number  read   assign genus  define sensitivity similarly   taxonomic rank  kraken may classify  read  level   species measure  precision require   define  effect  precision  assign  correct genus  example   assign  species     reason  define ranklevel precision         number  read label     correct taxon   measure rank    number  read label     measure rank     number  read incorrectly label   measure rank  example give  read     label  escherichia coli  label     coli  fergusonii  escherichia would improve genuslevel precision  label  enterobacteriaceae correct family  proteobacteria correct phylum would   effect  genuslevel precision  label    bacillus incorrect genus  firmicutes incorrect phylum would decrease  genuslevel precision  evaluate phymmbls accuracy follow  developers advice   select  genus confidence threshold   comparisons  select  read   simulate medium complexity simmc  data set cover  different genera  simulate short read   sanger sequence data   simmc set  select  last       read   run phymmbl    read  evaluate  genuslevel sensitivity  precision  phymmbls predictions  genus confidence thresholds      increments    find   threshold   yield  highest fscore  harmonic mean  sensitivity  precision     also  fscores within  percentage point   maximum additional file  table   therefore use   genus confidence threshold   comparisons although  selection   threshold depend   users individual need      extent arbitrary  threshold select   manner provide   proper comparison   selective classifier   kraken   threshold    time  accuracy result  use megablast   classifier  obtain   log data produce  phymmbl  phymmbl use megablast   alignment step  assign  taxonomic label   read  megablast  use  taxon associate   first report alignment megablast  run  default options speed  evaluate use  singlethreaded operation   program except  nbc phymmbl  alter    call   blastn program use one thread instead  two nbc  run   concurrent process operate  disjoint set  genomes   genomic library   total time   classifier  determine  sum  decompression  score time   genome wall clock time  record   classifiers  compare kraken    classifiers  use blast  phymmbl  nbc   metaphlan  classifiers   run    computer   amd opteron   ghz cpus     ram run red hat enterprise linux   data set use  speed evaluation   read    program   kraken   variants  metaphlan  use  read data set higher read number  use   faster program  minimize  effect   initial  final operations  take place   program execution although kraken    one   program  examine  explicitly perform operations  ensure  data   physical memory  classification  want   sure   program  evaluate   similar manner  evaluate speed   program  read  database file  imm file  blast databases  phymmbl kmer frequency list  nbc   bowtie index  metaphlan  memory three time  run  program  order  place  database content   operate system cache   store  physical memory reduce database size  generate   database   minikraken result  remove  first   every block   record   standard kraken database  shrink factor    select     smallest integer factor  would reduce  size  less     size   easily fit   memory  many common personal computers  users    ram available kraken allow  smaller shrink factor   use   give increase sensitivity use  draft genomes  construct  krakengb database  notice   several contigs  know adapter sequence   end  subsequent test  also find   sequence  sample  large amount  human sequence  consistently misclassified   database lead   conclude  contamination  likely present   draft genomes   attempt  counteract  contamination  remove   database  kmers  know adapter sequence  well   first  last  kmers     draft contigs    improve classification    eliminate  misclassification problem   reason  believe   draft genomes  use   kraken database  stringent measure   use  remove contaminant sequence   genomic library clade exclusion experiment  reanalyzing  simba data set   clade exclusion experiment  read   use  certain pair  measure  exclude rank   read origin lack  taxonomic entry  either   measure  exclude rank    use   particular experiment  addition  read   use   experiment unless  least two  taxa represent   database aside   exclude clade   exclude rank share  clade  origins taxon   measure rank  example  read  genus  would   use   experiment measure accuracy   class rank  exclude  genus rank unless  home class   least two  genera  genomes  krakens genomic library without  filter step   genus exclude      genus   class kraken could  possibly name  correct class   entries   database   class would  exclude  well     approach take  similar experiment   use  evaluate phymmbl  human microbiome data classification  classify  human microbiome project data use  kraken database make  complete refseq bacterial archaeal  viral genomes along   grch37 human genome  retrieve  sequence  three accession srs019120 srs014468  srs015055   ncbi sequence read archive   accession  two run submit  read  trim  remove low quality base  adapter sequence krona   use  generate  taxonomic distribution plot   sequence   pair read  join  read together  concatenate  mat   sequence  nnnnn   kraken ignores kmers  ambiguous nucleotides   kmers  span   character   affect classification  operation allow kraken  classify  pair  read   single unit rather    classify  mat separately",1
26,KSlam,"k-SLAM: accurate and ultra-fast taxonomic classification and gene identification for large metagenomic data sets
Algorithm outline k-SLAM is a metagenomic classifier that uses a sequence alignment method to infer taxonomy and identify genes. Reads and database genomes are split into short (k = 32) k-mers which are added to a list and sorted so that identical k-mers are placed next to one another. Iteration through the list allows k base overlaps between reads and genomes to be found, along with alignment position. The overlaps are then verified with a full Smith-Waterman pairwise sequence alignment. Neighboring alignments are chained together along each genome into a ‘pseudo-assembly’, this allows reads that map to low complexity and conserved regions to still be classified precisely as the chains often extend into unique sequence. Low scoring alignments are screened and taxonomy is inferred from the lowest common taxonomic ancestor of the valid overlaps. Alignments are also used to infer genes and variants. The sorted-list method of finding k-mer overlaps allows great speed and efficient parallelisation on modern hardware. k-mer based alignment For the following analysis assume: k is an integer chosen at compile time (default k = 32) and a k-mer is a sequence of k nucleotides. Each read is split up into overlapping k-mers (k − 1 base overlap) and the k-mers are added to a list. Each genome is split into non-overlapping k-mers (to save memory) and the k-mers are added to the same list. The list is sorted lexicographically, placing identical k-mers next to one another. The list is iterated over, finding overlaps between reads and genomes. For each of the overlaps found, a full Smith-Waterman pairwise local sequence alignment (using Mengyao Zhao's SIMD Smith-Waterman implementation (17)) is performed to ensure the overlap is valid and to find any variants. Alignments with a score lower than a user-chosen cutoff are screened (see Supplementary Data: Supplementary Data for a graph of sensitivity and specificity for various score cutoffs). k-mers are stored along with their offsets from the start of the sequence, the identifier of the sequence from which they were extracted and a flag that is set if the k-mer has been reverse-complemented. In order to find overlaps on both strands, k-SLAM compares each k-mer with its reverse complement and stores only the lexicographically smallest to save memory. A similar k-mer based method using lexicographic sorting and spaced k-mer seeds, albeit for protein alignments, was independently discovered and used in DIAMOND (18). Paired-end reads As read length is very important for taxonomic specificity, k-SLAM is designed to work with paired end reads of any insert size. Paired reads are treated initially as two single reads, which have their overlaps and alignments found using the above k-mer method. As bacterial sequence is often repetitive, it is highly likely that each end of the paired read aligns to multiple places on the same genome, hence a method is needed for detecting which pairings of these alignments are valid. For each read/genome pair, all of the alignments are sorted by offset from the start of the genome. The algorithm then makes alignment pairs from each R1's nearest neighbor R2s and vice-versa. This allows only a small subset of pairs to be considered instead of working with all possible pairs (avoiding N2 scaling). The library insert size is then inferred using a statistical method; insert sizes in the range of 0 ≤ I ≤ Q3 + 2(Q3 − Q1) are used to calculate a mean and standard deviation and all pairs with insert sizes I ≥ μ + 6σ are screened. Sequencing technologies k-SLAM is designed to work with data from all of the most used sequencing technologies. There are, however, some constraints on the reads that affect accuracy. The reads need to be longer than the length of the k-mer and have a sufficiently low error rate such that there is at least one error free k-mer in each read. This allows a k base overlap to be found that can then be verified with a full Smith–Waterman alignment. Longer reads will produce more alignments and greater taxonomic specificity. Taxonomic specificity is also improved by using paired-end reads. A lower error rate allows a longer k-mer and hence a shorter compute time. k-SLAM has been tested and found to be accurate on Illumina HiSeq and MiSeq platforms as well as 454 and Ion Torrent data. Pseudo-assembly Due to the similarity between the genomes of different bacterial species, there is a large probability that each read will map to more than one genome, this makes inferring taxonomy difficult as reads often map to long sections of conserved sequence. k-SLAM attempts to solve this problem by grouping reads that map to adjacent locations on the same genome together into ‘pseudo-assemblies’. A new alignment score is calculated for each chain, taking into account per base similarity, chain length and depth of coverage. These long chains of reads often extend beyond conserved sections and into regions specific to one particular strain. This allows all reads within the chain to be assigned to the lowest possible taxonomic level. Following is a description of the k-SLAM pseudo-assembly algorithm as applied to each genome: For each genome, sort the alignments by start position. Form chains of alignments that overlap by more than 20 bases. For each chain, calculate the following parameters: L=be−bs where L is the chain length (in nucleotides), bs and be are the positions of the first and final nucleotides, respectively. C=Nb/L where C is the coverage and Nb is the sum of the number of bases in each read. μs=ΣsNb where μs is mean score per nucleotide and s is the Smith–Waterman score for each alignment. S=CμsL where S is the chain's score which is applied to all of the alignments in the chain. Inferring taxonomy k-SLAM infers taxonomy using a lowest common ancestor technique similar to that in the Huson et al. program MEGAN (19). For each read, a score cutoff is calculated by multiplying the highest alignment score by a user chosen constant and all alignments below this cutoff are screened (see Supplementary Data: Supplementary Data for a graph of sensitivity and specificity for various fractional score cutoffs). Taxonomy is chosen based on the lowest common ancestor in the taxonomy tree of the remaining alignments. A matching gene is also inferred for each read from the position of the alignment along the genome. Inferring genes Genes are inferred using the GenBank format annotations. For each non-screened alignment, the gene with the most overlapping bases is chosen. For the XML output, genes with identical names, protein IDs or products that are assigned to the same taxonomy are combined into a single entry with an associated count. Synthetic metagenomes In order to calculate the taxonomic classification accuracy, a data set is required for which every read's taxonomic origin is known. This is impossible for real environmental metagenomes so the standard procedure is to produce these data sets in silico (20, 21). The first testing data set was entirely artificial, using paired-end 150 bp reads generated from 100 randomly selected NCBI genomes with an error profile five times greater than would be expected from Illumina reads (this error profile was taken from the Kraken (10) paper). This data set, which has all species in equal proportions, was designed so that it can be seen how the accuracy of each classifier changes for different species. The second data set was taken from Metabenchmark (22), an effort to evaluate the accuracy and speed of metagenome analysis tools. The data set used was >50 million reads and designed to accurately mimic the complexity, size and characteristics of real data. The researchers created the data sets by sampling read pairs from sequenced genomes in well defined proportions with error profiles added from real HiSeq data.",Classification,"kslam accurate  ultrafast taxonomic classification  gene identification  large metagenomic data sets
algorithm outline kslam   metagenomic classifier  use  sequence alignment method  infer taxonomy  identify genes read  database genomes  split  short    kmers   add   list  sort   identical kmers  place next  one another iteration   list allow  base overlap  read  genomes   find along  alignment position  overlap   verify   full smithwaterman pairwise sequence alignment neighbor alignments  chain together along  genome   pseudoassembly  allow read  map  low complexity  conserve regions  still  classify precisely   chain often extend  unique sequence low score alignments  screen  taxonomy  infer   lowest common taxonomic ancestor   valid overlap alignments  also use  infer genes  variants  sortedlist method  find kmer overlap allow great speed  efficient parallelisation  modern hardware kmer base alignment   follow analysis assume    integer choose  compile time default      kmer   sequence   nucleotides  read  split   overlap kmers    base overlap   kmers  add   list  genome  split  nonoverlapping kmers  save memory   kmers  add    list  list  sort lexicographically place identical kmers next  one another  list  iterate  find overlap  read  genomes     overlap find  full smithwaterman pairwise local sequence alignment use mengyao zhao' simd smithwaterman implementation   perform  ensure  overlap  valid   find  variants alignments   score lower   userchosen cutoff  screen see supplementary data supplementary data   graph  sensitivity  specificity  various score cutoffs kmers  store along   offset   start   sequence  identifier   sequence     extract   flag   set   kmer   reversecomplemented  order  find overlap   strand kslam compare  kmer   reverse complement  store   lexicographically smallest  save memory  similar kmer base method use lexicographic sort  space kmer seed albeit  protein alignments  independently discover  use  diamond  pairedend read  read length   important  taxonomic specificity kslam  design  work  pair end read   insert size pair read  treat initially  two single read    overlap  alignments find use   kmer method  bacterial sequence  often repetitive   highly likely   end   pair read align  multiple place    genome hence  method  need  detect  pair   alignments  valid   readgenome pair    alignments  sort  offset   start   genome  algorithm  make alignment pair   ' nearest neighbor r2s  viceversa  allow   small subset  pair   consider instead  work   possible pair avoid  scale  library insert size   infer use  statistical method insert size   range   ≤  ≤       use  calculate  mean  standard deviation   pair  insert size  ≥     screen sequence technologies kslam  design  work  data      use sequence technologies   however  constraints   read  affect accuracy  read need   longer   length   kmer    sufficiently low error rate      least one error free kmer   read  allow   base overlap   find     verify   full smithwaterman alignment longer read  produce  alignments  greater taxonomic specificity taxonomic specificity  also improve  use pairedend read  lower error rate allow  longer kmer  hence  shorter compute time kslam   test  find   accurate  illumina hiseq  miseq platforms  well    ion torrent data pseudoassembly due   similarity   genomes  different bacterial species    large probability   read  map    one genome  make infer taxonomy difficult  read often map  long section  conserve sequence kslam attempt  solve  problem  group read  map  adjacent locations    genome together  pseudoassemblies  new alignment score  calculate   chain take  account per base similarity chain length  depth  coverage  long chain  read often extend beyond conserve section   regions specific  one particular strain  allow  read within  chain   assign   lowest possible taxonomic level follow   description   kslam pseudoassembly algorithm  apply   genome   genome sort  alignments  start position form chain  alignments  overlap     base   chain calculate  follow parameters lbebs     chain length  nucleotides      position   first  final nucleotides respectively cnbl     coverage     sum   number  base   read μsσsnb    mean score per nucleotide     smithwaterman score   alignment scμsl     chain' score   apply     alignments   chain infer taxonomy kslam infer taxonomy use  lowest common ancestor technique similar     huson   program megan    read  score cutoff  calculate  multiply  highest alignment score   user choose constant   alignments   cutoff  screen see supplementary data supplementary data   graph  sensitivity  specificity  various fractional score cutoffs taxonomy  choose base   lowest common ancestor   taxonomy tree   remain alignments  match gene  also infer   read   position   alignment along  genome infer genes genes  infer use  genbank format annotations   nonscreened alignment  gene    overlap base  choose   xml output genes  identical name protein ids  products   assign    taxonomy  combine   single entry   associate count synthetic metagenomes  order  calculate  taxonomic classification accuracy  data set  require   every read' taxonomic origin  know   impossible  real environmental metagenomes   standard procedure   produce  data set  silico    first test data set  entirely artificial use pairedend   read generate   randomly select ncbi genomes   error profile five time greater  would  expect  illumina read  error profile  take   kraken  paper  data set    species  equal proportion  design      see   accuracy   classifier change  different species  second data set  take  metabenchmark   effort  evaluate  accuracy  speed  metagenome analysis tool  data set use   million read  design  accurately mimic  complexity size  characteristics  real data  researchers create  data set  sample read pair  sequence genomes  well define proportion  error profile add  real hiseq data",1
27,Centrifuge,"Centrifuge: rapid and sensitive classification of metagenomic sequences
Database sequence compression We implemented memory-efficient indexing schemes for the classification of microbial sequences based on the FM-index, which also permits very fast search operations. We further reduced the size of the index by compressing genomic sequences and building a modified version of the FM-index for those compressed genomes, as follows. First, we observed that for some bacterial species, large numbers of closely related strains and isolates have been sequenced, usually because they represent significant human pathogens. Such genomes include Salmonella enterica with 138 genomes, Escherichia coli with 131 genomes, and Helicobacter pylori with 73 genomes available (these figures represent the contents of RefSeq as of December 2015). As expected, the genomic sequences of strains within the same species are likely to be highly similar to one another. We leveraged this fact to remove such redundant genomic sequences, so that the storage size of our index can remain compact even as the number of sequenced isolates for these species increases. Figure 1 illustrates how we compress multiple genomes of the same species by storing near-identical sequences only once. First, we choose the two genomes (G1 and G2 in the figure) that are most similar among all genomes. We define the two most similar genomes as those that share the greatest number of k-mers (using k = 53 for this study) after k-mers are randomly sampled at a rate of 1% from the genomes of the same species. In order to facilitate this selection process, we used Jellyfish (Marcais and Kingsford 2011) to build a table indicating which k-mers belong to which genomes. Using the two most similar genomes allows for better compression as they tend to share larger chunks of genomic sequences than two randomly selected genomes. We then compared the two most similar genomes using nucmer (Kurtz et al. 2004), which outputs a list of the nearly or completely identical regions in both genomes. When combining the two genomes, we discard those sequences of G2 with ≥99% identity to G1 and retain the remaining sequences to use in our index. We then find the genome that is most similar to the combined sequences from G1 and G2 and combine this in the same manner as just described. This process is repeated for the rest of the genomes. As a result of this concatenation procedure, we obtained dramatic space reductions for many species; e.g., the total sequence was reduced from 661 to 74 Mbp (11% of the original sequence size) in S. enterica and from 655 to 107 Mbp (16%) in E. coli (see Table 1). Overall, the number of base pairs from ∼4300 bacterial and archaeal genomes was reduced from 15 to 9.1 billion base pairs (Gbp). The FM-index for these compressed sequences occupies 4.2 GB of memory, which is small enough to fit into the main memory (RAM) on a conventional desktop computer. As we demonstrate in the Supplemental Methods and Supplemental Table S1, this compression operation has only a negligible impact on classification sensitivity and accuracy. Classification based on the FM-index The FM-index provides several advantages over k-mer-based indexing schemes that store all k-mers in the target genomes. First, the size of the k-mer table is usually large; for example, Kraken's k-mer table for storing all 31-mers in ∼4300 prokaryotic genomes occupies ∼100 GB of disk space. Second, using a fixed value for k incurs a tradeoff between sensitivity and precision: Classification based on exact matches of large k-mers (e.g., 31 bp) provides higher precision but at the expense of lower sensitivity, especially when the data being analyzed originate from divergent species. To achieve higher sensitivity, smaller k-mer matches (e.g., 20–25 bp) can be used; however, this results in more false-positive matches. The FM-index provides a means to exploit both large and small k-mer matches by enabling rapid search of k-mers of any length, at speeds comparable to those of k-mer table indexing algorithms (see Results). Using this FM-index, Centrifuge classifies DNA sequences as follows. Suppose we are given a 100-bp read (note the Centrifuge can just as easily process very long reads, assembled contigs from a draft genome, or even entire chromosomes). We search both the read (forward) and its reverse complement from right to left (3′ to 5′) as illustrated in Figure 2A. Centrifuge begins with a short exact match (16-bp minimum) and extends the match as far as possible. In the example shown in Figure 2A, the first 40 bp match exactly, with a mismatch at the 41st base from the right. The rightmost 40-bp segment of the read is found in six species (A, B, C, D, E, and F) that had been stored in the Centrifuge database. The algorithm then resumes the search beginning at the 42nd base and stops at the next mismatch, which occurs at the 68th base. The 26-bp segment in the middle of the read is found in species G and H. We then continue to search for mappings in the rest of the read, identifying a 32-bp segment that matches species G. Note that only exact matches are considered throughout this process, which is a key factor in the speed of the algorithm. We perform the same procedure for the reverse complement of the read which, in this example, produces more mappings with smaller lengths (17, 16, 28, 18, and 17) compared to the forward strand. Based on the exact matches found in the read and its reverse complement, Centrifuge then classifies the read using only those mappings with at least one 22-bp match. Figure 2A shows three segment mappings on the forward strand read and one on the read's reverse complement that meet this length threshold. Centrifuge then scores each species using the following formula, which assigns greater weight to the longer segments After assessing a variety of formulas, we empirically found that the sum of squared lengths of segments provides the best classification precision. Because almost all sequences of 15 bp or shorter occur in the database by chance, we subtract 15 from the match length. Other values such as 0 and 7 bp work almost as well, while higher values such as 21 bp result in slightly lower precision and sensitivity. For the example in Figure 2, species A, B, C, D, E, and F are assigned the highest score (625), based on the relatively long 40-bp exact match. Species G and H get lower scores because they have considerably shorter matches, even though each has two distinct matches. Note that H has mappings on both the read and its reverse complement, and in this case, Centrifuge chooses the strand that gives the maximum score, rather than using the summed score on both strands, which might bias it toward palindromic sequences. Centrifuge can assign a sequence to multiple taxonomic categories; by default, it allows up to five labels per sequence. (Note that this strategy differs from Kraken, which always chooses a single taxonomic category, using the lowest common ancestor of all matching species.) In Figure 2, six different species match the read equally well. In order to reduce the number of assignments, Centrifuge traverses up the taxonomic tree. First, it considers the genus that includes the largest number of species, which, in this example (Fig. 2B), is genus I, which covers species A, B, and C. It then replaces these three species with the genus, thereby reducing the number of assignments to four (genus I plus species D, E, and F). If more than five taxonomic labels had remained, Centrifuge would repeat this process for other genera and subsequently for higher taxonomic units until it reduced the number of labels to five or fewer. The user can easily change the default threshold of five labels per sequence; for example, if this threshold is set to one, then Centrifuge will report only the lowest common ancestor as the taxonomic label, mimicking the behavior of Kraken. In the example shown in Figure 2, this label would be at the family level, which would lose some of the more specific information about which genera and species the reads matched best. If the size of the index is not a constraint, then the user can also use Centrifuge with uncompressed indexes, which classify reads using the same algorithm. Although considerably larger, the uncompressed indexes allow Centrifuge to classify reads at the strain or genome level; e.g., as E. coli K12 rather than just E. coli. Abundance analysis In addition to per-read classification, Centrifuge performs abundance analysis at any taxonomic rank (e.g., strain, species, genus). Because many genomes share near-identical segments of DNA with other species, reads originating from those segments will be classified as multiple species. Simply counting the number of the reads that are uniquely classified as a given genome (ignoring those that match other genomes) will therefore give poor estimates of that species’ abundance. To address this problem, we define the following statistical model and use it to find maximum likelihood estimates of abundance through an Expectation-Maximization (EM) algorithm. Detailed EM solutions to the model have been previously described and implemented in the Cufflinks (Trapnell et al. 2010) and Sailfish (Patro et al. 2014) software packages.",Classification,"centrifuge rapid  sensitive classification  metagenomic sequences
database sequence compression  implement memoryefficient index scheme   classification  microbial sequence base   fmindex  also permit  fast search operations   reduce  size   index  compress genomic sequence  build  modify version   fmindex   compress genomes  follow first  observe    bacterial species large number  closely relate strain  isolate   sequence usually   represent significant human pathogens  genomes include salmonella enterica   genomes escherichia coli   genomes  helicobacter pylori   genomes available  figure represent  content  refseq   december   expect  genomic sequence  strain within   species  likely   highly similar  one another  leverage  fact  remove  redundant genomic sequence    storage size   index  remain compact even   number  sequence isolate   species increase figure  illustrates   compress multiple genomes    species  store nearidentical sequence   first  choose  two genomes      figure    similar among  genomes  define  two  similar genomes    share  greatest number  kmers using      study  kmers  randomly sample   rate     genomes    species  order  facilitate  selection process  use jellyfish marcais  kingsford   build  table indicate  kmers belong   genomes use  two  similar genomes allow  better compression   tend  share larger chunk  genomic sequence  two randomly select genomes   compare  two  similar genomes use nucmer kurtz     output  list   nearly  completely identical regions   genomes  combine  two genomes  discard  sequence    ≥ identity    retain  remain sequence  use   index   find  genome    similar   combine sequence      combine     manner   describe  process  repeat   rest   genomes   result   concatenation procedure  obtain dramatic space reductions  many species   total sequence  reduce     mbp    original sequence size   enterica      mbp    coli see table  overall  number  base pair   bacterial  archaeal genomes  reduce     billion base pair gbp  fmindex   compress sequence occupy    memory   small enough  fit   main memory ram   conventional desktop computer   demonstrate   supplemental methods  supplemental table   compression operation    negligible impact  classification sensitivity  accuracy classification base   fmindex  fmindex provide several advantage  kmerbased index scheme  store  kmers   target genomes first  size   kmer table  usually large  example kraken' kmer table  store  mers   prokaryotic genomes occupy    disk space second use  fix value   incurs  tradeoff  sensitivity  precision classification base  exact match  large kmers    provide higher precision    expense  lower sensitivity especially   data  analyze originate  divergent species  achieve higher sensitivity smaller kmer match      use however  result   falsepositive match  fmindex provide  mean  exploit  large  small kmer match  enable rapid search  kmers   length  speed comparable    kmer table index algorithms see result use  fmindex centrifuge classify dna sequence  follow suppose   give   read note  centrifuge    easily process  long read assemble contigs   draft genome  even entire chromosomes  search   read forward   reverse complement  right  leave ′  ′  illustrate  figure  centrifuge begin   short exact match  minimum  extend  match  far  possible   example show  figure   first   match exactly   mismatch   41st base   right  rightmost  segment   read  find  six species           store   centrifuge database  algorithm  resume  search begin   42nd base  stop   next mismatch  occur   68th base   segment   middle   read  find  species      continue  search  mappings   rest   read identify   segment  match species  note   exact match  consider throughout  process    key factor   speed   algorithm  perform   procedure   reverse complement   read    example produce  mappings  smaller lengths       compare   forward strand base   exact match find   read   reverse complement centrifuge  classify  read use   mappings   least one  match figure  show three segment mappings   forward strand read  one   read' reverse complement  meet  length threshold centrifuge  score  species use  follow formula  assign greater weight   longer segment  assess  variety  formulas  empirically find   sum  square lengths  segment provide  best classification precision  almost  sequence     shorter occur   database  chance  subtract    match length  value       work almost  well  higher value     result  slightly lower precision  sensitivity   example  figure  species         assign  highest score  base   relatively long  exact match species    get lower score    considerably shorter match even though   two distinct match note    mappings    read   reverse complement    case centrifuge choose  strand  give  maximum score rather  use  sum score   strand  might bias  toward palindromic sequence centrifuge  assign  sequence  multiple taxonomic categories  default  allow   five label per sequence note   strategy differ  kraken  always choose  single taxonomic category use  lowest common ancestor   match species  figure  six different species match  read equally well  order  reduce  number  assignments centrifuge traverse   taxonomic tree first  consider  genus  include  largest number  species    example fig   genus   cover species       replace  three species   genus thereby reduce  number  assignments  four genus  plus species        five taxonomic label  remain centrifuge would repeat  process   genera  subsequently  higher taxonomic units   reduce  number  label  five  fewer  user  easily change  default threshold  five label per sequence  example   threshold  set  one  centrifuge  report   lowest common ancestor   taxonomic label mimic  behavior  kraken   example show  figure   label would    family level  would lose     specific information   genera  species  read match best   size   index    constraint   user  also use centrifuge  uncompress index  classify read use   algorithm although considerably larger  uncompress index allow centrifuge  classify read   strain  genome level    coli k12 rather    coli abundance analysis  addition  perread classification centrifuge perform abundance analysis   taxonomic rank  strain species genus  many genomes share nearidentical segment  dna   species read originate   segment   classify  multiple species simply count  number   read   uniquely classify   give genome ignore   match  genomes  therefore give poor estimate   species abundance  address  problem  define  follow statistical model  use   find maximum likelihood estimate  abundance   expectationmaximization  algorithm detail  solutions   model   previously describe  implement   cufflinks trapnell     sailfish patro    software package",1
28,Clark,"CLARK: fast and accurate classification of metagenomic and genomic sequences using discriminative k-mers
Building target-specific k-mer sets CLARK accepts inputs in fasta/fastq format; alternatively the input can be given as a text file containing the k-mer distribution (i.e., each line contains a k-mer and its number of occurrences). CLARK first builds an index from the target sequences, unless one already exists for the specified input files. If a user wants to classify objects at the genus level (or another taxonomic rank), he/she is expected to generate targets by grouping genomes of the same genus (or with the same taxonomic label). This strategy represents a major difference with other tools (such as LMAT, or KRAKEN). The index is a hash-table storing, for each distinct k-mer w (1) the ID for the target containing w, (2) the number of distinct targets containing w, and (3) the number of occurrences of w in all the targets. This hash-table uses separate chaining to resolve collisions (at each bucket). CLARK then removes any k-mer that appears in more than one target, except in the case of chromosome arm assignment. In the latter case, k-mers shared by the two arms of the same chromosome are used to define centromeric regions of overlap. Also, k-mers in the index may be removed based on their number of occurrences if the user has specified a minimum number of occurrences. These rare k-mers tend to be spurious from sequencing errors. Other metagenomic classifiers like KRAKEN and LMAT do not offer this protection against noise, which is very useful when target sequences are reads (or low-quality assemblies). Then, the resulting sets of target-specific k-mers are stored in disk for the next phase. The time and memory needed to create the index (for k=31) are given in Additional file 1: Table S1. This table also contains the time and memory required by NBC and KRAKEN. Observe that CLARK is faster than NBC and KRAKEN to create the index, and it uses less RAM and disk space than KRAKEN for classifying objects. The concept of “target-specific k-mers” is similar to the notion of “clade-specific marker genes” proposed in [7] or “genome-specific markers” recently proposed in [30]. While CLARK uses exact matching to identify the target-specific k-mers derived from any region in the genome, the authors in [7] disregard intergenic regions. The authors of [30] focus on strain-specific markers identified by approximate string matching, while CLARK uses exact matching. Another important difference is that the method presented in [30] relies on MEGABLAST [31] to perform the classification, which is several orders of magnitude slower than KRAKEN [11]. For users that want to run CLARK on workstations with limited amounts of RAM, we have designed CLARK-l (“light”). CLARK-l is a variant of CLARK that has a much smaller RAM footprint but can classify objects with similar speed and accuracy. The reduction in RAM can be achieved by constructing a hash-table of smaller size and by constructing smaller sets of discriminative k-mers. Instead of considering all k-mers in a target, CLARK-l samples a fraction of them. CLARK-l uses 27-mers (27-mers appeared to be a good tradeoff between speed, low memory usage and precision) and skips four consecutive/non-overlapping 27-mers. As a result, CLARK-l’s peak RAM usage is about 3.8 GB during the index creation, and 2.8 GB when computing the classification (see Additional file 1: Table S1). CLARK-l has also the advantage to be very fast in building the hash table. Table 1 includes the performance of CLARK-l. While the precision and sensitivity are lower compared to CLARK, CLARK-l still achieves high precision and high speed. Sequence classification In the full mode, once the index containing target-specific k-mers has been created, CLARK creates a “dictionary” that associates k-mers to targets. Then, CLARK iteratively processes each object: for each object sequence o CLARK queries the index to fetch the set of k-mers in o. A “hit” is obtained when a k-mer (either forward or reverse complement) matches a target-specific k-mer set. Object o is assigned to the target that has the highest number of hits (see algorithmic details in Additional file 1: Supplementary Note 1 and Additional file 1: Table S5). The confidence score is computed as h 1/(h 1+h 2), where h 1 is the number of hits for the highest target, and h 2 is the number of hits for the second-highest target. The rationale to remove common k-mers between targets (at any taxonomy level defined by the user) is that they increase the “noise” in the classification process. If they were present, more targets could obtain the same number of hits which would complicate the assignment. If such conflicts can be avoided, then there is no need to query the taxonomy tree, and find, for example, the lowest common ancestor taxons for “conflicting nodes” to resolve them as it is done in other tools (e.g., KRAKEN or LMAT). Observe in Additional file 1: Figure S1, that most of CLARK’s assignments have high confidence scores. Observe that at least 95% of all assignments in HiSeq, MiSeq, simBA-5 and simHC.20.500 made by CLARK in the full mode, have confidence scores equal to 1 (i.e., exactly one target gets hits), and the average confidence scores in all these assignments is 0.997. This implies that, on average, the number of hits for the top target (which will receive the assignment) is about 336 times higher than the second. Thus, CLARK, unlike LMAT or KRAKEN, does not need the taxonomy tree to classify objects, instead one “flat” level is clearly sufficient. If users are not interested in collecting confidence scores and all hit counts, then it is recommended to use the default mode of CLARK. In this mode, CLARK stops querying k-mers for an object as soon as there is at least one target that collects at least half of the total possible hits. Also, this mode loads in main memory about half of the target-specific k-mers. This is done by alternatively loading or skipping target-specific k-mers based on their index positions. CLARK runs significantly faster in default mode (2–5 times faster in our experiments) with negligible degradation of sensitivity and assignment rate. Also, the RAM usage is significantly lower than the full mode (up to 50% lower in our experiments). If speed is the primary concern, we have designed an “express” variant of CLARK called CLARK-E. CLARK-E is based upon Theorem 1 (see Additional file 1: Supplementary Note 1), which states that if an object originates from one of the targets then either one or no target will be hit from the k-mers in the object. Since we use target-specific k-mer sets, at most one target can be associated to the k-mers of an object. In addition, we reduce the number of queries to the database by considering a sample of the k-mers in the object. So CLARK-E only queries non-overlapping k-mers, and the object is assigned to the first target that obtains a hit. This optimization allows CLARK-E to be extremely fast compared to CLARK/KRAKEN (see Table 1), while maintaining high precision and sensitivity. Running time analysis All experiments presented in this study were run on a Dell PowerEdge T710 server (dual Intel Xeon X5660 2.8 Ghz, 12 cores, 192 GB of RAM). CLARK-l was also run on a Mac OS X, Version 10.9.5 (2.53 GHz Intel Core 2 Duo, 4 GB of RAM). When comparing KRAKEN to CLARK in their default mode, and KRAKEN-Q to CLARK-E, we always set KRAKEN to “preload” its database in main memory and print results to a file (instead of the standard output) to achieve the highest speed. For consistency, CLARK was also run under the same conditions. For the results in Table 1 and Table 2, CLARK (v1.0), NBC (v1.1), and KRAKEN (v0.10.4-beta) were run in single-threaded mode, three times on the same inputs in order to smooth fluctuations due to I/O and cache issues (the reported numbers are best values). We have also run the latest version of Kraken (v0.10.5-beta) and we did not observe a significant variation of accuracy and usage of RAM. However, we observed a 15% decrease in the classification speed compared to version v0.10.4-beta.",Classification,"clark fast  accurate classification  metagenomic  genomic sequence use discriminative kmers
building targetspecific kmer set clark accept input  fastafastq format alternatively  input   give   text file contain  kmer distribution   line contain  kmer   number  occurrences clark first build  index   target sequence unless one already exist   specify input file   user want  classify object   genus level  another taxonomic rank heshe  expect  generate target  group genomes    genus     taxonomic label  strategy represent  major difference   tool   lmat  kraken  index   hashtable store   distinct kmer       target contain    number  distinct target contain     number  occurrences      target  hashtable use separate chain  resolve collisions   bucket clark  remove  kmer  appear    one target except   case  chromosome arm assignment   latter case kmers share   two arm    chromosome  use  define centromeric regions  overlap also kmers   index may  remove base   number  occurrences   user  specify  minimum number  occurrences  rare kmers tend   spurious  sequence errors  metagenomic classifiers like kraken  lmat   offer  protection  noise    useful  target sequence  read  lowquality assemblies   result set  targetspecific kmers  store  disk   next phase  time  memory need  create  index    give  additional file  table   table also contain  time  memory require  nbc  kraken observe  clark  faster  nbc  kraken  create  index   use less ram  disk space  kraken  classify object  concept  “targetspecific kmers”  similar   notion  “cladespecific marker genes” propose    “genomespecific markers” recently propose    clark use exact match  identify  targetspecific kmers derive   region   genome  author   disregard intergenic regions  author   focus  strainspecific markers identify  approximate string match  clark use exact match another important difference    method present   rely  megablast   perform  classification   several order  magnitude slower  kraken   users  want  run clark  workstations  limit amount  ram   design clarkl “light” clarkl   variant  clark    much smaller ram footprint   classify object  similar speed  accuracy  reduction  ram   achieve  construct  hashtable  smaller size   construct smaller set  discriminative kmers instead  consider  kmers   target clarkl sample  fraction   clarkl use mers mers appear    good tradeoff  speed low memory usage  precision  skip four consecutivenonoverlapping mers   result clarkls peak ram usage       index creation     compute  classification see additional file  table  clarkl  also  advantage    fast  build  hash table table  include  performance  clarkl   precision  sensitivity  lower compare  clark clarkl still achieve high precision  high speed sequence classification   full mode   index contain targetspecific kmers   create clark create  “dictionary”  associate kmers  target  clark iteratively process  object   object sequence  clark query  index  fetch  set  kmers    “hit”  obtain   kmer either forward  reverse complement match  targetspecific kmer set object   assign   target    highest number  hit see algorithmic detail  additional file  supplementary note   additional file  table   confidence score  compute           number  hit   highest target      number  hit   secondhighest target  rationale  remove common kmers  target   taxonomy level define   user    increase  “noise”   classification process    present  target could obtain   number  hit  would complicate  assignment   conflict   avoid     need  query  taxonomy tree  find  example  lowest common ancestor taxons  “conflicting nodes”  resolve        tool  kraken  lmat observe  additional file  figure     clarks assignments  high confidence score observe   least    assignments  hiseq miseq simba  simhc make  clark   full mode  confidence score equal    exactly one target get hit   average confidence score    assignments    imply   average  number  hit   top target   receive  assignment    time higher   second thus clark unlike lmat  kraken   need  taxonomy tree  classify object instead one “flat” level  clearly sufficient  users   interest  collect confidence score   hit count    recommend  use  default mode  clark   mode clark stop query kmers   object  soon     least one target  collect  least half   total possible hit also  mode load  main memory  half   targetspecific kmers     alternatively load  skip targetspecific kmers base   index position clark run significantly faster  default mode  time faster   experiment  negligible degradation  sensitivity  assignment rate also  ram usage  significantly lower   full mode    lower   experiment  speed   primary concern   design  “express” variant  clark call clarke clarke  base upon theorem  see additional file  supplementary note   state    object originate  one   target  either one   target   hit   kmers   object since  use targetspecific kmer set   one target   associate   kmers   object  addition  reduce  number  query   database  consider  sample   kmers   object  clarke  query nonoverlapping kmers   object  assign   first target  obtain  hit  optimization allow clarke   extremely fast compare  clarkkraken see table   maintain high precision  sensitivity run time analysis  experiment present   study  run   dell poweredge t710 server dual intel xeon x5660  ghz  core    ram clarkl  also run   mac   version   ghz intel core  duo    ram  compare kraken  clark   default mode  krakenq  clarke  always set kraken  “preload”  database  main memory  print result   file instead   standard output  achieve  highest speed  consistency clark  also run    condition   result  table   table  clark  nbc   kraken v0beta  run  singlethreaded mode three time    input  order  smooth fluctuations due    cache issue  report number  best value   also run  latest version  kraken v0beta     observe  significant variation  accuracy  usage  ram however  observe   decrease   classification speed compare  version v0beta",1
29,kaiju,"Fast and sensitive taxonomic classification for metagenomics with Kaiju.
First, a sequencing read is translated into the six possible reading frames and the resulting amino acid sequences are split into fragments at stop codons. Fragments are then sorted either by their length (MEM mode) or by their BLOSUM62 score (Greedy mode). This sorted list of fragments is then searched against the reference protein database using the backwards search algorithm on the BWT. While MEM mode only allows exact matches, Greedy mode extends matches at their left end by allowing substitutions. Once the remaining fragments in the list are shorter than the best match obtained so far (MEM) or cannot achieve a better score (Greedy), the search stops and the taxon identifier of the corresponding database sequence is retrieved. Metagenome classifier. Kaiju classifies individual metagenomic reads using a reference database comprising the annotated protein-coding genes of a set of microbial genomes. We employ a search strategy, which finds maximal exact matching substrings between query and database using a modified version of the backwards search algorithm in the BWT20,21. The BWT22 is a text transformation that converts the reference sequence database into an easily searchable representation, which allows for exact string matching between a query sequence and the database in time proportional to the length of the query. While in the context of read mapping, MEMs have been used as a fast method for identifying seeds of mapping regions in the reference genome, for example, in (refs 23,24), we use MEMs to quickly find those sequences in the reference database that share the longest possible subsequence with the query. Backtracking through the BWT can be sped up by using a lookup table for occurrence counts of each alphabet letter, which was first proposed by Ferragina and Manzini20 and is often called FM-index. Kaiju employs a sparse representation of this table by using checkpoints, which allows for decreasing the otherwise large memory requirement due to the size of the amino acid alphabet. The initial suffix array used for calculating the BWT is also implemented as a sparse suffix array with adjustable size, which further reduces the index size with only little impact on runtime, because the suffix array is only needed for determining the name of the database sequence once the best match for a read is found. Thus, Kaiju is the first program to efficiently use the BWT and FM-index on a large protein database, allowing querying large sets of sequencing reads. Figure 5 illustrates the steps in Kaiju’s algorithm: First, Kaiju translates each read into the six possible reading frames, which are then split at stop codons into amino acid fragments. These fragments are sorted by length, and, beginning with the longest fragment, queried against the reference database using the backwards search in the BWT. Given a query fragment of length n and the minimum required match length m, the backwards search is started from all positions between n and n m in the query and the longest MEM is retained. If one or more matches of length l4m are found, m is set to l and the next fragment in the ordered list is queried against the database if its length is at least l, otherwise the search stops. Once the search is finished and one or more matches are found, the taxon identifier from the corresponding database sequence is retrieved from the suffix array and printed to the output. If equally long matches are found in multiple taxa, Kaiju determines their LCA from the taxonomic tree (Supplementary Fig. 6) and outputs its taxon identifier. Thus, each read is always classified to the lowest possible taxonomic level given the ambiguity of the search result. The minimum required MEM length m is the major parameter for trading sensitivity versus precision (with little impact on runtime). If the error rate e of the sequencing reads is known and the evolutionary distance between reference genome and sequenced genome is negligible, m can be estimated from e and the read length23. However, in metagenomics, the evolutionary distance, which adds variation on top of sequencing errors, is not known a priori. At least, one can estimate the false positive rate by counting random matches. To this end, we created a shuffled version of the microbial subset of NCBI’s NR protein database, using uShuffle25 with a window length of 100 amino acids, and searched for MEMs between simulated metagenomic reads and the shuffled database. Supplementary Figure 5 shows the cumulative sum of matches to the shuffled database sorted by the length of the match, and one can observe that B95% of them are r11 amino acids long. When classifying simulated reads against the original database, 475% of wrong classifications and only B2% of correct classifications have length r11. We therefore chose m ¼ 11 as the default minimum match length in Kaiju. Searching for MEMs is the fastest possible search strategy, but its sensitivity decreases with increasing evolutionary distance between query and target, where more and more amino acid substitutions occur and exact matches become shorter. Therefore, allowing for substitutions during the backwards search can bridge mismatches and extend the match at the cost of an exponential increase of runtime depending on the number of allowed mismatched positions. Because of the rapid expansion of the search space, especially with the 20 letter amino acid alphabet, one could employ a greedy heuristic, in which substitutions are only introduced at the end of a match instead of all positions in the query sequence. Therefore, we also implemented a Greedy search mode in Kaiju, which first locates all MEMs of a minimum seed length (default 7) and then extends them by allowing substitutions at the left ends of each seed match. From there, the backwards search continues until the next mismatch occurs. Eventually the search stops once the left end of the query is reached or if the maximum allowed number of substitutions has been reached. Since amino acid substitutions in homologous sequences are non-uniform, a further speed-up can be gained by prioritizing the most likely substitutions at each position. By using an amino acid substitution model, a total score for each match can be calculated, as in standard sequence alignment, which is then used to rank multiple matches and select the taxon from the database for classification. Therefore, after the translation of a read into a set of amino acid fragments, we rank the fragments by their BLOSUM62 score and start the database search with the highest scoring fragment. For each substituted amino acid, the modified fragment is placed back into the search list according to its new (now lower) score. Once a match is found, which has a higher score than all remaining fragments in the search list and a score above the minimum score threshold s, the search stops and this highest scoring match is used for classifying the read. If multiple matches to several different database entries have the same score, Kaiju classifies the read to their LCA as above. Again, the minimum required score s necessary for avoiding random matches can be estimated by using a shuffled database and we chose s ¼ 65 as default value for Kaiju’s Greedy mode. Kaiju is implemented as a command-line program in C/C þþ and is also available via a web server. Input files containing the (single-end or paired-end) reads can either be in FASTA or FASTQ format. Kaiju outputs one line for each read (or read pair), containing the read name and the NCBI taxon identifier of the assigned taxon, as well as the length or score of the match. Optionally, Kaiju can also produce a summary file with the number of reads assigned per taxon, which can be loaded into Krona26 for interactive visualization. We also include a utility program that can merge the classification results from different runs or programs, for example, for merging Kaiju and Kraken results. Performance evaluation. The primary goal of Kaiju’s protein-level classification is to improve classification of those parts of a metagenome that are only distantly related to the known sequences or belong to a branch of the phylogeny that is underrepresented in the reference database. We therefore devised a benchmark study, which addresses this problem by simulating the classification of metagenomic reads from a novel strain or species that is not contained in the reference database. For our benchmark data set, we downloaded a snapshot of all complete bacterial and archaeal genomes from the NCBI FTP server (date: 16 December 2014). Only those genomes were retained that are assigned to a species belonging to a genus and have a full chromosome with annotated proteins, resulting in a total of 2,724 genomes belonging to 692 distinct genera. Supplementary Fig. 4 shows the distribution of genomes to genera, illustrating the large variance in the number of sequenced genomes for each genus. For example, the genus Streptococcus contains 121 genomes, whereas 405 genera have only 1 available genome, 106 genera have 2 available genomes and so on. The distribution clearly illustrates a sampling bias and the sparseness across large parts of the phylogeny. From the total of 2,724 genomes, we extracted those genera that have at least 2 and at most 10 genomes assigned. This resulted in a list of 242 genera comprising 882 genomes, for which we measured the classification performance individually. For each of the 882 genomes, we simulated five sets of HTS reads and created a reference database not containing this genome that is then used to classify the simulated reads. Reads were simulated from the whole genome (including plasmids) using ART27. The four sets of Illumina reads contain 50k reads of length either 100 or 250 nt, both in single-end and paired-end mode. Another set of 50k Roche/454 reads with minimum length of 50 nt and mean length of 350 nt was also simulated using ART. To evaluate classification accuracy, we measured the number of classified reads, as well as sensitivity and precision on genus- and phylum-levels. Sensitivity was calculated as the percentage of reads assigned to the correct genus/phylum out of the total number of reads in the input. Precision was calculated as the percentage of reads assigned to the correct genus/phylum out of the number of classified reads, excluding reads classified correctly to a rank above genus/phylum-level. The same measurements were used in the study by Ounit et al.9. Kraken (v0.10.4b) and Clark (v1.1.3) were run in their default modes using k ¼ 31 for highest precision, and Clark was also run using k ¼ 20. Kaiju was run in MEM mode using minimum match lengths m ¼ 11y14 and in Greedy-1 and Greedy-5 modes (allowing only 1 or up to 5 substitutions) using minimum match scores s ¼ 55y80. Speed measurements were run on an HP Apollo 6000 System ProLiant XL230a Gen9 Server, which has two 64-bit Intel Xeon E5-2683 2 GHz CPUs (14 cores each), 128 GB DDR4 memory and a 500 GB 7200 r.p.m. SATA disk (HP 614829-002). Kraken and Clark were run in default modes with k ¼ 31 and Kaiju was run in MEM (m ¼ 12), as well as Greedy-1 and Greedy-5 (s ¼ 65) modes with an index that uses a suffix array exponent of 3. Performance was measured in processed reads (or read pairs) per second (r.p.s.) using 25 parallel threads. While Kaiju and Clark need to preload their index into memory before the classification starts, Kraken can either preload the index or only load necessary segments during the classification. We therefore measured Kraken’s speed using both options, and it turned out that Kraken runs faster without preloading on our hardware. We therefore report its performance without preloading. For each of the five types of simulated reads from our exclusion benchmark, we created a data set comprising 10k reads from each genome in the reference database, resulting in 27.24 m reads for each read type. Each combination of program and read type was measured four times to reduce impact of caching and I/O fluctuations and the fastest run of the replicates is reported.",Classification,"fast  sensitive taxonomic classification  metagenomics  kaiju
first  sequence read  translate   six possible read frame   result amino acid sequence  split  fragment  stop codons fragment   sort either   length mem mode    blosum62 score greedy mode  sort list  fragment   search   reference protein database use  backwards search algorithm   bwt  mem mode  allow exact match greedy mode extend match   leave end  allow substitutions   remain fragment   list  shorter   best match obtain  far mem  cannot achieve  better score greedy  search stop   taxon identifier   correspond database sequence  retrieve metagenome classifier kaiju classify individual metagenomic read use  reference database comprise  annotate proteincoding genes   set  microbial genomes  employ  search strategy  find maximal exact match substrings  query  database use  modify version   backwards search algorithm   bwt20  bwt22   text transformation  convert  reference sequence database   easily searchable representation  allow  exact string match   query sequence   database  time proportional   length   query    context  read map mems   use   fast method  identify seed  map regions   reference genome  example  refs   use mems  quickly find  sequence   reference database  share  longest possible subsequence   query backtrack   bwt   speed   use  lookup table  occurrence count   alphabet letter   first propose  ferragina  manzini20   often call fmindex kaiju employ  sparse representation   table  use checkpoints  allow  decrease  otherwise large memory requirement due   size   amino acid alphabet  initial suffix array use  calculate  bwt  also implement   sparse suffix array  adjustable size   reduce  index size   little impact  runtime   suffix array   need  determine  name   database sequence   best match   read  find thus kaiju   first program  efficiently use  bwt  fmindex   large protein database allow query large set  sequence read figure  illustrate  step  kaijus algorithm first kaiju translate  read   six possible read frame    split  stop codons  amino acid fragment  fragment  sort  length  begin   longest fragment query   reference database use  backwards search   bwt give  query fragment  length    minimum require match length   backwards search  start   position        query   longest mem  retain  one   match  length l4m  find   set     next fragment   order list  query   database   length   least  otherwise  search stop   search  finish  one   match  find  taxon identifier   correspond database sequence  retrieve   suffix array  print   output  equally long match  find  multiple taxa kaiju determine  lca   taxonomic tree supplementary fig   output  taxon identifier thus  read  always classify   lowest possible taxonomic level give  ambiguity   search result  minimum require mem length    major parameter  trade sensitivity versus precision  little impact  runtime   error rate    sequence read  know   evolutionary distance  reference genome  sequence genome  negligible    estimate     read length23 however  metagenomics  evolutionary distance  add variation  top  sequence errors   know  priori  least one  estimate  false positive rate  count random match   end  create  shuffle version   microbial subset  ncbis  protein database use ushuffle25   window length   amino acids  search  mems  simulate metagenomic read   shuffle database supplementary figure  show  cumulative sum  match   shuffle database sort   length   match  one  observe  b95    r11 amino acids long  classify simulate read   original database   wrong classifications     correct classifications  length r11  therefore choose      default minimum match length  kaiju search  mems   fastest possible search strategy   sensitivity decrease  increase evolutionary distance  query  target     amino acid substitutions occur  exact match become shorter therefore allow  substitutions   backwards search  bridge mismatch  extend  match   cost   exponential increase  runtime depend   number  allow mismatch position    rapid expansion   search space especially    letter amino acid alphabet one could employ  greedy heuristic   substitutions   introduce   end   match instead   position   query sequence therefore  also implement  greedy search mode  kaiju  first locate  mems   minimum seed length default    extend   allow substitutions   leave end   seed match    backwards search continue   next mismatch occur eventually  search stop   leave end   query  reach    maximum allow number  substitutions   reach since amino acid substitutions  homologous sequence  nonuniform   speedup   gain  prioritize   likely substitutions   position  use  amino acid substitution model  total score   match   calculate   standard sequence alignment    use  rank multiple match  select  taxon   database  classification therefore   translation   read   set  amino acid fragment  rank  fragment   blosum62 score  start  database search   highest score fragment   substitute amino acid  modify fragment  place back   search list accord   new  lower score   match  find    higher score   remain fragment   search list   score   minimum score threshold   search stop   highest score match  use  classify  read  multiple match  several different database entries    score kaiju classify  read   lca     minimum require score  necessary  avoid random match   estimate  use  shuffle database   choose     default value  kaijus greedy mode kaiju  implement   commandline program      also available via  web server input file contain  singleend  pairedend read  either   fasta  fastq format kaiju output one line   read  read pair contain  read name   ncbi taxon identifier   assign taxon  well   length  score   match optionally kaiju  also produce  summary file   number  read assign per taxon    load  krona26  interactive visualization  also include  utility program   merge  classification result  different run  program  example  merge kaiju  kraken result performance evaluation  primary goal  kaijus proteinlevel classification   improve classification   part   metagenome    distantly relate   know sequence  belong   branch   phylogeny   underrepresented   reference database  therefore devise  benchmark study  address  problem  simulate  classification  metagenomic read   novel strain  species    contain   reference database   benchmark data set  download  snapshot   complete bacterial  archaeal genomes   ncbi ftp server date  december    genomes  retain   assign   species belong   genus    full chromosome  annotate proteins result   total   genomes belong   distinct genera supplementary fig  show  distribution  genomes  genera illustrate  large variance   number  sequence genomes   genus  example  genus streptococcus contain  genomes whereas  genera    available genome  genera   available genomes     distribution clearly illustrate  sample bias   sparseness across large part   phylogeny   total   genomes  extract  genera    least      genomes assign  result   list   genera comprise  genomes    measure  classification performance individually      genomes  simulate five set  hts read  create  reference database  contain  genome    use  classify  simulate read read  simulate   whole genome include plasmids use art27  four set  illumina read contain  read  length either       singleend  pairedend mode another set   roche read  minimum length     mean length     also simulate use art  evaluate classification accuracy  measure  number  classify read  well  sensitivity  precision  genus  phylumlevels sensitivity  calculate   percentage  read assign   correct genusphylum    total number  read   input precision  calculate   percentage  read assign   correct genusphylum    number  classify read exclude read classify correctly   rank  genusphylumlevel   measurements  use   study  ounit   kraken v04b  clark   run   default modes use     highest precision  clark  also run use    kaiju  run  mem mode use minimum match lengths   11y14   greedy  greedy modes allow       substitutions use minimum match score   55y80 speed measurements  run    apollo  system proliant xl230a gen9 server   two bite intel xeon   ghz cpus  core    ddr4 memory      rpm sata disk   kraken  clark  run  default modes      kaiju  run  mem     well  greedy  greedy    modes   index  use  suffix array exponent   performance  measure  process read  read pair per second rps use  parallel thread  kaiju  clark need  preload  index  memory   classification start kraken  either preload  index   load necessary segment   classification  therefore measure krakens speed use  options   turn   kraken run faster without preloading   hardware  therefore report  performance without preloading     five type  simulate read   exclusion benchmark  create  data set comprise  read   genome   reference database result    read   read type  combination  program  read type  measure four time  reduce impact  cache   fluctuations   fastest run   replicate  report",1
30,KrakenUniq,"KrakenUniq: confident and fast metagenomics classification using unique k-mer counts
KrakenUniq was developed to provide efficient k-mer count information for all taxa identified in a metagenomics experiment. The main workflow is as follows: As reads are processed, each k-mer is assigned a taxon from the database (Fig. 1a). KrakenUniq instantiates a HyperLogLog data sketch for each taxon and adds the k-mers to it (Fig. 1b and Additional file 1: Section 1 on the HyperLogLog algorithm). After classification of a read, KrakenUniq traverses up the taxonomic tree and merges the estimators of each taxon with its parent. In its classification report, KrakenUniq includes the number of unique k-mers and the depth of k-mer coverage for each taxon that it observed in the input data Efficient k-mer cardinality estimation using the HyperLogLog algorithm Cardinality is the number of elements in a set without duplicates, e.g., the number of distinct words in a text. An exact count can be kept by storing the elements in a sorted list or linear probing hash table, but that requires memory proportional to the number of unique elements. When an accurate estimate of the cardinality is sufficient, however, the computation can be done efficiently with a very small amount of fixed memory. The HyperLogLog algorithm (HLL) [10], which is well suited for k-mer counting [14], keeps a summary or sketch of the data that is sufficient for precise estimation of the cardinality and requires only a small amount of constant space to estimate cardinalities up to billions. The method centers on the idea that long runs of leading zeros, which can be efficiently computed using machine instructions, are unlikely in random bitstrings. For example, about every fourth bitstring in a random series should start with 012 (one 0 bit before the first 1 bit), and about every 32nd hash starts with 000012. Conversely, if we know the maximum number of leading zeros k of the members of a random set, we can use 2k + 1 as a crude estimate of its cardinality (more details in Additional file 1: Section 1 on the HLL algorithm). HLL keeps m = 2p 1 byte counts of the maximum numbers of leading zeros on the data (its data sketch), with p, the precision parameter, typically between 10 and 18 (see Fig. 2). For cardinalities up to about m/4, we use the sparse representation of the registers suggested by Heule et al. [11] that has the much higher effective precision p′ of 25 by encoding each index and count in a vector of 4-byte values (see Fig. 2). To add a k-mer to its taxon’s sketch, the k-mer (with k up to 31) is first mapped by a hash function to a 64-bit hash value. Note that k-mers that contain non-A, C, G, or T characters (such as ambiguous IUPAC characters) are ignored by KrakenUniq. The first p bits of the hash value are used as index i, and the later 64-p = q bits for counting the number of leading zeros k. The value of the register M[i] in the sketch is updated if k is larger than the current value of M[i]. When the read classification is finished, the taxon sketches are aggregated up the taxonomy tree by taking the maximum of each register value. The resulting sketches are the same as if the k-mers were counted at their whole lineage from the beginning. KrakenUniq then computes cardinality estimates using the formula proposed by Ertl [12], which has theoretical and practical advantages and does not require empirical bias correction factors [10, 11]. In our tests, it performed better than Flajolet’s and Heule’s methods (Additional file 1: Figures S1 and S2). The expected relative error of the final cardinality estimate is approximately 1.04/sqrt(2p) [10]. With p = 14, the sketch uses 214 1-byte registers, i.e., 16 KB of space, and gives estimates with relative errors of less than 1% (Fig. 2). Note that KrakenUniq also incorporates an exact counting mode, which however uses significantly more memory and runtime without appreciable improvements in classification accuracy Results on 21 simulated and 10 biological test datasets We assessed KrakenUniq’s performance on the 34 datasets compiled by McIntyre et al. [15] (see Additional file 2: Table S3 for details on the datasets). We place greater emphasis on the 11 biological datasets, which contain more realistic laboratory and environmental contamination. In the first part of this section, we show that unique k-mer counts provide higher classification accuracy than read counts, and in the second part, we compare KrakenUniq with the results of 11 metagenomics classifiers. We ran KrakenUniq on three databases: “orig,” the database used by McIntyre et al.; “std,” which contains all current complete bacterial, archaeal, and viral genomes from RefSeq plus viral neighbor sequences and the human reference genome; and “nt,” which contains all microbial sequences (including fungi and protists) in the non-redundant nucleotide collection nr/nt provided by NCBI (see Additional file 1: Section 2 for details). The “std” database furthermore includes the UniVec and EmVec sequence sets of synthetic constructs and vector sequences, and low-complexity k-mers in microbial sequences were masked using NCBI’s dustmasker with default settings. We use two metrics to compare how well methods can separate true positives and false positives: (a) F1 score, i.e., the harmonic mean of precision p and recall r, and (b) recall at a maximum false discovery rate (FDR) of 5%. For each method, we compute and select the ideal thresholds based on the read count, k-mer count or abundance calls. Precision p is defined as the number of correctly called species (or genera) divided by the number of all called species (or genera) at a given threshold. Recall r is the proportion of species (or genera) that are in the test dataset and that are called at a given threshold. Higher F1 scores indicate a better separation between true positives and false positives. Higher recall means that more true species can be recovered while controlling the false positives. Because the NCBI taxonomy has been updated since the datasets were published, we manually updated the “truth” sets in several datasets (see Additional file 1: Section 2.3 for details on taxonomy fixes). Any cases that might have been missed would result in a lower apparent performance of KrakenUniq. Note that we exclude the over 10-year-old simulated datasets simHC, simMC, and simLC from Mavromatis et al. (2007), as well as the biological dataset JGI SRR033547 which has only 100 reads. Classification performance using unique k-mer or read count thresholds We first looked at the performance of the unique k-mer count thresholds versus read count thresholds (as would be used with Kraken). The k-mer count thresholds worked very well, particularly for the biological datasets (Table 1 and Additional file 2: Table S3). On the genus level, the average recall in the biological datasets increases by 4–9%, and the average F1 score increases 2–3%. On the species level, the average increase in recall in the biological sets is between 3 and 12%, and the F1 score increases by 1–2%. On the simulated datasets, the differences are less pronounced and vary between databases, even though on average the unique k-mer count is again better. However, only in two cases (genus recall on databases “orig” and “std”) the difference is higher than 1% in any direction. We find that simulated datasets often lack false positives with a decent number of reads but a lower number of unique k-mer counts, which we see in real data. Instead, in most simulated datasets, the number of unique k-mers is linearly increasing with the number of unique reads in both true and false positives (Additional file 1: Figure S3). In biological datasets, sequence contamination and lower read counts for the true positives make the task of separating true and false positives harder. Comparison of KrakenUniq with 11 other methods Next, we compared KrakenUniq’s unique k-mer counts with the results of 11 metagenomics classifiers from McIntyre et al. [15], which include the alignment-based methods Blast + Megan [16, 17], Diamond + Megan [17, 18], and MetaFlow [19]; the k-mer-based CLARK [20], CLARK-S [21], Kraken [9], LMAT [22], and NBC [23]; and the marker-based methods GOTTCHA [24], MetaPhlAn2 [25], and PhyloSift [26]. KrakenUniq with database “nt” has the highest average recall and F1 score across the biological datasets, as shown in Table 2. As seen before, using unique k-mer instead of read counts as thresholds increases the scores. While the database selection proves to be very important (KrakenUniq with database “std” is performing 10% worse than KrakenUniq with database “nt”), only Blast has higher average scores than KrakenUniq with k-mer count thresholds on the original database. On the simulated datasets, KrakenUniq with the “nt” database still ranks at the top, though, as seen previously, there is more variation (Additional file 1: Table S4). Notably, CLARK is as good as KrakenUniq, but Blast has much worse scores on the simulated datasets. Generating a better test dataset and selecting an appropriate k-mer threshold In the previous section, we demonstrated that KrakenUniq gives better recall and F1 scores than other classifiers on the test datasets, given the correct thresholds. How can the correct thresholds be determined on real data with varying sequencing depths and complex communities? The test datasets are not ideal for that the biological datasets lack complexity with a maximum of 25 species in some of the samples, while the simulated samples lack the features of biological datasets. We thus generated a third type of test dataset by sampling reads from real bacterial isolate sequencing runs, of which there are tens of thousands in the Sequence Read Archive (SRA). That way, we created a complex test dataset for which we know the ground truth, with all the features of real sequencing experiments, including lab contaminants and sequencing errors. We selected 280 SRA datasets from 280 different bacterial species that are linked to complete RefSeq genomes (see Additional file 1: Suppl. Methods Section 2.4). We randomly sampled between 1 hundred and 1 million reads (logarithmically distributed) from each experiment, which gave 34 million read pairs in total. Furthermore, we sub-sampled 5 read sets with between 1 and 20 million reads. All read sets were classified with KrakenUniq using the ‘“std” database. Consistent with the results of the previous section, we found that unique k-mer counts provide better thresholds than read counts both in terms of F1 score and recall in all test datasets (e.g., Fig. 3 on 10 million reads—species recall using k-mers is 0.85, recall using reads 0.76). With higher sequencing depth, the recall increased slightly—from 0.80 to 0.85 on the species level and from 0.87 to 0.89 on the genus level. The ideal values of the unique k-mer count thresholds, however, vary widely with different sequencing depths. We found that the ideal thresholds increase by about 2000 unique k-mers per 1 million reads (see Fig. 4). McIntyre et al. [15] found that k-mer-based methods show a positive relationship between sequencing depths and misclassified reads. Our analysis also shows that with deeper sequencing depths, higher thresholds are required to control the false-positive rate. In general, we find that for correctly identified species, we obtain up to approximately L-k unique k-mers per each read, where L is the read length because each read samples a different location in the genome. (Note that once the genome is completely covered, no more unique k-mers can be detected.) Thus, the k-mer threshold should always be several times higher than the read count threshold. For the discovery of pathogens in human patients, discussed in the next section, a read count threshold of 10 and unique k-mer count threshold of 1000 eliminated many background identifications while preserving all true positives, which were discovered from as few as 15 reads. Exact counting versus estimated cardinality KrakenUniq’s unique k-mer count is an estimate, raising the following question: does using an estimate—instead of the exact count—affect the classification performance? To answer this question, we implemented an exact counting mode in KrakenUniq. As expected, exact counting requires significantly more memory and runtime. On the full test dataset (with 34.3 mio paired reads sampled from 280 WGS experiments on bacterial isolates), the more efficient of two version of exact counting required 60% more memory and over 200% more runtime. At the same time, we observed virtually no improvement in term of classification performance (Table 3). A likely explanation for this finding is that over- or underestimation of the true cardinality by a small amount (e.g., 1%) rarely changes the ranking of the identifications. There will be cases, however, where a true species may fall just under a threshold due to the estimation error, and users may choose to use exact counting with KrakenUniq, although this will incur a large penalty in both runtime and memory consumption. Results on biological samples for infectious disease diagnosis Metagenomics is increasingly used to find species of low abundance. A special case is the emerging use of metagenomics for the diagnosis of infectious diseases [27, 28]. In this application, infected human tissues are sequenced directly to find the likely disease organism. Usually, the vast majority of the reads match (typically 95–99%) the host, and sometimes fewer than 100 reads out of many millions of reads are matched to the target species. Common skin bacteria from the patient or lab personnel and other contamination from sample collection or preparation can easily generate a similar number of reads, and thus mask the signal from the pathogen. To assess if the unique k-mer count metric in KrakenUniq could be used to rank and identify pathogen from human samples, we reanalyzed ten patient samples from a previously described series of neurological infections [4]. That study sequenced spinal cord mass and brain biopsies from ten hospitalized patients for whom routine tests for pathogens were inconclusive. In four of the ten cases, a likely diagnosis could be made with the help of metagenomics. To confirm the metagenomics classifications, the authors in the original study re-aligned all pathogen reads to individual genomes. Table 4 shows the results of our reanalysis of the confirmed pathogens in the four patients, including the number of reads and unique k-mers from the pathogen, as well as the number of bases covered by re-alignment to the genomes. Even though the read numbers are very low in two cases, the number of unique k-mers suggests that each read matches a different location in the genome. For example, in PT8, 15 reads contain 1570 unique k-mers, and re-alignment shows 2201 covered base pairs. In contrast, Table 5 shows examples of identifications from the same datasets that are not well-supported by k-mer counts. We also examined the likely source of the false-positive identifications by blasting the reads against the full nt database, and found rRNA of environmental bacteria, human RNA, and PhiX-174 mis-assignments (see Additional file 1: Suppl. Methods for details). Notably, the common laboratory and skin contaminants PhiX-174, Escherichia coli, Cutibacterium acnes, and Delftia were detected in most of the samples, too (see Additional file 1: Table S6). However, those identifications are solid in terms of their k-mer counts—the bacteria and PhiX-174 are present in the sample, and the reads cover their genomes rather randomly. To discount them, comparisons against a negative control or between multiple samples are require Further extensions in KrakenUniq KrakenUniq adds three further notable features to the classification engine. 1. Enabling strain identification by extending the taxonomy: The finest level of granularity for Kraken classifications are nodes in the NCBI taxonomy. This means that many strains cannot be resolved, because up to hundreds of strains share the same taxonomy ID. KrakenUniq allows extending the taxonomy with virtual nodes for genomes, chromosomes, and plasmids, and thus enabling identifications at the most specific levels (see Additional file 1: Suppl. Methods Section 3) 2. Integrating 100,000 viral strain sequences: RefSeq includes only one reference genome for most viral species, which means that a lot of the variation of viral strain is not covered in a standard RefSeq database. KrakenUniq sources viral strain sequences from the NCBI Viral Genome Resource that are validated as “neighbors” of RefSeq viruses, which leads to up to 20% more read classifications (see Additional file 1: Suppl. Methods Section 4). 3. Hierarchical classification with multiple databases: Researchers may want to include additional sequence sets, such as draft genomes, in some searches. KrakenUniq allows to chain databases and match each k-mer hierarchically, stopping when it found a match. For example, to mitigate the problem of host contamination in draft genomes, a search may use the host genome as the first database, then complete microbial genomes then draft microbial genomes. More details are available in Additional file 1: Suppl. Method Section 5. Timing and memory requirements The additional features of KrakenUniq come without a runtime penalty and very limited additional memory requirements. In fact, due to code improvements, KrakenUniq often runs faster than Kraken, particularly when most of the reads come from one species. On the test datasets, the mean classification speed in million base pairs per minute increased slightly from 410 to 421 Mbp/m (see Additional file 2: Table S3). When factoring in the time needed to summarize classification results by Kraken-report, which is required for Kraken but part of the classification binary of KrakenUniq, KrakenUniq is on average 50% faster. The memory requirements increase on average by 0.5 GB from 39.5 to 40 GB. On the pathogen ID patient data, where in most cases over 99% of the reads were either assigned to human or synthetic reads, KrakenUniq was significantly faster than Kraken (Additional file 1: Table S5). The classification speed increased from 467 to 733 Mbp/m. The average wall time was about 44% lower, and the average additional memory requirements were less than 1 GB, going from 118.0 to 118.4 GB. All timing comparisons were made after preloading the database and running with 10 parallel threads.",Classification,"krakenuniq confident  fast metagenomics classification use unique kmer counts
krakenuniq  develop  provide efficient kmer count information   taxa identify   metagenomics experiment  main workflow   follow  read  process  kmer  assign  taxon   database fig  krakenuniq instantiate  hyperloglog data sketch   taxon  add  kmers   fig   additional file  section    hyperloglog algorithm  classification   read krakenuniq traverse   taxonomic tree  merge  estimators   taxon   parent   classification report krakenuniq include  number  unique kmers   depth  kmer coverage   taxon   observe   input data efficient kmer cardinality estimation use  hyperloglog algorithm cardinality   number  elements   set without duplicate   number  distinct word   text  exact count   keep  store  elements   sort list  linear probe hash table   require memory proportional   number  unique elements   accurate estimate   cardinality  sufficient however  computation    efficiently    small amount  fix memory  hyperloglog algorithm hll    well suit  kmer count  keep  summary  sketch   data   sufficient  precise estimation   cardinality  require   small amount  constant space  estimate cardinalities   billions  method center   idea  long run  lead zero    efficiently compute use machine instructions  unlikely  random bitstrings  example  every fourth bitstring   random series  start   one  bit   first  bit   every 32nd hash start   conversely   know  maximum number  lead zeros    members   random set   use      crude estimate   cardinality  detail  additional file  section    hll algorithm hll keeps     byte count   maximum number  lead zero   data  data sketch    precision parameter typically     see fig   cardinalities      use  sparse representation   register suggest  heule       much higher effective precision ′    encode  index  count   vector  byte value see fig   add  kmer   taxons sketch  kmer       first map   hash function   bite hash value note  kmers  contain nona     character   ambiguous iupac character  ignore  krakenuniq  first  bits   hash value  use  index    later    bits  count  number  lead zeros   value   register    sketch  update    larger   current value     read classification  finish  taxon sketch  aggregate   taxonomy tree  take  maximum   register value  result sketch       kmers  count   whole lineage   begin krakenuniq  compute cardinality estimate use  formula propose  ertl    theoretical  practical advantage    require empirical bias correction factor     test  perform better  flajolets  heules methods additional file  figure     expect relative error   final cardinality estimate  approximately sqrt2p       sketch use  byte register     space  give estimate  relative errors  less   fig  note  krakenuniq also incorporate  exact count mode  however use significantly  memory  runtime without appreciable improvements  classification accuracy result   simulate   biological test datasets  assess krakenuniqs performance    datasets compile  mcintyre    see additional file  table   detail   datasets  place greater emphasis    biological datasets  contain  realistic laboratory  environmental contamination   first part   section  show  unique kmer count provide higher classification accuracy  read count    second part  compare krakenuniq   result   metagenomics classifiers  run krakenuniq  three databases “orig”  database use  mcintyre   “std”  contain  current complete bacterial archaeal  viral genomes  refseq plus viral neighbor sequence   human reference genome  “”  contain  microbial sequence include fungi  protists   nonredundant nucleotide collection nrnt provide  ncbi see additional file  section   detail  “std” database furthermore include  univec  emvec sequence set  synthetic construct  vector sequence  lowcomplexity kmers  microbial sequence  mask use ncbis dustmasker  default settings  use two metrics  compare  well methods  separate true positives  false positives   score   harmonic mean  precision   recall    recall   maximum false discovery rate fdr     method  compute  select  ideal thresholds base   read count kmer count  abundance call precision   define   number  correctly call species  genera divide   number   call species  genera   give threshold recall    proportion  species  genera     test dataset    call   give threshold higher  score indicate  better separation  true positives  false positives higher recall mean   true species   recover  control  false positives   ncbi taxonomy   update since  datasets  publish  manually update  “truth” set  several datasets see additional file  section   detail  taxonomy fix  case  might   miss would result   lower apparent performance  krakenuniq note   exclude   yearold simulate datasets simhc simmc  simlc  mavromatis     well   biological dataset jgi srr033547     read classification performance use unique kmer  read count thresholds  first look   performance   unique kmer count thresholds versus read count thresholds  would  use  kraken  kmer count thresholds work  well particularly   biological datasets table   additional file  table    genus level  average recall   biological datasets increase     average  score increase    species level  average increase  recall   biological set         score increase     simulate datasets  differences  less pronounce  vary  databases even though  average  unique kmer count   better however   two case genus recall  databases “orig”  “std”  difference  higher     direction  find  simulate datasets often lack false positives   decent number  read   lower number  unique kmer count   see  real data instead   simulate datasets  number  unique kmers  linearly increase   number  unique read   true  false positives additional file  figure   biological datasets sequence contamination  lower read count   true positives make  task  separate true  false positives harder comparison  krakenuniq    methods next  compare krakenuniqs unique kmer count   result   metagenomics classifiers  mcintyre     include  alignmentbased methods blast  megan   diamond  megan    metaflow   kmerbased clark  clarks  kraken  lmat   nbc    markerbased methods gottcha  metaphlan2   phylosift  krakenuniq  database “”   highest average recall   score across  biological datasets  show  table   see  use unique kmer instead  read count  thresholds increase  score   database selection prove    important krakenuniq  database “std”  perform  worse  krakenuniq  database “”  blast  higher average score  krakenuniq  kmer count thresholds   original database   simulate datasets krakenuniq   “” database still rank   top though  see previously    variation additional file  table  notably clark   good  krakenuniq  blast  much worse score   simulate datasets generate  better test dataset  select  appropriate kmer threshold   previous section  demonstrate  krakenuniq give better recall   score   classifiers   test datasets give  correct thresholds    correct thresholds  determine  real data  vary sequence depths  complex communities  test datasets   ideal    biological datasets lack complexity   maximum   species     sample   simulate sample lack  feature  biological datasets  thus generate  third type  test dataset  sample read  real bacterial isolate sequence run     tens  thousands   sequence read archive sra  way  create  complex test dataset    know  grind truth    feature  real sequence experiment include lab contaminants  sequence errors  select  sra datasets   different bacterial species   link  complete refseq genomes see additional file  suppl methods section   randomly sample   hundred   million read logarithmically distribute   experiment  give  million read pair  total furthermore  subsampled  read set      million read  read set  classify  krakenuniq use  “std” database consistent   result   previous section  find  unique kmer count provide better thresholds  read count   term   score  recall   test datasets  fig    million reads—species recall using kmers   recall use read   higher sequence depth  recall increase slightly—      species level        genus level  ideal value   unique kmer count thresholds however vary widely  different sequence depths  find   ideal thresholds increase    unique kmers per  million read see fig  mcintyre    find  kmerbased methods show  positive relationship  sequence depths  misclassified read  analysis also show   deeper sequence depths higher thresholds  require  control  falsepositive rate  general  find   correctly identify species  obtain   approximately  unique kmers per  read     read length   read sample  different location   genome note    genome  completely cover   unique kmers   detect thus  kmer threshold  always  several time higher   read count threshold   discovery  pathogens  human patients discuss   next section  read count threshold    unique kmer count threshold   eliminate many background identifications  preserve  true positives   discover      read exact count versus estimate cardinality krakenuniqs unique kmer count   estimate raise  follow question  use  estimate—instead   exact count—affect  classification performance  answer  question  implement  exact count mode  krakenuniq  expect exact count require significantly  memory  runtime   full test dataset   mio pair read sample   wgs experiment  bacterial isolate   efficient  two version  exact count require   memory     runtime    time  observe virtually  improvement  term  classification performance table   likely explanation   find     underestimation   true cardinality   small amount   rarely change  rank   identifications    case however   true species may fall    threshold due   estimation error  users may choose  use exact count  krakenuniq although   incur  large penalty   runtime  memory consumption result  biological sample  infectious disease diagnosis metagenomics  increasingly use  find species  low abundance  special case   emerge use  metagenomics   diagnosis  infectious diseases     application infect human tissue  sequence directly  find  likely disease organism usually  vast majority   read match typically   host  sometimes fewer   read   many millions  read  match   target species common skin bacteria   patient  lab personnel   contamination  sample collection  preparation  easily generate  similar number  read  thus mask  signal   pathogen  assess   unique kmer count metric  krakenuniq could  use  rank  identify pathogen  human sample  reanalyzed ten patient sample   previously describe series  neurological infections   study sequence spinal cord mass  brain biopsies  ten hospitalize patients   routine test  pathogens  inconclusive  four   ten case  likely diagnosis could  make   help  metagenomics  confirm  metagenomics classifications  author   original study realign  pathogen read  individual genomes table  shows  result   reanalysis   confirm pathogens   four patients include  number  read  unique kmers   pathogen  well   number  base cover  realignment   genomes even though  read number   low  two case  number  unique kmers suggest   read match  different location   genome  example  pt8  read contain  unique kmers  realignment show  cover base pair  contrast table  shows examples  identifications    datasets    wellsupported  kmer count  also examine  likely source   falsepositive identifications  blast  read   full  database  find rrna  environmental bacteria human rna  phix misassignments see additional file  suppl methods  detail notably  common laboratory  skin contaminants phix escherichia coli cutibacterium acnes  delftia  detect     sample  see additional file  table  however  identifications  solid  term   kmer counts— bacteria  phix  present   sample   read cover  genomes rather randomly  discount  comparisons   negative control   multiple sample  require  extensions  krakenuniq krakenuniq add three  notable feature   classification engine  enabling strain identification  extend  taxonomy  finest level  granularity  kraken classifications  nod   ncbi taxonomy  mean  many strain cannot  resolve    hundreds  strain share   taxonomy  krakenuniq allow extend  taxonomy  virtual nod  genomes chromosomes  plasmids  thus enable identifications    specific level see additional file  suppl methods section   integrating  viral strain sequence refseq include  one reference genome   viral species  mean   lot   variation  viral strain   cover   standard refseq database krakenuniq source viral strain sequence   ncbi viral genome resource   validate  “neighbors”  refseq viruses  lead      read classifications see additional file  suppl methods section   hierarchical classification  multiple databases researchers may want  include additional sequence set   draft genomes   search krakenuniq allow  chain databases  match  kmer hierarchically stop   find  match  example  mitigate  problem  host contamination  draft genomes  search may use  host genome   first database  complete microbial genomes  draft microbial genomes  detail  available  additional file  suppl method section  time  memory requirements  additional feature  krakenuniq come without  runtime penalty   limit additional memory requirements  fact due  code improvements krakenuniq often run faster  kraken particularly     read come  one species   test datasets  mean classification speed  million base pair per minute increase slightly     mbpm see additional file  table   factor   time need  summarize classification result  krakenreport   require  kraken  part   classification binary  krakenuniq krakenuniq   average  faster  memory requirements increase  average           pathogen  patient data    case     read  either assign  human  synthetic read krakenuniq  significantly faster  kraken additional file  table   classification speed increase     mbpm  average wall time    lower   average additional memory requirements  less    go       time comparisons  make  preloading  database  run   parallel thread",1
31,LiveKraken,"LiveKraken--real-time metagenomic classification of illumina data
Originally, Kraken has a linear workflow (Wood and Salzberg, 2014). Sequencing reads are read from FASTA or FASTQ files and subsequently classified using a pre-computed database. Since the reads are independent of each other, they can be processed in parallel. The lowest common ancestor (LCA) classification results found for each read are written to Kraken’s tabular report file. To make this workflow fit for the purpose of live taxonomic classification, similar to the approach taken in HiLive (Lindner et al., 2017), a new sequence reader module was implemented which allows reading sequencing data from Illumina’s binary basecall (BCL) format. LiveKraken can be used to analyze continuously and refine the metagenomic sample composition, using the same database structure as the original Kraken. Illumina sequencers process all reads in parallel in so called cycles, appending one base to all reads per cycle. For each cycle, BCL files are produced in Illumina’s BaseCalls directory, which is declared as input for LiveKraken instead of FASTA or FASTQ files. New data is collected by the BCL sequencing reader module in user-specified intervals of j sequencing cycles, starting with the first k-mer of size k. The collected data is sent to the classifier which refines the stored partial classification with the new sequence information. Temporary data structures of Kraken are stored for each read, such as the LCA list, a list of ambiguous nucleotides and the number of k-mer occurrences in the database. This leads to an overall increase of memory consumption proportional to the number of LCAs found for each read sequence. Additionally, and crucial for the iterative refinement, a variable is stored that is holding the position up to which each read was classified. After each refinement step, output in the same tabular format as known from Kraken is produced. This enables early classification while also ensuring that the classification output after reading the data from the last sequencing cycle is exactly the same that Kraken would produce Raw parts of sequenced reads are streamed directly from the sequencer into Kraken’s classification algorithm. K-mers are taxonomically classified using Kraken’s pre-computed map of each k-mer to the lowest common ancestor of all genomes containing the k-mer, as color coded in the taxonomy tree. The highest scoring path from the pruned sub-tree of the taxonomic tree is selected as classification of each read (Wood and Salzberg 2014). (b) Results: In this example, 2 358 788 unmasked reads from SRR062462 were transferred back into raw MiSeq data format. Results are reported after 40, 80, 120, 160 and 200 sequencing cycles or approximately 12, 9, 6, 3 and 0 h on an Illumina MiSeq before the sequencer finishes and data can be prepared for other tools to start. The results are visualized in a Sankey diagram of read classifications on species level after all cycles are reported. The top five groups with the most hits are shown, while groups with fewer hits are conflated as ‘other’. Reads which cannot be assigned on species level are denoted as unclassified. The unclassified nodes are optically narrowed by approximately 1 500 000 reads each for better recognition of relevant groups. Thickness of the flows encodes the number of reads going from one node to another, where blue flows represent unchanged or new classifications and red ones show changed classifications. While the number of unclassified reads decreases, the overall proportions of taxa stay the same. Misclassifications occur in negligible magnitude. The visualization of results as an interactive sankey-plot is part of LiveKraken LiveKraken can be installed via the included script install_kraken.sh analogous to Kraken with an additional dependency to the boost library. It has been tested with gcc v. 4.9.2 and v. 7.2.0 and boost v. 1.5.8. Furthermore, a Conda package is available (Grüning et al., 2017). LiveKraken uses the same command line interface as Kraken. LiveKraken builds on the well-known tool Kraken. Hence, we show its results in comparison to the classic Kraken approach. While we guarantee identical results as Kraken with the end of a sequencing run, we also show that preliminary classifications allow a reliable estimate of the sample composition long before the sequencer has finished. We ran LiveKraken on three datasets from the NIH Human Microbiome Project (NIH HMP Working Group et al., 2009; c.f. Table 1), returning results after every 40th sequencing cycle or approximately 12, 9, 6, 3 and 0 h before the sequencer finished, respectively. As reference database we used all bacteria and archea sequences from RefSeq (O’Leary et al., 2016) downloaded on June 2nd 2015. We compared the results to the output of Kraken on the full datasets (Table 1). An example is visualized in Figure 1b, showing that the number of unclassified reads decreases over time, but only a minor number of reads is misclassified in earlier stages. While the peak memory requirements of LiveKraken increase by <1% compared to Kraken in our experiments, speed decreases by 15% (c.f. Supplementary Fig. S1). It is still orders of magnitude faster than the sequencer and therefore not the runtime bottleneck. Our results confirm the hypothesis that a classification is already possible long before classical metagenomic tools can even be started.",Classification,"livekrakenrealtime metagenomic classification  illumina data
originally kraken   linear workflow wood  salzberg  sequence read  read  fasta  fastq file  subsequently classify use  precomputed database since  read  independent       process  parallel  lowest common ancestor lca classification result find   read  write  krakens tabular report file  make  workflow fit   purpose  live taxonomic classification similar   approach take  hilive lindner     new sequence reader module  implement  allow read sequence data  illuminas binary basecall bcl format livekraken   use  analyze continuously  refine  metagenomic sample composition use   database structure   original kraken illumina sequencers process  read  parallel   call cycle append one base   read per cycle   cycle bcl file  produce  illuminas basecalls directory   declare  input  livekraken instead  fasta  fastq file new data  collect   bcl sequence reader module  userspecified intervals   sequencing cycle start   first kmer  size   collect data  send   classifier  refine  store partial classification   new sequence information temporary data structure  kraken  store   read    lca list  list  ambiguous nucleotides   number  kmer occurrences   database  lead   overall increase  memory consumption proportional   number  lcas find   read sequence additionally  crucial   iterative refinement  variable  store   hold  position     read  classify   refinement step output    tabular format  know  kraken  produce  enable early classification  also ensure   classification output  read  data   last sequence cycle  exactly    kraken would produce raw part  sequence read  stream directly   sequencer  krakens classification algorithm kmers  taxonomically classify use krakens precomputed map   kmer   lowest common ancestor   genomes contain  kmer  color cod   taxonomy tree  highest score path   prune subtree   taxonomic tree  select  classification   read wood  salzberg   result   example    unmask read  srr062462  transfer back  raw miseq data format result  report        sequence cycle  approximately          illumina miseq   sequencer finish  data   prepare   tool  start  result  visualize   sankey diagram  read classifications  species level   cycle  report  top five group    hit  show  group  fewer hit  conflate   read  cannot  assign  species level  denote  unclassified  unclassified nod  optically narrow  approximately    read   better recognition  relevant group thickness   flow encode  number  read go  one node  another  blue flow represent unchanged  new classifications  red ones show change classifications   number  unclassified read decrease  overall proportion  taxa stay   misclassifications occur  negligible magnitude  visualization  result   interactive sankeyplot  part  livekraken livekraken   instal via  include script install_krakensh analogous  kraken   additional dependency   boost library    test  gcc       boost   furthermore  conda package  available grüning    livekraken use   command line interface  kraken livekraken build   wellknown tool kraken hence  show  result  comparison   classic kraken approach   guarantee identical result  kraken   end   sequence run  also show  preliminary classifications allow  reliable estimate   sample composition long   sequencer  finish  run livekraken  three datasets   nih human microbiome project nih hmp work group     table  return result  every 40th sequence cycle  approximately          sequencer finish respectively  reference database  use  bacteria  archea sequence  refseq oleary    download  june 2nd   compare  result   output  kraken   full datasets table   example  visualize  figure  show   number  unclassified read decrease  time    minor number  read  misclassified  earlier stag   peak memory requirements  livekraken increase   compare  kraken   experiment speed decrease    supplementary fig    still order  magnitude faster   sequencer  therefore   runtime bottleneck  result confirm  hypothesis   classification  already possible long  classical metagenomic tool  even  start",1
32,Marvel,"MARVEL, a Tool for Prediction of Bacteriophage Sequences in Metagenomic Bins
Training and Testing Datasets To build and test MARVEL, the RefSeq microbial dataset was downloaded (January 2018) and only genomes belonging to the Bacteria domain (NCBI txid: 2) and to dsDNA viruses from the Caudovirales order (NCBI txid:28883) were selected (this is the baseline dataset). Tailed phages were selected at this step as a representative group given that they constitute the majority of viruses present in most environmental samples (Ashelford et al., 2003; Filée et al., 2005; Ackermann, 2007). The baseline dataset was split into two subsets according to the GenBank record date: before January 2016; and January 2016 and thereafter. This time-based division is usually applied in classifiers to simulate the use of the tool on newly isolated sequences (Roux et al., 2015; Ren et al., 2017). We refer to the before-2016 subset as the training dataset, and to the 2016-and-later subset as the testing dataset. The training dataset has 1,247 phage genomes and 1,029 bacterial genomes, and it was used to train and generate a model for prediction of phage bins. The testing dataset has 335 bacterial genomes and 177 phage genomes. Training and testing datasets have no overlap and are available in MARVEL’s repository page1. Training and testing datasets were further processed to generate mock datasets of contigs with specific lengths. For each fragment length analyzed in this study (2, 4, 8, 12, and 16 kbp), complete genomes were randomly fragmented in 10 contigs of the specified length that may or may not have overlap. Next, contigs belonging to the same organism were clustered to form a simulated bin. This process was performed for both training and testing sets, and the resulting bins were used to train the machine learning algorithm, to asses MARVEL’s performance, and to compare MARVEL against VirSorter and VirFinder. Feature Extraction and Classifier Development As previous studies have shown, genomic features such as DNA k-mer profiles and GC content can be strong signals in linking or differentiating genome sequences from bacteria and viruses (Edwards et al., 2016; Ren et al., 2017). However, it is known that phages try to mimic host genome sequences in order to overcome their defenses (Carbone, 2008; Bahir et al., 2009). This causes classifiers based on k-mer frequencies to have poor performance in terms of overall accuracy and especially recall. In other words, when one of these classifiers identifies a phage genome, it is almost always correct, but it is likely to miss a majority of new phages present in environmental samples. Seeking more robust features, we focused our efforts on characteristics related to genome structure and protein translational mechanisms of each organism. Such characteristics require a second layer of information, which may be added by utilization of results from gene prediction programs, such as Prodigal (Hyatt et al., 2010) and GeneMark (Besemer et al., 2001). Therefore, we evaluated phage and bacterial genomes according to six of these genomic features extracted from the baseline dataset of RefSeq complete genomes. These six features are: average gene length, average spacing between genes, density of genes, frequency of strand shifts between neighboring genes, ATG relative frequency, and fraction of genes with significant hits against the pVOGs database (Grazziotin et al., 2017). Average gene length was computed by adding up the length of all predicted CDSs in the genome or in the contigs in a bin (in bp) divided by the total number of predicted CDSs. Average spacing was calculated as the mean length in bp of regions between two CDSs. Density of genes was calculated as the total number of CDSs divided by genome length measured in kbp. Frequency of strand shifts was computed by adding up the number of strand shifts between neighboring genes, and dividing by the total number of CDSs in the genome. ATG relative frequency was computed by counting the number of ATG triplets in one of the strands, in all contigs in a bin or in the complete genome, divided by the total number of 3-mers in that sequence (one strand). Finally, each CDS in a genome was searched using HMMscan (Eddy, 2011) against the pVOGs database of viral HMM profiles (Grazziotin et al., 2017) (downloaded in January 2018); a significant hit was noted when the e-value was less than or equal to 10-10. The number of significant hits was divided by the total number of CDSs to generate the fraction of genes with significant hits against the pVOGs database. All values based on predicted CDSs were extracted from GenBank files as available for download in January 2018 (exploratory step) or predicted in simulated fragments by Prodigal (Hyatt et al., 2010) as driven by Prokka (Seemann, 2014). Using Python Scikit Learn libraries (Pedregosa et al., 2011), we tried different machine learning approaches based on the six features listed above. Specifically: support vector machine (SVM), logistic regression, neural networks, and random forest. Classifiers were evaluated using the training set as well as k-fold cross-validation (k = 20), with the result that random forest was the best approach for our target prediction. Similar findings about suitability of random forest classifiers in bioinformatics have also been reported (Boulesteix et al., 2012; Zhang et al., 2017). The relative weight of each feature on a given dataset was calculated by the ID3 implementation of random forest (Quinlan, 1986). Features with low gain of information were removed from the final model, in order to simplify feature extraction in the final version of the tool. The following features were selected as more informative: gene density, strand shifts, and fraction of genes with significant hits against pVOGs database (see section “Results”). We then extracted these three informative features from a complete training set of 8 kbp simulated bins, and a random forest classifier was trained to be MARVEL’s prediction core. The random forest model was trained with 50 initial tree estimators and leaf pruning; other parameters were set to their default values. Tests With Simulated Metagenomic Bins Simulated bins containing different fragment lengths were generated for genomes of the testing set as previously described to asses MARVEL’s performance. Each test corresponding to a specific fragment length was performed in five randomly sampled replicates of 150 bins (75 bacteria and 75 dsDNA phages). Bins were submitted to MARVEL and predictions were evaluated for true positive rates, specificity, accuracy, and F1 score according to the following standard formulae: TPR=TPTP+FNSPC=TNTN+FPACC=TP+TNTP+FP+TN+FNF1=2TP2TP+FP+FN Where: TPR = True positive rate, SPC = Specificity, ACC = Accuracy, TP = True positive count, FP = False positive count, TN = True negative count, FN = False negative count Tests With Real Metagenomic Data From Composting Samples A dataset of Illumina raw reads from composting samples generated by our group (Antunes et al., 2016) was used to test MARVEL’s performance in real metagenomic data. Five samples were extracted from a composting unit, and whole community DNA was extracted to generate shotgun metagenomic reads; this dataset contains mostly bacterial sequences. Raw reads for all five samples were cross-assembled with metaSpades (Nurk et al., 2017) generating a set of contigs. Metabat2 (Kang et al., 2015) was used for binning with parameters: -m 1500 -s 10000. Other parameters remained with their default values. Resulting bins were evaluated regarding quality and the presence of Bacterial and Archaeal marker genes using CheckM (Parks et al., 2015). Pipeline Implementation MARVEL was coded in Python 3 and uses Prokka (Seemann, 2014) and HMMscan (Eddy, 2011) as important dependencies. As input, MARVEL requires a directory with metagenomic bins in FASTA format; it generates a results directory containing bins predicted as phages. An auxiliary script was made available to generate bins from Illumina paired-end reads using standard tools and methods (Breitwieser et al., 2017). Performance Comparison of MARVEL, VirSorter, and VirFinder Each contig of a simulated bin (10 contigs in total) was individually given as input to VirSorter and VirFinder. For a given tool, an entire bin was considered to be a positive prediction in case at least one of its contigs were predicted as viral (note that in our experimental set-up, there are no bins with both bacterial and viral sequences). A contig was considered viral if predicted in categories I and II for VirSorter, and if the q-value was less than or equal to 0.01 for VirFinder. Tests were performed for different fragment lengths and in 30 randomly sampled replicates of 100 bins (50 bacteria and 50 dsDNA phages). Average values of true positive rate, specificity, and accuracy were compared using the Wilcoxon signed-rank test and were considered significant if the p-value was less than 0.001. Running time was measured for all tools using two sets of bins (100 bins averaging 40 kbp and 100 bins averaging 160 kbp) in a standard desktop computer with a 64-bit Intel Core i7-4770 3.4 GHz × 6 CPUs and 8 GB RAM DDR3, running Linux distribution Ubuntu 16.",Classification,"marvel  tool  prediction  bacteriophage sequence  metagenomic bins
training  test datasets  build  test marvel  refseq microbial dataset  download january    genomes belong   bacteria domain ncbi txid    dsdna viruses   caudovirales order ncbi txid  select    baseline dataset tail phages  select   step   representative group give   constitute  majority  viruses present   environmental sample ashelford    filée    ackermann   baseline dataset  split  two subsets accord   genbank record date  january   january   thereafter  timebased division  usually apply  classifiers  simulate  use   tool  newly isolate sequence roux    ren     refer    subset   train dataset    andlater subset   test dataset  train dataset   phage genomes   bacterial genomes    use  train  generate  model  prediction  phage bin  test dataset   bacterial genomes   phage genomes train  test datasets   overlap   available  marvel repository page1 train  test datasets   process  generate mock datasets  contigs  specific lengths   fragment length analyze   study       kbp complete genomes  randomly fragment   contigs   specify length  may  may   overlap next contigs belong    organism  cluster  form  simulate bin  process  perform   train  test set   result bin  use  train  machine learn algorithm  asses marvel performance   compare marvel  virsorter  virfinder feature extraction  classifier development  previous study  show genomic feature   dna kmer profile   content   strong signal  link  differentiate genome sequence  bacteria  viruses edwards    ren    however   know  phages try  mimic host genome sequence  order  overcome  defenses carbone  bahir     cause classifiers base  kmer frequencies   poor performance  term  overall accuracy  especially recall   word  one   classifiers identify  phage genome   almost always correct    likely  miss  majority  new phages present  environmental sample seek  robust feature  focus  efforts  characteristics relate  genome structure  protein translational mechanisms   organism  characteristics require  second layer  information  may  add  utilization  result  gene prediction program   prodigal hyatt     genemark besemer    therefore  evaluate phage  bacterial genomes accord  six   genomic feature extract   baseline dataset  refseq complete genomes  six feature  average gene length average space  genes density  genes frequency  strand shift  neighbor genes atg relative frequency  fraction  genes  significant hit   pvogs database grazziotin    average gene length  compute  add   length   predict cdss   genome    contigs   bin   divide   total number  predict cdss average space  calculate   mean length    regions  two cdss density  genes  calculate   total number  cdss divide  genome length measure  kbp frequency  strand shift  compute  add   number  strand shift  neighbor genes  divide   total number  cdss   genome atg relative frequency  compute  count  number  atg triplets  one   strand   contigs   bin    complete genome divide   total number  mers   sequence one strand finally  cds   genome  search use hmmscan eddy    pvogs database  viral hmm profile grazziotin    download  january   significant hit  note   evalue  less   equal    number  significant hit  divide   total number  cdss  generate  fraction  genes  significant hit   pvogs database  value base  predict cdss  extract  genbank file  available  download  january  exploratory step  predict  simulate fragment  prodigal hyatt     drive  prokka seemann  use python scikit learn libraries pedregosa     try different machine learn approach base   six feature list  specifically support vector machine svm logistic regression neural network  random forest classifiers  evaluate use  train set  well  kfold crossvalidation      result  random forest   best approach   target prediction similar find  suitability  random forest classifiers  bioinformatics  also  report boulesteix    zhang     relative weight   feature   give dataset  calculate   id3 implementation  random forest quinlan  feature  low gain  information  remove   final model  order  simplify feature extraction   final version   tool  follow feature  select   informative gene density strand shift  fraction  genes  significant hit  pvogs database see section “results”   extract  three informative feature   complete train set   kbp simulate bin   random forest classifier  train   marvel prediction core  random forest model  train   initial tree estimators  leaf prune  parameters  set   default value test  simulate metagenomic bin simulate bin contain different fragment lengths  generate  genomes   test set  previously describe  asses marvel performance  test correspond   specific fragment length  perform  five randomly sample replicate   bin  bacteria   dsdna phages bin  submit  marvel  predictions  evaluate  true positive rat specificity accuracy   score accord   follow standard formulae tprtptpfnspctntnfpacctptntpfptnfnf12tp2tpfpfn  tpr  true positive rate spc  specificity acc  accuracy   true positive count   false positive count   true negative count   false negative count test  real metagenomic data  compost sample  dataset  illumina raw read  compost sample generate   group antunes     use  test marvel performance  real metagenomic data five sample  extract   compost unit  whole community dna  extract  generate shotgun metagenomic read  dataset contain mostly bacterial sequence raw read   five sample  crossassembled  metaspades nurk    generate  set  contigs metabat2 kang     use  bin  parameters      parameters remain   default value result bin  evaluate regard quality   presence  bacterial  archaeal marker genes use checkm park    pipeline implementation marvel  cod  python   use prokka seemann   hmmscan eddy   important dependencies  input marvel require  directory  metagenomic bin  fasta format  generate  result directory contain bin predict  phages  auxiliary script  make available  generate bin  illumina pairedend read use standard tool  methods breitwieser    performance comparison  marvel virsorter  virfinder  contig   simulate bin  contigs  total  individually give  input  virsorter  virfinder   give tool  entire bin  consider    positive prediction  case  least one   contigs  predict  viral note    experimental setup    bin   bacterial  viral sequence  contig  consider viral  predict  categories     virsorter    qvalue  less   equal    virfinder test  perform  different fragment lengths    randomly sample replicate   bin  bacteria   dsdna phages average value  true positive rate specificity  accuracy  compare use  wilcoxon signedrank test   consider significant   pvalue  less   run time  measure   tool use two set  bin  bin average  kbp   bin average  kbp   standard desktop computer   bite intel core   ghz   cpus    ram ddr3 run linux distribution ubuntu ",1
33,SMART,"Scalable metagenomics alignment research tool (SMART): a scalable, rapid, and complete search heuristic for the classification of metagenomic sequences from complex sequence populations
Computational infrastructure The University of Washington provides a shared high-performance computing cluster known as Hyak. Currently UW Hyak has 9,028 Intel Xeon processing cores with 834 computational nodes. Each node used to test computational scaling contained 16 CPU cores with 64 GB of memory. Construction of database The v209 release of NCBI Genbank was downloaded (September 2015) and each Genbank accession was linked using the NCBI Taxonomy database to a single species and class. Using parallelization across 156 cores and a MapReduce framework, the genomic DNA was then virtually cut at every 30 basepairs, and each 30-mer was linked to the corresponding species and class and sorted. Finally merge sort was used to combine all the sorted 30-mers for classification. The dataset was then split into shards based on the first four basepairs of each 30mer creating a 256 separate databases that could be deterministically searched. The databases were saved in a hashtable format that could be loaded at runtime into memory by each search program. Description of search heuristic A total of 256 search programs are started asynchronously in parallel with each program assigned a 4 basepair shard as part of the mapping step. Each search program then iterates through the list of sequences in FASTA or FASTQ format and slides a 30 basepair window if the first 4 basepairs match the assigned shard definition of the executing program. The remaining 26 basepairs are then used to execute an in-memory hash-table lookup (Fig. 1). The reverse complement is also checked for every read. Each successful match to a species, genus, or class is kept and recorded. In addition, a 1-edit distance permutation algorithm was created to generate every possible one base-pair substitution permutation of the 30-mer search to account for sequencing errors and single nucleotide polymorphisms, without accounting for insertions or deletions. The results of each program are sequentially reduced to create the final classification results. Matching is performed at the species level and multiple matches against different organisms are collected. If any match is mammalian then the read is classified as mammalian; the highest voted match at the species, genus, and class taxonomy levels are calculated for each read for the final classification. If the highest classification for a read is a tie, then the read is labeled as ambiguous for a given taxonomy level. Datasets tested Simulated datasets (HiSeq, MiSeq, and simBA5) were taken from the publicly available datasets that were used to evaluate Kraken [5]. In a previous clinical trial of acute conjunctivitis/epidemic keratoconjunctivitis (NV-422 Phase IIB/III, NovaBay, clinicaltrials.gov: NCT01532336), a total 500 patients with clinical signs and symptoms of epidemic keratoconjunctivitis were recruited worldwide. Institutional review board approval was obtained through Goodwyn IRB (Cincinnati, OH, approval number: CL1104) Clinical research adhered to the tenets of the Declaration of Helsinki and was conducted in accordance with Health Insurance Portability and Accountability Act regulations. Written informed consent was obtained before participation for all participants in the study. Conjunctival samples from the upper/lower tarsal conjunctiva and fornix were collected using sterile dry swabs (Copan diagnostics inc., Murrieta, CA). Genomic DNA was isolated from conjunctival swabs using Qiagen Blood & Tissue DNA Kit (Qiagen, Inc., Venlo, the Netherlands) as per protocol. Three samples were randomly selected for whole genome sequencing (WGS). One nanogram of genomic DNA from each sample was used to prepare libraries for WGS according to the manufacturer’s instruction using Illumina Nextera XT Sample Prep Kit (Illumina, Inc, San Diego, CA). The DNA libraries were sequenced using MiSeq System following the manufacturer’s standard protocols (Illumina, Inc, San Diego, CA). Three conjunctival samples were used from this clinical trial collected from patients on the day of enrollment prior to the initiation of either placebo or the investigative drug. The FASTQ files for these samples have been uploaded to the NCBI SRA archive (SRR3033169, SRR3033245, and SRR3033274). Flash was used to preprocess the paired end libraries and Sickle was used for quality trimming [30]. In addition, data from the Human Microbiome Project [31] was downloaded as an additional metagenomic dataset. Specifically, three gut microbiome datasets (SRS019120, SRS014468 and SRS015055) were downloaded from the NCBI Sequence Read Archive, and Sickle again was applied prior to analysis of the samples. Evaluation of accuracy and speed To allow for direct comparison of performance statistics, the same definition of sensitivity and precision were used as described by Wood et al. [5]. Briefly sensitivity was defined as the number of correct classifications of reads divided by the total number in each dataset. Precision was defined as the number of correct classifications divided by the total number of reads attempted to be classified. Comparison of SMART to Kraken and CLARK In order to compare the accuracy and performance of the three tools, DNA sequence files from all the bacterial, viral, and archaeal sections of RefSeq were downloaded. For Kraken, CLARK, and SMART, the same sequences were used to build a database in each tool respectively following the documentation provided. The simulated datasets were then analyzed by each tool on the same computational node (16 CPU cores with 64 GB of RAM) in the UW Hyak with multithreading enabled to the maximum number of CPUs. For Kraken, the database was preloaded into memory for maximal performance as suggested by the creators of Kraken for users with NFS filesystems. For CLARK, the standard mode (−m 1) was used to analyze the simulated files as the program failed to start with other modes due to the RAM limitation. In order to calculate throughput, each program was run sequentially three times and the lowest execution time was utilized to calculate throughput.",Classification,"scalable metagenomics alignment research tool smart  scalable rapid  complete search heuristic   classification  metagenomic sequence  complex sequence populations
computational infrastructure  university  washington provide  share highperformance compute cluster know  hyak currently  hyak   intel xeon process core   computational nod  node use  test computational scale contain  cpu core     memory construction  database  v209 release  ncbi genbank  download september    genbank accession  link use  ncbi taxonomy database   single species  class use parallelization across  core   mapreduce framework  genomic dna   virtually cut  every  basepairs   mer  link   correspond species  class  sort finally merge sort  use  combine   sort mers  classification  dataset   split  shards base   first four basepairs   30mer create   separate databases  could  deterministically search  databases  save   hashtable format  could  load  runtime  memory   search program description  search heuristic  total   search program  start asynchronously  parallel   program assign   basepair shard  part   map step  search program  iterate   list  sequence  fasta  fastq format  slide   basepair window   first  basepairs match  assign shard definition   execute program  remain  basepairs   use  execute  inmemory hashtable lookup fig   reverse complement  also check  every read  successful match   species genus  class  keep  record  addition  edit distance permutation algorithm  create  generate every possible one basepair substitution permutation   mer search  account  sequence errors  single nucleotide polymorphisms without account  insertions  deletions  result   program  sequentially reduce  create  final classification result match  perform   species level  multiple match  different organisms  collect   match  mammalian   read  classify  mammalian  highest vote match   species genus  class taxonomy level  calculate   read   final classification   highest classification   read   tie   read  label  ambiguous   give taxonomy level datasets test simulate datasets hiseq miseq  simba5  take   publicly available datasets   use  evaluate kraken    previous clinical trial  acute conjunctivitisepidemic keratoconjunctivitis  phase iibiii novabay clinicaltrialsgov nct01532336  total  patients  clinical sign  symptoms  epidemic keratoconjunctivitis  recruit worldwide institutional review board approval  obtain  goodwyn irb cincinnati  approval number cl1104 clinical research adhere   tenets   declaration  helsinki   conduct  accordance  health insurance portability  accountability act regulations write inform consent  obtain  participation   participants   study conjunctival sample   upperlower tarsal conjunctiva  fornix  collect use sterile dry swab copan diagnostics inc murrieta  genomic dna  isolate  conjunctival swab use qiagen blood  tissue dna kit qiagen inc venlo  netherlands  per protocol three sample  randomly select  whole genome sequence wgs one nanogram  genomic dna   sample  use  prepare libraries  wgs accord   manufacturers instruction use illumina nextera  sample prep kit illumina inc san diego   dna libraries  sequence use miseq system follow  manufacturers standard protocols illumina inc san diego  three conjunctival sample  use   clinical trial collect  patients   day  enrollment prior   initiation  either placebo   investigative drug  fastq file   sample   upload   ncbi sra archive srr3033169 srr3033245  srr3033274 flash  use  preprocess  pair end libraries  sickle  use  quality trim   addition data   human microbiome project   download   additional metagenomic dataset specifically three gut microbiome datasets srs019120 srs014468  srs015055  download   ncbi sequence read archive  sickle   apply prior  analysis   sample evaluation  accuracy  speed  allow  direct comparison  performance statistics   definition  sensitivity  precision  use  describe  wood    briefly sensitivity  define   number  correct classifications  read divide   total number   dataset precision  define   number  correct classifications divide   total number  read attempt   classify comparison  smart  kraken  clark  order  compare  accuracy  performance   three tool dna sequence file    bacterial viral  archaeal section  refseq  download  kraken clark  smart   sequence  use  build  database   tool respectively follow  documentation provide  simulate datasets   analyze   tool    computational node  cpu core     ram    hyak  multithreading enable   maximum number  cpus  kraken  database  preloaded  memory  maximal performance  suggest   creators  kraken  users  nfs filesystems  clark  standard mode    use  analyze  simulate file   program fail  start   modes due   ram limitation  order  calculate throughput  program  run sequentially three time   lowest execution time  utilize  calculate throughput",1
34,Taxonomer,"Taxonomer: an interactive metagenomics analysis portal for universal pathogen detection and host mRNA expression profiling
Binner module Identifying small numbers of pathogen sequences hidden among vast numbers of host and/or microbiota-derived sequencing reads is a major algorithmic challenge for metagenomics-based pathogen detection tools. The standard approach is to use digital subtraction [64], whereby all sequencing reads are first aligned to the host’s genome sequence. This is the approach used by SURPI [32], for example. During subtraction, reads of host origin are removed. Additional subtraction steps may be used for removal of non-relevant microbial sequences, including those known to represent reagent contamination (e.g. [43, 62]) or sequencing adaptors. A greatly reduced number of presumably relevant microbial sequences are then classified by alignment to larger reference databases. Since only the remaining reads are matched with selected reference sequences, pathogens can be missed entirely if they are homologous to sequences in the subtraction database. Taxonomer overcomes this inherent limitation of digital subtraction by means of its “Binner” module (Fig. 1a), which compares each read to every reference database in parallel, assigning them to broad, non-exclusive taxonomic categories. Taxonomer’s Binner database is created by counting unique 21 bp k-mers in different taxonomic/gene datasets using Kanalyze [65] (version 0.9.7). Each taxonomic/gene dataset represents a “bin” in which query sequences can be placed based on their k-mer content. Each database is assigned a unique bit flag that allows k-mers that belong to one or more bins to be recognized and counted. The database bins and flags are shown in Additional file 1: Table S7. The k-mer counts are merged into a binary file that contains the k-mers and the database flag. This binary file shares a similar organization to our classification databases, and is organized to optimize query speed. Reads are then assigned to the taxonomic group(s) with which most k-mers are shared. Ties are resolved as shown in Additional file 1: Table S8 and results summarized for visualization (Additional file 1: Table S9). High binning accuracy is possible because of the minimal intersections (0.47 %) of k-mer content from comprehensive human and microbial reference databases (Additional file 1: Figure S1a and b). Optimal k-mer count cutoffs were determined by Youden’s indexes and F1 scores [66] and were in the range of 3–13 (Additional file 1: Table S10, default, n = 11). To eliminate binning of reads containing adapter sequence, by default, the binner ignores k-mers present in Illumina Tru-Seq adapters. A database of External RNA Controls Consortium (ERCC) control sequences allows quantification of ERCC spike-in controls. To demonstrate the advantage of Taxonomer’s non-greedy binning algorithm, we compared high-level taxonomic assignments made by SURPI, which employs greedy digital subtraction using sequence alignments by SNAP [67], to those of Taxonomer’s alignment-free Binner (Additional file 1: Figure S2). While high-level taxonomic assignments agree for 73.8 % of RNA-seq reads, Taxonomer assigned 16 % of reads an ambiguous origin (i.e. they match equally to multiple databases), 96 % of these were classified as human by SURPI. This was mostly due to highly conserved ribosomal and mitochondrial sequences (data not shown), but similar effects were also apparent for fungal sequences (18 % classified as human by SURPI). Taxonomer’s Binner was also able to capture more phage/viral sequences (7426) than the alignment-based method (5798), and resulted in fewer unclassified sequencing reads (3.2 % vs. 4.5 %). Consistent with lower abundance of rRNA and mtRNA sequences in DNA sequencing data, Taxonomer had many fewer ambiguous assignments (0.04 %, of which 40 % were classified as human and 59 % as viral by SURPI; overall agreement 98.7 %). Classifier module Classification in Taxonomer is based on exact k-mer matching. Taxonomer uses databases that are optimized for rapid k-mer queries that store every reference in which a k-mer is found as well as an associated k-mer weight for every reference. The fundamental question for classification is how likely it is that a particular k-mer (Ki) originates from any reference sequence, refi. To answer this question, Taxonomer calculates a k-mer weight: 𝐾𝑊𝑟𝑒𝑓𝑖(𝐾𝑖)=𝐶𝑟𝑒𝑓(𝐾𝑖)/𝐶𝑑𝑏(𝐾𝑖)𝐶𝑑𝑏(𝐾𝑖)/𝑇𝑜𝑡𝑎𝑙𝑘𝑚𝑒𝑟𝑐𝑜𝑢𝑛𝑡 Where C represents a function that returns the count of Ki. Cref(Ki) indicates the count of the Ki in a particular reference. Cdb(Ki) indicates the count of Ki in the database. This weight provides a relative, database specific measure of how likely it is that a k-mer originated from a particular reference. In order to classify a query sequence, we calculate the sum of the k-mer weights for every reference that has a matching k-mer in the query sequence. Suppose that there are N possible k-mers from query sequence Q. Then, for every reference, refi, that shares a k-mer with Q, the total k-mer weight for refi is: 𝑇𝐾𝑊(𝑟𝑒𝑓𝑖)=𝛴𝑁𝑗=1𝐾𝑊𝑟𝑒𝑓𝑖(𝐾𝑗) Each read is assigned to the reference that has the maximum total k-mer weight. In the case of a tie, the query sequence is assigned to the taxonomic lowest common ancestor (LCA) [30]. Protonomer module We developed a mapping scheme between amino acids and their corresponding codons to facilitate mapping in protein space while using the same strategies and speed we developed for classification in nucleotide space. When the amino acid database is built for classification, Taxonomer assigns every amino acid to just one codon. This unique mapping, which we term a non-degenerate translation, is used to generate an artificial DNA sequence that corresponds to the protein sequence in the database. This DNA sequence is entered into Taxonomer’s nucleotide classification databases. Query reads are translated into all six reading frames using the same non-degenerate translation scheme used to build the database and each translated frame is then classified. K-mer weighting and read classification assignment are performed as described above. The default Protonomer database is subsets of UniRef90 and UniRef50 (see “Databases” for details). Empirically, we found a k-mer size of 30 (10 amino acids) to perform best. We chose to classify viruses in protein space because of their high mutation rates, genetic variability, and incomplete reference databases [58]. Figure 3 presents benchmark data for Protonomer and two other rapid protein search tools, RAPSearch2 [36] (employed by SURPI) and DIAMOND [37] (an ultrafast, BLAST-like protein search tool), using RNA-seq data from respiratory samples of 24 children with documented viral infections as determined by an FDA-cleared molecular test (eSensor Respiratory Virus Panel, GenMark) or targeted PCR [40] (Additional file 1: Table S11), for which complete viral genomes could be manually constructed (Geneious, version 6.1). Viral reads were defined by mapping all reads binned as “Viral” or “Unknown” to the manually constructed viral genomes. Sensitivity and specificity were determined based on detection of known viral reads (true positives) and non-viral reads (true negatives). Protonomer provides a single taxonomic identifier per read as the classification assignment, which makes interpretation of results extremely simple. Neither RAPSearch2 nor DIAMOND classify a read, instead they only provide BLAST-like alignment information. For benchmarking against RAPSearch2 and DIAMOND, the LCA of the alignment with the lowest E-value was assigned as the classification. All tools were benchmarked using the viral subset of UniRef90 as their database. Both Protonomer and RAPSearch2 process paired reads by concatenating them together with a “-” between mate pairs. DIAMOND does not support paired-end reads, so each pair was searched separately, and the hit with the lowest e-value from each read was used to make the classification assignments. Afterburner To increase recovery of distantly homologous viral proteins, Taxonomer offers two options. First, unclassified reads can be further analyzed using the Afterburner module, a degenerate k-mer matching engine that employs a collapsed amino-acid alphabet (Additional file 1: Figure S14). In a manner similar to that employed by DIAMOND [37], we used k-means clustering on the BLOSUM62 matrix to generate a compressed amino acid alphabet. By using the collapsed amino acid alphabet, we are able to achieve higher sensitivity in classification with sequences that are more diverged at the expense of a higher false positive rate when compared with Protonomer (Additional file 1: Figure S14). Importantly, Taxonomer is not restricted to short reads, allowing re-analysis of resulting contigs for still greater classification sensitivity (Figs. 3 and 5). Host gene expression estimations Taxonomer also uses its nucleotide classifier to assign reads to host reference transcripts. By default, these are transcripts and corresponding gene models (GTF file) from the ENSMBL human reference sequence, GRCh37.75. Empirically, we found that a k-mer size of 25 worked best for mapping reads to human transcripts. We benchmarked Taxonomer’s gene expression estimates against Sailfish’s [38] and Cufflinks’ [39] using both biological and synthetic data. To generate the benchmark data shown in Fig. 4a, we ran Taxonomer in a standalone fashion. We had Taxonomer output all ties between transcripts during the classification step; we then randomly assigned a read to a single transcript. We used these transcript level assignments to calculate gene level expression. We next employed a linear regression to correct for transcript assignment bias in a similar fashion to Sailfish. The reported correlations were then calculated using these corrected values. This level of gene expression analysis is not currently available through the web interface because of the way data are streamed; however, the results given from the web interface are a very good approximation (Spearman correlation >0.93 on a set of genes that both methods have positives counts and Spearman correlation >0.75 when the gene set is unrestricted). In the first experiment, we employed qPCR results taken from the microarray quality control study (MAQC) [48]; specifically, human brain tissue samples (Additional file 1: Table S12). We also compared performance using synthetic RNA-seq reads (2 × 76 bp, n = 15,000,000) generated with the Flux Simulator tool [68]; see Additional file 1: Table S13 for parameters. TopHat [69] was used to produce alignments for Cufflinks. Like Taxonomer, Sailfish does not need external alignment information. Databases The Classifier and Protonomer databases are modular and easily constructed, consisting only of multi-fasta files with a “parent tag” on their definition lines. These tags describe each reference sequence’s immediate phylogenetic parent-taxon. Bacterial classification is based on a marker gene approach (16S rRNA gene) and the Greengenes database (reference set with operational taxonomic units, OTU, clustered at 99 %, version 13_8 [45, 70], Additional file 1: Table S7). This reference set contains 203,452 OTU clusters from 1,262,986 reference sequences. The taxonomic lineage for each OTU was used to create a hierarchical taxonomy map to represent OTU relationships. To support the OTU “species” concept, the taxonomy was completed for ranks in the taxonomic lineage that had no value. Unique dummy species names from the highest taxonomic rank available were used to fill empty values. Versions of the Greengenes database were formatted for use within BLAST, the RDP Classifier, and Kraken. Fungal classification is also based on a marker gene approach (internal transcribed spacer, ITS, rRNA sequences) and the UNITE database [60] (version sh_taxonomy_qiime_ver6_dynamic_s_09.02.2014, Additional file 1: Table S7). This reference set contains 45,674 taxa (species hypothesis, SH) generated from 376,803 reference sequences with a default-clustering threshold of 98.5 % and expert taxonomic curation. Dummy names were created for ranks that had no value. Versions of the unite database were formatted for use with BLAST, the RDP Classifier, and Kraken. Viral classification and discovery. The virus classification database consists of the viral subset of UniRef90 [71] (release 2014_06) combined with the bacterial subset of UniRef50 (release 2015_03). The viral protein database was reduced to 289,486 viral sequences based on NCBI taxonomy. Phage sequences were separated, leaving a total of 200,880 references for other viruses. NCBI taxonomy was used to determine the sequence relationship. For viral classification and discovery benchmarks shown in Fig 3a–c and for contig-level classification, only the viral subset of UniRef90 was used. Additional classification databases For testing purposes, additional bacterial classification databases were constructed from RefSeq (identical to Kraken’s full database; n = 210,627 total references; n = 5242 bacterial references, using NCBI taxonomy) and the complete ribosomal database project databases download on 24 September 2014 (n = 2,929,433 references, using RDP taxonomy). Database construction Databases are constructed to maximize query speed. K-mers are stored in lexicographical order and k-mer minimizers are used to point to blocks of k-mers in the database. Once a block of k-mers is isolated, a binary search is used to complete the query. This scheme provides extraordinary query speeds, as demonstrated by Wood and Salzberg [30]. We employ the same basic database layout as Kraken, with the important difference that instead of storing just the LCA of a k-mer, we also store the k-mer count and every reference (up to an adjustable cutoff) with associated k-mer weight. Detailed information about the database format and layout is available upon request. Gene classification protocols We extracted reference sequences from widely used, curated public databases for benchmark experiments [12]. These reference sequences were used to generate synthetic read datasets having a variety of read-lengths and error rates using wgsim (https://github.com/lh3/wgsim). PCR-amplified 16S rRNA gene sequences from two metagenomics studies on stool [47] and the home environment [46] were also used. The analysis was limited to taxa with relative abundance >0.1 % per sample (10 random samples were selected from each study). Bacterial 16S rRNA From the SILVA 119 non-redundant small-subunit ribosomal sequence reference database [12], we extracted bacterial reference sequences between 1200 and 1650 bp of length and excluded references annotated as cyanobacteria, mitochondria, and chloroplasts. Only high quality references without ambiguous bases, alignment quality values >50 %, and sequence quality >70 % were included. All the above values are reported by SILVA. Percent identity to the closest Greengenes OTU was determined by MegaBLAST [72] using hits with a query coverage >80 %. Synthetic reads (100 bp single-end, 100 bp paired-end, 250 paired-end) were generated from these reference sequences at 5× coverage. Fungal ITS To test the accuracy of identifying fungal ITS sequences that are not represented in the UNITE database [60], we utilized the UNITE_public_dataset (version_15.01.14). Percent identity to the closest UNITE species hypothesis (SH, OTUs clustered at 98.5 %) was determined by MegaBLAST using hits with a query coverage >80 %. Synthetic reads (250 bp single-end) were generated from these reference sequences at 5× coverage. Due to the variable length of ITS sequences (mean 585 bp, range 51–2995 bp, n = 376,803), paired-end sequences were not generated. Classification criteria for reference methods BLAST Default MegaBLAST parameters were used. Top scoring references were identified and used to assign OTUs/SHs. Multiple OTUs/SHs were assigned to synthetic reads when more than one OTU/SH reference shared 100 % identity. If no OTU/SH had 100 % identity to a read, then all OTUs within 0.5 % of the top hit were assigned to the read. The taxonomy of the assigned OTUs/SHs was compared and the highest rank in common was used to assign a taxonomic value to the read. The percent identity was used to determine the assignment of the highest taxonomic rank. Sequence reads with >97 % identity to a reference were assigned to species, >90 % identity to genus, and <90 % to family when lineage information was available at this rank. RDP classifier RDP classifier analyses were performed on a local server (see below). Classifications were resolved to the rank with a minimum confidence level of ≥0.5. Kraken Kraken analyses were performed on a local server (see below). Kraken reports the taxon identifier for each read’s final taxonomic assignment. An accessory script (Kraken-filter) can be used to apply confidence scores, although we found this value had little impact on results of our benchmarks. The effect of applying different confidence scores is shown in Additional file 1: Figure S6. SURPI SURPI analyses were performed using an Amazon EC2 instance through the published Amazon Machine Image. SURPI reports the best hit for its mapping tools (SNAP [67], RAPSearch2), which were used for comparison. Taxonomer implementation Taxonomer was written in C with Python bindings through Cython. An implementation of Taxonomer that contains the entire pipeline functionality was written in C and drives the iobio web interface. Server specifications Benchmarking was performed on a machine with Red Hat Linux, 1 TB of RAM, and 80 CPUs. Number of CPUs was restricted to 16 unless otherwise noted. Web-service and visualization Taxonomer is publically available as a web-service built upon the iobio framework [33]. It is available at taxonomer.iobio.io. Complex metagenomic data can be processed quickly and effectively interpreted through web-based visualizations. Figure 1b illustrates the interface. As reads are being streamed to the analysis server, a pie chart is presented summarizing the results of the binning procedure. When one of the bacterial, fungal, viral, or phage bins of the pie chart is selected, the results of the Classifier/Protonomer modules are displayed in a sunburst visualization. Additional information is provided at the top of the web page about how many reads were sampled, the number of reads classified, and the detection threshold. The detection threshold informs a user about how abundant a particular organism must be in order to be detected with the number of reads sampled. This provides an indicator of the sensitivity of detection in the sample. In addition, a slider allows the user to select an absolute cutoff for the minimum number of reads required in order to be displayed in the sunburst. DNA and RNA-seq of patient samples Nucleic acid extraction Samples (75–200 μL) were extracted using the QIAamp Viral RNA extraction kit (Qiagen). Extraction was carried out as described by the manufacturer with the exception of the AW1 washing step. For this step, 250 μL of AW1 wash buffer was added to the QIAamp Mini column before centrifugation at 8000 rpm. Then, 80 μL of DNase I mix (Qiagen) containing 10 μL of RNase-free DNase I and 70 μL of Buffer RDD was added to the column for on column DNase digestion. After incubation at room temperature for 15 min, an additional 250 μL of AW1 was added to the column before centrifugation at 8000 rpm. The manufacturer’s suggested protocol was continued at this point with column washing using Buffer AW2. After all washing steps, RNA was eluted in 60 μL of water. Extraction for total DNA was performed using 75–200 μL of sample with the DNeasy Blood and Tissue Kit (Qiagen) according to the manufacturer’s instructions. DNA was eluted in 200 μL of nuclease-free water. Depletion of human DNA Microbial DNA was enriched with NEBNext Microbiome DNA Enrichment Kit (NEB). Briefly, MBD2-Fc-bound magnetic beads were prepared by combining 3 μL of MBD2-Fc protein with 30 μL of Protein A Magnetic Beads per sample and placing the mixture in a rotating mixer for 10 min at room temperature before washing with 1× Binding Buffer. Extracted DNA (200 ng in 200 μL) was added to 50 μL 5× Binding Buffer. The resulting 250 uL were added to MBD2-Fc-bound magnetic beads for 15 min at room temperature with rotation. The enriched microbial DNA was cleaned-up with Agencourt AMPure XP Beads (Beckman Coulter). Library generation For HiSeq and MiSeq sequencing, indexed cDNA libraries were produced from extracted RNA using the TruSeq RNA Sample Prep Kit v2 (Illumina) omitting poly-A selection. RNA was dried and resuspended in 19.5 μL of Elute, Prime, Fragment Mix. The remainder of the library preparation was conducted per manufacturer’s instructions. Before library generation from DNA, enriched microbial DNA was fragmented with the Covaris S2 Ultrasonicator using intensity 5, duty cycle 10 %, and 200 cycles/burst for 80 s all at 7 °C. Libraries generated from fragmented enriched microbial DNA were prepared using the KAPA Hyper Prep Kit (KAPA Biosystems) according to the manufacturer’s instructions. PCR cycles used for library amplification were dependent upon the amount of input DNA and 13 cycles were used for these experiments. Libraries were quantitated by qPCR using the KAPA SYBR FAST ABI Prism qPCR Kit (KAPA BioSciences) and the Applied Biosystems 7900HT Fast Real-Time PCR System (Applied Biosciences). Library size was determined with the Agilent High Sensitivity DNA Kit and Agilent 2100 Bioanalyzer. After pooling of indexed sequencing libraries, a second qPCR and bioanalyzer run was performed to estimate the final concentration before sequencing. For Ion Proton sequencing, indexed cDNA libraries were produced from extracted RNA using the SMARTer Universal Low Input RNA Kit (Clontech) with numbers of PCR cycles in the range of 10–15 based on RNA yield. Sequencing Pooled sequencing libraries were analyzed on a HiSeq 2500 (2 × 100 bp), MiSeq (2 × 250 bp, both Illumina), or Ion Proton (median read length 139 bp, Life Technologies) instruments according to manufacturers’ protocols. Statistical analyses For gene expression analyses, we report both the Pearson and Spearman correlations as was done before [38]. Correlation coefficients were calculated using the scipy library for python. The Pearson correlation of the log transformed gene expression estimates necessitates the removal of any genes whose estimated expression is 0. The log transform prevents outliers from dominating the correlation. We also report the Spearman correlation, for which the log transform is not as necessary since it is a correlation based on ranks. Thus the exclusion of genes with estimates of 0 can be avoided.",Classification,"taxonomer  interactive metagenomics analysis portal  universal pathogen detection  host mrna expression profiling
binner module identify small number  pathogen sequence hide among vast number  host andor microbiotaderived sequence read   major algorithmic challenge  metagenomicsbased pathogen detection tool  standard approach   use digital subtraction  whereby  sequence read  first align   host genome sequence    approach use  surpi   example  subtraction read  host origin  remove additional subtraction step may  use  removal  nonrelevant microbial sequence include  know  represent reagent contamination     sequence adaptors  greatly reduce number  presumably relevant microbial sequence   classify  alignment  larger reference databases since   remain read  match  select reference sequence pathogens   miss entirely    homologous  sequence   subtraction database taxonomer overcome  inherent limitation  digital subtraction  mean   “binner” module fig   compare  read  every reference database  parallel assign   broad nonexclusive taxonomic categories taxonomers binner database  create  count unique   kmers  different taxonomicgene datasets use kanalyze  version   taxonomicgene dataset represent  “bin”   query sequence   place base   kmer content  database  assign  unique bite flag  allow kmers  belong  one   bin   recognize  count  database bin  flag  show  additional file  table   kmer count  merge   binary file  contain  kmers   database flag  binary file share  similar organization   classification databases   organize  optimize query speed read   assign   taxonomic group    kmers  share tie  resolve  show  additional file  table   result summarize  visualization additional file  table  high bin accuracy  possible    minimal intersections    kmer content  comprehensive human  microbial reference databases additional file  figure s1a   optimal kmer count cutoffs  determine  youdens index   score      range   additional file  table s10 default     eliminate bin  read contain adapter sequence  default  binner ignore kmers present  illumina truseq adapters  database  external rna control consortium ercc control sequence allow quantification  ercc spikein control  demonstrate  advantage  taxonomers nongreedy bin algorithm  compare highlevel taxonomic assignments make  surpi  employ greedy digital subtraction use sequence alignments  snap     taxonomers alignmentfree binner additional file  figure   highlevel taxonomic assignments agree     rnaseq read taxonomer assign    read  ambiguous origin   match equally  multiple databases      classify  human  surpi   mostly due  highly conserve ribosomal  mitochondrial sequence data  show  similar effect  also apparent  fungal sequence   classify  human  surpi taxonomers binner  also able  capture  phageviral sequence    alignmentbased method   result  fewer unclassified sequence read      consistent  lower abundance  rrna  mtrna sequence  dna sequence data taxonomer  many fewer ambiguous assignments        classify  human     viral  surpi overall agreement   classifier module classification  taxonomer  base  exact kmer match taxonomer use databases   optimize  rapid kmer query  store every reference    kmer  find  well   associate kmer weight  every reference  fundamental question  classification   likely     particular kmer  originate   reference sequence refi  answer  question taxonomer calculate  kmer weight 𝐾𝑊𝑟𝑒𝑓𝑖𝐾𝑖𝐶𝑟𝑒𝑓𝐾𝑖𝐶𝑑𝑏𝐾𝑖𝐶𝑑𝑏𝐾𝑖𝑇𝑜𝑡𝑎𝑙𝑘𝑚𝑒𝑟𝑐𝑜𝑢𝑛𝑡   represent  function  return  count   crefki indicate  count      particular reference cdbki indicate  count     database  weight provide  relative database specific measure   likely     kmer originate   particular reference  order  classify  query sequence  calculate  sum   kmer weight  every reference    match kmer   query sequence suppose     possible kmers  query sequence    every reference refi  share  kmer    total kmer weight  refi  𝑇𝐾𝑊𝑟𝑒𝑓𝑖𝛴𝑁𝑗1𝐾𝑊𝑟𝑒𝑓𝑖𝐾𝑗  read  assign   reference    maximum total kmer weight   case   tie  query sequence  assign   taxonomic lowest common ancestor lca  protonomer module  develop  map scheme  amino acids   correspond codons  facilitate map  protein space  use   strategies  speed  develop  classification  nucleotide space   amino acid database  build  classification taxonomer assign every amino acid   one codon  unique map   term  nondegenerate translation  use  generate  artificial dna sequence  correspond   protein sequence   database  dna sequence  enter  taxonomers nucleotide classification databases query read  translate   six read frame use   nondegenerate translation scheme use  build  database   translate frame   classify kmer weight  read classification assignment  perform  describe   default protonomer database  subsets  uniref90  uniref50 see “databases”  detail empirically  find  kmer size    amino acids  perform best  choose  classify viruses  protein space    high mutation rat genetic variability  incomplete reference databases  figure  present benchmark data  protonomer  two  rapid protein search tool rapsearch2  employ  surpi  diamond   ultrafast blastlike protein search tool use rnaseq data  respiratory sample   children  document viral infections  determine   fdacleared molecular test esensor respiratory virus panel genmark  target pcr  additional file  table s11   complete viral genomes could  manually construct geneious version  viral read  define  map  read bin  “viral”  “unknown”   manually construct viral genomes sensitivity  specificity  determine base  detection  know viral read true positives  nonviral read true negative protonomer provide  single taxonomic identifier per read   classification assignment  make interpretation  result extremely simple neither rapsearch2  diamond classify  read instead   provide blastlike alignment information  benchmarking  rapsearch2  diamond  lca   alignment   lowest evalue  assign   classification  tool  benchmarked use  viral subset  uniref90   database  protonomer  rapsearch2 process pair read  concatenate  together   “”  mate pair diamond   support pairedend read   pair  search separately   hit   lowest evalue   read  use  make  classification assignments afterburner  increase recovery  distantly homologous viral proteins taxonomer offer two options first unclassified read    analyze use  afterburner module  degenerate kmer match engine  employ  collapse aminoacid alphabet additional file  figure s14   manner similar   employ  diamond   use kmeans cluster   blosum62 matrix  generate  compress amino acid alphabet  use  collapse amino acid alphabet   able  achieve higher sensitivity  classification  sequence    diverge   expense   higher false positive rate  compare  protonomer additional file  figure s14 importantly taxonomer   restrict  short read allow reanalysis  result contigs  still greater classification sensitivity figs    host gene expression estimations taxonomer also use  nucleotide classifier  assign read  host reference transcripts  default   transcripts  correspond gene model gtf file   ensmbl human reference sequence grch37 empirically  find   kmer size   work best  map read  human transcripts  benchmarked taxonomers gene expression estimate  sailfishs   cufflinks  use  biological  synthetic data  generate  benchmark data show  fig   run taxonomer   standalone fashion   taxonomer output  tie  transcripts   classification step   randomly assign  read   single transcript  use  transcript level assignments  calculate gene level expression  next employ  linear regression  correct  transcript assignment bias   similar fashion  sailfish  report correlations   calculate use  correct value  level  gene expression analysis   currently available   web interface    way data  stream however  result give   web interface    good approximation spearman correlation    set  genes   methods  positives count  spearman correlation    gene set  unrestricted   first experiment  employ qpcr result take   microarray quality control study maqc  specifically human brain tissue sample additional file  table s12  also compare performance use synthetic rnaseq read        generate   flux simulator tool  see additional file  table s13  parameters tophat   use  produce alignments  cufflinks like taxonomer sailfish   need external alignment information databases  classifier  protonomer databases  modular  easily construct consist   multifasta file   “parent tag”   definition line  tag describe  reference sequence immediate phylogenetic parenttaxon bacterial classification  base   marker gene approach  rrna gene   greengenes database reference set  operational taxonomic units otu cluster    version 13_8   additional file  table   reference set contain  otu cluster   reference sequence  taxonomic lineage   otu  use  create  hierarchical taxonomy map  represent otu relationships  support  otu “species” concept  taxonomy  complete  rank   taxonomic lineage    value unique dummy species name   highest taxonomic rank available  use  fill empty value versions   greengenes database  format  use within blast  rdp classifier  kraken fungal classification  also base   marker gene approach internal transcribe spacer  rrna sequence   unite database  version sh_taxonomy_qiime_ver6_dynamic_s_09 additional file  table   reference set contain  taxa species hypothesis  generate   reference sequence   defaultclustering threshold     expert taxonomic curation dummy name  create  rank    value versions   unite database  format  use  blast  rdp classifier  kraken viral classification  discovery  virus classification database consist   viral subset  uniref90  release 2014_06 combine   bacterial subset  uniref50 release 2015_03  viral protein database  reduce   viral sequence base  ncbi taxonomy phage sequence  separate leave  total   reference   viruses ncbi taxonomy  use  determine  sequence relationship  viral classification  discovery benchmarks show  fig 3ac   contiglevel classification   viral subset  uniref90  use additional classification databases  test purpose additional bacterial classification databases  construct  refseq identical  krakens full database    total reference    bacterial reference use ncbi taxonomy   complete ribosomal database project databases download   september     reference use rdp taxonomy database construction databases  construct  maximize query speed kmers  store  lexicographical order  kmer minimizers  use  point  block  kmers   database   block  kmers  isolate  binary search  use  complete  query  scheme provide extraordinary query speed  demonstrate  wood  salzberg   employ   basic database layout  kraken   important difference  instead  store   lca   kmer  also store  kmer count  every reference    adjustable cutoff  associate kmer weight detail information   database format  layout  available upon request gene classification protocols  extract reference sequence  widely use curated public databases  benchmark experiment   reference sequence  use  generate synthetic read datasets   variety  readlengths  error rat use wgsim  pcramplified  rrna gene sequence  two metagenomics study  stool    home environment   also use  analysis  limit  taxa  relative abundance   per sample  random sample  select   study bacterial  rrna   silva  nonredundant smallsubunit ribosomal sequence reference database   extract bacterial reference sequence       length  exclude reference annotate  cyanobacteria mitochondria  chloroplasts  high quality reference without ambiguous base alignment quality value    sequence quality    include    value  report  silva percent identity   closest greengenes otu  determine  megablast  use hit   query coverage   synthetic read   singleend   pairedend  pairedend  generate   reference sequence   coverage fungal   test  accuracy  identify fungal  sequence    represent   unite database   utilize  unite_public_dataset version_15 percent identity   closest unite species hypothesis  otus cluster     determine  megablast use hit   query coverage   synthetic read   singleend  generate   reference sequence   coverage due   variable length   sequence mean   range      pairedend sequence   generate classification criteria  reference methods blast default megablast parameters  use top score reference  identify  use  assign otusshs multiple otusshs  assign  synthetic read    one otush reference share   identity   otush    identity   read   otus within     top hit  assign   read  taxonomy   assign otusshs  compare   highest rank  common  use  assign  taxonomic value   read  percent identity  use  determine  assignment   highest taxonomic rank sequence read    identity   reference  assign  species   identity  genus     family  lineage information  available   rank rdp classifier rdp classifier analyse  perform   local server see  classifications  resolve   rank   minimum confidence level  ≥ kraken kraken analyse  perform   local server see  kraken report  taxon identifier   read final taxonomic assignment  accessory script krakenfilter   use  apply confidence score although  find  value  little impact  result   benchmarks  effect  apply different confidence score  show  additional file  figure  surpi surpi analyse  perform use  amazon ec2 instance   publish amazon machine image surpi report  best hit   map tool snap  rapsearch2   use  comparison taxonomer implementation taxonomer  write    python bind  cython  implementation  taxonomer  contain  entire pipeline functionality  write    drive  iobio web interface server specifications benchmarking  perform   machine  red hat linux    ram   cpus number  cpus  restrict   unless otherwise note webservice  visualization taxonomer  publically available   webservice build upon  iobio framework    available  taxonomeriobioio complex metagenomic data   process quickly  effectively interpret  webbased visualizations figure  illustrate  interface  read   stream   analysis server  pie chart  present summarize  result   bin procedure  one   bacterial fungal viral  phage bin   pie chart  select  result   classifierprotonomer modules  display   sunburst visualization additional information  provide   top   web page   many read  sample  number  read classify   detection threshold  detection threshold inform  user   abundant  particular organism must   order   detect   number  read sample  provide  indicator   sensitivity  detection   sample  addition  slider allow  user  select  absolute cutoff   minimum number  read require  order   display   sunburst dna  rnaseq  patient sample nucleic acid extraction sample    extract use  qiaamp viral rna extraction kit qiagen extraction  carry   describe   manufacturer   exception   aw1 wash step   step    aw1 wash buffer  add   qiaamp mini column  centrifugation   rpm     dnase  mix qiagen contain    rnasefree dnase      buffer rdd  add   column   column dnase digestion  incubation  room temperature   min  additional    aw1  add   column  centrifugation   rpm  manufacturers suggest protocol  continue   point  column wash use buffer aw2   wash step rna  elute     water extraction  total dna  perform use    sample   dneasy blood  tissue kit qiagen accord   manufacturers instructions dna  elute     nucleasefree water depletion  human dna microbial dna  enrich  nebnext microbiome dna enrichment kit neb briefly mbd2fcbound magnetic bead  prepare  combine    mbd2fc protein     protein  magnetic bead per sample  place  mixture   rotate mixer   min  room temperature  wash   bind buffer extract dna       add     bind buffer  result    add  mbd2fcbound magnetic bead   min  room temperature  rotation  enrich microbial dna  cleanedup  agencourt ampure  bead beckman coulter library generation  hiseq  miseq sequence index cdna libraries  produce  extract rna use  truseq rna sample prep kit  illumina omit polya selection rna  dry  resuspend     elute prime fragment mix  remainder   library preparation  conduct per manufacturers instructions  library generation  dna enrich microbial dna  fragment   covaris  ultrasonicator use intensity  duty cycle     cyclesburst       ° libraries generate  fragment enrich microbial dna  prepare use  kapa hyper prep kit kapa biosystems accord   manufacturers instructions pcr cycle use  library amplification  dependent upon  amount  input dna   cycle  use   experiment libraries  quantitated  qpcr use  kapa sybr fast abi prism qpcr kit kapa biosciences   apply biosystems 7900ht fast realtime pcr system apply biosciences library size  determine   agilent high sensitivity dna kit  agilent  bioanalyzer  pool  index sequence libraries  second qpcr  bioanalyzer run  perform  estimate  final concentration  sequence  ion proton sequence index cdna libraries  produce  extract rna use  smarter universal low input rna kit clontech  number  pcr cycle   range   base  rna yield sequence pool sequence libraries  analyze   hiseq      miseq      illumina  ion proton median read length   life technologies instrument accord  manufacturers protocols statistical analyse  gene expression analyse  report   pearson  spearman correlations      correlation coefficients  calculate use  scipy library  python  pearson correlation   log transform gene expression estimate necessitate  removal   genes whose estimate expression    log transform prevent outliers  dominate  correlation  also report  spearman correlation    log transform    necessary since    correlation base  rank thus  exclusion  genes  estimate     avoid",1
35,RNA-Code,"RNA-CODE: A Noncoding RNA Classification Tool for Short Reads in NGS Data Lacking Reference Genomes
We propose a method that combines homology search and family-specific de novo assembly to identify reads sequenced from ncRNAs. In particular, the homology search is applied to both the short reads and contigs produced by assembly programs. This method is designed based on two key observations. First, reads sequenced from ncRNAs tend to share higher sequence and structural similarity with the their native families than reads sequenced from other families. Thus, higher alignment scores by ncRNA homology search tools are expected. In particular, homology search is vital for identifying ncRNAs that go through cleavage and degradation. Reads sequenced from miRNAs are hard to assemble because only reads corresponding to mature miRNAs can be largely captured into RNA-seq data. None or a few can be mapped to other regions of the pre-miRNA due to fast degradation. Figure 1 shows the mapping results of reads sequenced from pre-miRNAs obtained from Arabidopsis. No contig or very short contigs can be produced based on the typical read mapping pattern. In addition, this read mapping pattern does not change with increase of expression levels, as shown in the three miRNAs in Figure 1. For these types of ncRNAs, applying homology search on short reads directly is indispensable. While homology search is important, applying it to short and fragmentary reads may introduce high false positive rate when detecting remote ncRNA homologs (data will be shown in Methods Section). Thus RNA-CODE employs the second observation that true ncRNA reads sequenced from the same gene can be assembled into contigs with significantly high alignment scores against their native families. On the contrary, reads aligned by chance are not likely to be assembled because they tend to share poor overlaps. Both properties are important in boosting sensitivity and accuracy of short reads classification. RNA-CODE consists of three key stages. First, RNA-CODE coarsely classifies reads into different ncRNA families using both secondary structure and sequence similarity. Then, a familyspecific sequence assembly is used to assemble aligned reads into contigs. Because the numbers of reads that are coarsely classified in the first step indicate the expression levels or abundance of ncRNA genes in this family, this step chooses de novo assembly parameters (such as kmer size or overlap threshold) accordingly. The produced contigs are generally longer than input reads and thus can be classified into ncRNA families with better sensitivity and accuracy in the last step. For miRNAs which cannot be assembled into contigs, we use their biogenesis-based property and homology search results for classification. The three-stage workflow with chosen tool for each stage is illustrated in Figure 2. Here, we highlight the rationale behind the design of the three stages. The first stage aims to classify a large number of input reads into different ncRNA families with high sensitivity. It employs existing homology search tools. For short and fragmentary reads, this stage can incur high FP rate. Thus, downstream analysis is needed to remove those falsely classified reads. In the second stage, de novo sequence assembly tools are employed to assembly classified reads into contigs. The familyspecific sequence assembly is expected to produce contigs corresponding to complete or partial ncRNA genes. However, because of the extremely uneven or low transcriptional levels of many types of ncRNAs or low abundance, a small overlap cutoff or kmer is needed to ensure appropriate connectivity for some families. As a result, some contigs are chimeric or simply consist of randomly aligned reads. The third stage is used to remove the false positives. All contigs are aligned to ncRNA families. Only ones with scores or lengths above given cutoffs are kept. For miRNAs that cannot form contigs, we use stringent homology search scores and known biogenesis-related properties as classification criteria. Every stage will be described in great detail below. Stage 1: SCFG-based filtration To maximize classification sensitivity, short reads are aligned to an SCFG model built from an RNA family of interest. An SCFG describes not only primary sequence of an RNA family but also its secondary structure formed by base pair interaction. The state-ofthe-art implementation of SCFG model is Covariance Model (CM). The software suite Infernal [11] builds a CM on a family of RNA sequences, and searches for homologs using inside-outside algorithms. A CM in Infernal is implemented as a tree-like structure in which each node models a single base or a base pair. Infernal is able to optimally align a sequence to this tree with the highest possible score. Short reads, however, pose challenges to the search algorithms because they are fragmentary sequences in which nucleotides expected to form base pairs could be missing. Due to missing bases, base pairs that could have been aligned to a base-pair node in a parse tree are not alignable any more. As a result, reads sequenced from this family may not be well aligned to the underlying CM. Truncated-CYK (trCYK) [14] is a specialized tool designed for fragmentary sequence search. It performs local RNA alignment against a CM of interest, recovering base pairs that are possibly missing and would otherwise be base paired. For every alignment, a score is provided by trCYK indicating the goodness of alignment. Homologous reads tend to yield higher scores and longer alignments than random reads. Here we report the performance comparison of two homology search tools that can be applied to short and fragmentary reads. One is the mostly commonly used homology search tool BLAST [19], which relies on sequence similarity only. The second tool is trCYK [14]. The goal is to compare the performance of trCYK with BLAST in classifying ncRNA reads of different lengths. Thus, for read length 25, 30 and 50, we sampled 5000 true reads from tRNA sequences obtained from Rfam. Another 5000 random reads generated from other RNA families were mixed with true tRNA reads. Seed sequences from Rfam were excluded from the test data. Covariance Model used in trCYK and formatted database used in BLAST were both built from seed sequences of tRNA. We then searched for tRNA reads in the mixed reads using trCYK and BLAST. The performance of both tools is visualized in the ROC curves in Figure 3. The figure demonstrates that trCYK has better performance than BLAST. However, both tools have high FP rates, showing the need for further screening. Like all alignment programs, a score cutoff is needed to distinguish homologous sequences from others. We set two cutoffs s and l, on alignment score and alignment length, respectively. s and l determine the strength of filtration. A low cutoff will lead to an overwhelming number of negative reads which could significantly slow down the next two stages. A high cutoff, however, will exclude remote homologous reads with poor conservation from further analysis. As trCYK does not provide such thresholds, we considered two strategies to determine the cutoffs. First, the expected alignment score for a homologous sequence of length L can be used as the cutoff. In order to ensure high sensitivity, the actual cutoff can be smaller than the expected score. Second, Monte-Carlo method can be used to evaluate the sensitivity and FP rate of a score cutoff using a large number of sequences that are generated from both ncRNAs and nonncRNAs. In this work, we used the second strategy. Figure 3 shows that some very short homologous reads have poor alignment scores. As the first stage defines the upper-bound of the classification sensitivity, we chose a loose cutoff s = 1 to guarantee that most positive reads can pass the filtration stage. We found that this threshold also applies to reads sequenced from other types of ncRNAs. With the increase of read length, this score threshold needs to be improved as well. As the first stage is designed to achieve high sensitivity, the default cutoff is set to 1. To further increase the sensitivity of filtration, we also accept reads with alignment score s greater than -1 and with alignment length l§30 bases. Stage 2: family-specific de novo assembly For reads that are coarsely classified to a family by the first stage, they will be input to de novo assembly tools. Compared to conducting de novo assembly on all the reads, the input sizes to assembly tools are significantly reduced. Thus, even memory intensive assembly tools can be applied. Multiple de novo assembly tools exist. Depending on the data properties, such as read length and sequencing error rates, sensible choices can be made. In this work, the de novo assembly programs are applied to RNA-seq data of non-model organisms or metagenomic data. Thus, specific properties of these two data should be considered when choosing assembly tools. Unlike genome assembly, highly diverse sequencing coverage is expected in both data sets. In RNA-seq data, heterogeneous expression levels of ncRNAs contribute to highly diverse sequencing coverage. In metagenomic data, different abundance of ncRNA genes lead to different sequencing coverage. Choosing one set of parameters (such as overlap threshold in overlap graph or kmer size in de Bruijn graph) for the whole data set is not likely to produce optimal results for downstream ncRNA analysis. Thus, the first requirement for the chosen assembly program is that users can adjust the parameters according to the output of the filtration stage. Specifically, although the first stage only coarsely classifies reads into gene families, it can be used to estimate the expression levels or abundance of genes in a family. For families with large number of reads classified, RNA-CODE assumes high sequencing coverage and thus uses stringent assembly parameters. On the other hand, for families with fewer number of classified reads, small overlap cutoffs (in an overlap graph-based assembly tools) or small kmers (in de Bruijn graph) should be used to ensure connectivity for lowly transcribed regions or low abundance genes. Second, many assembly programs removed kmers with low coverage as they may contain sequencing errors. In order to assembly ncRNAs of low expression or abundance, we use an assembly tool that can keep reads/kmers with low coverage. In this work, for very short reads (read length v50 bp), we applied and compared several assembly programs [20,21] and chose SSAKE [22] because it delivers better assembly performance on our experimental data. SSAKE is a specialized de novo assembly tool for unpaired short reads assembly. It is a graphbased greedy assembler that efficiently assembles millions of short reads following near-linear time complexity. During the assembly process, the 39 end of a contig is extended if its suffix overlaps with the prefix of another read. SSAKE generates contigs progressively by searching all prefixes stored in a hash table for the longest possible prefix-suffix match whose overlap is above a threshold. We modified the codes of SSAKE to make it accept any overlap cutoff. To follow standard notation for assembly algorithms, we use k to represent either the kmer size in De Bruijn graph or the overlap threshold in an overlap graph for assembly. The length of overlap threshold k is an important parameter in SSAKE. A higher k usually results in fewer but more accurate contigs. A lower k leads to higher contiguity. But incorrect extension may happen because the probability of a random prefix-suffix match is high. In de novo assembly, there does not exist optimal overlap threshold. Although the expression or abundance is not known a prior, we estimate it using the number of classified reads in the first stage and choose k accordingly. In addition, it is observed that using a single-k will lead to suboptimal performance of de novo assemblers. In the second stage, a range of k’s are chosen and used on short reads assembly. For each k, all reads are used in assembly. The contigs from different assemblies are subsequently pooled together for further analysis. Stage 3: contig selection Some randomly aligned reads in stage 1 can be removed by stage 2 because they are not part of any contig. However, due to the loose overlap cutoff or small kmers, some reads can still be assembled into contigs and thus produced by stage 2. There are three types of contigs with reference to an ncRNA gene family: 1) positive contigs that are assembled by reads originated from this family, 2) negative contigs that are assembled from false reads that are not part of the underlying gene family, and 3) chimeric contigs that are formed by both true and false reads. Negative and chimeric contigs can be formed due to small overlaps we allowed in the multiple-k assembly. The probability that a contig is extended with a negative read due to a random prefix-suffix match is high for a small overlap cutoff. Sketches of the three types of contigs are presented in Figure 4. To distinguish positive contigs from negative ones, we align contigs to the underlying CM. We chose to use trCYK for the following reasons. 1) Both sequence and structural information of a contig should be utilized. 2) Many contigs may not be complete RNA genes especially when the gene transcription level is low. Thus we need to consider missing bases while aligning contigs to the underlying CM. After trCYK is applied to all contigs from stage 2, if there exist contigs with alignment scores greater than a pre-determined cutoff, the gene of interest is considered to be transcribed. As trCYK is a local alignment tool, it is common that only part of the contig is aligned to the underlying CM. Thus only reads that assemble the aligned part are classified into this RNA family. This feature could be very effective when multiple correct segments exist in a chimeric contig, although we did not observe such cases in our experiment. Bad segments interleaved by correct ones could potentially be removed. MIRNA families Normally, all classified reads need to pass through the entire pipeline. But for miRNA families, as no contigs might be formed, we used two criteria for read classification. First, the alignment score and length of the trCYK alignments in the first stage must pass the pre-determined threshold. Second, for all reads that align to the miRNA* region, we examined whether they can form a stem with reads aligned to the mature miRNA region. If not, the reads aligning to miRNA* region will be removed. As a result, many reads that cannot form any contig can be still classified into miRNAs based only on the homology search results in the first stage.",Classification,"rnacode  noncoding rna classification tool  short read  ngs data lack reference genomes
 propose  method  combine homology search  familyspecific  novo assembly  identify read sequence  ncrnas  particular  homology search  apply    short read  contigs produce  assembly program  method  design base  two key observations first read sequence  ncrnas tend  share higher sequence  structural similarity    native families  read sequence   families thus higher alignment score  ncrna homology search tool  expect  particular homology search  vital  identify ncrnas    cleavage  degradation read sequence  mirnas  hard  assemble   read correspond  mature mirnas   largely capture  rnaseq data none      map   regions   premirna due  fast degradation figure  show  map result  read sequence  premirnas obtain  arabidopsis  contig   short contigs   produce base   typical read map pattern  addition  read map pattern   change  increase  expression level  show   three mirnas  figure    type  ncrnas apply homology search  short read directly  indispensable  homology search  important apply   short  fragmentary read may introduce high false positive rate  detect remote ncrna homologs data   show  methods section thus rnacode employ  second observation  true ncrna read sequence    gene   assemble  contigs  significantly high alignment score   native families   contrary read align  chance   likely   assemble   tend  share poor overlap  properties  important  boost sensitivity  accuracy  short read classification rnacode consist  three key stag first rnacode coarsely classify read  different ncrna families use  secondary structure  sequence similarity   familyspecific sequence assembly  use  assemble align read  contigs   number  read   coarsely classify   first step indicate  expression level  abundance  ncrna genes   family  step choose  novo assembly parameters   kmer size  overlap threshold accordingly  produce contigs  generally longer  input read  thus   classify  ncrna families  better sensitivity  accuracy   last step  mirnas  cannot  assemble  contigs  use  biogenesisbased property  homology search result  classification  threestage workflow  choose tool   stage  illustrate  figure    highlight  rationale behind  design   three stag  first stage aim  classify  large number  input read  different ncrna families  high sensitivity  employ exist homology search tool  short  fragmentary read  stage  incur high  rate thus downstream analysis  need  remove  falsely classify read   second stage  novo sequence assembly tool  employ  assembly classify read  contigs  familyspecific sequence assembly  expect  produce contigs correspond  complete  partial ncrna genes however    extremely uneven  low transcriptional level  many type  ncrnas  low abundance  small overlap cutoff  kmer  need  ensure appropriate connectivity   families   result  contigs  chimeric  simply consist  randomly align read  third stage  use  remove  false positives  contigs  align  ncrna families  ones  score  lengths  give cutoffs  keep  mirnas  cannot form contigs  use stringent homology search score  know biogenesisrelated properties  classification criteria every stage   describe  great detail  stage  scfgbased filtration  maximize classification sensitivity short read  align   scfg model build   rna family  interest  scfg describe   primary sequence   rna family  also  secondary structure form  base pair interaction  stateoftheart implementation  scfg model  covariance model   software suite infernal  build     family  rna sequence  search  homologs use insideoutside algorithms    infernal  implement   treelike structure    node model  single base   base pair infernal  able  optimally align  sequence   tree   highest possible score short read however pose challenge   search algorithms    fragmentary sequence   nucleotides expect  form base pair could  miss due  miss base base pair  could   align   basepair node   parse tree   alignable     result read sequence   family may   well align   underlie  truncatedcyk trcyk    specialize tool design  fragmentary sequence search  perform local rna alignment     interest recover base pair   possibly miss  would otherwise  base pair  every alignment  score  provide  trcyk indicate  goodness  alignment homologous read tend  yield higher score  longer alignments  random read   report  performance comparison  two homology search tool    apply  short  fragmentary read one   mostly commonly use homology search tool blast   rely  sequence similarity   second tool  trcyk   goal   compare  performance  trcyk  blast  classify ncrna read  different lengths thus  read length      sample  true read  trna sequence obtain  rfam another  random read generate   rna families  mix  true trna read seed sequence  rfam  exclude   test data covariance model use  trcyk  format database use  blast   build  seed sequence  trna   search  trna read   mix read use trcyk  blast  performance   tool  visualize   roc curve  figure   figure demonstrate  trcyk  better performance  blast however  tool  high  rat show  need   screen like  alignment program  score cutoff  need  distinguish homologous sequence  others  set two cutoffs     alignment score  alignment length respectively    determine  strength  filtration  low cutoff  lead   overwhelm number  negative read  could significantly slow   next two stag  high cutoff however  exclude remote homologous read  poor conservation   analysis  trcyk   provide  thresholds  consider two strategies  determine  cutoffs first  expect alignment score   homologous sequence  length    use   cutoff  order  ensure high sensitivity  actual cutoff   smaller   expect score second montecarlo method   use  evaluate  sensitivity   rate   score cutoff use  large number  sequence   generate   ncrnas  nonncrnas   work  use  second strategy figure  show    short homologous read  poor alignment score   first stage define  upperbound   classification sensitivity  choose  loose cutoff     guarantee   positive read  pass  filtration stage  find   threshold also apply  read sequence   type  ncrnas   increase  read length  score threshold need   improve  well   first stage  design  achieve high sensitivity  default cutoff  set     increase  sensitivity  filtration  also accept read  alignment score  greater     alignment length § base stage  familyspecific  novo assembly  read   coarsely classify   family   first stage    input   novo assembly tool compare  conduct  novo assembly    read  input size  assembly tool  significantly reduce thus even memory intensive assembly tool   apply multiple  novo assembly tool exist depend   data properties   read length  sequence error rat sensible choices   make   work   novo assembly program  apply  rnaseq data  nonmodel organisms  metagenomic data thus specific properties   two data   consider  choose assembly tool unlike genome assembly highly diverse sequence coverage  expect   data set  rnaseq data heterogeneous expression level  ncrnas contribute  highly diverse sequence coverage  metagenomic data different abundance  ncrna genes lead  different sequence coverage choose one set  parameters   overlap threshold  overlap graph  kmer size   bruijn graph   whole data set   likely  produce optimal result  downstream ncrna analysis thus  first requirement   choose assembly program   users  adjust  parameters accord   output   filtration stage specifically although  first stage  coarsely classify read  gene families    use  estimate  expression level  abundance  genes   family  families  large number  read classify rnacode assume high sequence coverage  thus use stringent assembly parameters    hand  families  fewer number  classify read small overlap cutoffs   overlap graphbased assembly tool  small kmers   bruijn graph   use  ensure connectivity  lowly transcribe regions  low abundance genes second many assembly program remove kmers  low coverage   may contain sequence errors  order  assembly ncrnas  low expression  abundance  use  assembly tool   keep readskmers  low coverage   work   short read read length v50   apply  compare several assembly program   choose ssake    deliver better assembly performance   experimental data ssake   specialize  novo assembly tool  unpaired short read assembly    graphbased greedy assembler  efficiently assemble millions  short read follow nearlinear time complexity   assembly process   end   contig  extend   suffix overlap   prefix  another read ssake generate contigs progressively  search  prefix store   hash table   longest possible prefixsuffix match whose overlap    threshold  modify  cod  ssake  make  accept  overlap cutoff  follow standard notation  assembly algorithms  use   represent either  kmer size   bruijn graph   overlap threshold   overlap graph  assembly  length  overlap threshold    important parameter  ssake  higher  usually result  fewer   accurate contigs  lower  lead  higher contiguity  incorrect extension may happen   probability   random prefixsuffix match  high   novo assembly    exist optimal overlap threshold although  expression  abundance   know  prior  estimate  use  number  classify read   first stage  choose  accordingly  addition   observe  use  singlek  lead  suboptimal performance   novo assemblers   second stage  range    choose  use  short read assembly     read  use  assembly  contigs  different assemblies  subsequently pool together   analysis stage  contig selection  randomly align read  stage    remove  stage      part   contig however due   loose overlap cutoff  small kmers  read  still  assemble  contigs  thus produce  stage    three type  contigs  reference   ncrna gene family  positive contigs   assemble  read originate   family  negative contigs   assemble  false read    part   underlie gene family   chimeric contigs   form   true  false read negative  chimeric contigs   form due  small overlap  allow   multiplek assembly  probability   contig  extend   negative read due   random prefixsuffix match  high   small overlap cutoff sketch   three type  contigs  present  figure   distinguish positive contigs  negative ones  align contigs   underlie   choose  use trcyk   follow reason   sequence  structural information   contig   utilize  many contigs may   complete rna genes especially   gene transcription level  low thus  need  consider miss base  align contigs   underlie   trcyk  apply   contigs  stage    exist contigs  alignment score greater   predetermine cutoff  gene  interest  consider   transcribe  trcyk   local alignment tool   common   part   contig  align   underlie  thus  read  assemble  align part  classify   rna family  feature could   effective  multiple correct segment exist   chimeric contig although    observe  case   experiment bad segment interleave  correct ones could potentially  remove mirna families normally  classify read need  pass   entire pipeline   mirna families   contigs might  form  use two criteria  read classification first  alignment score  length   trcyk alignments   first stage must pass  predetermine threshold second   read  align   mirna* region  examine whether   form  stem  read align   mature mirna region    read align  mirna* region   remove   result many read  cannot form  contig   still classify  mirnas base    homology search result   first stage",1
36,MetaOthello,"A novel data structure to support ultra-fast taxonomic classification of metagenomic sequences with k-mer signatures
2.1 k-mer taxon signatures A k-mer is a length k subsequence of genomic sequences; for any sequence of length L, there exist a maximum of L  k þ 1 possible k-mers. Metagenomic reference material consists of one or more complete reference genomes belonging to an organism. Increasingly sophisticated sequencing techniques have permitted discovery of distinct reference genomes for a single species of organism, thereby capturing genomic variations that are often important to the functionality of the microbial species. The number of genomes (whether draft or complete) available as metagenomic reference material increases with each new discovery. If we consider each dataset as a collection of k-mers, a given taxon can be described by the set of k-mers present in the reference sequences belonging to its taxonomic subtree. The problem of classifying a metagenomic read thus simplifies to the identification of the taxon that best matches the set of k-mers associated with the target read. When k is sufficiently large (e.g. kP20), the majority of k-mers are unique to the species carrying them. These species-specific k-mers may serve as signatures, directly implicating the appropriate taxonomic classification. However, a significant proportion of k-mers is present in multiple species, making them unique only to higher-ranking taxa. In this paper, we formalize the taxonomic specificity of a k-mer as the signature of a taxon: A k-mer is considered to be a signature of a taxon if (i) the k-mer does not appear in any genomic references belonging to ancestors or siblings of the target taxon, but only to sequences belonging to the taxon’s subtree, and (ii) the k-mer is not a signature of any lower-ranked taxon in the subtree. Equivalently, the taxon evincing a k-mer signature is the lowest common ancestor (LCA) of all species in the taxonomy whose reference genomes contain that k-mer (Ames et al., 2013; Tu et al., 2014). In this way, as illustrated in Figure 1A, the set of all k-mers present in the genomic references of a taxonomy can be divided into disjoint collections, each of which contains the set of signature k-mers belonging to a single node in the taxonomy tree. Formally, let S be the set of all k-mers present in genomic references annotated by the taxonomy and let T ¼ f1; 2; ... ; jTjg be the taxa (nodes) present in the taxonomy. Then S can be divided into jTj disjoint sets, S ¼ fS0; S1; ... ; St; ... ; Sð Þ jTj1 g, where for any node t 2 T, St corresponds to the set of k-mer signatures belonging to taxon t. Thus, there exists a mapping, g : S ! T, such that g(s) ¼ t if the k-mer, s 2 S, is a signature of the taxon, t 2 T. 2.2 k-mer classification with l-Othello The core data structure of MetaOthello is called l-Othello. l-Othello is essentially a hashing classifier, first designed for fast-forwarding information base queries (Yu et al., 2016). It is capable of classifying a key to the appropriate member of a large collection of categories with high efficiency in memory and speed. In our particular application, l-Othello supports the mapping between a k-mer signature and the corresponding taxon. 2 X.Liu et al. 2.2.1 Overview of the l-Othello data structure l-Othello maintains a query function between any given k-mer and a taxon: s : SU ! f0; 1; ... ; 2l  1g, where SU is the universal set of k-mers (i.e. SU ¼ rk; r ¼ fA; C; G; Tg). l is determined by the total number of taxa T, where l ¼ d log2jTje. The algorithm satisfies the following properties: (i) l-Othello is always able to retrieve the correct taxon ID corresponding to a valid k-mer signature; that is, for any s 2 S; sð Þ¼ s t, where t is the ID of the taxon to which s is specific. (ii) When provided an alien k-mer (i.e. a k-mer that does not appear in the reference sequences), l-Othello is able to recognize the alien with high success rate but may carry a slight risk of assigning it to a random taxon in the taxonomy. Figure 2 shows an example l-Othello structure. An l-Othello data structure works by maintaining a pair of hash functions, hha; hbi and two arrays of l-bit integers, A and B. ha : SU ! f0; 1; ... ma  1g and hb : SU ! f0; 1; ... ; mb  1g, where ma and mb are integer values determined during l-Othello construction. The relationships among the elements of A and B can be viewed as a bipartite graph G ¼ ð Þ U; V; E , where nodes in U and V correspond to elements in A and B respectively. A query of k-mer s on the graph yields a node index i ¼ hað Þs in U and a node index j ¼ hbð Þs in V. The classification of k-mer s is determined by the values at A h½  að Þs and B h½  bð Þs , via a  (bitwise XOR) operation: sð Þ¼ s A h½  að Þs  B h½  bð Þs : The bit-wise XOR operation of integers has the following property: For any l-bit integer x, x  x ¼ 0; x  0 ¼ x. When l-Othello is properly constructed, sð Þ¼ s t for any k-mer specific to taxon t, i.e. s 2 St. The success of l-Othello relies on assigning bitmap values on both sides of the bipartite graph, where the operation between two nodes can directly generate the class membership. Setting the bitmap of two nodes with no initial values is fairly simple and can be achieved in multiple ways. For example, when l ¼ 1 and assuming the membership of a key is 1, we can assign the bit values of the two nodes ui and vj as either A i½¼ 0; B j ½ ¼ 1 or A i½¼ 1;B j ½ ¼ 0. However, if the value of one node has already been determined by another key, then only the remaining value is altered. For example, if A i½  is already set as 0, given A i½   B j ½ ¼ 1, then B j ½ ¼ 1. In the worst-case scenario, both A i½  and B j ½  have already been determined by their involvement with other keys. This situation creates a cycle in the graph when edge ui and vj is added, possibly resulting in a conflict and failed assignment of a bit value. When a conflict arises, we have two options: remove the k-mer or select a different hash function. Theorem 1 shows that for a randomly selected pair of hash functions hha; hbi, the probability of G being cyclic is extremely low. In our experiment, fewer than 100 kmers among 6 billion were removed due to conflicts. Additionally, because multiple k-mers manifest in one read, losing one k-mer does not significantly affect the accuracy of the algorithm, so we may omit kmers whose inclusion would cause a conflict. THEOREM 1 Suppose ha, hb are randomly selected from a family of random hash functions such that ha : S ! f0; 1; ... ; ma  1g and hb : S ! f0; 1; ... ; mb  1g. Given a set of k-mers S ¼ S0 [ S1 [ S2l 1, let n ¼ jSj. Construct a bipartite graph G ¼ ð Þ U; V; E , where an edge ui; vj 2 E if and only if there is a k-mer s 2 S such that hað Þ¼ s i and hbð Þ¼ s j. Let v be the number of cycles in G. Then v converges to a Poisson distribution with parameter k ¼  1 2 ln 1  c2  , where c ¼ nffiffiffiffiffiffiffiffiffi mamb p . The proof of Theorem 1 can be found in Supplementary Section S1. We recommend the values of ma and mb as follows: Let ma and mb A B Fig. 1. Illustration of MetaOthello algorithm. (A) An example of taxonomy with reference sequences in the leaf nodes. 3-mers that are signatures to each taxon are highlighted in bold font with different shades. (B) A two-step approach to read classification Fig. 2. An example of l-Othello with l ¼ 3 classifying n ¼ 5 k-mers. Left: Bipartite graph G and corresponding bitmaps A and B. Each edge in G represents a k-mer. Hash functions ha and hb map the k-mer s into corresponding locations in A and B. Right: Query of s returns sðsÞ¼ð001Þ2 ¼ 1 MetaOthello 3 be powers of 2, where ma is the smallest value such that ma  1:33n, and mb is the smallest value such that mb  n. As a result, we have 2:67n  ma þ mb < 4n and 0:14 < k < 0:41. In practice, we can always expect the number of cycles in an l-Othello to be less than 2 with probability 97%, and smaller than 9 with probability higher than 99.999%. 2.2.2 Time and space complexity for l-Othello construction and query Construction Complexity: The construction of the l-Othello data structure generally follows a depth-first traversal of the bipartite graph; thus the complexity is O(n), where n is the total number of k-mers. The memory complexity is O(ln), where l is the number of bits to encode the number of categories. We can further reduce the memory cost by dividing the set of k-mers into smaller groups based on prefixes, commonly of length 3, corresponding to g ¼ 64 groups. Each group contains approximately n g k-mers. For each group, we build an l-Othello, using time O n g   with memory cost O n g  . In total, constructing g l-Othello structures still takes O(n) time. Query Complexity: For each query sð Þs , l-Othello computes two hash values hað Þs and hbð Þs , and accesses two memory locations A ½  hað Þs and B h½  bð Þs . The time complexity is O(1). This procedure only includes a few basic arithmetic operations, resulting in an extremely fast execution speed. 2.2.3 On alien k-mers An alien k-mer is defined as a k-mer that is not included during the construction of an l-Othello. In the context of taxonomic classification, they are those k-mers that are not included in any of the reference materials. Alien k-mers often arise due to noise or genomic sequences belonging to a novel species sampled by a sequence read. We have designed two strategies to detect alien k-mers. First, we would like to increase the randomness of assignments in the case of alien k-mers; secondly, we may add another bit in the bitmap (i.e. let l > log djTje), doubling the number of categories, so that an alien k-mer has a much higher chance of being assigned to alien categories (i.e., categories not used by existing taxon labels). Here we show how we may leverage the randomness of alien assignment to predict an alien k-mer within the l-Othello itself. We first discuss the query result for l ¼ 1, and then we extend it to l-Othello. When l ¼ 1, Othello classifies k-mers (s 2 S ¼ S0 [ S1) into S0 and S1. Each element in A or B is a 1-bit value. For a query of an alien k-mer s0 62 S, l-Othello still returns a value s s0 ð Þ2f0; 1g. For alien keys, s s0 ð Þ¼ A ha s0 ½  ð Þ  B hb s0 ½  ð Þ . Let a0 and a1 be the fraction of 0s and 1s in the bitmaps A respectively, i.e. a0 ¼ jftjA t½ ¼0gj ma ; a1 ¼ jftjA t½ ¼1gj ma . Similarly b0 and b1 are the fractions in B. Suppose ha and hb are uniformly distributed random hash functions, and s0 is an arbitrary k-mer in the universal set; then s s0 ð Þ returns 1 with probability p1 ¼ a0b1 þ a1b0. Similarly, s s0 ð Þ returns 0 with probability p0 ¼ a0b0 þ a1b1. For l-Othello, a similar property also holds. Let p(t) be the probability that the query of an alien k-mer returns exactly t. s s0 ð Þ¼ t indicates A ha s0 ½  ð Þ  B hb s0 ½ ¼ ð Þ t. Note that ha and hb are uniform random hash functions and are not correlated. Hence, p tð Þ ¼ P s s 0 ð Þ ð Þ ¼ t ¼ 2 Xl 1 x¼0 axbx  t where ax is the fraction of elements has value x in a and bx  t is the fraction of elements has value x  t in B. Given a particular l-Othello, we can always compute p(t) values for all t ¼ 0; 1; ... ; 2l  1 using time O 22l þ n  . These p(t) values are affected by the occurrence frequency of each l-bit integer, namely ax and bx for all 0  x < 2l . In some cases, these values are not uniformly distributed, which may result in imbalance among p(t). Under such circumstances, we can tune these values by flipping the bitmaps of a connected component in the bipartite graph without changing sð Þs for s 2 S. In practice, we can always tune the values so that p(t) is of the same order of magnitude for all t, and all of them are approximately 2l . Two tuning approaches are described in Supplementary Section S2. We can also explicitly detect alien k-mers by by increasing l, thus intentionally expanding the number of targeted categories, where the majority of them are dummy (alien). Due to the randomness of class assignment in the presence of an alien, many alien k-mers are likely to fall in these dummy categories, and are thus recognized as alien. Formally, if for some s; s s ð ÞjTj; s is an alien. Thus, alien k-mers are recognized in this stage with probability P2l 1 x¼jTj p xð Þ 2l jTj 2l . 2.3 Taxonomic classification of sequencing reads As illustrated in Figure 1B, given any sequencing read, our algorithm iterates over each k-mer from the beginning of the read and, for each k-mer, retrieves the taxon to which it is specific using l-Othello. Taxonomic classification of the read is determined by assembling the taxa for all k-mers in the read. The classification is straightforward when all k-mers indicate the same taxon, but this is not often the case. Disparate taxa are considered to be consistent if they belong to the same path in the taxonomy, meaning that one assignment is the higher rank of the other. When these taxa belong to different branches, they represent conflicting information. The issue is further complicated by the possibility of false taxonomic information returned from querying alien k-mers, where the k-mer in the read does not appear in any of the reference sequences. To tackle this challenge, we have designed a window-based classification approach. A window is defined as a sequence of consecutive k-mers that are assigned to the same taxon of a given level. The window-based approach guards against false-positive assignments due to alien k-mers. Assuming that the taxon ID returned by an alien k-mer is random, the chance of having two consecutive alien k-mers return the same taxon ID is 2 Xl 1 t¼0  p tð Þ2  2l : This value is very small, regardless of k. Additionally, each window corresponds to a maximum read subsequence that matches the reference sequences. Thus, the longer the window, the longer the subsequence match, and the less likely the match is random. In comparison, other algorithms such as Kraken and Clark count the total number of k-mer matches, regardless of their spatial distribution across the read. If multiple taxon windows are available, MetaOthello scores each of them using the summed squares of window sizes as in the following formula; the taxon with the maximum score will be selected: Score tðÞ¼ X wt i 2 ; where wt i denotes the number of k-mers in the ith window classified to taxon t. 4 X.Liu et al. A k-mer signature belonging to a taxon is also specific to its higher-ranking taxa, so at higher taxonomic ranks, there exist more k-mers to distinguish a taxon from its siblings. Thus, longer k-mer windows and more-accurate classifications are expected at higher taxonomic ranks. Under this assumption, a ‘top-down’ strategy is adopted during read classification. Given a read sequence, MetaOthello starts the classification at the top rank and continues the classification down the ranks until there does not exist a sufficiently large k-mer window supporting the level. Based on the k-mer distribution in each taxon, MetaOthello establishes a threshold on minimum window size when the classification on that taxon requires. Theorem 2 shows that the minimum window-size threshold can be precomputed for each taxon prior to read classification. The minimum window size required for a taxon is determined by the probability of an alien k-mer query on l-Othello returning a taxon rooted in t and the acceptable false-positive rate. The larger the size of the taxon subtree, the higher the probability that a random alien k-mer may match to t and thus the longer the window required for reliable classification. Additionally, a larger window size will be required in order to lower the false-positive rate. THEOREM 2. Given a user-defined false-positive rate k and the total read number M, the minimum window-size threshold required for a taxon t can be computed as logp tð Þ k ð Þ 1k M, where pt denotes the probability that an alien k-mer query on l-Othello returns a value in the taxon subtree with root t. The proof is presented in Supplementary Section S3. For example, when t is a genus-level node, supposing l ¼ 12, then p tðÞ ð Þ 1 þ 7 2l ¼ 1 256. Given 10 million reads and suppose k ¼ 0:001, then logp tð Þ k ð Þ 1k M ¼ 3:42, and only windows larger than three will be taken into consideration when determining the read assignment.",Classification," novel data structure  support ultrafast taxonomic classification  metagenomic sequence  kmer signatures
 kmer taxon signatures  kmer   length  subsequence  genomic sequence   sequence  length   exist  maximum       possible kmers metagenomic reference material consist  one   complete reference genomes belong   organism increasingly sophisticate sequence techniques  permit discovery  distinct reference genomes   single species  organism thereby capture genomic variations   often important   functionality   microbial species  number  genomes whether draft  complete available  metagenomic reference material increase   new discovery   consider  dataset   collection  kmers  give taxon   describe   set  kmers present   reference sequence belong   taxonomic subtree  problem  classify  metagenomic read thus simplify   identification   taxon  best match  set  kmers associate   target read    sufficiently large  kp20  majority  kmers  unique   species carry   speciesspecific kmers may serve  signatures directly implicate  appropriate taxonomic classification however  significant proportion  kmers  present  multiple species make  unique   higherranking taxa   paper  formalize  taxonomic specificity   kmer   signature   taxon  kmer  consider    signature   taxon    kmer   appear   genomic reference belong  ancestors  siblings   target taxon    sequence belong   taxons subtree    kmer    signature   lowerranked taxon   subtree equivalently  taxon evince  kmer signature   lowest common ancestor lca   species   taxonomy whose reference genomes contain  kmer ames          way  illustrate  figure   set   kmers present   genomic reference   taxonomy   divide  disjoint collections    contain  set  signature kmers belong   single node   taxonomy tree formally let    set   kmers present  genomic reference annotate   taxonomy  let       jtjg   taxa nod present   taxonomy     divide  jtj disjoint set   fs0         jtj1     node     correspond   set  kmer signatures belong  taxon  thus  exist  map             kmer      signature   taxon     kmer classification  lothello  core data structure  metaothello  call lothello lothello  essentially  hash classifier first design  fastforwarding information base query       capable  classify  key   appropriate member   large collection  categories  high efficiency  memory  speed   particular application lothello support  map   kmer signature   correspond taxon  xliu    overview   lothello data structure lothello maintain  query function   give kmer   taxon                universal set  kmers             determine   total number  taxa      log2jtje  algorithm satisfy  follow properties  lothello  always able  retrieve  correct taxon  correspond   valid kmer signature                   taxon     specific   provide  alien kmer   kmer    appear   reference sequence lothello  able  recognize  alien  high success rate  may carry  slight risk  assign    random taxon   taxonomy figure  show  example lothello structure  lothello data structure work  maintain  pair  hash function hha hbi  two array  lbit integers                               integer value determine  lothello construction  relationships among  elements       view   bipartite graph          nod     correspond  elements     respectively  query  kmer    graph yield  node index   hað      node index   hbð     classification  kmer   determine   value              via   bitwise xor operation                 bitwise xor operation  integers   follow property   lbit integer             lothello  properly construct       kmer specific  taxon       success  lothello rely  assign bitmap value   side   bipartite graph   operation  two nod  directly generate  class membership set  bitmap  two nod   initial value  fairly simple    achieve  multiple ways  example      assume  membership   key     assign  bite value   two nod     either  i½¼         i½¼      however   value  one node  already  determine  another key    remain value  alter  example      already set   give                  worstcase scenario           already  determine   involvement   key  situation create  cycle   graph  edge     add possibly result   conflict  fail assignment   bite value   conflict arise   two options remove  kmer  select  different hash function theorem  show    randomly select pair  hash function hha hbi  probability    cyclic  extremely low   experiment fewer   kmers among  billion  remove due  conflict additionally  multiple kmers manifest  one read lose one kmer   significantly affect  accuracy   algorithm   may omit kmers whose inclusion would cause  conflict theorem  suppose    randomly select   family  random hash function                          give  set  kmers       s2l  let   jsj construct  bipartite graph           edge            kmer      hað     hbð    let    number  cycle     converge   poisson distribution  parameter               nffiffiffiffiffiffiffiffiffi mamb    proof  theorem    find  supplementary section   recommend  value      follow let      fig  illustration  metaothello algorithm   example  taxonomy  reference sequence   leaf nod mers   signatures   taxon  highlight  bold font  different shade   twostep approach  read classification fig   example  lothello     classify    kmers leave bipartite graph   correspond bitmaps     edge   represent  kmer hash function    map  kmer   correspond locations     right query   return sðsþ¼ð001þ2   metaothello   power       smallest value          smallest value        result                 practice   always expect  number  cycle   lothello   less    probability   smaller    probability higher    time  space complexity  lothello construction  query construction complexity  construction   lothello data structure generally follow  depthfirst traversal   bipartite graph thus  complexity       total number  kmers  memory complexity  oln     number  bits  encode  number  categories    reduce  memory cost  divide  set  kmers  smaller group base  prefix commonly  length  correspond     group  group contain approximately   kmers   group  build  lothello use time       memory cost       total construct  lothello structure still take  time query complexity   query    lothello compute two hash value hað   hbð    access two memory locations    hað          time complexity    procedure  include   basic arithmetic operations result   extremely fast execution speed   alien kmers  alien kmer  define   kmer    include   construction   lothello   context  taxonomic classification    kmers    include     reference materials alien kmers often arise due  noise  genomic sequence belong   novel species sample   sequence read   design two strategies  detect alien kmers first  would like  increase  randomness  assignments   case  alien kmers secondly  may add another bite   bitmap  let   log djtje double  number  categories    alien kmer   much higher chance   assign  alien categories  categories  use  exist taxon label   show   may leverage  randomness  alien assignment  predict  alien kmer within  lothello   first discuss  query result        extend   lothello     othello classify kmers             element       bite value   query   alien kmer    lothello still return  value    þ2f0   alien key                     let      fraction       bitmaps  respectively    jftja  ¼0gj     jftja  ¼1gj   similarly      fraction   suppose     uniformly distribute random hash function     arbitrary kmer   universal set      return   probability   a0b1  a1b0 similarly     return   probability   a0b0  a1b1  lothello  similar property also hold let    probability   query   alien kmer return exactly       indicate                 note      uniform random hash function    correlate hence                   x¼0 axbx       fraction  elements  value          fraction  elements  value      give  particular lothello   always compute  value            use time         value  affect   occurrence frequency   lbit integer namely              case  value   uniformly distribute  may result  imbalance among    circumstances   tune  value  flip  bitmaps   connect component   bipartite graph without change        practice   always tune  value        order  magnitude         approximately   two tune approach  describe  supplementary section    also explicitly detect alien kmers   increase  thus intentionally expand  number  target categories   majority    dummy alien due   randomness  class assignment   presence   alien many alien kmers  likely  fall   dummy categories   thus recognize  alien formally        jtj    alien thus alien kmers  recognize   stage  probability p2l  x¼jtj     jtj    taxonomic classification  sequence read  illustrate  figure  give  sequence read  algorithm iterate   kmer   begin   read    kmer retrieve  taxon     specific use lothello taxonomic classification   read  determine  assemble  taxa   kmers   read  classification  straightforward   kmers indicate   taxon     often  case disparate taxa  consider   consistent   belong    path   taxonomy mean  one assignment   higher rank      taxa belong  different branch  represent conflict information  issue   complicate   possibility  false taxonomic information return  query alien kmers   kmer   read   appear     reference sequence  tackle  challenge   design  windowbased classification approach  window  define   sequence  consecutive kmers   assign    taxon   give level  windowbased approach guard  falsepositive assignments due  alien kmers assume   taxon  return   alien kmer  random  chance   two consecutive alien kmers return   taxon      t¼0         value   small regardless   additionally  window correspond   maximum read subsequence  match  reference sequence thus  longer  window  longer  subsequence match   less likely  match  random  comparison  algorithms   kraken  clark count  total number  kmer match regardless   spatial distribution across  read  multiple taxon windows  available metaothello score    use  sum square  window size    follow formula  taxon   maximum score   select score tðþ¼         denote  number  kmers   ith window classify  taxon   xliu    kmer signature belong   taxon  also specific   higherranking taxa   higher taxonomic rank  exist  kmers  distinguish  taxon   siblings thus longer kmer windows  moreaccurate classifications  expect  higher taxonomic rank   assumption  topdown strategy  adopt  read classification give  read sequence metaothello start  classification   top rank  continue  classification   rank     exist  sufficiently large kmer window support  level base   kmer distribution   taxon metaothello establish  threshold  minimum window size   classification   taxon require theorem  show   minimum windowsize threshold   precomputed   taxon prior  read classification  minimum window size require   taxon  determine   probability   alien kmer query  lothello return  taxon root     acceptable falsepositive rate  larger  size   taxon subtree  higher  probability   random alien kmer may match    thus  longer  window require  reliable classification additionally  larger window size   require  order  lower  falsepositive rate theorem  give  userdefined falsepositive rate    total read number   minimum windowsize threshold require   taxon    compute  logp          denote  probability   alien kmer query  lothello return  value   taxon subtree  root   proof  present  supplementary section   example     genuslevel node suppose      tðþ          give  million read  suppose     logp            windows larger  three   take  consideration  determine  read assignment",1
37,taxMaps,"taxMaps: comprehensive and highly accurate taxonomic classification of short-read data in reasonable time
Database creation Data from the RefSeq Genomes and BLAST nt databases were retrieved through the NCBI FTP server and organized in various databases (see Supplemental Table S1). For each database, duplicate sequence entries were removed, and all ambiguous nucleotides converted to N characters. Then, for every distinct k-mer, we computed the LCA between all taxonomic IDs of the sequences containing it, derived from the NCBI Taxonomy database (NCBI Resource Coordinators 2016). K-mers were assembled, through extension, into sequences that share the same LCA. This procedure is done on the fly as the algorithm traverses the database and k-mers are read and classified. For every sequence record in the database, compression is initialized by the creation of a sequence, corresponding to the first k-mer of the record and classified as its LCA. Then, for every k-mer, if the LCA classification matches that of the sequence, the sequence is extended with the last base of the k-mer. Otherwise, a new sequence consisting of that k-mer and corresponding classification is initiated. The newly assembled sequences are then indexed (FM-index) using GEM (Marco-Sola et al. 2012). This reassembly process and use of the FM-index result in a reduction of the memory footprint, allowing for very large databases to be merged and simultaneously queried. Although the overall strategy is similar, in essence, to the one used in Kraken, the fact that the operation is performed on k-mers of length equal or greater than a target read length allows for nonexact searches to be conducted in the same manner as they would against the original database, meaning that for every alignment in the raw database, there will be at least one k-mer in the compressed database, where the same alignment is possible, thus rendering this compression lossless for the purpose of taxonomic classification. This not only eliminates most of the database sequence redundancy, consequently improving mapping performance stability, but it also significantly reduces the number of post-mapping computations to be performed. This is particularly true for samples containing DNA or RNA from organisms that are highly represented in the databases (e.g., E. coli) or for which the repeat content is particularly high. Compressed indexes can be downloaded from ftp://ftp.nygenome.org/taxmaps. Classification algorithm Reads are mapped in single-end mode to an indexed database (k ≥ read length) using GEM mapper, which guarantees that all optimal alignments are retrieved, up to the user-defined maximum edit distance (-e, default = 0.2) parameter. Each read is then taxonomically classified as the LCA of all database sequences returned. For paired-end classification, reads are classified independently. If the classification of the two ends is discordant, meaning that they are different and the root-to-leaf (RTL) path of one end is not fully included in the RTL path of the other end, the pair is classified as the LCA of both single-end classifications. If the RTL path of one end is contained in the RTL path of the other end, the pair is then classified as the lower taxon of the longest RTL path. In situations in which no database match is found for one of the two reads, the pair is classified solely on one read. taxMaps also has a stricter paired-end classification scheme, for which both ends are required to have database hits. In that scheme, the pair is always classified as the LCA of both single-end classifications, even when one RTL path is contained in the other, ensuring maximum precision at the expense of a higher rank classification. Implementation taxMaps is fully implemented in Python and works as a transparent pipeline-generating script upon user-defined parameters. It reads data in FASTQ format but can also extract unmapped reads from BAM files through SAMtools (Li et al. 2009). Processing steps such as adapter removal, low-quality end trimming, and low complexity filtering are carried out by Cutadapt (Martin 2011) and PRINSEQ lite (Schmieder and Edwards 2011) as part of the taxMaps pipeline, upon user-specified options. Users can also specify multiple indexes to be queried and define, on an index-specific basis, the maximum edit distance and number of threads used by GEM (Marco-Sola et al. 2012). Apart from that, taxMaps offers one single-end and two paired-end classification modes (described above). Mapping and classification results are given as tab-delimited files, including full mapping information for each read in GEM format along with the corresponding taxonomic classification. Finally, results for all represented taxa are summarized in a table and an interactive report is generated using Krona (Ondov et al. 2011). Simulated metagenomics data sets To build the simulated data sets, we first selected taxa for which the RTL path included all the major taxonomic ranks and had at least one contiguous sequence longer than 100 kb in NCBI's nucleotide database (NCBI Resource Coordinators 2016) and then, for each of the 4089 selected taxa (Supplemental Fig. S2), we randomly extracted a single 100-kb sequence chunk. From these sequences, 55 simulated data sets, each consisting of 10 million read pairs, were generated using a version of wgsim forked from SAMtools (Li et al. 2009) (https://github.com/lh3/wgsim), by combining five different read lengths (75, 125, 150, 250, and 300 bp) with 11 edit distances (0.0, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18, and 0.20) and the following additional parameters: fragment length of 550 bp, indel fraction of 0.15 and a maximum fraction of ambiguous bases allowed of 0.003. Interleaved FASTQ files were converted to FASTA files for BLASTN and MegaBLAST, since these programs were not designed to handle the FASTQ format. Each read ID contains the taxonomic identifier of the sequence from which it was simulated as well as the read length and edit distance of the data set. All data sets are available at ftp://ftp.nygenome.org/taxmaps/Benchmark/Datasets. We additionally selected 1 million read pairs (2 × 125 bp, edit distance = 0.08) that were clustered into bins of increasing multiplicity (number of alignment best hits) on the uncompressed NCBI's nucleotide database. These were used to compare GEM mapper and BLASTN performance on compressed and uncompressed databases. We ran taxMaps, Kraken, Centrifuge, BLASTN, and MegaBLAST on each of the 55 simulated data sets using default parameters and the NCBI's nucleotide database as reference for all methods. For taxMaps databases, the choice of k-mer depended on the read length (k = read length). For BLASTN, the number of read pairs analyzed was reduced to 100,000 by random sampling due to time constraints. Given that BLASTN and MegaBLAST are not taxonomic classifiers per se, the LCA of all best hits for each read was determined. For paired-end classification, the criteria used in taxMaps was applied. To estimate sensitivity and precision, classifications were split into four distinct categories: (1) correct, if the correct taxon is included in the RTL path of the assigned taxon; (2) concordant, if the assigned taxon is different from the correct taxon, but it is included in the RTL path of the correct taxon; (3) incorrect, if the assigned taxon is not included in the RTL path of the correct taxon, nor is the correct taxon included in the RTL path of the assigned taxon; and (4) unclassified, if no taxon was assigned. Rank-level sensitivity is then given by the number of correct classifications at a particular rank over the total number of possible classifications, and rank-level precision corresponds to the number of correct classifications at a particular rank over the number of correct and incorrect classifications at that same rank. Paired-end rank-level sensitivity and precision of each program was calculated at eight major taxonomic ranks (species, genus, family, order, class, phylum, kingdom, and root), for every edit distance and read length combination (Supplemental Fig. S3). Similarly, single-end rank-level sensitivity and precision data were also collected for each program's output in single-end mode (Supplemental Fig. S4). All corresponding F-scores can be found in Supplemental Table S3. In addition to the sensitivity and precision metrics, wall clock time data was collected for each program on all paired-end data sets (Supplemental Fig. S5). taxMaps, MegaBLAST, Centrifuge, and BLASTN were run on a computer cluster running CentOS 7.1 on either Intel Xeon E5-2697 2.60 GHz CPUs or Intel Xeon CPU E5-2680 2.80 GHz CPUs. Due to the high memory requirements, Kraken was run on a large-memory shared host running CentOS 6.5 on Intel Xeon CPU E7-8830 2.13 GHz CPUs. All programs were run using 16 CPUs per job, except for BLASTN, which was run on eight CPUs given the long-term commitment required of these resources. The wall clock time reported for BLASTN was then extrapolated to match the number of reads classified and numbers of CPUs used by the other programs. Strain-level accuracy assessment For all of the bacterial strain genomes available in the refseq_complete_genomes database, we have determined the number of k-mers (k = 125) that are unique to each strain. This allowed us to select 500 bacterial strains that are evenly distributed along the spectrum of percentage of strain-specific sequence and, for each, simulate 1 million 125-bp read pairs with an average divergence of 4% (ftp://ftp.nygenome.org/taxmaps/Benchmark/Datasets). Reads were then classified using taxMaps with default parameters on both paired-end and single-end modes against the original database. For each strain, strain-level sensitivity, precision, and corresponding F-score were computed. Mock community data sets To assess the classification accuracy on real data, we used two mock community single-end data sets, HiSeq and MiSeq, from a previously published benchmark (Wood and Salzberg 2014). Each data set was originally composed of 10,000 single-end reads from 10 different bacterial species. After adapter clipping using Cutadapt (Martin 2011), removal of sequences shorter than 31 bp and the complete removal of Streptococcus pneumoniae from the HiSeq data set due to the presence of chimeric reads that were likely artifacts, there were 8850 and 9953 reads left on the HiSeq and MiSeq data sets, respectively. For each data set, apart from running taxMaps (k = 125 and k = 300 for HiSeq and MiSeq, respectively), Kraken, Centrifuge, MegaBLAST, and BLASTN, we additionally ran the protein homology-based classifiers Kaiju and DIAMOND. DIAMOND classification followed the same criteria as BLASTN and MegaBLAST. Moreover, a filtering strategy was implemented for both BLAST programs, using the criteria (minimum bit score, win-score, and top-percent) described by the authors of MEGAN (Huson et al. 2007). We selected a win-score of 100 and minimum bit score cutoffs of 60 and two values, 5% and 10%, were explored for the top-percent cutoff (Supplemental Fig. S7). All methods used the refseq_complete_genomes database, with the exception of Kaiju and DIAMOND that used the correspondent set of annotated proteins (available at ftp://ftp.nygenome.org/taxmaps/Benchmark/Refseq_complete_genomes_DB). For each tool, rank-level sensitivity and precision were computed. Corresponding F-scores are given in Supplemental Table S4. We have also estimated genus abundance based on read classification for the HiSeq, MiSeq, and the two recently published data sets HC/LC and ZymoBIOMICS (McIntyre et al. 2017). For every genus, abundance estimates Aobs from each tool were then compared to the truth set value Aexp and the relative difference between the two Drel was calculated as D rel ( A obs , A exp ) = A obs − A exp max ( A obs , A exp ) . Real metagenomics samples We downloaded seven Illumina data sets of real metagenomics samples from the Sequence Read Archive (SRA) (Leinonen et al. 2011). Their description and corresponding accession numbers can be found in Supplemental Table S2. On all data sets, adapter sequences were clipped, and low-quality end bases trimmed (Q < 20). Reads were classified with paired-end and single-end modes using taxMaps (k = 300), Kraken, Centrifuge, Kaiju, and DIAMOND. For each data set, apart from determining the number of classified reads by each method, we computed a novel rank-level metric called classification concordance. This metric is defined as the percentage of read pairs for which the independent classification of both ends is either the same or concordant at that particular rank, as long as one of the ends has been classified at that rank or below. For instance, if one end is classified as Escherichia coli and the other as Enterobacteriaceae, the classification for that read pair is considered to be concordant at the species level and at all ranks above. If the second end had been classified as Proteus vulgaris instead, the classification would be concordant at the family level and at all ranks above. To assess whether classification concordance could be used as a proxy for precision, we calculated the Spearman's rank correlation ρ between the two metrics on the simulated data sets, for all methods and at all ranks with the exception of “root.”",Classification,"taxmaps comprehensive  highly accurate taxonomic classification  shortread data  reasonable time
database creation data   refseq genomes  blast  databases  retrieve   ncbi ftp server  organize  various databases see supplemental table    database duplicate sequence entries  remove   ambiguous nucleotides convert   character   every distinct kmer  compute  lca   taxonomic ids   sequence contain  derive   ncbi taxonomy database ncbi resource coordinators  kmers  assemble  extension  sequence  share   lca  procedure     fly   algorithm traverse  database  kmers  read  classify  every sequence record   database compression  initialize   creation   sequence correspond   first kmer   record  classify   lca   every kmer   lca classification match    sequence  sequence  extend   last base   kmer otherwise  new sequence consist   kmer  correspond classification  initiate  newly assemble sequence   index fmindex use gem marcosola     reassembly process  use   fmindex result   reduction   memory footprint allow   large databases   merge  simultaneously query although  overall strategy  similar  essence   one use  kraken  fact   operation  perform  kmers  length equal  greater   target read length allow  nonexact search   conduct    manner   would   original database mean   every alignment   raw database     least one kmer   compress database    alignment  possible thus render  compression lossless   purpose  taxonomic classification    eliminate    database sequence redundancy consequently improve map performance stability   also significantly reduce  number  postmapping computations   perform   particularly true  sample contain dna  rna  organisms   highly represent   databases   coli     repeat content  particularly high compress index   download  ftpftpnygenomeorgtaxmaps classification algorithm read  map  singleend mode   index database  ≥ read length use gem mapper  guarantee   optimal alignments  retrieve    userdefined maximum edit distance  default   parameter  read   taxonomically classify   lca   database sequence return  pairedend classification read  classify independently   classification   two end  discordant mean    different   roottoleaf rtl path  one end   fully include   rtl path    end  pair  classify   lca   singleend classifications   rtl path  one end  contain   rtl path    end  pair   classify   lower taxon   longest rtl path  situations    database match  find  one   two read  pair  classify solely  one read taxmaps also   stricter pairedend classification scheme    end  require   database hit   scheme  pair  always classify   lca   singleend classifications even  one rtl path  contain    ensure maximum precision   expense   higher rank classification implementation taxmaps  fully implement  python  work   transparent pipelinegenerating script upon userdefined parameters  read data  fastq format   also extract unmapped read  bam file  samtools     process step   adapter removal lowquality end trim  low complexity filter  carry   cutadapt martin   prinseq lite schmieder  edwards   part   taxmaps pipeline upon userspecified options users  also specify multiple index   query  define   indexspecific basis  maximum edit distance  number  thread use  gem marcosola    apart   taxmaps offer one singleend  two pairedend classification modes describe  map  classification result  give  tabdelimited file include full map information   read  gem format along   correspond taxonomic classification finally result   represent taxa  summarize   table   interactive report  generate use krona ondov    simulate metagenomics data set  build  simulate data set  first select taxa    rtl path include   major taxonomic rank    least one contiguous sequence longer     ncbi' nucleotide database ncbi resource coordinators         select taxa supplemental fig   randomly extract  single  sequence chunk   sequence  simulate data set  consist   million read pair  generate use  version  wgsim fork  samtools       combine five different read lengths          edit distance               follow additional parameters fragment length    indel fraction     maximum fraction  ambiguous base allow   interleave fastq file  convert  fasta file  blastn  megablast since  program   design  handle  fastq format  read  contain  taxonomic identifier   sequence     simulate  well   read length  edit distance   data set  data set  available  ftpftpnygenomeorgtaxmapsbenchmarkdatasets  additionally select  million read pair     edit distance     cluster  bin  increase multiplicity number  alignment best hit   uncompress ncbi' nucleotide database   use  compare gem mapper  blastn performance  compress  uncompress databases  run taxmaps kraken centrifuge blastn  megablast      simulate data set use default parameters   ncbi' nucleotide database  reference   methods  taxmaps databases  choice  kmer depend   read length   read length  blastn  number  read pair analyze  reduce    random sample due  time constraints give  blastn  megablast   taxonomic classifiers per   lca   best hit   read  determine  pairedend classification  criteria use  taxmaps  apply  estimate sensitivity  precision classifications  split  four distinct categories  correct   correct taxon  include   rtl path   assign taxon  concordant   assign taxon  different   correct taxon    include   rtl path   correct taxon  incorrect   assign taxon   include   rtl path   correct taxon    correct taxon include   rtl path   assign taxon   unclassified   taxon  assign ranklevel sensitivity   give   number  correct classifications   particular rank   total number  possible classifications  ranklevel precision correspond   number  correct classifications   particular rank   number  correct  incorrect classifications    rank pairedend ranklevel sensitivity  precision   program  calculate  eight major taxonomic rank species genus family order class phylum kingdom  root  every edit distance  read length combination supplemental fig  similarly singleend ranklevel sensitivity  precision data  also collect   program' output  singleend mode supplemental fig   correspond fscores   find  supplemental table   addition   sensitivity  precision metrics wall clock time data  collect   program   pairedend data set supplemental fig  taxmaps megablast centrifuge  blastn  run   computer cluster run centos   either intel xeon   ghz cpus  intel xeon cpu   ghz cpus due   high memory requirements kraken  run   largememory share host run centos   intel xeon cpu   ghz cpus  program  run use  cpus per job except  blastn   run  eight cpus give  longterm commitment require   resources  wall clock time report  blastn   extrapolate  match  number  read classify  number  cpus use    program strainlevel accuracy assessment     bacterial strain genomes available   refseq_complete_genomes database   determine  number  kmers      unique   strain  allow   select  bacterial strain   evenly distribute along  spectrum  percentage  strainspecific sequence    simulate  million  read pair   average divergence   ftpftpnygenomeorgtaxmapsbenchmarkdatasets read   classify use taxmaps  default parameters   pairedend  singleend modes   original database   strain strainlevel sensitivity precision  correspond fscore  compute mock community data set  assess  classification accuracy  real data  use two mock community singleend data set hiseq  miseq   previously publish benchmark wood  salzberg   data set  originally compose   singleend read   different bacterial species  adapter clip use cutadapt martin  removal  sequence shorter      complete removal  streptococcus pneumoniae   hiseq data set due   presence  chimeric read   likely artifacts      read leave   hiseq  miseq data set respectively   data set apart  run taxmaps         hiseq  miseq respectively kraken centrifuge megablast  blastn  additionally run  protein homologybased classifiers kaiju  diamond diamond classification follow   criteria  blastn  megablast moreover  filter strategy  implement   blast program use  criteria minimum bite score winscore  toppercent describe   author  megan huson     select  winscore    minimum bite score cutoffs    two value     explore   toppercent cutoff supplemental fig   methods use  refseq_complete_genomes database   exception  kaiju  diamond  use  correspondent set  annotate proteins available  ftpftpnygenomeorgtaxmapsbenchmarkrefseq_complete_genomes_db   tool ranklevel sensitivity  precision  compute correspond fscores  give  supplemental table    also estimate genus abundance base  read classification   hiseq miseq   two recently publish data set hclc  zymobiomics mcintyre     every genus abundance estimate aobs   tool   compare   truth set value aexp   relative difference   two drel  calculate   rel   obs   exp    obs   exp max   obs   exp   real metagenomics sample  download seven illumina data set  real metagenomics sample   sequence read archive sra leinonen     description  correspond accession number   find  supplemental table    data set adapter sequence  clip  lowquality end base trim    read  classify  pairedend  singleend modes use taxmaps    kraken centrifuge kaiju  diamond   data set apart  determine  number  classify read   method  compute  novel ranklevel metric call classification concordance  metric  define   percentage  read pair    independent classification   end  either    concordant   particular rank  long  one   end   classify   rank    instance  one end  classify  escherichia coli     enterobacteriaceae  classification   read pair  consider   concordant   species level    rank    second end   classify  proteus vulgaris instead  classification would  concordant   family level    rank   assess whether classification concordance could  use   proxy  precision  calculate  spearman' rank correlation    two metrics   simulate data set   methods    rank   exception  “root”",1
38,MEGAN,"MEGAN analysis of metagenomic data
Sequence comparisons In our studies, we performed sequence comparisons against the NCBI-NR database of nonredundant protein sequences using BLASTX with the default settings, the NCBI-NT database on nucleotide sequences using BLASTN with the default settings, and against whole-genome sequences obtained from dog, elephant, and human, using BLASTZ. Sequence comparison is a computationally challenging task that is likely to grow even more demanding as databases continue to grow and larger metagenome data sets are analyzed. For example, comparing the mammoth data set against NCBI-NR took almost 180 h real time on a cluster of 64 CPUs. We estimate that performing the same computation on the 1.6 million reads of the complete Sargasso Sea data set would require ∼1000 h real time on our system. Analysis using MEGAN At the startup, MEGAN loads the complete NCBI taxonomy, currently containing >280,000 taxa, which can then be interactively explored using customized tree-navigation features. However, the main application of MEGAN is to process the results of a comparison of reads against a database of known sequences. The program parses files generated by BLASTX, BLASTN, or BLASTZ, and saves the results as a series of read–taxon matches in a program-specific metafile. (Additional parsers may be added to process the results generated by other sequence comparison methods.) The program assigns reads to taxa using the LCA algorithm and then displays the induced taxonomy. Nodes in the taxonomy can be collapsed or expanded to produce summaries at different levels of the taxonomy. Additionally, the program provides a search tool to search for specific taxa, and an Inspector tool to view individual BLAST matches (see Fig. 9). The approach uses several thresholds. First, the min-score filter sets a threshold for the score that an alignment must achieve to be considered in the calculations. For reads of length ∼100 bp and using BLASTX to compare against NCBI-NR, a min-score of 35 or higher is recommended; while for reads of length ∼800 bp, a min-score of 100 is more suitable. Second, to help distinguish between hits due to sequence identity and those due to homology, the top-percent filter is used to retain only those hits for a given read r whose scores lie within a given percentage of the highest score involving r. (Note that this is not the same as keeping a certain percentage of the hits.) The smaller the set value is, the more specific a calculated assignment will be, but also the greater the chance of producing an over-prediction, that is, a false prediction due to the absence of the true taxon in the database. A useful range of values is 10%–20%. Third, a win-score threshold can be set such that, for any given read, if any match scores above the threshold, then for that read, only those matches are considered that score above the threshold. Fourth, to help reduce false positives, the min-support filter is used to set a threshold for the minimum number of reads that must be assigned to a taxon t, or to any of its descendants in the taxonomical tree. After the main computation, all reads that are assigned to a taxon that does not meet this requirement are reassigned to the special taxon “Not Assigned.” By default, this parameter is set to 2. The result of the LCA algorithm is presented to the user as the partial taxonomy T that is induced by the set of taxa that have been identified (see Fig. 5). The program allows the user to explore the results at many different taxonomical levels, by providing methods for collapsing and expanding different parts of the taxonomy T. Each node in T represents a taxon t and can be queried to determine which reads have been assigned directly to t, and how many reads have been assigned to taxa below t. Additionally, the program allows the user to view the sequence alignments upon which specific assignments are based",Classification,"megan analysis  metagenomic data
sequence comparisons   study  perform sequence comparisons   ncbinr database  nonredundant protein sequence use blastx   default settings  ncbint database  nucleotide sequence use blastn   default settings   wholegenome sequence obtain  dog elephant  human use blastz sequence comparison   computationally challenge task   likely  grow even  demand  databases continue  grow  larger metagenome data set  analyze  example compare  mammoth data set  ncbinr take almost   real time   cluster   cpus  estimate  perform   computation    million read   complete sargasso sea data set would require   real time   system analysis use megan   startup megan load  complete ncbi taxonomy currently contain  taxa     interactively explore use customize treenavigation feature however  main application  megan   process  result   comparison  read   database  know sequence  program parse file generate  blastx blastn  blastz  save  result   series  readtaxon match   programspecific metafile additional parsers may  add  process  result generate   sequence comparison methods  program assign read  taxa use  lca algorithm   display  induce taxonomy nod   taxonomy   collapse  expand  produce summaries  different level   taxonomy additionally  program provide  search tool  search  specific taxa   inspector tool  view individual blast match see fig   approach use several thresholds first  minscore filter set  threshold   score   alignment must achieve   consider   calculations  read  length    use blastx  compare  ncbinr  minscore    higher  recommend   read  length    minscore     suitable second  help distinguish  hit due  sequence identity   due  homology  toppercent filter  use  retain   hit   give read  whose score lie within  give percentage   highest score involve  note        keep  certain percentage   hit  smaller  set value    specific  calculate assignment    also  greater  chance  produce  overprediction    false prediction due   absence   true taxon   database  useful range  value   third  winscore threshold   set     give read   match score   threshold    read   match  consider  score   threshold fourth  help reduce false positives  minsupport filter  use  set  threshold   minimum number  read  must  assign   taxon       descendants   taxonomical tree   main computation  read   assign   taxon    meet  requirement  reassign   special taxon “ assigned”  default  parameter  set    result   lca algorithm  present   user   partial taxonomy    induce   set  taxa    identify see fig   program allow  user  explore  result  many different taxonomical level  provide methods  collapse  expand different part   taxonomy   node   represent  taxon     query  determine  read   assign directly     many read   assign  taxa   additionally  program allow  user  view  sequence alignments upon  specific assignments  base",1
39,MetaShot,"MetaShot: an accurate workflow for taxon classification of host-associated microbiome from shotgun metagenomic data
The MetaShot workflow implements a two-step similarity-based approach to attain the best compromise between computational efficiency and assignment accuracy. Indeed, in consideration that the large majority of shotgun reads derive from the host we first carry out a fast similarity-based screening to detect candidate microbial reads, then a fine-grained taxonomic assessment of the much smaller set of putative microbial reads is carried out by using also an iterative taxon refinement procedure (see Supplementary Material for details). A software package implementing the MetaShot pipeline is freely available at https://github.com/bfosso/MetaShot and includes a utility tool for extracting all reads assigned to a specific NCBI taxonomic ID or all those left unassigned. In order to carry out a comparative assessment of MetaShot performance with respect to Kraken (Wood and Salzberg, 2014) and MetaPhlAn2 (Truong et al., 2015), two state of the art tools for analyzing shotgun metagenomics data, we used ART (Huang et al., 2012) to generate an in silico designed human microbiota with a composition resembling a typical human sample, containing human, bacterial and viral sequences (see Table 1A and Supplementary Material for a more detailed description). The simulated dataset also included reads from PhiX phage, which have been shown to contaminate many assembled microbial genomes (Mukherjee et al., 2015) and from human endogenous retroviruses (HERV), which escape detection by most tools designed for analyzing shotgun metagenomics data because they are simply labeled as host reads. Indeed, under specific conditions these viruses can be expressed and may play a role in disease pathogenesis (Agoni et al., 2013; Li et al., 2015). Moreover, in order to compare MetaShot, Kraken and MetaPhlAn2 on a controlled real dataset we analyzed a bacterial and viral mock community (Conceicao-Neto et al., 2015) available in the NCBI-SRA archive (SRR3458569). The results of the benchmark assessment displayed in Table 1 clearly show that MetaShot outperforms Kraken and MetaPhlAn2 in terms of the overall accuracy of reads assignment for the Prokaryotes and Viruses simulated datasets, at the Family, Genus and Species levels. In addition, MetaShot performs better that Kraken and MetaPhlAn2 also in terms of taxon assignment accuracy at Species and Genus levels at both qualitative (see Supplementary Tables S1–S4) and quantitative levels (See Supplementary Figs S2 and S3). Finally, in order to test MetaShot on a real dataset we analyzed DNA-seq (528 034 456 100 bp x 2 PE reads) and RNA-Seq (61 318 866 100 bp x 2 PE reads) data from a sample of cervical squamous cell carcinoma of the uterus. While it is known that about 95% of these cancers harbor human papillomavirus (HPV) genomes, the specific serotype involved varies, the most common ones being HPV16, HPV18, and HPV31 (Growdon and Del Carmen, 2008). We previously established by PCR assessment that this test sample contained HPV31. Indeed, HPV31 was detected only by MetaShot in both DNA-Seq and RNA-Seq datasets (25 359 reads over 25 368 total viral reads in DNA-Seq data and 13 684 reads over 14 150 total viral reads in RNA-Seq data) whereas Kraken detected much fewer viral reads (2656 and 1565 in total for DNA-Seq and RNA-Seq data, respectively), notably not including HPV31 (see Supplementary Table S1) which also MetaPhlAn2 was unable to detect. These results confirm the optimal performance of MetaShot with respect to Kraken and MetaPhlAn2 also in the case of real data analysis. The MetaShot output consists of: (i) an HTML interactive table reporting for each node in the inferred taxonomy the taxon name, the NCBI taxonomy ID and the number of assigned reads; (ii) a CSV file containing the same information reported in the interactive table; (iii) a Krona graph (Ondov et al., 2011) to graphically inspect the inferred microbiome. A remarkable unique feature of MetaShot is the possibility to extract all unassigned reads or the set of reads assigned to a specific taxon, defined by the NCBI taxonomy ID. This feature is particularly useful for downstream analyses such as OTU generation, contig assembly for the characterization of unassigned reads, or functional annotation of the reads belonging to a specific species/strain. In addition, this feature may allow for shotgun mapping species-specific DNA-seq reads to their target genome, if available, to prevent the possibility of artifacts, usually associated with a strong positional mapping bias, due to chimeric contamination in GenBank reference sequences (Mukherjee et al., 2015). Moreover, in the case of shotgun RNA-Seq reads, mapping to their target genome may precisely assess their relevant expression profile. The price for the overall better accuracy of MetaShot is a lower computational efficiency. MetaShot is about 2 and 3 times slower than Kraken and MetaPhlAn2, respectively, for the complete analysis of the simulated benchmark dataset (see Supplementary Material).",Classification,"metashot  accurate workflow  taxon classification  hostassociated microbiome  shotgun metagenomic data
 metashot workflow implement  twostep similaritybased approach  attain  best compromise  computational efficiency  assignment accuracy indeed  consideration   large majority  shotgun read derive   host  first carry   fast similaritybased screen  detect candidate microbial read   finegrained taxonomic assessment   much smaller set  putative microbial read  carry   use also  iterative taxon refinement procedure see supplementary material  detail  software package implement  metashot pipeline  freely available    include  utility tool  extract  read assign   specific ncbi taxonomic     leave unassigned  order  carry   comparative assessment  metashot performance  respect  kraken wood  salzberg   metaphlan2 truong    two state   art tool  analyze shotgun metagenomics data  use art huang     generate   silico design human microbiota   composition resemble  typical human sample contain human bacterial  viral sequence see table   supplementary material    detail description  simulate dataset also include read  phix phage    show  contaminate many assemble microbial genomes mukherjee      human endogenous retroviruses herv  escape detection   tool design  analyze shotgun metagenomics data    simply label  host read indeed  specific condition  viruses   express  may play  role  disease pathogenesis agoni        moreover  order  compare metashot kraken  metaphlan2   control real dataset  analyze  bacterial  viral mock community conceicaoneto    available   ncbisra archive srr3458569  result   benchmark assessment display  table  clearly show  metashot outperform kraken  metaphlan2  term   overall accuracy  read assignment   prokaryotes  viruses simulate datasets   family genus  species level  addition metashot perform better  kraken  metaphlan2 also  term  taxon assignment accuracy  species  genus level   qualitative see supplementary table s1s4  quantitative level see supplementary figs    finally  order  test metashot   real dataset  analyze dnaseq         read  rnaseq         read data   sample  cervical squamous cell carcinoma   uterus    know      cancers harbor human papillomavirus hpv genomes  specific serotype involve vary   common ones  hpv16 hpv18  hpv31 growdon  del carmen   previously establish  pcr assessment   test sample contain hpv31 indeed hpv31  detect   metashot   dnaseq  rnaseq datasets   read    total viral read  dnaseq data    read    total viral read  rnaseq data whereas kraken detect much fewer viral read     total  dnaseq  rnaseq data respectively notably  include hpv31 see supplementary table   also metaphlan2  unable  detect  result confirm  optimal performance  metashot  respect  kraken  metaphlan2 also   case  real data analysis  metashot output consist    html interactive table report   node   infer taxonomy  taxon name  ncbi taxonomy    number  assign read   csv file contain   information report   interactive table iii  krona graph ondov     graphically inspect  infer microbiome  remarkable unique feature  metashot   possibility  extract  unassigned read   set  read assign   specific taxon define   ncbi taxonomy   feature  particularly useful  downstream analyse   otu generation contig assembly   characterization  unassigned read  functional annotation   read belong   specific speciesstrain  addition  feature may allow  shotgun map speciesspecific dnaseq read   target genome  available  prevent  possibility  artifacts usually associate   strong positional map bias due  chimeric contamination  genbank reference sequence mukherjee    moreover   case  shotgun rnaseq read map   target genome may precisely assess  relevant expression profile  price   overall better accuracy  metashot   lower computational efficiency metashot      time slower  kraken  metaphlan2 respectively   complete analysis   simulate benchmark dataset see supplementary material",1
40,RIEMS,"RIEMS: a software pipeline for sensitive and comprehensive taxonomic classification of reads from metagenomics datasets
Implementation, integrated software and databases RIEMS is implemented as a unix shellscript written in bourne-again shell (bash). The script files are available under the GNU General Public License Version 3 together with the validation datasets and the “novel viruses dataset” at http://www.fli.bund.de/no_cache/en/startseite/institutes/institute-of-diagnostic-virology/labs-working-groups/laboratory-for-ngs-and-microarray-diagnostics.html. RIEMS only relies on validated standard software and databases. All software applications are installed on a local server (specifications see below). RIEMS has access to the 454 genome sequencer software suite (works with version 2.6 and later; Roche, Mannheim, Germany; available free of charge after registration at http://454.com/contact-us/software-request.asp), in particular the assembler Newbler and the GS reference mapper as well as the standard flowgram format (sff) and the FASTA nucleic acid (fna) tool commands. Furthermore, the NCBI BLAST software suite (version 2.2.26+; available at ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/) and the NCBI databases (available at ftp://ftp.ncbi.nlm.nih.gov/blast/db/) for nucleotide (nt), protein (nr), and taxonomy are integrated [23]. In addition, the Emboss software package (version 6.3.1; http://emboss.sourceforge.net/download/) [39] is included, in particular the applications for open reading frame (ORF) detection and amino acid translation. All software applications are used with default settings. The BLAST hit selection is based on the e-value (cut-off 0.001); the hit with the lowest e-value is selected. RIEMS accepts sequences input in the common sequence formats sff (as generated by Lifetechnologies IonTorrent and 454/Roche Genome Sequencers), FASTQ (for instance Illumina), or FASTA; in case of FASTA files preferably in combination with the corresponding quality files. Hardware and operating system RIEMS and all necessary applications are installed locally on a server equipped with four Intel Xeon E7450 processors (each 6 cores with a frequency of 2400 MHz) and 64 GB PC2-5300 (DDR2-667 MHz) ECC RAM. The server is linked with 2 × 4 Gbit/s ports to the storage area network. The operating system is CentOS release 5.10 with Linux kernel release 2.6.18-371.1.2.e15. Delineation of RIEMS data processing A general overview of the RIEMS pipeline is depicted in Figure 6. RIEMS is subdivided into a ‘Basic analysis’ which taxonomically assigns the bulk of the dataset and a ‘Further analysis’ dealing with all sequences remaining taxonomically unassigned after the ‘Basic analysis’. Both the basic and the further analysis follow the general strategy of sequentially applying different tools with decreasing stringency in order to achieve a fast and reliable overall analysis. After both the basic and the further analysis are finished, all results are summarized in a spread sheet arranged taxonomically. Prior to all analyses, all reads are quality trimmed for high reliability of the final classifications. For this purpose, the GS reference mapper application is used which intrinsically performs a quality trimming. Thereto, a poly-A sequence of 120 nucleotides in length is generated and used as reference for a mapping (Figure 6). The trim points which are determined during this quick initial mapping are then used as fixed trim points in all analyses. In parallel, a pre-screening of the dataset against a user provided reference set can optionally be performed like shown in the analyses of the Clinical PathoScope dataset (Figure 4). Basic analysis The main steps of the ‘Basic analysis’ (Figure 6B) are identification of the sample background, assignment of reads to the background species, identification of further abundant species in the remaining dataset and a final blast analysis with decreasing stringency, first using megablast followed by blastn. The first step of the actual analysis is the identification of the sample background, i.e. the species the bulk of the reads can be assigned to. For this purpose, a subset of 5,000 reads is randomly extracted from the trimmed reads and assembled into contigs using the 454 newbler assembler. Subsequently, the resulting contigs are analysed using megablast against the NCBI nucleotide database “nt” (all GenBank + EMBL + DDBJ + PDB sequences) [23]. Based on GenBank identifiers (GIs) of the best hits, the associated taxonomy IDs of the corresponding species are determined in the NCBI taxonomy database. This information is used to retrieve all sequences of each detected species from the nucleotide database. Successively, each of these sequence sets is first used as reference for a mapping and thereafter a megablast [23] analysis in which reads excluded from the mapping due to length restriction or partially mapped reads are included (a detailed depiction of the repetitive taxonomy based sequence retrieval and alignment procedure can be found in Additional file 1: Figures S1, S2, and S3). In case an identified organism is a eukaryote, all reads that were classified to this organism by mapping are blasted against a partial database containing only viral sequences in order to identify viral reads potentially classified as eukaryotic reads due to viral sequences which may be part of eukaryotic genomes (Additional file 1: Figure S3). All reads that were classified in one of these steps are exempted from subsequent analyses. This procedure (generating and assembling a random sequence subset, followed by BLAST, sequence retrieval and mapping plus scanning for viral sequences) is repeated with the remaining unassigned reads until either no new species are identified by blasting the generated contigs or no new contigs can be assembled. This repeated procedure enables a rapid, automatic breakdown of the dataset without user provided knowledge of the organisms making up the sample background. If after the initial repeated background analyses more than 10,000 reads are still unassigned and cannot be assembled into contigs, another random sub-setting process is initiated. However, in this process, only 100 reads are taken from the remaining unassigned reads and are analysed directly using megablast [23] without a prior assembly. The taxonomy IDs of the hits are determined according to their GenBank Identifiers (GIs) obtained from the BLAST result. Only if more than eight reads are assigned to the same taxonomy ID the corresponding sequences are retrieved from the nucleotide database and used as reference for a mapping and a subsequent megablast analysis like in the initial background analysis (Additional file 1: Figures S2 and S3). All reads that remain unassigned after the initial screening are assembled into contigs. Reads that remained unassembled in this step are assigned to the contigs by megablast if at least 80% of the bases can be aligned to the contig. Subsequently, all contigs are searched against the local nucleotide database using blastn [23]. If BLAST aligns contigs only partially and the unaligned part consists of more than 30 consecutive nucleotides, this unaligned part is again searched in the database by blastn. Based on the assignment of a contig to an organism, reads that were aligned to that contig by megablast are assigned to the respective organism as well (Additional file 1: Figure S4). Unassigned and partially assigned contigs are saved in a file for the further analysis (see below). Those reads that were neither assembled into a contig nor assigned to a contig by megablast in the previous assembly step are sequentially investigated by multiple BLAST analyses. The first of these BLAST analyses is a megablast against a database encompassing the nucleotide sequences of all organisms that were identified in the partial datasets and used as references for mappings. Secondly, the remaining unassigned reads are searched in the NCBI nt database again using megablast. Finally, blastn is used to search the unaligned remainder in the NCBI nt database. In all of these BLAST searches, unaligned parts of more than 30 consecutive nucleotides of partially aligned reads are searched a second time (Additional file 1: Figure S5). Further analysis The ‘Further analysis’ can optionally be executed and starts with additional nucleotide BLAST analyses of the reads. Then, for the subsequent analyses the sequence type is switched to the amino acid sequences coded by the sequences that remained unassigned (reads and contigs). Due to the higher information content of the 20 letter amino acid alphabet, usually additional sequences can be classified. The initial nucleotide read sequence analyses within the ‘Further analysis’ are performed without low complexity filtering first using megablast and subsequently blastn both searching in the NCBI nt database. Deactivation of the low complexity filter further decreases the stringency of the assignments because even low complexity regions of the sequences are considered (Additional file 1: Figure S6). Hence, there is the chance that additional reads can be classified. The next step is the translation of the unclassified reads (Emboss sixpack [39]) and contigs (Emboss getorf [39]) into amino acid sequences in all six frames. The only limitation we apply is a lower size cut-off of 20 amino acids because shorter peptide sequences cannot be analysed by blastp to yield a truly significant result; hence all ORFs with a minimum length of 20 codons are translated independently of start codons until reaching a stop codon. These settings assure that even in the case of sequencing noise causing non-sense mutations by insertions, deletions or single base exchanges the coded aa sequences can be analysed at least as partial sequences. In the case of short reads, i.e. Illumina HiSeq style reads, the lower size cut-off of 20 aa still permits their analysis. The deduced amino acid sequences are binned according to their length in order to select the proper substitution matrices for blastp according to [43,44] (<35 amino acids PAM-30, 35 – 50 amino acids PAM-70, 50 – 85 amino acids BLOSUM-80 and >85 amino acids BLOSUM-62). For contigs, all deduced aa sequences are analysed by blastp vs. NCBI nr database (Additional file 1: Figure S7). Blastp analyses (vs. NCBI nr database) of aa sequences deduced from reads (Additional file 1: Figure S8) start with the longest sequences translated from the reads, and only proceeds with the next shorter aa sequence deduced from that read if no hit was found in the database. If a similar aa sequence was detected all additional aa sequences for the respective read are exempted from further blast analyses. Comparison with other software tools For the comparison of RIEMS with other tools with regard to sensitivity and specificity, we used Kraken [37] with the available Kraken Minidatabase (as of 30.09.2014), Clinical PathoScope [36] (v. 1.0.3, and database as of 30.09.2014) and the MetaPhlAn Clade Specific marker BLAST database [38] (as of 30.09.2014) in combination with Megablast for read classification; all three were used with default settings as recommended by the respective authors in combination with the validated databases provided at the project websites. For the comparisons we used different datasets, namely our simulated sample (see Table 1) with and without deviations, a small dataset comprising roughly 2,800 genuine Illumina MiSeq reads representing 2 different novel viruses, and the simulated sample #1 from the Clinical PathoScope project (http://sourceforge.net/projects/PathoScope). Here we had to modify the read accessions of reads representing Streptococcal sequences, because the authors didn’t label the reads according to the two different Streptococcal species present in the dataset. This was done by majority according to the classifications of the 4 tools in use. Roughly 4800 reads had to be discarded because of inconclusive classifications. Like RIEMS, the three tools put out read-wise classifications which were analysed using R (v3.1.0) [45] and R-Studio (v 0.98.507; RStudio, Inc. www.rstudio.com) to calculate the sensitivity and specificity. To enable a robust comparison of the different tools, classifications were consolidated to the species level and only unambiguous read-to-species classifications were taken into account. In case of Clinical PathoScope, reads longer than 100 bases are by default split into fragments; RIEMS has the capability to split reads if only partial matches occur. In both cases, results obtained for the sub-sequences were consolidated at the read level and again only counted if the results were unambiguous.",Classification,"riems  software pipeline  sensitive  comprehensive taxonomic classification  read  metagenomics datasets
implementation integrate software  databases riems  implement   unix shellscript write  bourneagain shell bash  script file  available   gnu general public license version  together   validation datasets   “novel viruses dataset”   riems  rely  validate standard software  databases  software applications  instal   local server specifications see  riems  access    genome sequencer software suite work  version   later roche mannheim germany available free  charge  registration    particular  assembler newbler    reference mapper  well   standard flowgram format sff   fasta nucleic acid fna tool command furthermore  ncbi blast software suite version  available  ftpftpncbinlmnihgovblastexecutablesblast   ncbi databases available  ftpftpncbinlmnihgovblastdb  nucleotide  protein   taxonomy  integrate   addition  emboss software package version     include  particular  applications  open read frame orf detection  amino acid translation  software applications  use  default settings  blast hit selection  base   evalue cutoff   hit   lowest evalue  select riems accept sequence input   common sequence format sff  generate  lifetechnologies iontorrent  roche genome sequencers fastq  instance illumina  fasta  case  fasta file preferably  combination   correspond quality file hardware  operate system riems   necessary applications  instal locally   server equip  four intel xeon e7450 processors   core   frequency   mhz    pc2 ddr2 mhz ecc ram  server  link     gbits port   storage area network  operate system  centos release   linux kernel release e15 delineation  riems data process  general overview   riems pipeline  depict  figure  riems  subdivide   basic analysis  taxonomically assign  bulk   dataset    analysis deal   sequence remain taxonomically unassigned   basic analysis   basic    analysis follow  general strategy  sequentially apply different tool  decrease stringency  order  achieve  fast  reliable overall analysis    basic    analysis  finish  result  summarize   spread sheet arrange taxonomically prior   analyse  read  quality trim  high reliability   final classifications   purpose   reference mapper application  use  intrinsically perform  quality trim thereto  polya sequence   nucleotides  length  generate  use  reference   map figure   trim point   determine   quick initial map   use  fix trim point   analyse  parallel  prescreening   dataset   user provide reference set  optionally  perform like show   analyse   clinical pathoscope dataset figure  basic analysis  main step   basic analysis figure   identification   sample background assignment  read   background species identification   abundant species   remain dataset   final blast analysis  decrease stringency first use megablast follow  blastn  first step   actual analysis   identification   sample background   species  bulk   read   assign    purpose  subset   read  randomly extract   trim read  assemble  contigs use   newbler assembler subsequently  result contigs  analyse use megablast   ncbi nucleotide database “”  genbank  embl  ddbj  pdb sequence  base  genbank identifiers gi   best hit  associate taxonomy ids   correspond species  determine   ncbi taxonomy database  information  use  retrieve  sequence   detect species   nucleotide database successively    sequence set  first use  reference   map  thereafter  megablast  analysis   read exclude   map due  length restriction  partially map read  include  detail depiction   repetitive taxonomy base sequence retrieval  alignment procedure   find  additional file  figure      case  identify organism   eukaryote  read   classify   organism  map  blast   partial database contain  viral sequence  order  identify viral read potentially classify  eukaryotic read due  viral sequence  may  part  eukaryotic genomes additional file  figure   read   classify  one   step  exempt  subsequent analyse  procedure generate  assemble  random sequence subset follow  blast sequence retrieval  map plus scan  viral sequence  repeat   remain unassigned read  either  new species  identify  blast  generate contigs   new contigs   assemble  repeat procedure enable  rapid automatic breakdown   dataset without user provide knowledge   organisms make   sample background    initial repeat background analyse    read  still unassigned  cannot  assemble  contigs another random subsetting process  initiate however   process   read  take   remain unassigned read   analyse directly use megablast  without  prior assembly  taxonomy ids   hit  determine accord   genbank identifiers gi obtain   blast result     eight read  assign    taxonomy   correspond sequence  retrieve   nucleotide database  use  reference   map   subsequent megablast analysis like   initial background analysis additional file  figure     read  remain unassigned   initial screen  assemble  contigs read  remain unassembled   step  assign   contigs  megablast   least    base   align   contig subsequently  contigs  search   local nucleotide database use blastn   blast align contigs  partially   unaligned part consist     consecutive nucleotides  unaligned part   search   database  blastn base   assignment   contig   organism read   align   contig  megablast  assign   respective organism  well additional file  figure  unassigned  partially assign contigs  save   file    analysis see   read   neither assemble   contig  assign   contig  megablast   previous assembly step  sequentially investigate  multiple blast analyse  first   blast analyse   megablast   database encompass  nucleotide sequence   organisms   identify   partial datasets  use  reference  mappings secondly  remain unassigned read  search   ncbi  database  use megablast finally blastn  use  search  unaligned remainder   ncbi  database     blast search unaligned part     consecutive nucleotides  partially align read  search  second time additional file  figure   analysis   analysis  optionally  execute  start  additional nucleotide blast analyse   read    subsequent analyse  sequence type  switch   amino acid sequence cod   sequence  remain unassigned read  contigs due   higher information content    letter amino acid alphabet usually additional sequence   classify  initial nucleotide read sequence analyse within   analysis  perform without low complexity filter first use megablast  subsequently blastn  search   ncbi  database deactivation   low complexity filter  decrease  stringency   assignments  even low complexity regions   sequence  consider additional file  figure  hence    chance  additional read   classify  next step   translation   unclassified read emboss sixpack   contigs emboss getorf   amino acid sequence   six frame   limitation  apply   lower size cutoff   amino acids  shorter peptide sequence cannot  analyse  blastp  yield  truly significant result hence  orfs   minimum length   codons  translate independently  start codons  reach  stop codon  settings assure  even   case  sequence noise cause nonsense mutations  insertions deletions  single base exchange  cod  sequence   analyse  least  partial sequence   case  short read  illumina hiseq style read  lower size cutoff    still permit  analysis  deduce amino acid sequence  bin accord   length  order  select  proper substitution matrices  blastp accord    amino acids pam    amino acids pam    amino acids blosum   amino acids blosum  contigs  deduce  sequence  analyse  blastp  ncbi  database additional file  figure  blastp analyse  ncbi  database   sequence deduce  read additional file  figure  start   longest sequence translate   read   proceed   next shorter  sequence deduce   read   hit  find   database   similar  sequence  detect  additional  sequence   respective read  exempt   blast analyse comparison   software tool   comparison  riems   tool  regard  sensitivity  specificity  use kraken    available kraken minidatabase    clinical pathoscope     database      metaphlan clade specific marker blast database      combination  megablast  read classification  three  use  default settings  recommend   respective author  combination   validate databases provide   project websites   comparisons  use different datasets namely  simulate sample see table    without deviations  small dataset comprise roughly  genuine illumina miseq read represent  different novel viruses   simulate sample #   clinical pathoscope project      modify  read accession  read represent streptococcal sequence   author didnt label  read accord   two different streptococcal species present   dataset     majority accord   classifications    tool  use roughly  read    discard   inconclusive classifications like riems  three tool put  readwise classifications   analyse use     rstudio   rstudio inc wwwrstudiocom  calculate  sensitivity  specificity  enable  robust comparison   different tool classifications  consolidate   species level   unambiguous readtospecies classifications  take  account  case  clinical pathoscope read longer   base   default split  fragment riems   capability  split read   partial match occur   case result obtain   subsequences  consolidate   read level    count   result  unambiguous",1
41,GOTTCHA,"Accurate read-based metagenome characterization using a hierarchical suite of unique signatures
GOTTCHA's unique reference genome databases FASTA-encoded databases of unique signatures for prokaryotic and viral genomes were generated and used for this work (instructions provided at https://github.com/LANL-Bioinformatics/GOTTCHA). Briefly, databases of unique genome segments at multiple taxonomic levels (e.g. family, species, genus, strain-level, etc.) are used for taxonomic classification of reads. Variants of these databases, in which all human 24-mers were removed were also generated and used in this study. These 24-mers were derived from the GRCh37.p10 (Genome Reference Consortium), HuRef (J. Craig Venter Institute), and CHM1_1.0 (Washington Univesity School of Medicine) assemblies and include unplaced scaffolds. Synthetic metagenomes Several metagenome datasets were used to establish and validate the appropriate criteria for accurate taxonomic classification and abundance calculation using our GOTTCHA databases. Of the 16 synthetic metagenomic datasets (MG1–MG16) analyzed in this study, six were created for this study as high-complexity, high-coverage (HCHC) metagenomes with a total read amount mimicking that of a single Illumina HiSeq 2000 lane that varied in: community composition (100, >200 or >300 organisms), relative abundance (even or log-normal distribution), and per-base quality scores and error rates. Each dataset consisted of 300 million (M) 100-bp, paired-end reads. Read sets were derived from either a constant number of genomes (even: MG1, MG3, MG5) or the numbers of cells were randomly selected for each species from along a log-normal distribution curve (log-normal: MG2, MG4, MG6). Table ​Table11 and Supplementary Table S1 summarize these synthetic metagenomic communities. Synthetic data were generated using MetaSim and a customized Illumina error model, with per-base quality assignments derived as follows. Errors were modeled by mapping real Illumina HiSeq 2000 100-bp reads against high-quality genome references whose assemblies were considered ‘finished’ (12). Sequencing errors were recorded and collated according to the read base position (1–100). An exactly-matched base was recorded in one lot while incorrectly-mapped bases were recorded in a separate, per-base lot. The per-base error probability provided to MetaSim was obtained by dividing the number of error bases at a specific position by 10M. Briefly, position-based quality scores were assigned for both error and error-free positions: error-free bases receive qualities randomly chosen from the set of exactly-matched base qualities recorded for that position in the read (of the 10M reads), whereas error-containing base qualities were randomly chosen from quality scores recorded for their position-specific lot. Ten previously published synthetic metagenomes incorporating a MAQ error model (6) were also used in the cross-tool comparison for their considerably lower sequencing coverage: two high-complexity, medium-coverage (HCMC) metagenomes, each with 100 organisms and 1M reads (MG7, MG8); and two low-complexity, low-coverage (LCLC) metagenomes, each with 25 organisms and 250k reads (MG9–MG16). HMP mock metagenomes Genomic mixtures of 22 organisms of fixed concentrations were created by the Human Microbiome Project (HMP) to test their sequencing protocols and analytical pipelines. Two mixture types are available: an even (EVEN) mixture, where aliquots were based on equimolar rRNA operon counts per organism; and a staggered (STAG) mixture, where the rRNA operon counts can vary by up to 4 orders of magnitude according to the following table: http://downloads.hmpdacc.org/data/HMMC/HMPRP_sT1-Mock.pdf. Each mixture was then sequenced on both the Illumina Genome Analyzer II and the 454 GS FLX Titanium. Raw sequence data was downloaded from the HMP website (http://www.hmpdacc.org/HMMC). Since it was highly unlikely that the observed sequence distribution would correlate with input concentrations at the library preparation stage (1,13–19), a round of BWA (20) mapping using only the known community members as reference was used to compute each individual's actual relative abundance. Although there were 22 organisms in the mock community, the analyses presented are limited to completed bacterial genomes, and thus exclude the eukaryote (Candida albicans) and the incomplete bacterial genome (Actinomyces odontolyticus), leaving just 20 reference organisms (MG17–MG20). Air filter metagenome In March 2011, genomic DNA was extracted from an air filter wash (phosphate buffered saline, 1% TritonX) and spiked with random amounts of DNA from the biothreat agent Francisella tularensis SCHU S4. Two Illumina libraries were created from the extracted 20 ng of DNA: one constructed immediately after DNA extraction and one amplified using the Qiagen Repli-G whole genome multiple displacement amplification (MDA) kit. Amplification yielded 4.3 μg of MDA DNA from which a library was prepared with the ‘Preparing Samples for Sequencing Genome DNA’ protocol without modification, and sequenced as a single read (SR) 36 bp run on the Illumina Genome Analyzer IIx, generating 91 252 832 reads. The unamplified sample was prepared with the same Illumina protocol, but required speed vacuum-concentrating to a usable volume before fragmentation on a Covaris’ E210 Focused-ultrasonicator. Because the 20 ng of starting material was much less than the 1–5 μg of DNA required by the standard protocol, the following modifications were applied to prevent large-adapter and primer dimers from forming: only 25% of the required adapter oligo mix was used and the volume made up for with water, and enriched using only 25% of the PCR primers before cleaning with Agencourt AMPure beads (Beckman Coulter), yielding 13 ng DNA/μl. The unamplified library was sequenced across six lanes of the Illumina HiSeq 2000 as a SR 36 bp run (v1 cBot kit for cluster generation; v1.5 sequencing kit), generating 631 706 030 reads; however a temporary instrumental malfunction resulted in the generation of only 30-bp reads. Spiked human stool metagenome Stool was collected from a single individual, divided into three samples, and spiked with various concentrations of several pathogens at the Center for Disease Control and Prevention (CDC) in Atlanta, GA. Bacterial pathogens included the A1122 vaccine strain of Yersinia pestis and the Sterne vaccine strain of Bacillus anthracis, both tested at dilutions of 108, 107 and 106 CFU/ml. Viral pathogens included Human adenovirus B (HAdV-3 strain), Mamastrovirus 1 (Human astrovirus 2) and Enterovirus C (Human poliovirus 1 strain Sabin vaccine strain). Stock concentrations for Adenovirus, Poliovirus and Astrovirus were 4.07 × 108 (diluted 1:50, 1:500, 1:5000), 4.14 × 109 (diluted 1:500, 1:5000, 1:50000), and 5.83 × 109 (diluted 1:50, 1:500, 1:5000) genome copies/ml, respectively. The organisms were radiation-inactivated and RNA was extracted using TRIzol LS (Invitrogen). The three samples were each filtered through a 0.1-μm centrifugal filter and cleaned up further using the Qiagen RNeasy kit. RNA concentrations were determined by Qubit RNA assay and confirmed using the Agilent Bioanalyzer 2100 with either the RNA Nano or RNA Pico chips. Approximately 70–80 ul of each sample averaging 371 ng RNA/μl solution was shipped in Eppendorf LoBind tubes sealed with Parafilm to Los Alamos National Laboratory for sequencing. Sequencing libraries were generated for the three fecal RNA samples using Illumina's TruSeq v2 RNA Sample Prep kit, which includes cDNA conversion and PCR enrichment. The three libraries had an average size of 330 bp with a range of 200–700 bp. Each library was sequenced in one lane each of the same HiSeq paired-end 101 bp run. GOTTCHA-based read classification GOTTCHA-based analyses begin with trimming input read datasets by quality, followed by fragmentation of reads into uniform sizes. Reads are first fragmented at any nucleotide < Q20, and the remaining read fragments are split into as many non-overlapping 30-mers (subreads) as possible. Currently, when using data from Pacific Biosciences (RS or RS II), lowering the quality threshold to Q10 appears to provide results comparable to less error-prone reads. Terminal fragments whose lengths are between 30 and 59 bp are retained as subreads as-is without splitting. Unlike other metagenome profiling tools that report classification accuracy on a per-read basis, GOTTCHA's classification accuracy is organism-based and, since exact read matching is currently implemented, longer read fragment lengths increase the chance of a mismatch. For this reason, sequence reads larger than 30 bp are broken down into fragments. We tested the recoverable signal-to-noise ratio of read fragments of length 24–30, 40 and 50 bp and found that the shorter fragments increase signal output, but those of 24 bp in length also increased the classification error rates, so a value of 30 bp was selected. These data suggest, however, that allowing 1–2 mismatches may be beneficial for increasing read recruitment of homologous, but slightly divergent, sequences, however this would require additional parameter optimizations. The trimmed subreads are then mapped to either the prokaryotic and/or viral GOTTCHA databases, using the maximal exact matches (mem) option of the short-read aligner BWA: bwa mem -k 24 -T 0 -B 100 -O 100 -E 100 -t 12, where k is the minimum seed length, T is the minimum alignment score, B is the mismatch penalty, O is the gap open penalty, E is the gap extension penalty, and t is the number of threads. SAM alignment results are then profiled and filtered with the GOTTCHA profiler using the following filter parameters: minLen = 100, minCov = 0.005, minHits = 10, cCov = 0.006, minMLHL = 5. Binary classification GOTTCHA classification occurs at the organism level rather than at the more traditional read level. For the synthetic and mock datasets, an algorithm-identified organism is labeled a true positive (TP), if the organism is known to be present in the sample, or as a false positive (FP) if it is known not to be present. A false negative (FN) is called if the algorithm does not find an organism known to be present. True negatives (TN) are dependent on the number of organisms in the reference database used, and as such, are only reported for GOTTCHA. Specifically, TN = (no. of genomes in database) – (TP + FP + FN). Binary classifications are based on the final output of each program. For the GOTTCHA profiler, this includes a filtering step. GOTTCHA results were filtered in a serial, two-stage process: the first considers only those species whose linear coverage met or exceeded 0.5%. The second rejects any species whose mean linear hit length was <5 and linear coverage <0.6%. Statistical measures and parameter filtering In the GOTTCHA workflow, an organism's linear coverage is GOTTCHA's primary classification parameter. In order to determine the minimum threshold above which an organism should be classified as present in a sample, we evaluated the dependence of the classification precision [ = TP/(TP + FP)] and recall [ = TP/(TP + FN)] over all valid ranges of linear coverage for metagenomes with sequencing amounts that varied over 3 orders of magnitude, from 250k to 300M reads (Supplementary Table S1, Supplementary Figure S1). Precision–recall (PR) curves (Supplementary Figure S2) were created by repeatedly profiling the SAM alignment files for different values of the linear coverage for datasets MG7–MG16. For computational efficiency, rather than computing precision and recall values throughout the entire range of linear coverage (0–100%), we computed these values up to the greatest linear coverage found in the sample. At this point and for all higher values of linear coverage, no true positives are found, and hence, precision = recall = 0. Both plots (Supplementary Figure S2A and S2B) include the random guess performance line and the precision–recall values yielding optimal F1-scores are indicated. We then plotted the best F1-scores [ = 2 × precision × recall/(precision + recall)] obtainable across the entire range of linear coverages in order to assess the best possible performance of each tool among the diverse set of metagenomes (Supplementary Figure S2C). Profiling tool comparisons Several tools were used to help classify metagenome reads and the results were compared (Supplementary Table S2). All tools, including GOTTCHA utilized the same quality-trimmed input dataset. MetaPhlAn v1.7.7 (6) was run with default settings, using Bowtie2 (21) against its database of unique clade-specific marker genes released on 15 October 2012. mOTUs v1 was run using default parameters (7) and reports the species directly with NCBI taxonomic IDs. Kraken v0.10.2-beta was run with default parameters with the preload option using its database available on 3 November 2013 (11), and the Kraken reporter classifies ambiguous reads with the LCA method (22). BLASTn v2.2.28+ (23) was run on 1 million randomly sampled reads for the fecal metagenomic analysis and with all reads in all other cases. BWA v0.7.4-r385 (20) used as a stand-alone tool was run locally using the aln and samse single-end reads option and the SAM file was processed with samtools (24). For both the BWA and BLASTn results, the NCBI taxonomy was assigned to each read using lowest common ancestor (LCA) method. Additionally, because BWA and BLAST are not specifically metagenome read taxonomy assignment tools, we imposed a cutoff such that only species with 10 or more read counts were reported, in order to limit the number of false positives but without compromising true positives. All tools were run with 12 threads. precision, recall, F-score, false discovery rate and accuracy, were calculated for each tool as detailed above. Tool resource utilization Each tool's resource utilization was tracked while processing the HMP mock datasets using 12 threads. The workstation consisted of four Intel quad-core Xeon E7440 (2.40 GHz) processors for 16 cores total, 132 GB RAM, and attached to six Seagate SAS drives (model ST9146852SS) in hardware RAID5 via a LSI Logic/Symbios Logic MegaRAID SAS 1078 controller. Assessing the proportion of genomes that are used for classification Using 100 randomly selected genomes, we investigated how much of their own sequence information is used (i.e. can be classified) by the various tools, and how much of this sequence information contributed deleteriously to the overall genome/taxon assignment (incorrect assignment). For this analysis the 100 genomes were decomposed into all possible 31-mers (so as to maintain minimum compatibility with all classification tools), where each consecutive 31-mer was chosen so as to overlap 30-bp with the previous 31-mer. Each tool was then used to classify these ‘reads’, according to the tool-specific protocol (above) and the read assignments recorded. A correct assignment (TP) was recorded when a 31-mer was assigned to its correct source genome, and an incorrect assignment (FP) was recorded when the 31-mer was assigned to a genome other than its true source. For each tool, the fraction of assigned (TP+FP), and the proportion of correctly assigned (TP/(TP + FP)), and incorrectly assigned reads (FP/(TP + FP)) are displayed using the boxplot function in R, and as a function of taxonomic hierarchy (such that reads placed incorrectly at the lower levels of taxonomy may still be classified correctly at higher levels of taxonomy). Community profiling of synthetic and HMP mock metagenomes Species identification proceeded straightforward with MetaPhlAn, mOTUs and Kraken as we applied no post-filtering to the data. Data generated from the aligners BWA and BLASTn were filtered so that species (genomes) recruiting <10 hits were discarded so as to limit the gross over-reporting of false positives. Relative abundance calculations proceed only upon finalization of species identification. Species relative abundances are provided by MetaPhlAn, mOTUs and Kraken, whereas those for BWA and BLASTn were calculated by fractions of all hit counts. GOTTCHA relative abundances were calculated by calculating the ratio of all bases mapped to the species signatures with the total linear length of all signatures mapped. Community profiling of real, spiked metagenomes Species identification and relative abundances proceeded as described above. The relative abundance of the organisms detected by each tool were organized into a pivot table, and a subset of these organisms—restricted only to those detected by GOTTCHA—formed the basis of a summary heat map using the matrix2png web tool (25). Extreme values were trimmed by 5% (outlier effect). The air filter metagenome values ranged from 3.1 × 10−5 (black) to 0.031 (red), whereas the human stool metagenome ranged from 2.7 × 10−8 (black) to 0.0052 (red); gray values represent organisms unidentified by the respective tool. The human stool metagenome was quite large, so the BLASTn analysis was limited to a random subset of 1M reads only. This dataset also included viral targets which mOTUs and MetaPhlAn are currently incapable of classifying. As such, no viral information is provided for these two tools. In order to correlate spike level with detectability, GOTTCHA hit counts are reported in addition to the relative abundances of the heat map. GOTTCHA classification efficacy of novel genomes through hold-out analyses Nearly 2000 prokaryotic draft genomes of varying degrees of novelty that were not included in the creation of the GOTTCHA databases were passed through the GOTTCHA workflow (with some modification) to attempt to identify the parent taxa. These draft genomes were obtained from NCBI in the form of draft genome assemblies. Since genome assembly collapses sequencing reads into a single, contiguous representation that is essentially devoid of the redundant input data (reads) needed to distinguish signal from noise, we used Jellyfish (26) to decompose all contigs into all possible 30-mers, where each 30-mer occurs one or more times. Since contig qualities were not readily available, no quality-based trimming was implemented. Each k-mer's multiplicity, however, was retained and these ‘read’ fragments were then mapped to the GOTTCHA database, the taxonomic level of which was determined by the taxonomic novelty of the genome analyzed. For example, a genome considered novel at the genus level was mapped to the family GOTTCHA database to attempt to place it in the proper family.",Classification,"accurate readbased metagenome characterization use  hierarchical suite  unique signatures
gottcha' unique reference genome databases fastaencoded databases  unique signatures  prokaryotic  viral genomes  generate  use   work instructions provide   briefly databases  unique genome segment  multiple taxonomic level  family species genus strainlevel etc  use  taxonomic classification  read variants   databases    human mers  remove  also generate  use   study  mers  derive   grch37p10 genome reference consortium huref  craig venter institute  chm1_1 washington univesity school  medicine assemblies  include unplaced scaffold synthetic metagenomes several metagenome datasets  use  establish  validate  appropriate criteria  accurate taxonomic classification  abundance calculation use  gottcha databases    synthetic metagenomic datasets mg1mg16 analyze   study six  create   study  highcomplexity highcoverage hchc metagenomes   total read amount mimic    single illumina hiseq  lane  vary  community composition     organisms relative abundance even  lognormal distribution  perbase quality score  error rat  dataset consist   million   pairedend read read set  derive  either  constant number  genomes even mg1 mg3 mg5   number  cells  randomly select   species  along  lognormal distribution curve lognormal mg2 mg4 mg6 table ​table11  supplementary table  summarize  synthetic metagenomic communities synthetic data  generate use metasim   customize illumina error model  perbase quality assignments derive  follow errors  model  map real illumina hiseq   read  highquality genome reference whose assemblies  consider finish  sequence errors  record  collate accord   read base position   exactlymatched base  record  one lot  incorrectlymapped base  record   separate perbase lot  perbase error probability provide  metasim  obtain  divide  number  error base   specific position   briefly positionbased quality score  assign   error  errorfree position errorfree base receive qualities randomly choose   set  exactlymatched base qualities record   position   read    read whereas errorcontaining base qualities  randomly choose  quality score record   positionspecific lot ten previously publish synthetic metagenomes incorporate  maq error model   also use   crosstool comparison   considerably lower sequence coverage two highcomplexity mediumcoverage hcmc metagenomes    organisms   read mg7 mg8  two lowcomplexity lowcoverage lclc metagenomes    organisms   read mg9mg16 hmp mock metagenomes genomic mixtures   organisms  fix concentrations  create   human microbiome project hmp  test  sequence protocols  analytical pipelines two mixture type  available  even even mixture  aliquots  base  equimolar rrna operon count per organism   stagger stag mixture   rrna operon count  vary     order  magnitude accord   follow table   mixture   sequence    illumina genome analyzer      flx titanium raw sequence data  download   hmp website  since   highly unlikely   observe sequence distribution would correlate  input concentrations   library preparation stage   round  bwa  map use   know community members  reference  use  compute  individual' actual relative abundance although    organisms   mock community  analyse present  limit  complete bacterial genomes  thus exclude  eukaryote candida albicans   incomplete bacterial genome actinomyces odontolyticus leave   reference organisms mg17mg20 air filter metagenome  march  genomic dna  extract   air filter wash phosphate buffer saline  tritonx  spike  random amount  dna   biothreat agent francisella tularensis schu  two illumina libraries  create   extract    dna one construct immediately  dna extraction  one amplify use  qiagen replig whole genome multiple displacement amplification mda kit amplification yield    mda dna    library  prepare   prepare sample  sequence genome dna protocol without modification  sequence   single read    run   illumina genome analyzer iix generate    read  unamplified sample  prepare    illumina protocol  require speed vacuumconcentrating   usable volume  fragmentation   covaris e210 focusedultrasonicator      start material  much less      dna require   standard protocol  follow modifications  apply  prevent largeadapter  primer dimers  form     require adapter oligo mix  use   volume make    water  enrich use     pcr primers  clean  agencourt ampure bead beckman coulter yield   dnaμl  unamplified library  sequence across six lanes   illumina hiseq       run  cbot kit  cluster generation  sequence kit generate    read however  temporary instrumental malfunction result   generation    read spike human stool metagenome stool  collect   single individual divide  three sample  spike  various concentrations  several pathogens   center  disease control  prevention cdc  atlanta  bacterial pathogens include  a1122 vaccine strain  yersinia pestis   sterne vaccine strain  bacillus anthracis  test  dilutions      cfuml viral pathogens include human adenovirus  hadv strain mamastrovirus  human astrovirus   enterovirus  human poliovirus  strain sabin vaccine strain stock concentrations  adenovirus poliovirus  astrovirus     dilute       dilute        dilute    genome copiesml respectively  organisms  radiationinactivated  rna  extract use trizol  invitrogen  three sample   filter    centrifugal filter  clean   use  qiagen rneasy kit rna concentrations  determine  qubit rna assay  confirm use  agilent bioanalyzer   either  rna nano  rna pico chip approximately     sample average   rnaμl solution  ship  eppendorf lobind tube seal  parafilm  los alamos national laboratory  sequence sequence libraries  generate   three fecal rna sample use illumina' truseq  rna sample prep kit  include cdna conversion  pcr enrichment  three libraries   average size      range     library  sequence  one lane     hiseq pairedend   run gottchabased read classification gottchabased analyse begin  trim input read datasets  quality follow  fragmentation  read  uniform size read  first fragment   nucleotide  q20   remain read fragment  split   many nonoverlapping mers subreads  possible currently  use data  pacific biosciences     lower  quality threshold  q10 appear  provide result comparable  less errorprone read terminal fragment whose lengths        retain  subreads asis without split unlike  metagenome profile tool  report classification accuracy   perread basis gottcha' classification accuracy  organismbased  since exact read match  currently implement longer read fragment lengths increase  chance   mismatch   reason sequence read larger     break   fragment  test  recoverable signaltonoise ratio  read fragment  length       find   shorter fragment increase signal output       length also increase  classification error rat   value     select  data suggest however  allow  mismatch may  beneficial  increase read recruitment  homologous  slightly divergent sequence however  would require additional parameter optimizations  trim subreads   map  either  prokaryotic andor viral gottcha databases use  maximal exact match mem option   shortread aligner bwa bwa mem                 minimum seed length    minimum alignment score    mismatch penalty    gap open penalty    gap extension penalty     number  thread sam alignment result   profile  filter   gottcha profiler use  follow filter parameters minlen   mincov   minhits   ccov   minmlhl   binary classification gottcha classification occur   organism level rather     traditional read level   synthetic  mock datasets  algorithmidentified organism  label  true positive    organism  know   present   sample    false positive     know    present  false negative   call   algorithm   find  organism know   present true negative   dependent   number  organisms   reference database use      report  gottcha specifically     genomes  database       binary classifications  base   final output   program   gottcha profiler  include  filter step gottcha result  filter   serial twostage process  first consider   species whose linear coverage meet  exceed   second reject  species whose mean linear hit length    linear coverage  statistical measure  parameter filter   gottcha workflow  organism' linear coverage  gottcha' primary classification parameter  order  determine  minimum threshold    organism   classify  present   sample  evaluate  dependence   classification precision   tptp    recall   tptp     valid range  linear coverage  metagenomes  sequence amount  vary   order  magnitude     read supplementary table  supplementary figure  precisionrecall  curve supplementary figure   create  repeatedly profile  sam alignment file  different value   linear coverage  datasets mg7mg16  computational efficiency rather  compute precision  recall value throughout  entire range  linear coverage   compute  value    greatest linear coverage find   sample   point    higher value  linear coverage  true positives  find  hence precision  recall    plot supplementary figure s2a  s2b include  random guess performance line   precisionrecall value yield optimal f1scores  indicate   plot  best f1scores     precision  recallprecision  recall obtainable across  entire range  linear coverages  order  assess  best possible performance   tool among  diverse set  metagenomes supplementary figure s2c profile tool comparisons several tool  use  help classify metagenome read   result  compare supplementary table   tool include gottcha utilize   qualitytrimmed input dataset metaphlan    run  default settings use bowtie2    database  unique cladespecific marker genes release   october  motus   run use default parameters   report  species directly  ncbi taxonomic ids kraken v0beta  run  default parameters   preload option use  database available   november     kraken reporter classify ambiguous read   lca method  blastn    run   million randomly sample read   fecal metagenomic analysis    read    case bwa v0r385  use   standalone tool  run locally use  aln  samse singleend read option   sam file  process  samtools     bwa  blastn result  ncbi taxonomy  assign   read use lowest common ancestor lca method additionally  bwa  blast   specifically metagenome read taxonomy assignment tool  impose  cutoff    species     read count  report  order  limit  number  false positives  without compromise true positives  tool  run   thread precision recall fscore false discovery rate  accuracy  calculate   tool  detail  tool resource utilization  tool' resource utilization  track  process  hmp mock datasets use  thread  workstation consist  four intel quadcore xeon e7440  ghz processors   core total   ram  attach  six seagate sas drive model st9146852ss  hardware raid5 via  lsi logicsymbios logic megaraid sas  controller assess  proportion  genomes   use  classification use  randomly select genomes  investigate  much    sequence information  use    classify   various tool   much   sequence information contribute deleteriously   overall genometaxon assignment incorrect assignment   analysis   genomes  decompose   possible mers    maintain minimum compatibility   classification tool   consecutive mer  choose    overlap    previous mer  tool   use  classify  read accord   toolspecific protocol    read assignments record  correct assignment   record   mer  assign   correct source genome   incorrect assignment   record   mer  assign   genome    true source   tool  fraction  assign tpfp   proportion  correctly assign tptp    incorrectly assign read fptp    display use  boxplot function      function  taxonomic hierarchy   read place incorrectly   lower level  taxonomy may still  classify correctly  higher level  taxonomy community profile  synthetic  hmp mock metagenomes species identification proceed straightforward  metaphlan motus  kraken   apply  postfiltering   data data generate   aligners bwa  blastn  filter   species genomes recruit  hit  discard    limit  gross overreporting  false positives relative abundance calculations proceed  upon finalization  species identification species relative abundances  provide  metaphlan motus  kraken whereas   bwa  blastn  calculate  fraction   hit count gottcha relative abundances  calculate  calculate  ratio   base map   species signatures   total linear length   signatures map community profile  real spike metagenomes species identification  relative abundances proceed  describe   relative abundance   organisms detect   tool  organize   pivot table   subset   organisms—restricted    detect  gottcha—formed  basis   summary heat map use  matrix2png web tool  extreme value  trim   outlier effect  air filter metagenome value range     black   red whereas  human stool metagenome range     black   red gray value represent organisms unidentified   respective tool  human stool metagenome  quite large   blastn analysis  limit   random subset   read   dataset also include viral target  motus  metaphlan  currently incapable  classify    viral information  provide   two tool  order  correlate spike level  detectability gottcha hit count  report  addition   relative abundances   heat map gottcha classification efficacy  novel genomes  holdout analyse nearly  prokaryotic draft genomes  vary degrees  novelty    include   creation   gottcha databases  pass   gottcha workflow   modification  attempt  identify  parent taxa  draft genomes  obtain  ncbi   form  draft genome assemblies since genome assembly collapse sequence read   single contiguous representation   essentially devoid   redundant input data read need  distinguish signal  noise  use jellyfish   decompose  contigs   possible mers   mer occur one   time since contig qualities   readily available  qualitybased trim  implement  kmer' multiplicity however  retain   read fragment   map   gottcha database  taxonomic level    determine   taxonomic novelty   genome analyze  example  genome consider novel   genus level  map   family gottcha database  attempt  place    proper family",1
42,Genometa,"Genometa - A Fast and Accurate Classifier for Short Metagenomic Shotgun Reads
Genometa The program is an extensive modification of the established Integrated Genome Browser (IGB) genome browser [14]. IGB was selected because of its functionality, clear user interface and extensible, well-documented Java source code. The IGB codebase was forked in order to develop Genometa in a separated subversion repository. SAM to BAM conversion is implemented using the Picard Java library (http://picard.sourceforge.net/), and reads from BAM files are counted, mapped to metadata and subsequently displayed in a histogram and in the genome browser. Initially, support for the Bowtie algorithm [15] has been included in the graphical interface and BWA [16] will be included in the future. Reference sequences for metagenomics 1190 prokaryotic chromosomes from various sources were concatenated and used to build a metagenomic reference sequence. These include the August 2010 versions of the NCBI RefSeq collection, the Human Microbiome project [17], the Genomic Encyclopedia for Bacteria and Archaea [18], the Metahit programme [3], and the Moore Foundation Marine Microbial Genome Sequencing Project (http://www.moore.org/marine-micro.aspx). The earliest sequenced strain genome from each species was included using an in-house script. The included genomes are listed within the Genometa program itself. Short reads can then be mapped onto this reference in a similar fashion to that routinely used in genome resequencing. This reference sequence was used to derive the results mentioned in this paper. Reference sequences will be updated regularly using complete and draft genomes from these resources as well as novel resources arising in the future. We have designed a Java program named RefSelector, downloadable from the Genometa website, which allows customisation of the reference sequence. Users modify the taxa in the included list, run RefSelector, and receive a new reference sequence containing only those genomes included in the modified list. Accuracy on simulated metagenomes The advantage of using simulated data to assess a program is that the true origin and method of production of the datasets are known, and hence positive predictions can indeed be verified. Because short reads offer the lowest cost per bp and have been underutilised by metagenome researchers to date, 50 bp (starting from the first base) of sequence were filtered from metagenomic projects with longer reads using an in-house PERL script. As an exception, short reads were extracted from Sanger reads at position 100 to avoid ambiguous bases present at read starts. Default bowtie settings were used for analyses of the SimLC [19] and simulated ocean metagenome datasets. SimLC is a dataset constructed by other researchers to allow objective assessments of metagenome analysis programs. The simulated ocean metagenome was created with Metasim [20] using 100000 reads from ten marine strains: 17391 reads from Marinomonas sp. MWYL1, 15665 from Shewanella loihica PV-4, 12473 reads from Oceanobacillus iheyensis HTE831, 11890 reads from Nitrosococcus oceani ATCC19707, 10509 reads from Alcanivorax borkumensis SK2, 9252 reads from Synechococcus elongatus PCC6301, 6911 reads from Halobacterium salinarum R1, 5946 reads from Prochlorococcus marinus CCMP1375, 5619 reads from Nitrosopumilus maritimus SCM1 and 4344 reads from Candidatus Pelagibacter ubique HTCC1062. Alignment speed comparison of BLAST and Bowtie In order to test the alignment speed of BLAST and Bowtie on the same datasets reads were collected from a human gut study [21], a human stool diarrhea study [22], a vineyard study [2] and a cystic fibrosis lung dataset [23]. Bowtie was run within Genometa using default settings with the exception of -p (number of threads) being raised to achieve optimal resource usage. BLAST v2.2.13 was run with the following settings: (blastall -p blastn -d allSpecies_august2010.fa -i $input -e 1e-10 -a 7 -b 30 -v 30 -F F -o $output1). Analyses were run on the same machine, and results were normalised to ensure comparability. Extrapolation of BLAST results was considered only after 24 hours had elapsed in order to save power. Benchmarking algorithms on an Illumina human gut microbiome dataset The first 100,000 Illumina 100 bp reads were extracted from an Illumina human gut dataset derived from stool samples (SRR042027, Human Microbiome Project, [17]). Only 100,000 reads were taken as this represents the upper limit which could (in late 2011) be submitted to one of the metagenome webservers tested. Only the first reads of each read pair were used to ensure comparability between the programs under test, since most lack functionality to deal with paired end reads. The exact bowtie command used for the Genometa assessment was: (bowtie -t allSpecies_august2010 –sam -p 15 -f gut.fa gut.sam). Megan [24], MG-RAST [25], and Carma3 [26] were all run with default settings.",Classification,"genometa   fast  accurate classifier  short metagenomic shotgun reads
genometa  program   extensive modification   establish integrate genome browser igb genome browser  igb  select    functionality clear user interface  extensible welldocumented java source code  igb codebase  fork  order  develop genometa   separate subversion repository sam  bam conversion  implement use  picard java library   read  bam file  count map  metadata  subsequently display   histogram    genome browser initially support   bowtie algorithm    include   graphical interface  bwa    include   future reference sequence  metagenomics  prokaryotic chromosomes  various source  concatenate  use  build  metagenomic reference sequence  include  august  versions   ncbi refseq collection  human microbiome project   genomic encyclopedia  bacteria  archaea   metahit programme    moore foundation marine microbial genome sequence project   earliest sequence strain genome   species  include use  inhouse script  include genomes  list within  genometa program  short read    map onto  reference   similar fashion   routinely use  genome resequencing  reference sequence  use  derive  result mention   paper reference sequence   update regularly use complete  draft genomes   resources  well  novel resources arise   future   design  java program name refselector downloadable   genometa website  allow customisation   reference sequence users modify  taxa   include list run refselector  receive  new reference sequence contain   genomes include   modify list accuracy  simulate metagenomes  advantage  use simulate data  assess  program    true origin  method  production   datasets  know  hence positive predictions  indeed  verify  short read offer  lowest cost per     underutilised  metagenome researchers  date   start   first base  sequence  filter  metagenomic project  longer read use  inhouse perl script   exception short read  extract  sanger read  position   avoid ambiguous base present  read start default bowtie settings  use  analyse   simlc   simulate ocean metagenome datasets simlc   dataset construct   researchers  allow objective assessments  metagenome analysis program  simulate ocean metagenome  create  metasim  use  read  ten marine strain  read  marinomonas  mwyl1   shewanella loihica   read  oceanobacillus iheyensis hte831  read  nitrosococcus oceani atcc19707  read  alcanivorax borkumensis sk2  read  synechococcus elongatus pcc6301  read  halobacterium salinarum   read  prochlorococcus marinus ccmp1375  read  nitrosopumilus maritimus scm1   read  candidatus pelagibacter ubique htcc1062 alignment speed comparison  blast  bowtie  order  test  alignment speed  blast  bowtie    datasets read  collect   human gut study   human stool diarrhea study   vineyard study    cystic fibrosis lung dataset  bowtie  run within genometa use default settings   exception   number  thread  raise  achieve optimal resource usage blast   run   follow settings blastall  blastn  allspecies_august2010fa  input            output1 analyse  run    machine  result  normalise  ensure comparability extrapolation  blast result  consider    hours  elapse  order  save power benchmarking algorithms   illumina human gut microbiome dataset  first  illumina   read  extract   illumina human gut dataset derive  stool sample srr042027 human microbiome project    read  take   represent  upper limit  could  late   submit  one   metagenome webservers test   first read   read pair  use  ensure comparability   program  test since  lack functionality  deal  pair end read  exact bowtie command use   genometa assessment  bowtie  allspecies_august2010 sam    gutfa gutsam megan  mgrast   carma3    run  default settings",1
43,LMAT,"Scalable metagenomic taxonomy classification using a reference genome database
k-mer/taxonomy database A reference genome database consists of a collection of genome sequences with each genome sequence assigned a taxonomic identifier. The first step is to convert this ‘raw’ reference genome database into a searchable k-mer/taxonomy database by storing every overlapping k-mer along with select taxonomic IDs. A reference database was constructed from complete and partial microbial genome sequences from the NCBI genome database on October 17, 2011. The ‘raw’ genome database included 301 935 distinct genomic segments (plasmids, chromosomes and other genomic segments) and contains 67 073 viral, 4366 bacterial and 236 archaea segments. The remaining segments are shared among draft eukaryotic microbial genomes that were included as assemblies with many contigs and supplementary mitochondrial genomes from eukaryotes. The reference set includes 1272 bacterial species, 121 archaeal species, 3048 viral species and 335 eukaryotic species. Microbial genome segments range in length from a small number of single read contigs of length less than 100 bases up to a 13 033 779-base chromosome (for Sorangium cellulosum). Taxonomic IDs represent nodes in the taxonomy tree and cover all ranks from an individual genome or strain up to the highest order domains. Figure 1 shows an example representation for the searchable database. As input, database construction requires (i) an NCBI taxonomy tree, (ii) a reference genome sequence database and (iii) mappings between the genome sequence identifiers and taxonomy identifiers from the taxonomy tree. Then, all overlapping k-mers from the genome database are computed, and the LCA for the taxonomic IDs for each k-mer is identified. Finally, a post-order tree traversal up to the LCA counts the number of genomes that contain the k-mer for each taxonomy node in the traversal. Our initial expectation was to use k-mer counts to weigh each k-mer’s contribution to a candidate taxonomic label assignment. However, the weighting procedure was found to be sensitive to genome representation bias and therefore a binary scoring scheme was chosen In our experiments, we use a ‘full’ k-mer/taxonomy database (kFull) and a smaller database built from a ‘marker library’ (kML). A marker library contains the most taxonomically informative set of k-mers present in the raw genome database. The marker library is created by separating k-mers into disjoint groups. A k-mer is in exactly one group. Each group has a unique label, which consists of the names of all genomes (or more generally, sequences including genome, plasmid, segment or chromosome) that contain the k-mers in the group. For example, k-mers that occur in exactly the genomes A, B and C are in one group ‘A, B, C’, k-mers that occur only in genomes B and C are another group ‘B, C’ and k-mers in genomes A, C and D are a third group ‘A, C, D’. As a more concrete example, suppose k-mer1 is present in Yersinia pestis KIM, Y.pestis CO92 and Yersinia pseudotuberculosis IP32953. k-mer2 is present in Y.pestis CO92 and Y.pseudotuberculosis IP32953. Both k-mers have the LCA of Yersinia, but in building the marker library, they are in two separate groups based on the exact set of genomes that contain the k-mer: k-mer1’s group is labeled ‘Y.pestis KIM, Y.pestis CO92, Y.pseudotuberculosis IP32953’, and k-mer2’s group is labeled ‘Y.pestis CO92, Y.pseudotuberculosis IP32953’. All the k-mers in groups containing more than 1000 k-mers are included in the marker library. So, if there are 1000 k-mers in k-mer1’s group, then all those k-mers go into the marker library. If there are 999 k-mers in k-mer2’s group, then none of those k-mers go into the marker library. k-mers whose LCA is above the taxonomic rank of family are not included in the marker library. A k-mer/taxonomy database is created from the marker library’s set of k-mers. 2.2 Scoring a read’s taxonomic IDs In the k-mer/taxonomy database, each k-mer of length k is associated with a list of taxonomic IDs as outlined in Section 2.1. The first step of determining a taxonomy ID for a read from the query set is to assign a score to each taxonomy ID of each k-mer in the read. The score is derived from the proportion of k-mers of the read that occurs under that taxonomy node normalized by the proportion of k-mers of a random read that also appears under that taxonomy node. To illustrate the details of the scoring algorithm, the example in Figure 2 shows a query read with three k-mers. The tax IDs of the read’s constituent k-mers are retrieved from the k-mer/taxonomy database. For each read s of length l, a binary classification table C of size forumla is constructed, in which the K rows represent the k-mers retrieved from the read and the T columns represent candidate taxonomic IDs (k-mers are stored in the database in canonical order removing strand specificity. Only canonically ordered k-mers are used to query the database.). A ‘1’ entry indicates the tax ID (column) belongs to the associated k-mer (row). For each tax ID j, the proportion of k-mers having that tax ID is computed (shown in Fig. 2 in the last row): forumla⁠. In the example, forumla for genome G2. The scoring method uses a random model to limit genome representation bias in the database and avoid assigning taxonomic labels by random chance when a novel organism is not represented in the reference database. The score Sj of read s for a taxonomic ID j is defined as forumla⁠, where PRj represents the proportion of k-mers associated with taxonomy ID j in the random model. The random model estimates PRj, the chance of assigning a read of length l to taxonomy ID j owing to random chance (for simplicity the random score is not shown in Fig. 2). The random model is precomputed for read length l and assumes a random nucleotide composition, which can optionally sample explicitly from a range of GC content values. Reads are randomly generated, and then are searched against the database to calculate a Pj value for each random read r and observed taxonomy ID j. PRj is set to the maximum observed value forumla⁠. Further details of random model construction are included in the Supplementary Material. 2.3 Rank-flexible read classification Taxonomy classification combines LCA selection with the read label score evaluation. Candidate labels are examined in order by decreasing read label score as shown in Figure 3 (for illustration, the numerator of the read label score Pj is shown). The most specific taxonomic label is selected such that no other taxonomic label from a conflicting lineage has a comparable read label score. Comparable is defined as a score within one standard deviation of the best candidate using the read label score distribution for the read. The best candidate is found using the taxonomic lineage from the highest scoring taxonomy label. The path from the highest scoring node to the LCA is created (LCAs for individual k-mers identified off-line are used as a starting point to find the LCA for all retrieved k-mers online, which in some cases can reduce run time costs) and each subsequent label is evaluated for consistency with the lineage. When conflicting labels are encountered, the lineage is pruned further up the tree to resolve the conflict. In the example shown in Figure 3, the lineage from G1 to the LCA n5 is constructed first. When G3 is examined, it is identified as conflicting with G1 and the candidate lineage is pruned to node n1. The process continues until the ninth label is encountered (⁠forumla⁠), which for the purpose of illustration has a score below the threshold of comparable scores. The classification procedure terminates with the read assigned label n3. Classifications are divided into categories. The ‘LCA Match’ category means the LCA has a lower read label score but there are multiple conflicting labels with higher comparable read label score. This is the traditional LCA assignment algorithm (Huson et al., 2007) where multiple significant matches from competing lineages are found. The ‘Direct Match’ category means the classification reflects matches to the taxonomy without a conflicting lineage but the classification could be from any rank, including strain, species, genus or higher. Higher order rank assignments in the Direct Match category imply that the read label score for lower rank assignments were below the comparable score threshold and suggest sequence novelty in the classified sequence. ‘Novel Match’ is reported when multiple child nodes from competing lineages have comparable read label scores above the threshold but the LCA has a higher read label score. This indicates novelty in the query read and could occur when a substantial subset of the read’s k-mers are found in competing lineages, but the combination of k-mers observed at the LCA node is more significant. 2.4 Database ingest A key feature of our approach is to store the k-mer/taxonomy database in a file rather than build it anew for each query run; this permits shifting computational costs to the off-line taxonomy/genome ingest phase. The database is created once, and then is used repeatedly during classification. To enable database search during classification, the file is mapped into the memory address space of the classification program, allowing k-mers and associated taxonomy IDs to be accessed directly. Previous work at Lawrence Livermore National Laboratory (LLNL) has modified the Jemalloc memory management library (Evans, 2006) to enable memory allocation from an address range memory-mapped to a file residing on a storage device. Jemalloc is a drop-in replacement for regular malloc routines for allocating memory. Our modification to Jemalloc allows for an additional step to specify the database filename (Jemalloc memory-maps to temporary files). We have chosen to use the memory-map file approach rather than implementing an out-of-core indexing algorithm because of the ease of programming using the memory-map abstraction for the persistent storage of data structures. We place the memory-mapped files onto a ramdisk for in-memory performance. An ingest utility creates the database, which includes a hash table whose keys are k-mers and values are sets of taxonomy identifiers. Supplementary Table S2 shows numbers of k-mers present and the total storage required for several of the databases. The current default settings use 619 GB (kFull) and 39 GB (kML). The databases use 6 bytes per taxon, including genome counts information. We found that these counts are extraneous information and only 2 bytes per taxon identifier are required. Using 2 bytes per taxon allows for 65 566 distinct taxa (the current database has 18 498 distinct taxa). Future work is expected to reduce the database size to 413 GB (kFull) and 26 GB (kML). The ingest pipeline for the full database took ∼17 h using up to 256 2.3 GHz Advanced Micro Devices (AMD) compute nodes each with 32 GB running single-threaded tasks that dumped intermediate results to files in the parallel file system. 2.5 Test data To compare performance with existing state-of-the-art published tools, accuracy was compared with PhymmBL (Brady and Salzberg, 2011), MetaPhlAn (Segata et al., 2012) and Genometa (Davenport et al., 2012). PhymmBL balances classifying known species with classifying novel organisms but uses BLAST, which does not scale well with sequencer output (Angiuoli et al., 2011). MetaPhlAn uses a small marker library making it scalable but it does not attempt to label every read. It is also optimized to do relative abundance estimation, which LMAT currently does not do. Genometa replaces BLAST with a potentially faster search algorithm (Bowtie2 or BWA) and attempts to assign a taxonomic label to every read. Ideally, the same reference database would be used for all programs. However, adapting our database to work with Genometa and PhymmBL required significant customization and was not technically feasible with MetaPhlAn. Therefore, the existing reference database of each tool was used. The Genometa database is the oldest database created in 2010, our reference database was created in fall 2011, and PhymmBL and MetaPhlAn use databases created in mid 2012. The published PhymmBL dataset that uses a read length of 100 was chosen as the test query dataset. As the test data were created before each reference database was created, the query species should be present in each reference database. Three additional simulated test query sets were used to evaluate the accuracy of our method in the presence of query sequences absent from the reference database for viruses, prokaryotes and eukaryotes (fungi and protists). MetaSim was used with a 100 bp (Barthelson et al., 2011) and 80 bp (Richter et al., 2008) Illumina error model to generate the novel bacterial and viral dataset, respectively. Eukaryotes were taken from single species sequencing data deposited in the Short Read Archive (SRA) and include Trypanosoma evansi, Candida albicans, Coccidioides immitis, Aspergillus fumigatus and Entamoeba histolytica (read lengths ranged from 36 to 200). For the bacterial dataset, 100 sequences not found in the reference database (determined by GenBank Identifier and header comparison) were selected at random to serve as the candidate test set with 1 000 000 simulated reads and equal concentrations of the 100 bacteria. The 100 strains were made up of 75 distinct species, 14 of which were species not found in our reference database. For the viral case, 6921 reads were generated, assuming equal concentrations of 25 viral genomes, which made up 25 distinct species, 10 of which were not found in the reference database. Not every test sequence in the ‘novel’ datasets proved to be divergent from the reference database. A detailed description of the test data is given in the Supplementary Material. To measure run time, three non-synthetic metagenomic samples representing a viral metagenome (SRX022172), a human microbiome metagenome (ERR011121) and a single species raw read ‘metagenome’ (DRR000184) were taken from the SRA.",Classification,"scalable metagenomic taxonomy classification use  reference genome database
kmertaxonomy database  reference genome database consist   collection  genome sequence   genome sequence assign  taxonomic identifier  first step   convert  raw reference genome database   searchable kmertaxonomy database  store every overlap kmer along  select taxonomic ids  reference database  construct  complete  partial microbial genome sequence   ncbi genome database  october    raw genome database include   distinct genomic segment plasmids chromosomes   genomic segment  contain   viral  bacterial   archaea segment  remain segment  share among draft eukaryotic microbial genomes   include  assemblies  many contigs  supplementary mitochondrial genomes  eukaryotes  reference set include  bacterial species  archaeal species  viral species   eukaryotic species microbial genome segment range  length   small number  single read contigs  length less   base      base chromosome  sorangium cellulosum taxonomic ids represent nod   taxonomy tree  cover  rank   individual genome  strain    highest order domains figure  show  example representation   searchable database  input database construction require   ncbi taxonomy tree   reference genome sequence database  iii mappings   genome sequence identifiers  taxonomy identifiers   taxonomy tree   overlap kmers   genome database  compute   lca   taxonomic ids   kmer  identify finally  postorder tree traversal    lca count  number  genomes  contain  kmer   taxonomy node   traversal  initial expectation   use kmer count  weigh  kmers contribution   candidate taxonomic label assignment however  weight procedure  find   sensitive  genome representation bias  therefore  binary score scheme  choose   experiment  use  full kmertaxonomy database kfull   smaller database build   marker library kml  marker library contain   taxonomically informative set  kmers present   raw genome database  marker library  create  separate kmers  disjoint group  kmer   exactly one group  group   unique label  consist   name   genomes   generally sequence include genome plasmid segment  chromosome  contain  kmers   group  example kmers  occur  exactly  genomes       one group    kmers  occur   genomes     another group    kmers  genomes       third group       concrete example suppose kmer1  present  yersinia pestis kim ypestis co92  yersinia pseudotuberculosis ip32953 kmer2  present  ypestis co92  ypseudotuberculosis ip32953  kmers   lca  yersinia   build  marker library    two separate group base   exact set  genomes  contain  kmer kmer1s group  label ypestis kim ypestis co92 ypseudotuberculosis ip32953  kmer2s group  label ypestis co92 ypseudotuberculosis ip32953   kmers  group contain    kmers  include   marker library      kmers  kmer1s group    kmers    marker library     kmers  kmer2s group  none   kmers    marker library kmers whose lca    taxonomic rank  family   include   marker library  kmertaxonomy database  create   marker librarys set  kmers  score  read taxonomic ids   kmertaxonomy database  kmer  length   associate   list  taxonomic ids  outline  section   first step  determine  taxonomy    read   query set   assign  score   taxonomy    kmer   read  score  derive   proportion  kmers   read  occur   taxonomy node normalize   proportion  kmers   random read  also appear   taxonomy node  illustrate  detail   score algorithm  example  figure  show  query read  three kmers  tax ids   read constituent kmers  retrieve   kmertaxonomy database   read   length   binary classification table   size forumla  construct     row represent  kmers retrieve   read    columns represent candidate taxonomic ids kmers  store   database  canonical order remove strand specificity  canonically order kmers  use  query  database   entry indicate  tax  column belong   associate kmer row   tax    proportion  kmers   tax   compute show  fig    last row forumla⁠   example forumla  genome   score method use  random model  limit genome representation bias   database  avoid assign taxonomic label  random chance   novel organism   represent   reference database  score   read    taxonomic    define  forumla⁠  prj represent  proportion  kmers associate  taxonomy     random model  random model estimate prj  chance  assign  read  length   taxonomy   owe  random chance  simplicity  random score   show  fig   random model  precomputed  read length   assume  random nucleotide composition   optionally sample explicitly   range   content value read  randomly generate    search   database  calculate   value   random read   observe taxonomy   prj  set   maximum observe value forumla⁠  detail  random model construction  include   supplementary material  rankflexible read classification taxonomy classification combine lca selection   read label score evaluation candidate label  examine  order  decrease read label score  show  figure   illustration  numerator   read label score   show   specific taxonomic label  select     taxonomic label   conflict lineage   comparable read label score comparable  define   score within one standard deviation   best candidate use  read label score distribution   read  best candidate  find use  taxonomic lineage   highest score taxonomy label  path   highest score node   lca  create lcas  individual kmers identify offline  use   start point  find  lca   retrieve kmers online    case  reduce run time cost   subsequent label  evaluate  consistency   lineage  conflict label  encounter  lineage  prune    tree  resolve  conflict   example show  figure   lineage     lca   construct first    examine   identify  conflict     candidate lineage  prune  node   process continue   ninth label  encounter ⁠forumla⁠    purpose  illustration   score   threshold  comparable score  classification procedure terminate   read assign label  classifications  divide  categories  lca match category mean  lca   lower read label score    multiple conflict label  higher comparable read label score    traditional lca assignment algorithm huson     multiple significant match  compete lineages  find  direct match category mean  classification reflect match   taxonomy without  conflict lineage   classification could    rank include strain species genus  higher higher order rank assignments   direct match category imply   read label score  lower rank assignments    comparable score threshold  suggest sequence novelty   classify sequence novel match  report  multiple child nod  compete lineages  comparable read label score   threshold   lca   higher read label score  indicate novelty   query read  could occur   substantial subset   read kmers  find  compete lineages   combination  kmers observe   lca node   significant  database ingest  key feature   approach   store  kmertaxonomy database   file rather  build  anew   query run  permit shift computational cost   offline taxonomygenome ingest phase  database  create     use repeatedly  classification  enable database search  classification  file  map   memory address space   classification program allow kmers  associate taxonomy ids   access directly previous work  lawrence livermore national laboratory llnl  modify  jemalloc memory management library evans   enable memory allocation   address range memorymapped   file reside   storage device jemalloc   dropin replacement  regular malloc routines  allocate memory  modification  jemalloc allow   additional step  specify  database filename jemalloc memorymaps  temporary file   choose  use  memorymap file approach rather  implement  outofcore index algorithm    ease  program use  memorymap abstraction   persistent storage  data structure  place  memorymapped file onto  ramdisk  inmemory performance  ingest utility create  database  include  hash table whose key  kmers  value  set  taxonomy identifiers supplementary table  show number  kmers present   total storage require  several   databases  current default settings use   kfull    kml  databases use  bytes per taxon include genome count information  find   count  extraneous information    bytes per taxon identifier  require use  bytes per taxon allow    distinct taxa  current database    distinct taxa future work  expect  reduce  database size    kfull    kml  ingest pipeline   full database take   use     ghz advance micro devices amd compute nod     run singlethreaded task  dump intermediate result  file   parallel file system  test data  compare performance  exist stateoftheart publish tool accuracy  compare  phymmbl brady  salzberg  metaphlan segata     genometa davenport    phymmbl balance classify know species  classify novel organisms  use blast    scale well  sequencer output angiuoli    metaphlan use  small marker library make  scalable     attempt  label every read   also optimize   relative abundance estimation  lmat currently    genometa replace blast   potentially faster search algorithm bowtie2  bwa  attempt  assign  taxonomic label  every read ideally   reference database would  use   program however adapt  database  work  genometa  phymmbl require significant customization    technically feasible  metaphlan therefore  exist reference database   tool  use  genometa database   oldest database create    reference database  create  fall   phymmbl  metaphlan use databases create  mid   publish phymmbl dataset  use  read length    choose   test query dataset   test data  create   reference database  create  query species   present   reference database three additional simulate test query set  use  evaluate  accuracy   method   presence  query sequence absent   reference database  viruses prokaryotes  eukaryotes fungi  protists metasim  use     barthelson       richter    illumina error model  generate  novel bacterial  viral dataset respectively eukaryotes  take  single species sequence data deposit   short read archive sra  include trypanosoma evansi candida albicans coccidioides immitis aspergillus fumigatus  entamoeba histolytica read lengths range       bacterial dataset  sequence  find   reference database determine  genbank identifier  header comparison  select  random  serve   candidate test set     simulate read  equal concentrations    bacteria   strain  make    distinct species     species  find   reference database   viral case  read  generate assume equal concentrations   viral genomes  make   distinct species      find   reference database  every test sequence   novel datasets prove   divergent   reference database  detail description   test data  give   supplementary material  measure run time three nonsynthetic metagenomic sample represent  viral metagenome srx022172  human microbiome metagenome err011121   single species raw read metagenome drr000184  take   sra",1
44,DUDes,"DUDes: a top-down taxonomic profiler for metagenomics
The DUDes workflow requires three main inputs: a set of reads, references sequences and a taxonomic tree structure (Fig. 1). By mapping the reads against the reference sequences, a SAM file (Li et al., 2009) is created. The linkage between the reference sequences and the taxonomic information is performed by DUDesDB and stored in a database file. Both files serve as input to DUDes profiler which performs an iterative top-down analysis on the taxonomic tree structure. The first step of the DUDes algorithm is to assign a set of reference sequences and read matches for each node of the taxonomic tree (e.g. the root node contains all sequences and matches, while the Escherichia node (genus) will contain only the sequences corresponding to the organisms that belong to this classification and their respective matches). The general idea is to start at the root node and go deeper into the tree (Fig. 2), evaluating all taxonomic levels and identifying nodes with significant matches that are the ones that will be considered present in the sample. The presence of a node in a certain taxonomic level is defined by the following steps Comparison between DUD and LCA. DUD is a top-down approach that starts from low taxonomic levels. LCA is a bottom-up method that solves ambiguities from high taxonomic levels. The node marked with a circle is considered the LCA for its children nodes. Dotted arrows represents how DUD can be more specific in higher taxonomic levels pointing to candidates nodes while LCA is more conservative and goes back to the LCA Steps for identifications on a taxonomic level. (a) Taxonomic tree structure with the tested level in black and the reference sequences of each node (R1-Rn). Nodes A and B are the first ones to be compared against each other (b) Bin generation: reference sequences (green) and their respective read matches are subdivided into bins. For each bin, a match-based score is calculated (c)P-value estimation: 1—Bin scores of the node A (blue, references R1-R4) are compared against bin scores of node B (red, references R5-R7). The black line represents the cutoff value based on the top 50% of bin scores from node A. 2—Only the best bins are selected based on the cutoff. Permutations are then performed among the selected bins. 3—The distribution is generated by the randomly permuted difference of means between bin scores. The red dotted line represents the critical value and the green the observed difference of means. In this example node A is therefore significant against node B (P-value ≤ critical value). (d) Identification: steps b and c are then repeated for all pair of nodes. In this example, node A is identified because it was the only significant node against all the others (rows) and any other node could be significant against it (columns) 2.1 Bin generation First, we create a set of bins for each node of the tree. Bins are sub-sequences from the reference sequences assigned to the node (Fig. 3b). They are non-overlapping and equal-sized. The bin size is fixed for all nodes and it is defined by default as the 25th percentile of the sequence lengths of the whole reference database. The bin size should be a balance point between the number of small sequences in the database and speed requirements: the smaller the bin size, the more calculations are needed. The larger the bin size, the bigger the chance that higher nodes lack of full bins. In our analysis the 25th percentile provides a good trade-off between those, ensuring that the majority of reference sequences will have at least one full bin. Each one of these sub-regions will have a bin score based on read matches, defined as the sum of all match scores (ms). The match score is defined for each match m as: msm=l−e (1) where l is the length and e the edit distance of the match (minimum number of edit operations—insertions, deletions and substitutions of letters—transforming one sequence into another (Levenshtein, 1966)). Bin scores do not only take the number of matches to the bins into account, but also the number of mismatches and indels in those sub-regions. 2.2 P-value estimation DUDes performs a pairwise comparison between all nodes on a taxonomic level, estimating a P-value for each pair with permutation tests. The permutations occur between the nodes’ bin scores. Only bins with scores higher than zero are considered. Additionally, only the best bins are permuted: a cutoff is chosen, based on the number of bins representing the top 50% scores of the main node (Fig. 3c-1). This cutoff is useful in order not to prioritize nodes with larger or more references sequences, meaning that the comparison of a certain node X against node Y can have a different cutoff value from the comparison of the same node Y against X. For example: consider a node X with 100 bins and a node Y with only 70 bins. When comparing X against Y, only the first 50 bins from node X are going to be compared against the first 50 bins of node Y. When comparing Y against X, only 35 bins from both nodes are going to be considered (Supplementary Figure S1). When one of the nodes does not have enough bins to be compared, the cutoff will be reset to the total number of bins of the node with fewer bins (Supplementary Figure S2). The cutoff is also useful to remove poorly mapped bins with low scores and to normalize the number of bins between the two compared nodes, allowing a fair comparison between them. A limit of five bins is required for each node to allow a significant permutation. Permutations between the selected bin scores are performed 1000 times by default. The values are randomly shuffled and separated in two groups based on the cutoff value. The random difference of means between the groups is calculated, generating a distribution like the one shown in Figure 3c-3. A one-sided P-value is then estimated based on the observed difference of means between their actual bin scores (Supplementary Information—P-value estimation). The estimated P-value is considered significant if it is lower than a certain critical value. Because many hypotheses are being tested, multiple testing correction is necessary to control the type I error, that is, the risk of falsely rejecting a hypothesis that is true (Goeman and Solari, 2014). Here we applied two methods: Bonferroni correction locally (taxonomic level) and the Meinshausen procedure globally (tree level) (Meinshausen, 2008). Two methods were applied together because multiple tests occur in two ways: several taxonomic levels on the tree and several nodes for each taxonomic level are tested. The Meinshausen procedure was chosen for being a hierarchical approach that can achieve larger power for coarser resolution levels. It was applied to control the multiple testing error generated by several comparisons on each taxonomic level of the tree. Bonferroni correction was applied locally to correct for the multiple tests performed among the nodes in a taxonomic level. Although the Bonferroni method is highly conservative in general, this is not critical in our application since the number of nodes on a taxonomic level is usually fairly small. The critical value cv for each node n is calculated as: cvn=αN−1LnL (2) where α is the significance level threshold (default 0.05) and N is the total number of nodes in the tested taxonomic level (minus itself), comprising the Bonferroni correction. Additionally, L is the total number of leaves of the tree and Ln the total number of leaves below node n. They stand for the Meinshausen procedure correction. All nodes are tested against each other at the same level and, if the P-value of the comparison is below the critical value, the comparison is set as significant as pictured in the table in Figure 3d. 2.3 Identification After all nodes within a taxonomic level have been compared against each other, they are evaluated using two criteria: how many times they are not significant and how many times other nodes are significant against them (columns and rows in Fig. 3d, respectively). The number of occurrences of those two metric are summed for each node and DUDes selects the one with the minimum value as identified, therefore present on the sample. In a metagenomic dataset it is possible that more than one node is identified at once, when their bins have similar abundances. In this case, two or more nodes with sufficient number of bins and matches that could not be significant against each other are going to be identified concurrently. That means that more than one node is going to be considered present in the current taxonomic level, leading to more than one path on the following tree analysis. The three-step-algorithm (consisting of bin generation, P-value estimation and identification) continues to the next taxonomic level only for the children of the identified nodes. The process is iterated until it reaches the leaf nodes of the tree (here considered the deepest possible uncommon descendent—at species level by default) or until no more identifications are possible, due to lack of minimum matches or bins support. Here we can point out an advantage of the DUD method over the LCA. When the LCA is applied, the results tend to be very conservative because it solves ambiguous identifications by going back one taxonomic level to the LCA. Instead, the DUD approach will always try to go for a deeper taxonomic level, even when ambiguities are found (Fig. 2). That way it is possible to have identifications in higher taxonomic levels. Besides, when the provided data does not allow a specific identification on higher levels, it is still possible to propose a set of likely candidates based on the concurrent identification, being more specific than going back in the taxonomic tree. At the end of the tree iteration, one or more paths on the tree and their leaf nodes are identified as candidate TGs to be present on the sample. Because metagenomic samples can contain hundreds to thousands of organisms (Handelsmanl et al., 1998), a filtering step is performed to remove identified matches and allow more iterations. DUDes perform this step by filtering out the direct matches on the identified candidates’ references sequences. Furthermore, all matches from the reads that had at least one direct match are analyzed. If those read matches have a ms lower than the direct match, they are considered indirect matches, and are filtered out as well. With this new set of matches, a new iteration is started from the root node. Several iterations are performed until the number of matches is below a certain threshold or until all matches were filtered. At the end, a relative abundance value is calculated for the final candidates. These are based on the direct matches of the identified leaf nodes and normalized by the length of their respective reference sequences. Each identified leaf node n has an abundance ab calculated as: abn=∑ri=1∑tj=1msl (3) where r is the number of references sequences belonging to the node n, t are the matches belonging to the reference i, ms is the match score and l is the length of the reference i. The abundance of the parent nodes are based on the cumulative sum of their children nodes’ abundance. DUDes outputs a file with a set of final candidates in the BioBoxes Profiling Output Format v0.9.3 (https://github.com/bioboxes/rfc/blob/master/data-format/profiling.mkd). When strain identification is selected, DUDes outputs an additional file with all identified strains and their relative abundances. 2.4 Strain identification Optionally, DUDes will try to extend the species identification and provide a set of probable strains present in the sample. The process of strain identification works identically as the three-step-algorithm but starting the analysis from each one of the identified species nodes. Sequences among strains usually have high similarity in their composition. This makes the identification process more challenging. For that reason we implemented a post-filtering process to better select a candidate strain. Given a set of identified strains by the three-step method, we choose one representative candidate, which has the maximum summed value of ms in this set. Alternatively we provide a second output, reporting all other strains identified and their relative abundance. 2.5 DUDesDB DUDesDB pre-processes the taxonomic tree structure and the reference sequences, generating a database file to be used by DUDes profiler. The current version of DUDesDB supports the NCBI taxonomic tree (NCBI, 2015) and uses the GI or accession.version identifier to make the link between reference sequences and tree nodes. Because the strain level is not directly defined in the NCBI taxonomic tree structure, we considered any unclassified node with the tag no rank after the species level as a strain node. 2.6 Mapping DUDes can handle multiple matches and account for the mapping quality with match scores, improving its identification capabilities. By default, the number of allowed matches should be as high as possible, allowing all matches when feasible. Since that can be computational impracticable, we used a default value of 60 matches for each read. Other mapping parameters can be found in the Supplementary information—Tools and parameters. 2.7 Experiments DUDes evaluation was performed in four distinct datasets: two synthetic communities and two real metagenomic samples. First we analyzed synthetic metagenomic data with available ground truth to evaluate how precise is our identification method. We chose a common set for metagenomics evaluations—the Human Microbiome Project (HMP) mock community (Turnbaugh et al., 2007), an in vitro synthetic mixture of 22 organisms (20 Bacteria, 1 Archaea and 1 Eukaryote) that mimics errors and organism abundances from real metagenomics samples. Only Bacteria and Archaea were considered in this evaluation. Further, this set was also divided in sub-sets of different percentages of reads and compared against other metagenomics analysis tools: kraken (Wood and Salzberg, 2014), GOTTCHA (Freitas et al., 2015) and MetaPhlAn2 (Truong et al., 2015). They were chosen for having rather different approaches to solve the taxonomic profiling problem and for having good results in recent metagenomics studies (Lindgreen et al., 2016). Kraken is a read binning tool that uses a k-mer approach to classify each read in a given sample with focus in high performance. GOTTCHA is a taxonomic profiler that uses non-redundant signature databases and aims for lower false discovery rates. MetaPhlAn2 relies on a curated database of approximately 1 Million unique clade-specific marker genes for profiling metagenomic samples. A second synthetic community consisted of 64 laboratory-mixed microbial genomic DNAs was also evaluated (Shakya et al., 2013). This community made of organisms of known sequences has a very broad diversity among bacteria and archaea and a wide range of genetic variation at different taxonomic levels. At the same time, this dataset provides a large number of sequenced reads (∼110 M), allowing a more realistic performance evaluation. We also applied DUDes to real metagenomic samples of gut microbiomes from the outbreak of Shiga-toxigenic Escherichia coli (STEC) in Germany in 2011 (Loman et al., 2013). With this dataset we evaluated how well DUDes performs in a real scenario to profile a pathogenic sample, and compared the results with the previously known experiments. Furthermore, we evaluated this set based on previous known information (e.g. lab experiments, other tools based on the LCA approach) performing a more specific profiling. Lastly, we profile a marine dataset from Tara Oceans (Sunagawa et al., 2015) with Bacteria, Archaea, Virus and Eukaryotes present on the sample, showing the versatility of the tool. Datasets’ details are shown in Supplementary Table S1. The HMP and STEC samples were pre-processed with the digital normalization algorithm (Brown et al., 2012) for decreasing sampling variation and for error correction. In both analysis, the reference database for DUDes and kraken was generated with the set of complete genomes sequences (Bacteria and Archaea) together with the taxonomic tree structure, both from NCBI (NCBI, 2015) from March 26, 2015. The 64-organim set was used without any filter. For this set we used the above database with the addition of four non-complete genome sequences [taxid: 901, 52598, 314267, 304736] to have all species in the sample available. For the Tara dataset we made a custom database, containing only expected marine organisms. Bacterial, Archaeal and Viral taxons were obtained from the references sequences used in the Tara Oceans Project (Sunagawa et al., 2015) and the Eukaryotic set of taxons was obtained from the MMETSP (Keeling et al., 2014). All NCBI refseq sequences relative to those taxons were collected to generate the database (from January 31, 2016). For MetaPhlAn2 and GOTTCHA (all sets) we used their provided database, v20 and v20150825, respectively. Bowtie2 (Langmead and Salzberg, 2012) was used for read mapping. Parameters and usage details of each tool can be found in the Supplementary Materials—Tools and parameters. We evaluated the output from each tool based on a binary classification of the sorted taxonomic profile. The binary classification is valid for TGs of a certain taxonomic level. True positives are all TGs present in the sample and correctly identified, false positives are the identified TGs known to not be present in the sample, false negatives are the TGs that are present but could not be identified and true negatives are all TGs on the database not identified and known not to be present in the sample.",Classification,"dudes  topdown taxonomic profiler  metagenomics
 dudes workflow require three main input  set  read reference sequence   taxonomic tree structure fig   map  read   reference sequence  sam file      create  linkage   reference sequence   taxonomic information  perform  dudesdb  store   database file  file serve  input  dudes profiler  perform  iterative topdown analysis   taxonomic tree structure  first step   dudes algorithm   assign  set  reference sequence  read match   node   taxonomic tree   root node contain  sequence  match   escherichia node genus  contain   sequence correspond   organisms  belong   classification   respective match  general idea   start   root node   deeper   tree fig  evaluate  taxonomic level  identify nod  significant match    ones    consider present   sample  presence   node   certain taxonomic level  define   follow step comparison  dud  lca dud   topdown approach  start  low taxonomic level lca   bottomup method  solve ambiguities  high taxonomic level  node mark   circle  consider  lca   children nod dot arrows represent  dud    specific  higher taxonomic level point  candidates nod  lca   conservative  go back   lca step  identifications   taxonomic level  taxonomic tree structure   test level  black   reference sequence   node r1rn nod      first ones   compare     bin generation reference sequence green   respective read match  subdivide  bin   bin  matchbased score  calculate cpvalue estimation —bin score   node  blue reference r1r4  compare  bin score  node  red reference r5r7  black line represent  cutoff value base   top   bin score  node  —  best bin  select base   cutoff permutations   perform among  select bin — distribution  generate   randomly permute difference  mean  bin score  red dot line represent  critical value   green  observe difference  mean   example node   therefore significant  node  pvalue ≤ critical value  identification step      repeat   pair  nod   example node   identify      significant node    others row    node could  significant   columns  bin generation first  create  set  bin   node   tree bin  subsequences   reference sequence assign   node fig    nonoverlapping  equalsized  bin size  fix   nod    define  default   25th percentile   sequence lengths   whole reference database  bin size    balance point   number  small sequence   database  speed requirements  smaller  bin size   calculations  need  larger  bin size  bigger  chance  higher nod lack  full bin   analysis  25th percentile provide  good tradeoff   ensure   majority  reference sequence    least one full bin  one   subregions    bin score base  read match define   sum   match score   match score  define   match   msmle      length    edit distance   match minimum number  edit operations—insertions deletions  substitutions  letters—transforming one sequence  another levenshtein  bin score    take  number  match   bin  account  also  number  mismatch  indels   subregions  pvalue estimation dudes perform  pairwise comparison   nod   taxonomic level estimate  pvalue   pair  permutation test  permutations occur   nod bin score  bin  score higher  zero  consider additionally   best bin  permute  cutoff  choose base   number  bin represent  top  score   main node fig   cutoff  useful  order   prioritize nod  larger   reference sequence mean   comparison   certain node   node     different cutoff value   comparison    node     example consider  node    bin   node     bin  compare      first  bin  node   go   compare   first  bin  node   compare      bin   nod  go   consider supplementary figure   one   nod    enough bin   compare  cutoff   reset   total number  bin   node  fewer bin supplementary figure   cutoff  also useful  remove poorly map bin  low score   normalize  number  bin   two compare nod allow  fair comparison    limit  five bin  require   node  allow  significant permutation permutations   select bin score  perform  time  default  value  randomly shuffle  separate  two group base   cutoff value  random difference  mean   group  calculate generate  distribution like  one show  figure   onesided pvalue   estimate base   observe difference  mean   actual bin score supplementary information—pvalue estimation  estimate pvalue  consider significant    lower   certain critical value  many hypotheses   test multiple test correction  necessary  control  type  error    risk  falsely reject  hypothesis   true goeman  solari    apply two methods bonferroni correction locally taxonomic level   meinshausen procedure globally tree level meinshausen  two methods  apply together  multiple test occur  two ways several taxonomic level   tree  several nod   taxonomic level  test  meinshausen procedure  choose    hierarchical approach   achieve larger power  coarser resolution level   apply  control  multiple test error generate  several comparisons   taxonomic level   tree bonferroni correction  apply locally  correct   multiple test perform among  nod   taxonomic level although  bonferroni method  highly conservative  general    critical   application since  number  nod   taxonomic level  usually fairly small  critical value    node   calculate  cvnαn1lnl      significance level threshold default      total number  nod   test taxonomic level minus  comprise  bonferroni correction additionally    total number  leave   tree    total number  leave  node   stand   meinshausen procedure correction  nod  test       level    pvalue   comparison    critical value  comparison  set  significant  picture   table  figure   identification   nod within  taxonomic level   compare      evaluate use two criteria  many time    significant   many time  nod  significant   columns  row  fig  respectively  number  occurrences   two metric  sum   node  dudes select  one   minimum value  identify therefore present   sample   metagenomic dataset   possible    one node  identify     bin  similar abundances   case two   nod  sufficient number  bin  match  could   significant     go   identify concurrently  mean    one node  go   consider present   current taxonomic level lead    one path   follow tree analysis  threestepalgorithm consist  bin generation pvalue estimation  identification continue   next taxonomic level    children   identify nod  process  iterate   reach  leaf nod   tree  consider  deepest possible uncommon descendent— species level  default     identifications  possible due  lack  minimum match  bin support    point   advantage   dud method   lca   lca  apply  result tend    conservative   solve ambiguous identifications  go back one taxonomic level   lca instead  dud approach  always try     deeper taxonomic level even  ambiguities  find fig   way   possible   identifications  higher taxonomic level besides   provide data   allow  specific identification  higher level   still possible  propose  set  likely candidates base   concurrent identification   specific  go back   taxonomic tree   end   tree iteration one   paths   tree   leaf nod  identify  candidate tgs   present   sample  metagenomic sample  contain hundreds  thousands  organisms handelsmanl     filter step  perform  remove identify match  allow  iterations dudes perform  step  filter   direct match   identify candidates reference sequence furthermore  match   read    least one direct match  analyze   read match    lower   direct match   consider indirect match   filter   well   new set  match  new iteration  start   root node several iterations  perform   number  match    certain threshold    match  filter   end  relative abundance value  calculate   final candidates   base   direct match   identify leaf nod  normalize   length   respective reference sequence  identify leaf node    abundance  calculate  abn∑∑tj1msl      number  reference sequence belong   node     match belong   reference     match score     length   reference   abundance   parent nod  base   cumulative sum   children nod abundance dudes output  file   set  final candidates   bioboxes profile output format    strain identification  select dudes output  additional file   identify strain   relative abundances  strain identification optionally dudes  try  extend  species identification  provide  set  probable strain present   sample  process  strain identification work identically   threestepalgorithm  start  analysis   one   identify species nod sequence among strain usually  high similarity   composition  make  identification process  challenge   reason  implement  postfiltering process  better select  candidate strain give  set  identify strain   threestep method  choose one representative candidate    maximum sum value     set alternatively  provide  second output report   strain identify   relative abundance  dudesdb dudesdb preprocesses  taxonomic tree structure   reference sequence generate  database file   use  dudes profiler  current version  dudesdb support  ncbi taxonomic tree ncbi   use    accessionversion identifier  make  link  reference sequence  tree nod   strain level   directly define   ncbi taxonomic tree structure  consider  unclassified node   tag  rank   species level   strain node  map dudes  handle multiple match  account   map quality  match score improve  identification capabilities  default  number  allow match    high  possible allow  match  feasible since    computational impracticable  use  default value   match   read  map parameters   find   supplementary information—tools  parameters  experiment dudes evaluation  perform  four distinct datasets two synthetic communities  two real metagenomic sample first  analyze synthetic metagenomic data  available grind truth  evaluate  precise   identification method  choose  common set  metagenomics evaluations— human microbiome project hmp mock community turnbaugh      vitro synthetic mixture   organisms  bacteria  archaea   eukaryote  mimic errors  organism abundances  real metagenomics sample  bacteria  archaea  consider   evaluation   set  also divide  subsets  different percentages  read  compare   metagenomics analysis tool kraken wood  salzberg  gottcha freitas     metaphlan2 truong      choose   rather different approach  solve  taxonomic profile problem    good result  recent metagenomics study lindgreen    kraken   read bin tool  use  kmer approach  classify  read   give sample  focus  high performance gottcha   taxonomic profiler  use nonredundant signature databases  aim  lower false discovery rat metaphlan2 rely   curated database  approximately  million unique cladespecific marker genes  profile metagenomic sample  second synthetic community consist   laboratorymixed microbial genomic dnas  also evaluate shakya     community make  organisms  know sequence    broad diversity among bacteria  archaea   wide range  genetic variation  different taxonomic level    time  dataset provide  large number  sequence read   allow   realistic performance evaluation  also apply dudes  real metagenomic sample  gut microbiomes   outbreak  shigatoxigenic escherichia coli stec  germany   loman      dataset  evaluate  well dudes perform   real scenario  profile  pathogenic sample  compare  result   previously know experiment furthermore  evaluate  set base  previous know information  lab experiment  tool base   lca approach perform   specific profile lastly  profile  marine dataset  tara oceans sunagawa     bacteria archaea virus  eukaryotes present   sample show  versatility   tool datasets detail  show  supplementary table   hmp  stec sample  preprocessed   digital normalization algorithm brown     decrease sample variation   error correction   analysis  reference database  dudes  kraken  generate   set  complete genomes sequence bacteria  archaea together   taxonomic tree structure   ncbi ncbi   march    organim set  use without  filter   set  use   database   addition  four noncomplete genome sequence taxid        species   sample available   tara dataset  make  custom database contain  expect marine organisms bacterial archaeal  viral taxons  obtain   reference sequence use   tara oceans project sunagawa      eukaryotic set  taxons  obtain   mmetsp keel     ncbi refseq sequence relative   taxons  collect  generate  database  january    metaphlan2  gottcha  set  use  provide database v20  v20150825 respectively bowtie2 langmead  salzberg   use  read map parameters  usage detail   tool   find   supplementary materials—tools  parameters  evaluate  output   tool base   binary classification   sort taxonomic profile  binary classification  valid  tgs   certain taxonomic level true positives   tgs present   sample  correctly identify false positives   identify tgs know    present   sample false negative   tgs   present  could   identify  true negative   tgs   database  identify  know    present   sample",1
45,NBC,"NBC: the Naive Bayes Classification tool webserver for taxonomic classification of metagenomic reads.
We selected a previously benchmarked dataset (Gerlach et al., 2009): the Biogas reactor dataset (Schlüter et al., 2008), composed of 353 213 reads of average 230 bp length. We selected a real dataset as opposed to a synthetic one because we did not want to tailor the dataset to any specific database, since the database will vary on each web site. This comparison fairly assesses each webserver’s performance on a ‘real’dataset containing known and novel organisms. We conducted our tests against NBC and five other webservers in July and August of 2010. WebCARMA and MG-RAST require no parameters. Phylopythia requires the type of model to match against. MG-RAST requires an E-value cutoff under the SEED viewer (which we selected the highest). We selected default BLAST parameters for the NT database for Galaxy. For NBC, we used an Nmer size of 15 and the default 1032 organism genomelist. For CAMERA, we only retained the best top-hit organism for each read and used the ‘All Prokaryotes’ BLASTN database (and used the default parameters for the rest). We implement the NBC approach in Rosen et al. (2008) that assigns each read a log-likelihood score. We introduce two functions of NBC: (i) the novice functionality and (ii) the expert functionality. We expect that most users will fit into the ‘novice’category, which will enable them to upload their FASTA file of reads and obtain a file of summarized results matching each read to its most likely organism, given the training database. The parameters that (expert and novice) users can choose from are as follows: Upload File: the FASTA formatted file of metagenomic reads. The webserver also accepts .zip, .gz and .tgz of several FASTA files. Genome list: the algorithm speed depends linearly on the number of genomes that one scores against. So, if an expert user has prior knowledge about the expected microbes in the environment, he/she can select only those microbes that should be scored against. This will both speed up the computation time and reduce false positives of the algorithm. Nmer length: the user can select different Nmer feature sizes, but it is recommended that the novice user use N =15 since it works well for both long and short reads (Rosen et al., 2008). Email: The user’s email address is required so that they can be notified as to where to retrieve the results when the job is completed. Output: For a beginner, we suggest to (i) upload a FASTA file with the metagenomic reads and (ii) enter an email address. The output is a link to a directory that contains your original upload file (renamed as userAnalysisFile.txt), the genomes that were scored against (masterGenomeList.txt) and a summary of the matches for each read (summarized_results.txt). The expert user may be particularly interested in the *.csv.gz files where he/she can analyze the ‘score distribution’ of each read more in depth. 3 DISCUSSION In Figure 1, we show the percentage of reads (out of the whole dataset) that ranked in the top eight genera for each algorithm. We see that all methods are in unanimous agreement for Clostridium and Bacillus, while most methods (except Galaxy) agree for prominence of Methanoculleus. CAMERA supports NBC’s findings of Pseudomonas and Burkholderia, known to be found in sewage treatment plants (Vinneras et al., 2006). [The biogas reactor contained ∼2% chicken manure so it can have the traits of sludge waste (Schlüter et al., 2008)]. In Hery et al. (2010), Pseudomonas and Sorangium have been found in sludge wastes. Streptosporangium and Streptomyces are commonly found in vegetable gardens (Nolan et al., 2010), which is quite reasonable since this is an agricultural bioreactor. Therefore, NBC potentially has found significant populations of genera that other classifiers have missed. Thermosinus is not in NBC’s completed microbial training database and therefore, it did not find any matches. NBC took 21 h to run and classified all 100% of the reads compared with 12 h/23% for WebCARMA, 5 h/99% for CAMERA, 2–3 h/140% for Galaxy 1, and a few weeks 2/56.2% for MG-RAST. NBC runs on a 4-core Intel machine and speed would linearly increase with distributed computing in the future.",Classification,"nbc  naive bay classification tool webserver  taxonomic classification  metagenomic reads
 select  previously benchmarked dataset gerlach     biogas reactor dataset schlüter    compose    read  average   length  select  real dataset  oppose   synthetic one     want  tailor  dataset   specific database since  database  vary   web site  comparison fairly assess  webservers performance   realdataset contain know  novel organisms  conduct  test  nbc  five  webservers  july  august   webcarma  mgrast require  parameters phylopythia require  type  model  match  mgrast require  evalue cutoff   seed viewer   select  highest  select default blast parameters    database  galaxy  nbc  use  nmer size     default  organism genomelist  camera   retain  best tophit organism   read  use   prokaryotes blastn database  use  default parameters   rest  implement  nbc approach  rosen     assign  read  loglikelihood score  introduce two function  nbc   novice functionality    expert functionality  expect   users  fit   novicecategory   enable   upload  fasta file  read  obtain  file  summarize result match  read    likely organism give  train database  parameters  expert  novice users  choose    follow upload file  fasta format file  metagenomic read  webserver also accept zip   tgz  several fasta file genome list  algorithm speed depend linearly   number  genomes  one score     expert user  prior knowledge   expect microbes   environment heshe  select   microbes    score     speed   computation time  reduce false positives   algorithm nmer length  user  select different nmer feature size    recommend   novice user use   since  work well   long  short read rosen    email  users email address  require      notify     retrieve  result   job  complete output   beginner  suggest   upload  fasta file   metagenomic read   enter  email address  output   link   directory  contain  original upload file rename  useranalysisfiletxt  genomes   score  mastergenomelisttxt   summary   match   read summarized_resultstxt  expert user may  particularly interest   *csvgz file  heshe  analyze  score distribution   read   depth  discussion  figure   show  percentage  read    whole dataset  rank   top eight genera   algorithm  see   methods   unanimous agreement  clostridium  bacillus   methods except galaxy agree  prominence  methanoculleus camera support nbcs find  pseudomonas  burkholderia know   find  sewage treatment plant vinneras     biogas reactor contain  chicken manure      traits  sludge waste schlüter     hery    pseudomonas  sorangium   find  sludge waste streptosporangium  streptomyces  commonly find  vegetable garden nolan      quite reasonable since    agricultural bioreactor therefore nbc potentially  find significant populations  genera   classifiers  miss thermosinus    nbcs complete microbial train database  therefore    find  match nbc take    run  classify     read compare     webcarma    camera    galaxy     weeks   mgrast nbc run   core intel machine  speed would linearly increase  distribute compute   future",1
46,Kodoja,"Kodoja: A workflow for virus detection in plants using k-mer analysis of RNA-sequencing data
The Kodoja workflow The Kodoja workflow combines two existing tools, Kraken [16] for taxonomic classification using k-mers at the nucleotide level and Kaiju [17] for sequence matching at the protein level. Kodoja has three main components, summarized in Fig. 1: (a) kodoja_build for database generation for Kraken and Kaiju, (b) kodoja_search for the taxonomic classification of RNA-seq reads and (c) kodoja_retrieve for extraction of viral sequences by species for downstream analysis. Kodoja_build: database generationFor virus classification, the main Kodoja components (Kraken [16] and Kaiju [17]) each require a database generated from the genome or proteome of known plant viruses, and (if available) the genome or proteome of the plant host. Data download and database generation are achieved using the kodoja_build module. This module downloads genomes and protein sequence files from RefSeq [20], and then implements code from Kraken and Kaiju to generate tool-specific databases. The user can specify if all viruses or only plant viruses are included in the databases. If a host genome is available (either provided by the user or in RefSeq [20]), this can also be added to the database for host sequence classification. To make Kodoja easy to use, ready-made plant-specific viral databases for Kraken and Kaiju are provided for download at https://doi.org/10.5281/zenodo.1406071. These were generated by downloading all complete virus and viroid genomes and protein sequence files in NCBI RefSeq (Release 89) [20] and selecting plant viruses using information from the Virus-Host DB [21]. For Kraken, k-mer size is specified when building the database, and a k-mer size of 31 was used for the RNA-seq datasets. Kodoja_search: taxonomic classification of virus readsKodoja_search is the main Kodoja component. RNA-seq reads are first quality checked using Trimmomatic [22], which trims and discards low-quality reads. FastQC (www.bioinformatics.babraham.ac.uk/projects/fastqc/) is used for summarizing the read quality after trimming, and the FASTQC report forms part of Kodoja’s results. Kraken [16] is then used for the nucleotide-level classification. Kraken is a sequence classification algorithm for assigning taxonomic labels to short sequences [16]. It does this through dividing each sequence into k-mers and querying each against a k-mer database. K-mers which are shared between organisms are mapped to the lowest common ancestor, and this information is then used to build a subtree of the general taxonomy tree for the classification of the sequences. In the tree, each node has a weight equal to the number of k-mers in the sequence associated with the node’s taxon. Each root-to-leaf path in the tree is scored by adding all the weights in the path. The leaf of the path with the largest score is the classification used for the sequence [16]. The use of the k-mer database makes the classification algorithm very fast compared to alignment-based methods [11]. In the next step, full length sequence reads are translated and classified at the protein-level using Kaiju [17]. Kaiju translates the sequences into six frames and splits the resulting translations into fragments using translation termination codons (UAG, UAA, UGA). Kaiju balances precision and sensitivity by using a minimum fragment length parameter. We used a minimum fragment length of 15 and the number of mismatches permitted was one. Fragments are queried against a protein database using a modified version of the backwards search algorithm in the Burrows–Wheeler transform [23]. A key component of sequence classification for both Kraken and Kaiju is the tool-specific database. We have provided pre-computed plant virus databases that can be used directly with the Kodoja workflow, but custom databases can also be made using kodoja_build (see Kodoja_build: database generation). Implementation of the kodoja_search module results in reads being assigned to taxonomic classes by both Kraken and Kaiju. Reads assigned to the same virus class by both tools (set intersection) are designated as stringent assignments; and reads assigned to a virus class by either Kraken or Kaiju (set union) are assigned as non-stringent assignments. The assignments are given in a results summary, which includes the reads counts for each type of assignment. Full results from Kraken and Kaiju are also provided so that users can analyse these data further, outside of the Kodoja workflow. Kodoja_retrieve: extraction of viral readsThis module can be used to extract species-specific sequences for downstream analysis outside of the Kodoja workflow. The user can specify retrieval of reads classified to a species, and/or genus, using either stringent or non-stringent assignments. The ability to retrieve and download all reads assigned to a specific virus gives the user the potential to assemble complete viral genomes for further analysis. Kodoja workflow availabilityKodoja is available for direct installation and use at the command line in Linux through Bioconda [18] (https://anaconda.org/bioconda/kodoja). Alternatively, the code can be downloaded from github (https://github.com/abaizan/kodoja). Kodoja is also provided as a package in Galaxy, an open source web-based analytical environment for data analysis [19]. This is available on GitHub (https://github.com/abaizan/kodoja_galaxy) and the Galaxy Tool Shed (https://toolshed.g2.bx.psu.edu/view/abaizan/kodoja). Developing Kodoja as a package within Galaxy makes it available to researchers with a local installation of Galaxy, and allows analysis to be completed with no command line input. By using an open source workflow platform in this way, the tool can also potentially be used on a cloud-based Galaxy server. Benchmarking Kodoja using existing datasets Kodoja was tested on three publicly available RNA-seq grapevine datasets [24] analysed for the presence of viral sequences [25]. In the original work sequencing data for 11 grapevine samples was obtained, including multiple samples from skin, grain and seed [24]. In the analysis work viral sequences were identified using contig building and subsequence blast alignment of contigs to a reference viral database [25]. For the Kodoja benchmarking, we selected one library from grain (G1R1) [Sequence Read Archive (SRA) identifier SRR866540], skin (S3R1) (SRA: SRR866571) and seed (S3R3) (SRA:SRR866576); representative of those datasets with the largest and most diverse viromes. These datasets are denoted GV1, GV2 and GV3, respectively, in the current analysis. Assembly and alignment for confirmationTo confirm the viruses predicted by Kodoja, kodoja_retrieve was used to extract reads assigned to each virus. Reads for each virus were then assembled using Trinity [26] with minimum contig length of 200 nucleotides. The longest contig for each virus was then aligned against the NCBI non-redundant nucleotide database using blastn, and the match with lowest e-value selected for taxonomic comparison. Where too few reads were available for contig assembly, all reads assigned to a virus species by Kodoja were aligned. Applying Kodoja to virus detection in raspberry (Rubus idaeus) Kodoja was then applied to RNA-seq libraries generated from two raspberry plants of variety Glen Dee (denoted D5 and D6) collected from a commercial raspberry plantation in Angus, Scotland, UK. Both plants showed viral infection symptoms: D5 showed vein yellowing and D6 showed leaf blade yellowing RNA-sequencingSymptomatic leaves were collected from each plant (D5 and D6) and frozen at −80 °C for long-term storage (15 months). Two samples of leaf were placed in a clean, autoclaved 2 ml Eppendorf tube together with a sterile 3 mm glass bead, frozen with liquid nitrogen and then powdered using a bead beater (Qiagen TissueLyser). Then 100 mg of powdered leaf was resuspended in a mixture of 450 µl Qiagen RNeasy Plant Mini Kit buffer RLT, 45 µl Ambion Plant RNA isolation aid and 4.5 µl 2-mercaptoethanol. Thereafter the RNA extraction followed the manufacturer’s instructions to the RNeasy kit, and the RNA was eluted in RNAse-free water. The RNA was supplied to the Glasgow Polyomics facility (UK) for quality control, ribosomal RNA depletion, library preparation (paired-end 200 bp) and high-throughput sequencing using an Illumina NextSeq instrument (RD_PE2×75_33M). The raw data files for sample D5 and D6 comprised 64 M and 62 M reads, respectively (available from the European Nucleotide Archive [27] under accessions ERR2784286 and ERR2784287, respectively). Kodoja analysis of raspberry RNA-seq datasetsThe Kodoja workflow was run on the two raspberry RNA-seq datasets, using the draft genome of black raspberry (Rubus occidentalis)[28] as the host in the Kraken database build. Assembly and alignment for confirmationTo confirm the predicted viruses, kodoja_retrieve was used to extract reads assigned to each virus species; contigs were assembled and aligned to a reference database as described in Assembly and alignment for confirmation. PCR confirmation of virus sequencesTo confirm that the viruses identified by Kodoja were present, new samples of total RNA were extracted from the frozen leaves of sample D5 and D6 using the Thompson buffer method as described previously [29] and eluted in RNAse-free water. For detection of Raspberry leaf mottle virus (RLMV) the plant RNA was converted to cDNA using SuperScript III (Invitrogen) reverse transcriptase and random hexamer primer following the manufacturer’s instructions. For other RNA viruses [Raspberry leaf blotch virus (RLBV) and Beet ringspot virus (BRSV)] the extracted plant RNA was added directly to a 25 µl illustra Ready-to-Go RT-PCR bead (GE Healthcare) reaction together with virus-specific PCR primers (Table 1). To detect the DNA plant virus Rubus yellow net virus (RYNV), six 1 cm diameter frozen D5 and D6 leaf discs were extracted using the DNeasy Plant Mini Kit (Qiagen) according to the manufacturer’s instructions. RYNV was detected in the eluted DNA by amplification in a 25 µl illustra Ready-to-Go PCR bead (GE Healthcare) reaction with virus-specific primers (Table 1). Positive controls for virus-detection were RNAs extracted from raspberry plants previously demonstrated to carry specific viruses. ",VirusDetection,"kodoja  workflow  virus detection  plant use kmer analysis  rnasequencing data
 kodoja workflow  kodoja workflow combine two exist tool kraken   taxonomic classification use kmers   nucleotide level  kaiju   sequence match   protein level kodoja  three main components summarize  fig   kodoja_build  database generation  kraken  kaiju  kodoja_search   taxonomic classification  rnaseq read   kodoja_retrieve  extraction  viral sequence  species  downstream analysis kodoja_build database generationfor virus classification  main kodoja components kraken   kaiju   require  database generate   genome  proteome  know plant viruses   available  genome  proteome   plant host data download  database generation  achieve use  kodoja_build module  module download genomes  protein sequence file  refseq    implement code  kraken  kaiju  generate toolspecific databases  user  specify   viruses   plant viruses  include   databases   host genome  available either provide   user   refseq    also  add   database  host sequence classification  make kodoja easy  use readymade plantspecific viral databases  kraken  kaiju  provide  download     generate  download  complete virus  viroid genomes  protein sequence file  ncbi refseq release    select plant viruses use information   virushost    kraken kmer size  specify  build  database   kmer size    use   rnaseq datasets kodoja_search taxonomic classification  virus readskodoja_search   main kodoja component rnaseq read  first quality check use trimmomatic   trim  discard lowquality read fastqc wwwbioinformaticsbabrahamacukprojectsfastqc  use  summarize  read quality  trim   fastqc report form part  kodojas result kraken    use   nucleotidelevel classification kraken   sequence classification algorithm  assign taxonomic label  short sequence      divide  sequence  kmers  query    kmer database kmers   share  organisms  map   lowest common ancestor   information   use  build  subtree   general taxonomy tree   classification   sequence   tree  node   weight equal   number  kmers   sequence associate   nod taxon  roottoleaf path   tree  score  add   weight   path  leaf   path   largest score   classification use   sequence   use   kmer database make  classification algorithm  fast compare  alignmentbased methods    next step full length sequence read  translate  classify   proteinlevel use kaiju  kaiju translate  sequence  six frame  split  result translations  fragment use translation termination codons uag uaa uga kaiju balance precision  sensitivity  use  minimum fragment length parameter  use  minimum fragment length     number  mismatch permit  one fragment  query   protein database use  modify version   backwards search algorithm   burrowswheeler transform   key component  sequence classification   kraken  kaiju   toolspecific database   provide precomputed plant virus databases    use directly   kodoja workflow  custom databases  also  make use kodoja_build see kodoja_build database generation implementation   kodoja_search module result  read  assign  taxonomic class   kraken  kaiju read assign    virus class   tool set intersection  designate  stringent assignments  read assign   virus class  either kraken  kaiju set union  assign  nonstringent assignments  assignments  give   result summary  include  read count   type  assignment full result  kraken  kaiju  also provide   users  analyse  data  outside   kodoja workflow kodoja_retrieve extraction  viral readsthis module   use  extract speciesspecific sequence  downstream analysis outside   kodoja workflow  user  specify retrieval  read classify   species andor genus use either stringent  nonstringent assignments  ability  retrieve  download  read assign   specific virus give  user  potential  assemble complete viral genomes   analysis kodoja workflow availabilitykodoja  available  direct installation  use   command line  linux  bioconda   alternatively  code   download  github  kodoja  also provide   package  galaxy  open source webbased analytical environment  data analysis    available  github    galaxy tool shed  develop kodoja   package within galaxy make  available  researchers   local installation  galaxy  allow analysis   complete   command line input  use  open source workflow platform   way  tool  also potentially  use   cloudbased galaxy server benchmarking kodoja use exist datasets kodoja  test  three publicly available rnaseq grapevine datasets  analyse   presence  viral sequence    original work sequence data   grapevine sample  obtain include multiple sample  skin grain  seed    analysis work viral sequence  identify use contig build  subsequence blast alignment  contigs   reference viral database    kodoja benchmarking  select one library  grain g1r1 sequence read archive sra identifier srr866540 skin s3r1 sra srr866571  seed s3r3 srasrr866576 representative   datasets   largest   diverse viromes  datasets  denote gv1 gv2  gv3 respectively   current analysis assembly  alignment  confirmationto confirm  viruses predict  kodoja kodoja_retrieve  use  extract read assign   virus read   virus   assemble use trinity   minimum contig length   nucleotides  longest contig   virus   align   ncbi nonredundant nucleotide database use blastn   match  lowest evalue select  taxonomic comparison    read  available  contig assembly  read assign   virus species  kodoja  align apply kodoja  virus detection  raspberry rubus idaeus kodoja   apply  rnaseq libraries generate  two raspberry plant  variety glen dee denote    collect   commercial raspberry plantation  angus scotland   plant show viral infection symptoms  show vein yellow   show leaf blade yellow rnasequencingsymptomatic leave  collect   plant     freeze   °  longterm storage  months two sample  leaf  place   clean autoclave   eppendorf tube together   sterile   glass bead freeze  liquid nitrogen   powder use  bead beater qiagen tissuelyser     powder leaf  resuspend   mixture    qiagen rneasy plant mini kit buffer rlt   ambion plant rna isolation aid    mercaptoethanol thereafter  rna extraction follow  manufacturers instructions   rneasy kit   rna  elute  rnasefree water  rna  supply   glasgow polyomics facility   quality control ribosomal rna depletion library preparation pairedend    highthroughput sequence use  illumina nextseq instrument rd_pe275_33m  raw data file  sample    comprise      read respectively available   european nucleotide archive   accession err2784286  err2784287 respectively kodoja analysis  raspberry rnaseq datasetsthe kodoja workflow  run   two raspberry rnaseq datasets use  draft genome  black raspberry rubus occidentalis   host   kraken database build assembly  alignment  confirmationto confirm  predict viruses kodoja_retrieve  use  extract read assign   virus species contigs  assemble  align   reference database  describe  assembly  alignment  confirmation pcr confirmation  virus sequencesto confirm   viruses identify  kodoja  present new sample  total rna  extract   freeze leave  sample    use  thompson buffer method  describe previously   elute  rnasefree water  detection  raspberry leaf mottle virus rlmv  plant rna  convert  cdna use superscript iii invitrogen reverse transcriptase  random hexamer primer follow  manufacturers instructions   rna viruses raspberry leaf blotch virus rlbv  beet ringspot virus brsv  extract plant rna  add directly     illustra readytogo rtpcr bead  healthcare reaction together  virusspecific pcr primers table   detect  dna plant virus rubus yellow net virus rynv six   diameter freeze    leaf discs  extract use  dneasy plant mini kit qiagen accord   manufacturers instructions rynv  detect   elute dna  amplification     illustra readytogo pcr bead  healthcare reaction  virusspecific primers table  positive control  virusdetection  rnas extract  raspberry plant previously demonstrate  carry specific viruses ",2
47,DisCVR,"DisCVR: Rapid viral diagnosis from high-throughput sequencing data
The k-mer databases A k-mer is a short sequence of k nucleotides. A k-mer dataset is generated iteratively by sliding a window of size k along a sequence one nucleotide at a time. Extracting k-mers and counting their frequencies in a set of sequences can be computationally intensive, especially when k is large and the sequences are numerous. Dedicated k-mer counting programs, such as Jellyfish (Marçais and Kingsford 2011) and Khmer (Zhang et al. 2014), can be incorporated into abundance-based tools in order to optimize speed. KAnalyze (Audano and Vannberg 2014) was chosen for integration into DisCVR because the k-mers it generates are sorted lexicographically, thus making the search for matches very efficient. KAnalyze also uses the canonical representation of a k-mer, which is lexicographically the smaller of a k-mer and its reverse complement. These features allow the program to work with 3 Gb RAM. For the purpose of this study, we define a virus k-mer as a k-mer that uniquely represents a virus or set of related viruses, to the exclusion of the host. A shared k-mer is defined as a k-mer that is common to a virus and the host. By excluding shared k-mers, it is not necessary for the user to remove host reads before using DisCVR, thus speeding up the overall processing time. If k is small, many copies of shared k-mers are generated, and if k is large, many copies of virus k-mers are found. Choosing the optimal k-mer size depends on balancing the advantages of speed (small k) with those of specificity and sensitivity (large k). Furthermore, it is necessary to reduce the number of low-complexity k-mers in the virus k-mer database, as these may be repetitive in sequence and present in otherwise unrelated viruses. The filtering of low-complexity k-mers and the selection of the size of k is explained in Supplementary Section S1 (Shannon 1948; Sims et al. 2009; Wu et al. 2009). For constructing the virus k-mer databases, three comprehensive datasets of complete or partial viral sequences were extracted from the NCBI taxonomy database. The first, the human hemorrhagic virus dataset (shortened below to ‘hemorrhagic dataset’), contained 33,367 sequences of the hemorrhagic fever viruses listed by the Centers for Disease Control and Prevention (Centers for Disease Control and Prevention, n.d.). The second, the human respiratory virus dataset (‘respiratory dataset’), contained 442,282 sequences of viruses associated with respiratory disease. The third, the human pathogenic virus dataset (‘pathogenic dataset’), consisted of 1,762,968 sequences of viruses identified in the UK Health and Safety Executive list of biological agents (Health and Safety Executive: The Approved List of Biological Agents 2013). 2.2 Database build DisCVR operates via three modules concerned with database build, sample classification and validation Currently, the database build module includes three virus k-mer databases, derived from the hemorrhagic, respiratory, and pathogenic datasets, for use in the sample classification module. In addition, some of the sequences in these datasets, defined largely by their presence in the NCBI RefSeq database, are used as a set of reference genome sequences in the validation module. DisCVR also allows the user to create customized databases and sets of reference sequences using the command-line utility scripts provided with the DisCVR distribution. The database build module involves selecting the relevant viral dataset, collecting the k-mers, and removing those that are shared with the host or are of low complexity. Each remaining k-mer is then identified with a taxonomic tag and an indication of the number of times it occurs in the sequences. The k-mers are further subdivided into those that exist in a single virus (i.e. specific k-mers) and those that exist in multiple viruses (i.e. nonspecific k-mers). These assignments are made at the level of species and strain and are used in the output to illustrate the degree of specificity of the k-mers matching a virus Sample classification To analyze an HTS dataset, the file is loaded into DisCVR via the GUI. The k-mers are extracted and their frequencies are calculated, the single copy k-mers, which are mainly attributed to sequencing errors (Manekar and Sathe 2018), and low-complexity k-mers, which commonly give confounding matches that have nothing to do with homology (Altschul et al. 1994), are filtered out, and the remaining k-mers are compared with the chosen virus k-mer database. As the number of k-mers in the sample can be enormous, various data structures were considered to optimize the classification on machines with limited RAM. Although searching the trie is fast O(n), where n is the size of the k-mer, it requires O(n2) overall time to build, and the space needed is quadratic. Instead, DisCVR uses a fast searching algorithm that groups similar k-mers together. Briefly, the k-mers in the virus database are divided among smaller sub-files according to the first five nucleotides. The same procedure is used to divide the k-mers derived from the entire HTS dataset. Searching commences by loading the corresponding sub-files from the virus k-mer database and the sample k-mers into memory, and performing a binary search for the presence of each sample k-mer among the database k-mers. Only matched k-mers are retrieved. Finally, DisCVR displays a straightforward list of all the virus hits detected, along with summary statistics and taxonomic information on the sample k-mers (Fig. 2). 2.4 Validation DisCVR helps the user to assess the significance of the findings by facilitating an examination of k-mer distribution (allowing up to three mismatches) across a reference sequence representing the target genome. As an alternative, it also incorporates an examination of sequence read distribution carried out by using Tanoti (Sreenu, n.d.), which is a BLAST-guided, reference-based short read aligner that is particularly tolerant of mismatches. In each case, the output is a graph showing the depth and coverage of k-mers or sequence reads across the reference genome and a summary of statistics for the mapping results Accuracy The respiratory database was used to analyze published RNA-seq data from nasopharyngeal swab samples (n = 89) that had been collected from adults with upper respiratory tract infections (Thorburn et al. 2015) (Supplementary Table S2; the average number of reads per sample was 660,640, range 30,872–1,278,122). The samples had been tested using a standard real-time PCR (RT-PCR) assay for human rhinovirus (HRV), influenza viruses A and B (IFA/IFB), respiratory syncytial virus (RSV), adenovirus (ADV), human metapneumovirus (hMPV), parainfluenza viruses (PIV) 1–4, and human coronaviruses (HCoV) HKU1, NL63, OC43 and 229E (Thorburn et al. 2015). The top hit for each sample (i.e. the virus having the greatest number of distinct k-mers) using DisCVR was compared with the virus detected previously by RT-PCR. The samples were also classified using three independent k-mer-based programs that require command-line usage on a Linux operating system: Kraken (Wood and Salzberg 2014), KrakenHLL (Breitwieser and Salzberg 2018), and CLARK (Ounit et al. 2015). As the pre-built database for Kraken only contains the RefSeq viral genomes (11,489 sequences), a more comprehensive k-mer database was built for each program from the same 442,282 sequences in the respiratory dataset in order to standardize the results. This successfully accommodated within species sequence diversity, which is not normally taken into account using the pre-built database. The initial objective was to determine the number of distinct k-mers that would maximize both sensitivity (effectiveness in identifying samples containing viruses) and specificity (effectiveness in identifying samples lacking viruses) for DisCVR. The output of DisCVR was categorized on the basis of the number of distinct k-mers for the top hit, and that of the other programs was assessed on the basis of the number of reads assigned to the top hit. For each tool, sensitivity and specificity were defined as TP/(TP + FN) and TN/(TN + FP), respectively, where TP, FN, TN, and FP are the number of true positive, false negative, true negative, and false positive samples relative to the RT-PCR results. We define samples as (1) true positive when the top virus hit was detected by both RT-PCR and DisCVR, (2) true negative when neither RT-PCR nor DisCVR detected a virus, (3) false negative when a virus was detected by RT-PCR but not by DisCVR, and (4) false positive when a virus was detected by DisCVR but not by RT-PCR. Receiver Operating Characteristics (ROC) curves were generated for DisCVR, Kraken, KrakenHLL and CLARK using the pROC package in R and Youden’s statistic",VirusDetection,"discvr rapid viral diagnosis  highthroughput sequence data
 kmer databases  kmer   short sequence   nucleotides  kmer dataset  generate iteratively  slide  window  size  along  sequence one nucleotide   time extract kmers  count  frequencies   set  sequence   computationally intensive especially    large   sequence  numerous dedicate kmer count program   jellyfish marçais  kingsford   khmer zhang      incorporate  abundancebased tool  order  optimize speed kanalyze audano  vannberg   choose  integration  discvr   kmers  generate  sort lexicographically thus make  search  match  efficient kanalyze also use  canonical representation   kmer   lexicographically  smaller   kmer   reverse complement  feature allow  program  work    ram   purpose   study  define  virus kmer   kmer  uniquely represent  virus  set  relate viruses   exclusion   host  share kmer  define   kmer   common   virus   host  exclude share kmers    necessary   user  remove host read  use discvr thus speed   overall process time    small many copy  share kmers  generate     large many copy  virus kmers  find choose  optimal kmer size depend  balance  advantage  speed small     specificity  sensitivity large  furthermore   necessary  reduce  number  lowcomplexity kmers   virus kmer database   may  repetitive  sequence  present  otherwise unrelated viruses  filter  lowcomplexity kmers   selection   size    explain  supplementary section  shannon  sims         construct  virus kmer databases three comprehensive datasets  complete  partial viral sequence  extract   ncbi taxonomy database  first  human hemorrhagic virus dataset shorten   hemorrhagic dataset contain  sequence   hemorrhagic fever viruses list   center  disease control  prevention center  disease control  prevention   second  human respiratory virus dataset respiratory dataset contain  sequence  viruses associate  respiratory disease  third  human pathogenic virus dataset pathogenic dataset consist   sequence  viruses identify    health  safety executive list  biological agents health  safety executive  approve list  biological agents   database build discvr operate via three modules concern  database build sample classification  validation currently  database build module include three virus kmer databases derive   hemorrhagic respiratory  pathogenic datasets  use   sample classification module  addition    sequence   datasets define largely   presence   ncbi refseq database  use   set  reference genome sequence   validation module discvr also allow  user  create customize databases  set  reference sequence use  commandline utility script provide   discvr distribution  database build module involve select  relevant viral dataset collect  kmers  remove    share   host    low complexity  remain kmer   identify   taxonomic tag   indication   number  time  occur   sequence  kmers   subdivide    exist   single virus  specific kmers    exist  multiple viruses  nonspecific kmers  assignments  make   level  species  strain   use   output  illustrate  degree  specificity   kmers match  virus sample classification  analyze  hts dataset  file  load  discvr via  gui  kmers  extract   frequencies  calculate  single copy kmers   mainly attribute  sequence errors manekar  sathe   lowcomplexity kmers  commonly give confound match   nothing    homology altschul     filter    remain kmers  compare   choose virus kmer database   number  kmers   sample   enormous various data structure  consider  optimize  classification  machine  limit ram although search  trie  fast      size   kmer  require on2 overall time  build   space need  quadratic instead discvr use  fast search algorithm  group similar kmers together briefly  kmers   virus database  divide among smaller subfiles accord   first five nucleotides   procedure  use  divide  kmers derive   entire hts dataset search commence  load  correspond subfiles   virus kmer database   sample kmers  memory  perform  binary search   presence   sample kmer among  database kmers  match kmers  retrieve finally discvr display  straightforward list    virus hit detect along  summary statistics  taxonomic information   sample kmers fig   validation discvr help  user  assess  significance   find  facilitate  examination  kmer distribution allow   three mismatch across  reference sequence represent  target genome   alternative  also incorporate  examination  sequence read distribution carry   use tanoti sreenu     blastguided referencebased short read aligner   particularly tolerant  mismatch   case  output   graph show  depth  coverage  kmers  sequence read across  reference genome   summary  statistics   map result accuracy  respiratory database  use  analyze publish rnaseq data  nasopharyngeal swab sample       collect  adults  upper respiratory tract infections thorburn    supplementary table   average number  read per sample   range   sample   test use  standard realtime pcr rtpcr assay  human rhinovirus hrv influenza viruses    ifaifb respiratory syncytial virus rsv adenovirus adv human metapneumovirus hmpv parainfluenza viruses piv   human coronaviruses hcov hku1 nl63 oc43   thorburn     top hit   sample   virus   greatest number  distinct kmers use discvr  compare   virus detect previously  rtpcr  sample  also classify use three independent kmerbased program  require commandline usage   linux operate system kraken wood  salzberg  krakenhll breitwieser  salzberg   clark ounit      prebuilt database  kraken  contain  refseq viral genomes  sequence   comprehensive kmer database  build   program     sequence   respiratory dataset  order  standardize  result  successfully accommodate within species sequence diversity    normally take  account use  prebuilt database  initial objective   determine  number  distinct kmers  would maximize  sensitivity effectiveness  identify sample contain viruses  specificity effectiveness  identify sample lack viruses  discvr  output  discvr  categorize   basis   number  distinct kmers   top hit      program  assess   basis   number  read assign   top hit   tool sensitivity  specificity  define  tptp    tntn   respectively         number  true positive false negative true negative  false positive sample relative   rtpcr result  define sample   true positive   top virus hit  detect   rtpcr  discvr  true negative  neither rtpcr  discvr detect  virus  false negative   virus  detect  rtpcr    discvr   false positive   virus  detect  discvr    rtpcr receiver operate characteristics roc curve  generate  discvr kraken krakenhll  clark use  proc package    youdens statistic",2
48,Metavisitor,"Metavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data
Metavisitor consists of a set of Galaxy tools (Fig 1) that can be combined to (i) retrieve up-to-date nucleotide as well as protein sequences of viral genomes deposited in Genbank [17] and index these sequences for subsequent alignments; (ii) extract sequencing reads that do not align to the host genomes, known symbionts or parasites; (iii) perform de novo assembly of these reads using assembly tools available in Galaxy, align the de novo contigs against the viral nucleotide or protein blast databases using blastn or blastx, respectively, and generate reports from blast outputs to help in known viruses diagnosis or in candidate virus discovery; (iv) use CAP3 (optional, see Use Case 3–3), blast and viral scaffolds for selected viruses to generate guided final viral sequence assemblies of blast sequence hits. Below, we group analysis steps in functional tasks i to iv and provide details on the Metavisitor tools. These tasks are linked together to build full workflows adapted to the analysis of the use cases described in the result section. Get reference viral sequences The “Get reference viral sequences” task is performed using the “Retrieve FASTA from NCBI” tool that sends a query string to the Genbank database [17] and retrieves the corresponding nucleotide or protein sequences. For the viral nucleotide and protein sequences referred to as “vir1”, we used the tool to query Genbank (oct 2015) and retrieve viruses sequences filtered out from cellular organisms and bacteriophage sequences (see S1 Fig). However, users can change the tool settings by entering query strings that fit their specific needs. As retrieving vir1 from NCBI takes several hours, we allow users to skip the step by directly accessing the nucleotides or protein vir1 datasets on the Mississippi server (http://mississippi.fr) or to download them from figshare (https://dx.doi.org/10.6084/m9.figshare.3179026). For convenience, nucleotide and protein blast indexes of vir1 are also available in the public library of the Mississippi server,but can also be generated using the “NCBI BLAST+ makeblastdb” Galaxy tool [18]. Bowtie [19] as well as bowtie2 [20] indexes of the vir1 nucleotide sequences have been generated in the Mississippi Galaxy instance using the corresponding “data manager” Galaxy tools. Finally, users can upload their own viral nucleotide and protein sequences using ftp and transfer them to a Galaxy history (Fig 1), where they can use the Galaxy data manager tools to produce the blast and bowtie indexes necessary for Metavisitor. (ii) Prepare data The “Prepare data” task (Fig 1) processes Illumina sequencing datasets in order to optimize the subsequent de novo assembly of viral sequencing reads. Fastq files of sequence reads are first clipped from library adapters and converted to fasta format using our tool “Clip adapter” tool (S1 Table). The clipped reads may be further converted to a fasta file of unique sequences headed by a character string that contains a unique identifier and the number of times that the sequences were found in the dataset, thus reducing the size of the dataset without loss of information. This optional treatment removes sequence duplicates and drastically reduces the workload of the next steps as well as the coverage variations after de novo assembly (see Use Cases 1–1 to 1–3). Clipped reads are then depleted from non-viral sequences by sequential alignments to the host genome, to other genomes from known or potential symbionts and parasites, as well as to PhiX174 genome sequences which are commonly used as internal controls in Illumina sequencing and may contaminate the datasets (Fig 1). The sequence reads that did not match the reference genomes are retained and returned as a fasta file that can be used subsequently by a de novo assembly tool. Note that these subtraction steps can be skipped when the host genome is not known or if the aim of the user is to discover endogenous viral elements [21]. (iii) Assembly, Blast and Parsing De novo assembly. In the task “Assemble, Blast and Parse” (iii), retained RNA sequences are subjected to de novo assembly. For short reads (<50 nt), we tested several rounds of de novo assembly by Velvet [7] using the Oases software package [22] (Fig 1) and k-mer lengths ranging from 15 to 35 (S1 Table). For reads between 50 nt and 100 nt, we also used the Oases with k-mer lengths ranging from 13 to 69. Finally in Use Case 3–3, we used the Trinity assembly software which is available as a Galaxy tool and was reported to performs well with long reads [23]. Trinity as well as SPAdes [24] assembly softwares were also tested as alternate option to Oases in the Use Case 2–2 (S1 Table), giving similar outputs. It is noteworthy that users can adapt a Metavisitor workflow using any assembly software available in the Galaxy tool shed. Blast. Next, de novo assembled contigs are aligned to both nucleotide and protein vir1 BLAST databases built from the viral reference sequences (Fig 1) using the blastn or blastx Galaxy tools [18]. These tools search nucleotide or protein databases using nucleotide or translated nucleotide queries, respectively [25]. The default parameters are adjusted in order to report only the 5 best alignments per contig (Maximum hits option is set to 5) and to generate a tabular blast output that includes the 12 standard columns plus a column containing the length of the aligned subject sequences (extended columns option, “slen” checked). Note that this additional column in the blast output is required for subsequent parsing of the blast output by the “Parse blast output and compile hits” tool. Parsing. Tabular outputs generated by blastn and blastx alignments are processed by the “Parse blast output and compile hits” tool (S1 Table), which returns 4 files, namely “blast analysis, by subjects”, “hits”, “Blast aligned sequences” and “Blast unaligned sequences”. In the “blast analysis, by subjects” file (S2 Fig), the subject sequences in the viral nucleotide or protein blast databases that produced significant blast alignments (hits) with de novo assembled contigs are listed, together with those contigs and hit information (% Identity, Alignment Length, start and end coordinates of hits relatively to the subject sequence, percentage of the contig length covered by the hit, E-value and Bit Score of the hit). In addition, for each subject sequence in the list, the length in nucleotide or amino-acid of the subject sequence (Subject Length), the summed coverage of the subject by all contig hits (Total Subject Coverage) as well as the fraction of the subject length that this coverage represents (Relative Subject Coverage), and the best (Best Bit Score) and mean (Mean Bit Score) bit scores produced by contig hits are computed and indicated. A simplified output can be generated without contigs and blast information by using the “compact” option for the reporting mode of the “Parse blast output and compile hits” tool. Note that the total and relative subject coverages indicate how much of the virus sequence is covered by the reconstructed contigs, whereas the Bit scores allow to estimate the distances between the reconstructed contigs and the subject sequence. The “hits” file contains the sequences of contig portions that produced significant alignment in the BLAST step (i.e. query hit sequences), flanked by additional contig nucleotides 5’ and 3’ to the hit (the size of these margins is set to 5 by default and can be modified by the user). These margins allow to include sequences that might not have significant homology but could still be of viral origin. Finally, the “Blast aligned sequences” file contains contigs that produced significant blast hits, whereas the “Blast unaligned sequences” file contains those that did not. (iv) Blast-Guided Scaffolding This last task allows to integrate hit sequences matching a candidate virus into a virus scaffold (Fig 1). First, blastn or blastx hits are retrieved from the “hits” file using the tool “Pick Fasta sequences” (S1 Table) and the appropriate query string (for instance, “Dengue” will retrieve hit sequences that significantly blast aligned with Dengue virus sequences). Next, these hit sequences can be further clustered in longer contigs using the “cap3 Sequence Assembly” Galaxy tool (S1 Table) adapted from CAP3 [26]. Finally, if there are still multiple unlinked contigs at this stage, they can be integrated (uppercase characters) in the matched viral sequence taken as a scaffold (lowercase characters). This scaffolding is achieved by (a) retrieving the viral sequence from the NCBI nucleotide database to be used as the backbone of the scaffold, generating a blast index from this sequence and aligning the contigs to this index with blastn or tblastx tools (b) running the “blast_to_scaffold” tools (S1 Table), taking as inputs the contigs, the viral guide sequence and the blastn or blastx output (Fig 1, bottom). Availability of Metavisitor All Metavisitor tools, workflows and use cases are available on the Galaxy server http://mississippi.snv.jussieu.fr. Readers can import in their personal account the published Metavisitor use case histories and their corresponding workflows to re-run the described analyses or adapt them to their studies. We made all tools and workflows that compose Metavisitor available from the main Galaxy tool shed (https://toolshed.g2.bx.psu.edu/), in the form of a tool suite (suite_metavisitor_1_2) which thus can be installed and used on any Galaxy server instance. The Metavisitor workflows are also available from the myexperiment repository (http://www.myexperiment.org/) They can be freely modified or complemented with additional analysis steps within the Galaxy environment.",VirusDetection,"metavisitor  suite  galaxy tool  simple  rapid detection  discovery  viruses  deep sequence data
metavisitor consist   set  galaxy tool fig     combine   retrieve uptodate nucleotide  well  protein sequence  viral genomes deposit  genbank   index  sequence  subsequent alignments  extract sequence read    align   host genomes know symbionts  parasites iii perform  novo assembly   read use assembly tool available  galaxy align   novo contigs   viral nucleotide  protein blast databases use blastn  blastx respectively  generate report  blast output  help  know viruses diagnosis   candidate virus discovery  use cap3 optional see use case  blast  viral scaffold  select viruses  generate guide final viral sequence assemblies  blast sequence hit   group analysis step  functional task     provide detail   metavisitor tool  task  link together  build full workflows adapt   analysis   use case describe   result section get reference viral sequence  “get reference viral sequences” task  perform use  “retrieve fasta  ncbi” tool  send  query string   genbank database   retrieve  correspond nucleotide  protein sequence   viral nucleotide  protein sequence refer   “vir1”  use  tool  query genbank oct   retrieve viruses sequence filter   cellular organisms  bacteriophage sequence see  fig however users  change  tool settings  enter query string  fit  specific need  retrieve vir1  ncbi take several hours  allow users  skip  step  directly access  nucleotides  protein vir1 datasets   mississippi server    download   figshare   convenience nucleotide  protein blast index  vir1  also available   public library   mississippi serverbut  also  generate use  “ncbi blast makeblastdb” galaxy tool  bowtie   well  bowtie2  index   vir1 nucleotide sequence   generate   mississippi galaxy instance use  correspond “data manager” galaxy tool finally users  upload   viral nucleotide  protein sequence use ftp  transfer    galaxy history fig     use  galaxy data manager tool  produce  blast  bowtie index necessary  metavisitor  prepare data  “prepare data” task fig  process illumina sequence datasets  order  optimize  subsequent  novo assembly  viral sequence read fastq file  sequence read  first clip  library adapters  convert  fasta format use  tool “clip adapter” tool  table  clip read may   convert   fasta file  unique sequence head   character string  contain  unique identifier   number  time   sequence  find   dataset thus reduce  size   dataset without loss  information  optional treatment remove sequence duplicate  drastically reduce  workload   next step  well   coverage variations   novo assembly see use case    clip read   deplete  nonviral sequence  sequential alignments   host genome   genomes  know  potential symbionts  parasites  well   phix174 genome sequence   commonly use  internal control  illumina sequence  may contaminate  datasets fig   sequence read    match  reference genomes  retain  return   fasta file    use subsequently    novo assembly tool note   subtraction step   skip   host genome   know    aim   user   discover endogenous viral elements  iii assembly blast  parse  novo assembly   task “assemble blast  parse” iii retain rna sequence  subject   novo assembly  short read    test several round   novo assembly  velvet  use  oases software package  fig   kmer lengths range      table  read        also use  oases  kmer lengths range     finally  use case   use  trinity assembly software   available   galaxy tool   report  perform well  long read  trinity  well  spade  assembly softwares  also test  alternate option  oases   use case   table give similar output   noteworthy  users  adapt  metavisitor workflow use  assembly software available   galaxy tool shed blast next  novo assemble contigs  align   nucleotide  protein vir1 blast databases build   viral reference sequence fig  use  blastn  blastx galaxy tool   tool search nucleotide  protein databases use nucleotide  translate nucleotide query respectively   default parameters  adjust  order  report    best alignments per contig maximum hit option  set     generate  tabular blast output  include   standard columns plus  column contain  length   align subject sequence extend columns option “slen” check note   additional column   blast output  require  subsequent parse   blast output   “parse blast output  compile hits” tool parse tabular output generate  blastn  blastx alignments  process   “parse blast output  compile hits” tool  table  return  file namely “blast analysis  subjects” “hits” “blast align sequences”  “blast unaligned sequences”   “blast analysis  subjects” file  fig  subject sequence   viral nucleotide  protein blast databases  produce significant blast alignments hit   novo assemble contigs  list together   contigs  hit information  identity alignment length start  end coordinate  hit relatively   subject sequence percentage   contig length cover   hit evalue  bite score   hit  addition   subject sequence   list  length  nucleotide  aminoacid   subject sequence subject length  sum coverage   subject   contig hit total subject coverage  well   fraction   subject length   coverage represent relative subject coverage   best best bite score  mean mean bite score bite score produce  contig hit  compute  indicate  simplify output   generate without contigs  blast information  use  “compact” option   report mode   “parse blast output  compile hits” tool note   total  relative subject coverages indicate  much   virus sequence  cover   reconstruct contigs whereas  bite score allow  estimate  distance   reconstruct contigs   subject sequence  “hits” file contain  sequence  contig portion  produce significant alignment   blast step  query hit sequence flank  additional contig nucleotides      hit  size   margins  set    default    modify   user  margins allow  include sequence  might   significant homology  could still   viral origin finally  “blast align sequences” file contain contigs  produce significant blast hit whereas  “blast unaligned sequences” file contain      blastguided scaffold  last task allow  integrate hit sequence match  candidate virus   virus scaffold fig  first blastn  blastx hit  retrieve   “hits” file use  tool “pick fasta sequences”  table   appropriate query string  instance “dengue”  retrieve hit sequence  significantly blast align  dengue virus sequence next  hit sequence    cluster  longer contigs use  “cap3 sequence assembly” galaxy tool  table adapt  cap3  finally    still multiple unlinked contigs   stage    integrate uppercase character   match viral sequence take   scaffold lowercase character  scaffold  achieve   retrieve  viral sequence   ncbi nucleotide database   use   backbone   scaffold generate  blast index   sequence  align  contigs   index  blastn  tblastx tool  run  “blast_to_scaffold” tool  table take  input  contigs  viral guide sequence   blastn  blastx output fig  bottom availability  metavisitor  metavisitor tool workflows  use case  available   galaxy server  readers  import   personal account  publish metavisitor use case histories   correspond workflows  rerun  describe analyse  adapt    study  make  tool  workflows  compose metavisitor available   main galaxy tool shed    form   tool suite suite_metavisitor_1_2  thus   instal  use   galaxy server instance  metavisitor workflows  also available   myexperiment repository     freely modify  complement  additional analysis step within  galaxy environment",2
49,VirFind,"Development of a virus detection and discovery pipeline using next generation sequencing
Sample sources Samples exhibiting virus-like symptoms were either plants maintained at the University of Arkansas-Fayetteville, or provided by collaborators in California, Michigan, Mississippi and Oregon. Laboratory tests (ELISA or RT-PCR) detected viruses in only a subset of samples, indicative of the presence of novel strains or species in others. Plant leaves or phloem were harvested and kept at −80 °C until nucleic acid extraction. Sample preparation methods >Thirty one plant samples (sample nos. 1–31, Table 1) were used for nucleic acid extraction. Sample nos. 1 and 2 were subjected to a total nucleic acid extraction protocol (Poudel et al., 2013), whereas sample nos. 3–31 were processed using a dsRNA-enrichment protocol (Yoshikawa and Converse, 1990). Samples 1, 5–8, 10, 13–21 and 31 were known to be infected by an array of viruses whereas samples 2–4, 9, 11, 12, 22–30 were never tested for viruses before. Reverse transcription was performed essentially as described before (Tzanetakis et al., 2005) using Maxima™ reverse transcriptase (Thermo Fisher Scientific, Waltham, MA) with 0.4 µM PDAP213′5 (emaravirus specific primer) (Di Bello and Tzanetakis, 2013) for samples 1 and 2, or BG4A-RT and KpnI-RT primers (0.4 µM each, Table 2) for samples 3–31. 5 µl of the cDNA were used in a 100 µl PCR reaction, with 0.8 µM PDAP213′5 primer for samples 1 and 2, or BG4A-PCR and KpnI-PCR primers (0.8 µM each, Table 2) for samples 3–31, and chemical composition as previously described (Poudel et al., 2013). The PCR program consisted of 2 min denaturation at 94 °C followed by 35 cycles of 20 s at 94 °C, 20 s at 45 °C, and 30 s at 72 °C, concluding with 10 min extension at 72 °C. The PCR products were visualized in 2% TBE-agarose gels stained with GelRed® (Biotium, Hayward, CA) and DOP-PCR products between 300 and 1000 bp were purified using GeneJET PCR Purification Kit (Thermo Fisher Scientific). DNA was quantified on a NanoDrop™ spectrophotometer (Thermo Fisher Scientific), normalized to the same amount for each sample, multiplexed as indicated in Table 1, and sequenced in 10 separate NGS reactions (NGS dataset nos. 1–10) using Illumina (Center for Genome Research and Biocomputing, Oregon State University, Corvallis, OR) or 454 Junior sequencing Development of VirFind VirFind was developed as an automated tool to process NGS outputs. The pipeline is run on a Dell high performance computer node with AMD Opteron 6200 Series processors (64 cores) and 512 Gb RAM housed at Arkansas High Performance Computing Center. A detailed flowchart of the steps performed by VirFind is presented in Fig. 1. Briefly, NGS sequence files are converted to fasta format. Sequences are then trimmed at both 5′ and 3′ ends to remove any adapters and primers, and collapsed using FASTX-Toolkit (http://hannonlab.cshl.edu/fastx_toolkit) and seq_crumbs (http://bioinf.comav.upv.es). Host sequences are removed from further processing after mapping to reference genomes using Bowtie 2 (Langmead and Salzberg, 2012). De novo sequence assembly is performed on unmapped reads using Velvet (Zerbino and Birney, 2008) with k-mer (overlapping value)=31. For datasets with average sequence length ≤50 nt (primarily siRNA sequences), additional Velvet assemblies with k-mer=15 or 19 are constructed. Short sequences may lead to false positives in Blast and for this reason only contigs and singlets of ≥90 nt are subjected to Blastn search against the GenBank nt database. Hits to GenBank nt are filtered out with virus and non-virus fasta reads together with their corresponding Blastn reports in tabular format. Sequences without any matches are then subjected to Blastx search against all GenBank virus protein sequences. CAP3 assemblies are also constructed on top of the Blast outputs. Official virus taxonomy information (order, family, sub-family, genus, species) derived from International Committee on Taxonomy of Viruses (ICTV) Master species list (http://talk.ictvonline.org/files/ictv_documents/m/msl/default.aspx) is presented on both reports. The remaining non-hit sequences are further processed using a Python script (http://cgpdb.ucdavis.edu/DNA_SixFrames_Translation) to translate all six frames which are consequently examined for the presence of conserved domains (Marchler-Bauer et al., 2009) against the NCBI Conserved Domain Database (CDD). For the web interface submission, users need to complete a sequence submission form that contains the following options: (i) trimming of adapter/primer, (ii) mapping to reference genomes to remove host sequences, (iii) cut-off e-values of the Blastn and Blastx steps to define sequence relatedness to those found in GenBank, and (iv) conserved domain search of the remaining unmatched sequences. Sequence files are then uploaded to the VirFind ftp server for analysis. When all steps are completed, output files are compressed and mailed to users with information on how to download results from the server. Virus detection and discovery NGS dataset nos. 4–6 (Table 1) were each spiked with a random 270 nt virus/viroid GenBank sequence which was used to test the ability of VirFind to detect a single copy virus-like sequence. For sample nos. 22–27 and 29–30 where VirFind identified novel viruses, PCR primers were developed (Table 2) and used to amplify and sequence parts of the viruses׳ genomes, confirming their presence in individual samples.",VirusDetection,"development   virus detection  discovery pipeline use next generation sequencing
sample source sample exhibit viruslike symptoms  either plant maintain   university  arkansasfayetteville  provide  collaborators  california michigan mississippi  oregon laboratory test elisa  rtpcr detect viruses    subset  sample indicative   presence  novel strain  species  others plant leave  phloem  harvest  keep   °  nucleic acid extraction sample preparation methods thirty one plant sample sample nos  table   use  nucleic acid extraction sample nos     subject   total nucleic acid extraction protocol poudel    whereas sample nos   process use  dsrnaenrichment protocol yoshikawa  converse  sample        know   infect   array  viruses whereas sample       never test  viruses  reverse transcription  perform essentially  describe  tzanetakis    use maxima™ reverse transcriptase thermo fisher scientific waltham     pdap213′ emaravirus specific primer  bello  tzanetakis   sample     bg4art  kpnirt primers    table   sample      cdna  use     pcr reaction    pdap213′ primer  sample     bg4apcr  kpnipcr primers    table   sample   chemical composition  previously describe poudel     pcr program consist   min denaturation   ° follow   cycle      °     °      ° conclude   min extension   °  pcr products  visualize   tbeagarose gel stain  gelred® biotium hayward   doppcr products       purify use genejet pcr purification kit thermo fisher scientific dna  quantify   nanodrop™ spectrophotometer thermo fisher scientific normalize    amount   sample multiplexed  indicate  table   sequence   separate ngs reactions ngs dataset nos  use illumina center  genome research  biocomputing oregon state university corvallis    junior sequence development  virfind virfind  develop   automate tool  process ngs output  pipeline  run   dell high performance computer node  amd opteron  series processors  core    ram house  arkansas high performance compute center  detail flowchart   step perform  virfind  present  fig  briefly ngs sequence file  convert  fasta format sequence   trim   ′  ′ end  remove  adapters  primers  collapse use fastxtoolkit   seq_crumbs  host sequence  remove   process  map  reference genomes use bowtie  langmead  salzberg   novo sequence assembly  perform  unmapped read use velvet zerbino  birney   kmer overlap value  datasets  average sequence length ≤  primarily sirna sequence additional velvet assemblies  kmer    construct short sequence may lead  false positives  blast    reason  contigs  singlets  ≥   subject  blastn search   genbank  database hit  genbank   filter   virus  nonvirus fasta read together   correspond blastn report  tabular format sequence without  match   subject  blastx search   genbank virus protein sequence cap3 assemblies  also construct  top   blast output official virus taxonomy information order family subfamily genus species derive  international committee  taxonomy  viruses ictv master species list   present   report  remain nonhit sequence   process use  python script   translate  six frame   consequently examine   presence  conserve domains marchlerbauer      ncbi conserve domain database cdd   web interface submission users need  complete  sequence submission form  contain  follow options  trim  adapterprimer  map  reference genomes  remove host sequence iii cutoff evalues   blastn  blastx step  define sequence relatedness   find  genbank   conserve domain search   remain unmatched sequence sequence file   upload   virfind ftp server  analysis   step  complete output file  compress  mail  users  information    download result   server virus detection  discovery ngs dataset nos  table    spike   random   virusviroid genbank sequence   use  test  ability  virfind  detect  single copy viruslike sequence  sample nos     virfind identify novel viruses pcr primers  develop table   use  amplify  sequence part   viruses׳ genomes confirm  presence  individual sample",2
50,VirusFinder,"VirusFinder: software for efficient and accurate detection of viruses and their integration sites in host genomes through next generation sequencing data
Preprocessing VirusFinder’s input can either be raw sequencing reads (in Fastq format) or an alignment file (in BAM format). If user provides only raw sequencing reads, VirusFinder will first use the alignment tool Bowtie 2 [12] to map these reads to a human reference genome, which can either be NCBI build 37/36 (http://www.ncbi.nlm.nih.gov/projects/mapview/) or UCSC hg19/hg18 (http://hgdownload.cse.ucsc.edu/downloads.html#human). VirusFinder runs Bowtie 2 in its sensitive end-to-end mode, in which Bowtie 2 does not trim (or ""soft clip"") characters from short reads in order to achieve high alignment speed. With the alignment file generated by Bowtie 2 or provided by the user, VirusFinder then garners all reads unmapped to the human reference genome for downstream analysis. Here in this step, user is allowed to provide the sequence of the virus being examined as an input parameter of VirusFinder. VirusFinder will skip step (2) of the pipeline if user provides the virus sequence. (2) Virus detection This step is used to detect the specific type(s) of virus(es) present in the sample. This step will be skipped if user supplies the virus sequence to VirusFinder. If the virus type is unknown, however, VirusFinder first aligns the unmapped reads collected in step (1) to a virus database (virus DB). The current version of VirusFinder (release 1.0) uses the same virus DB, virus.fa, as the one included with the RINS package (http://khavarilab.stanford.edu/resources.html) [8]. This virus DB contains viruses of all known classes (32,102 in total) [8]. User can replace virus.fa with an alternative virus DB, Genome Information Broker for Viruses (GIB-V) [13] (http://gib-v.genes.nig.ac.jp/), which collects 25,525 virus reference sequences, or a smaller set of viruses of user interest. Next, VirusFinder de novo assembles the reads aligned to the virus DB into contigs and maps contigs to both the human genome and the virus DB. All contigs that are mapped to the human genome are discarded. The alignment scores of the nonhuman contigs, which align only to the virus DB, are then used to rank the viruses, to which they are mapped. The sequence of the top ranking virus is then applied to the next analysis step. It may be worth mentioning that our virus detection method as described here used RINS [8] as a starting point. However, different from RINS that identifies viruses by recruiting all reads mapped to the virus DB, which can at the same time align to the human genome, VirusFinder utilizes only the reads mapped to the virus DB and unmapped to the human genome for virus detection. By using less reads than RINS and more importantly with a simplified pipeline, VirusFinder achieves significant speedup over RINS without sacrificing its accuracy. We have tested VirusFinder on more than 20 samples (including unpublished ones) infected with viruses of various types and VirusFinder detected correct virus types for all the test samples (see section below for results on publically available data). (3) Virus integration site detection VirusFinder combines the human reference genome with the virus sequence (designated as a separate pseudo-chromosome, chrVirus) identified in previous step (2) or provided by the user. It then uses the mapping tool BWA [14] to align the reads recruited in step (1) to this new reference. Another tool VirusSeq [10] also concatenates the human genome with virus sequences. But VirusSeq includes a fixed set of, i.e. 18, virus sequences in its reference genome and hence cannot be applied directly to detect virus insertion sites in samples infected with viruses other than the18 predefined ones. By concatenating the viruses detected in step (2) on the fly, VirusFinder is readily applicable to samples harboring viruses of arbitrary types (as long as they are represented in the virus DB). From the resultant alignment file, VirusFinder calls inter-chromosomal structural variants (SVs) using CREST [15]. The breakpoints of the SVs that involve both the virus and human genome, if there are any, are then reported. CREST utilizes soft-clipped reads as breakpoint positions of SVs. On a WGS sample with a modest 30× coverage, CREST can take several days to complete. To speed up our pipeline, before executing CREST, we run a much faster SV-detecting tool SVDetect [16] on the alignment file to calculate potential regions harboring virus integration sites. Then we modified CREST to make it search primarily within the regions identified by SVDetect. By blending SVDetect with CREST, we are able to reduce the computational time of SV calling significantly from several days on a WGS sample to around an hour. When the pipeline terminates, three files, ‘virus.txt’, ‘contig.txt’ and ‘integration-sites.txt’, are created in the working directory of VirusFinder. These files contain candidate viruses identified by VirusFinder, contigs mapped to these virus sequences, and detected virus insertion sites, respectively. For each virus insertion event, VirusFinder provides its breakpoints in both the virus sequence and the human genome. For detailed explanations of these files, please read our user manual in the Supplementary Material S1. (4) Software implementation The entire pipeline of VirusFinder, from the initial preprocessing step to the final virus integration site detection, is fully automated. As far as we know, this is the first fully automatic pipeline combining virus detection (step 2) seamlessly with virus integration site identification (step 3) and, thus, the first NGS software enabling the automatic detection of virus integration sites in samples for which viruses may not necessarily be determined beforehand. The aforementioned tool VirusSeq provides both virus detection script and virus integration site identification script too. Unfortunately, in VirusSeq, they are separate programs and cannot work directly together. Another advantage of VirusFinder is that it is capable of analyzing large-scale NGS data efficiently by improving significantly its computational pipelines for viruses and their integration sites detection. VirusFinder further improves its analysis speed by blending a fast aligner Bowtie 2 in the time-consuming step (1) with a slower yet more sensitive aligner BWA on a smaller subset of reads in step (3). VirusFinder was implemented in Perl programming language and has been tested on various Linux platforms. It depends on several third-party tools, including BLAST+ (or BLAST) [17], BLAT [18], SAMtools [19], and Trinity [20], in addition to the aforementioned Bowtie 2, BWA, SVDetect, and CREST. All these tools are publically available. Their download URLs and brief descriptions are provided in the Supplementary Material S1. Different from other tools, CREST requires the installation of a BLAT server. To ease the distribution of VirusFinder, we modified CREST into a standalone tool, which, together with several other software that CREST requires, is now included in the release package of VirusFinder. This removed completely the requirement to install a BLAT server on user’s system.",VirusDetection,"virusfinder software  efficient  accurate detection  viruses   integration sit  host genomes  next generation sequence data
preprocessing virusfinders input  either  raw sequence read  fastq format   alignment file  bam format  user provide  raw sequence read virusfinder  first use  alignment tool bowtie    map  read   human reference genome   either  ncbi build    ucsc hg19hg18 #human virusfinder run bowtie    sensitive endtoend mode   bowtie    trim  ""soft clip"" character  short read  order  achieve high alignment speed   alignment file generate  bowtie   provide   user virusfinder  garner  read unmapped   human reference genome  downstream analysis    step user  allow  provide  sequence   virus  examine   input parameter  virusfinder virusfinder  skip step    pipeline  user provide  virus sequence  virus detection  step  use  detect  specific type  viruses present   sample  step   skip  user supply  virus sequence  virusfinder   virus type  unknown however virusfinder first align  unmapped read collect  step    virus database virus   current version  virusfinder release  use   virus  virusfa   one include   rins package    virus  contain viruses   know class   total  user  replace virusfa   alternative virus  genome information broker  viruses gibv    collect  virus reference sequence   smaller set  viruses  user interest next virusfinder  novo assemble  read align   virus   contigs  map contigs    human genome   virus   contigs   map   human genome  discard  alignment score   nonhuman contigs  align    virus    use  rank  viruses     map  sequence   top rank virus   apply   next analysis step  may  worth mention   virus detection method  describe  use rins    start point however different  rins  identify viruses  recruit  read map   virus       time align   human genome virusfinder utilize   read map   virus   unmapped   human genome  virus detection  use less read  rins   importantly   simplify pipeline virusfinder achieve significant speedup  rins without sacrifice  accuracy   test virusfinder     sample include unpublished ones infect  viruses  various type  virusfinder detect correct virus type    test sample see section   result  publically available data  virus integration site detection virusfinder combine  human reference genome   virus sequence designate   separate pseudochromosome chrvirus identify  previous step   provide   user   use  map tool bwa   align  read recruit  step    new reference another tool virusseq  also concatenate  human genome  virus sequence  virusseq include  fix set    virus sequence   reference genome  hence cannot  apply directly  detect virus insertion sit  sample infect  viruses   the18 predefined ones  concatenate  viruses detect  step    fly virusfinder  readily applicable  sample harbor viruses  arbitrary type  long    represent   virus    resultant alignment file virusfinder call interchromosomal structural variants svs use crest   breakpoints   svs  involve   virus  human genome       report crest utilize softclipped read  breakpoint position  svs   wgs sample   modest  coverage crest  take several days  complete  speed   pipeline  execute crest  run  much faster svdetecting tool svdetect    alignment file  calculate potential regions harbor virus integration sit   modify crest  make  search primarily within  regions identify  svdetect  blend svdetect  crest   able  reduce  computational time   call significantly  several days   wgs sample  around  hour   pipeline terminate three file virustxt contigtxt  integrationsitestxt  create   work directory  virusfinder  file contain candidate viruses identify  virusfinder contigs map   virus sequence  detect virus insertion sit respectively   virus insertion event virusfinder provide  breakpoints    virus sequence   human genome  detail explanations   file please read  user manual   supplementary material   software implementation  entire pipeline  virusfinder   initial preprocessing step   final virus integration site detection  fully automate  far   know    first fully automatic pipeline combine virus detection step  seamlessly  virus integration site identification step   thus  first ngs software enable  automatic detection  virus integration sit  sample   viruses may  necessarily  determine beforehand  aforementioned tool virusseq provide  virus detection script  virus integration site identification script  unfortunately  virusseq   separate program  cannot work directly together another advantage  virusfinder     capable  analyze largescale ngs data efficiently  improve significantly  computational pipelines  viruses   integration sit detection virusfinder  improve  analysis speed  blend  fast aligner bowtie    timeconsuming step    slower yet  sensitive aligner bwa   smaller subset  read  step  virusfinder  implement  perl program language    test  various linux platforms  depend  several thirdparty tool include blast  blast  blat  samtools   trinity   addition   aforementioned bowtie  bwa svdetect  crest   tool  publically available  download urls  brief descriptions  provide   supplementary material  different   tool crest require  installation   blat server  ease  distribution  virusfinder  modify crest   standalone tool  together  several  software  crest require   include   release package  virusfinder  remove completely  requirement  install  blat server  users system",2
51,VirusSeeker,"VirusSeeker, a computational pipeline for virus discovery and virome composition analysis
Sample collection, preparation and sequencing Stool was collected from an adult rhesus macaque (Macaca mulatta) that was a control animal that received sham vaccines in a previous published study (Barouch et al., 2015). Total RNA plus DNA were extracted from 100 to 200 mg of frozen feces and randomly amplified as described previously (Finkbeiner et al., 2009, Wang et al., 2003). Amplification product was used for NEBNext library construction (New England BioLabs). Libraries were multiplexed on an Illumina MiSeq instrument (Washington University Center for Genome Sciences) using the paired-end 2×250 protocol. The dataset is available at http://pathology.wustl.edu/virusseeker/index.htm. 4.1.1. VirusSeeker-Virome workflow The VS-Virome pipeline is controlled by a master Perl script VirusSeeker-Virome.pl. The input to the pipeline is a directory path. The directory holds sequencing data from one or multiple samples each with two files containing FASTQ format sequence reads for read1 and read2. 1. Sequence preprocessing. The preprocessing of sequence files consists of the following steps: 1.1) trim adapter and/or primer sequences using cutadapt (Martin, 2011); 1.2) join read1 and 2 (of paired end reads) together to form a longer read if they overlap by defined criteria using fastq-join in the ea-utils package (https://github.com/ExpressionAnalysis/ea-utils) (Aronesty, 2011); 1.3) quality filtering of reads (trim low quality nucleotides, poly A sequences, remove reads with low average quality score) to obtain good quality sequences using PRINSEQ (Edwards, 2011); 1.4) Remove redundant sequences. Identical or nearly-identical sequences are frequently present in NGS data, either due to the sheer depth of NGS or because many of the pre-sequencing sample preparation methods involve PCR amplification. To reduce the computing cost of downstream analysis and to reduce amplification artifacts, CD-HIT (Li and Godzik, 2006) is used to cluster similar sequences. The default parameters in VS-Virome are set to cluster sequences that share ≥95% identity over 95% of the sequence length. The longest sequence from each cluster is retained as the representative sequence and used for downstream analysis. These are the “unique sequences”. 1.5) Mask repetitive sequences and sequence quality control. Many eukaryotic genomes contain stretches of highly repetitive DNA sequences which cause problems in BLAST-based similarity searches and result in high rates of false-positive alignments. Tantan (Frith, 2011) and RepeatMasker (http://www.repeatmasker.org) (Smit et al., 1996) are used to mask interspersed repeats and low complexity DNA sequences. A sequence fails the quality control criteria if it does not contain a stretch of at least 50 consecutive non-“N” nucleotides (i.e., “Filtered sequence”) or if greater than 40% of the total length of the sequence is masked (i.e., ""low complexity sequence""). These sequences are removed from further analysis. Remaining sequences are “good sequences”.1.6) remove host sequences by aligning sequences to reference genome using BWA-MEM (Li and Durbin, 2010) and MegaBLAST. Any sequence mapped to ""Host"" genomic sequence is removed from further analysis. After sequence preprocessing we obtain high quality, unique, non-host sequences. 2. Identification of candidate viral sequences. The high quality unique sequences from step 1 were subjected to BLASTn alignment against the virus-only nucleotide database to detect those that share nucleotide sequence similarity to known viruses. Remaining sequences are then aligned using BLASTx against the virus-only protein database to detect viruses sharing protein sequence similarity to known viruses. Sequences that have a significant hit at either step are “candidate viral sequences” which are then classified as bacteriophage and eukaryotic viral sequence based on the taxonomy identity of the best BLAST hit. Sequences classified as ""phage"" are pooled into a single file and removed from further analysis. Taxonomy information of bacteriophage sequences is output into corresponding files. 3. False positive removal. The candidate eukaryotic viral sequences from step 2 are sequentially queried against the NCBI Bacteria reference genomes using BWA-MEM, NT database using MegaBLAST (e-value cutoff 1E-10), BLASTn (e-value cutoff 1E-10), and NCBI NR database using BLASTx (e-value cutoff 1E-3). Sequences with significant hits are classified as eukaryotic virus, phage, or non-viral (if it hits host, mouse, fungal, bacterial or other) based on the taxonomy identity of the best BLAST hit. If a sequence aligns to both a virus and a sequence derived from another organism type (e.g. bacteria or fungi) among the top 50 best hits with an e-value below the cutoff it is classified as “ambiguous”. All eukaryotic viral sequences are further classified into viral families based on the taxonomy ID of the best hit. Sequences without any significant hit after BLASTx against NCBI NR database are placed in the “unassigned” category. 4. Report of the findings. The final output for each sample is a single file summarizing all the eukaryotic viruses identified in the dataset ranked by increasing sequence similarity to the most closely related viruses. Similar outputs for bacteriophage sequences are also generated. 4.1.2. VirusSeeker-Discovery workflow The VS-Discovery pipeline is controlled by a master Perl script VirusSeeker-Discovery.pl. The input to the pipeline is the same as that for the VS-Virome. 1. Sequence preprocessing. The preprocessing of sequence files is similar to VS-Virome except with the following minor changes. First, host sequences are filtered prior to the initial deduplication using CD-HIT. Subsequently, a two-step assembly process is implemented. From the CD-HIT processing, the longest read and the top 3 longest reads from each CD-HIT sequence cluster were extracted. Reads were assembled using Newbler v2.8 (454 Life Sciences, Branford, CT). Singleton and outliers from all the first step assemblies were extracted and used as the input for a second step assembly using Newbler. All the contigs from the first step assemblies were extracted and used as the input for a second step assembly using Phrap (http://www.phrap.org). All the contigs, singletons and outliers from the second step assemblies were extracted and CD-HIT was used to further remove redundant sequences. Repeatmasker is applied as described above. 2. Identification of candidate viral sequences, false positive removal and report of the findings as described above except for the following minor change. Sequences are not queried against the NCBI Bacteria reference genomes to avoid removal of contigs that share similarity with bacteria over only a small region. 4.1.3. Genome annotation and phylogenetic analysis ORFs were predicted and annotated using Artemis (Rutherford et al., 2000). Multiple sequence alignments were performed with MUSCLE (Edgar, 2004). Phylogenetic analysis was performed using the maximum likelihood method in MEGA7 program (Kumar et al., 2016) with 500 bootstrap replicates. The percentage of replicate trees in which the associated taxa clustered together in the bootstrap test is shown next to the branches. Only values greater than 70 are shown. 4.1.4. Viruses analyzed and sequence accession numbers used for analyses The viruses analyzed included the following genera, species, and strains: Bunyaviridae Bunyamwera Genus: Bunyamwera virus (BUNV, NP_047211.1), La Crosse virus (LACV, AAA62607.1), Schmallenberg virus (SBV, AGU16231.1), Severe fever with thrombocytopenia syndrome virus (SFTSV, BAN58179.1), Pacui virus (PACV, AIN55741.1), Ngari virus (NRIV, AFY23376.1), Abbey lake orthobunyavirus Ab-BUNV, AIA08883.1), Batai virus (BATV, AFY52608.1); Phlebovirus Genus: Heartland virus (HRTV, AFP33395.1), Rift Valley fever virus (RVFV, YP_003848704.1), Toscana virus (TOSV, CAA48478.1), Uukuniemi virus (UUKV, BAA01590.1), Severe fever with thrombocytopenia syndrome virus (SFTSV, BAN58179.1), Silver water virus (SILV, AIU95032.1), Soybean cyst nematode associated Uukuniemi virus (ScPV, AEF56734.1); Hantavirus Genus: Hantaan virus (HTNV), Saaremaa virus (SAAV, CAC85165.2), Dobrava-Belgrade virus (DOBV, NP_942555.1); Recently proposed Gouleakovirus genus: Gouleako virus (GOLV, AEJ38175.1), Cumuto virus (CUMV, AHH60917.1); unclassified ssRNA negative-strand viruses: Botrytis cinerea negative-stranded RNA virus 1 (BCNV, YP_009182153.1), Macrophomina phaseolina negative-stranded RNA virus 1 (MpNSRV1, ALD89106.2). Picobirnaviridae: Dromedary picobirnavirus (Dromedary PBV, AIY31294.1), Dromedary picobirnavirus (Dromedary PBV, AIY31286.1), Fox fecal picobirnavirus (Fox PBV, AIB06801.1), Fox picobirnavirus (Fox PBV, AGK45545.1), Human picobirnavirus 4-GA-91(HPBV, AAG53584.1 GII), Human picobirnavirus 1-CHN-97 (HPBV, AAG53583.1 GI), Human picobirnavirus (HPBV, YP_239361.1), Human picobirnavirus VS6600008 (HPBV, AIG71990.1), Human picobirnavirus (HPBV, AHX00958.1), Human picobirnavirus (HPBV, BAJ53294.1 GII), Otarine picobirnavirus (Otarine PBV, AFJ79071.1), Picobirnavirus GI/PBV/turkey/USA/MN-1/2011 (Turkey PBV, AHZ46150.1), Picobirnavirus bovine/RUBV-P/IND/2005 (Bovine PBV, ACT64131.1), Picobirnavirus monkey/CHN-49/2002 (Monkey PBV, AFK81928.1), Porcine picobirnavirus (Porcine PBV, AHI59999.1). 4.1.5. Nucleotide sequence accession number The following sequences have been deposited in the NCBI database under corresponding GenBank accession numbers: KY174981: Camula virus L segment, KY174982: PBV 196-06 RNA 1 segment; KY174983: RNA 2 segment.",VirusDetection,"virusseeker  computational pipeline  virus discovery  virome composition analysis
sample collection preparation  sequence stool  collect   adult rhesus macaque macaca mulatta    control animal  receive sham vaccines   previous publish study barouch    total rna plus dna  extract       freeze feces  randomly amplify  describe previously finkbeiner    wang    amplification product  use  nebnext library construction new england biolabs libraries  multiplexed   illumina miseq instrument washington university center  genome sciences use  pairedend  protocol  dataset  available    virusseekervirome workflow  vsvirome pipeline  control   master perl script virusseekerviromepl  input   pipeline   directory path  directory hold sequence data  one  multiple sample   two file contain fastq format sequence read  read1  read2  sequence preprocessing  preprocessing  sequence file consist   follow step  trim adapter andor primer sequence use cutadapt martin   join read1    pair end read together  form  longer read   overlap  define criteria use fastqjoin   eautils package  aronesty   quality filter  read trim low quality nucleotides poly  sequence remove read  low average quality score  obtain good quality sequence use prinseq edwards   remove redundant sequence identical  nearlyidentical sequence  frequently present  ngs data either due   sheer depth  ngs   many   presequencing sample preparation methods involve pcr amplification  reduce  compute cost  downstream analysis   reduce amplification artifacts cdhit   godzik   use  cluster similar sequence  default parameters  vsvirome  set  cluster sequence  share ≥ identity     sequence length  longest sequence   cluster  retain   representative sequence  use  downstream analysis    “unique sequences”  mask repetitive sequence  sequence quality control many eukaryotic genomes contain stretch  highly repetitive dna sequence  cause problems  blastbased similarity search  result  high rat  falsepositive alignments tantan frith   repeatmasker  smite     use  mask intersperse repeat  low complexity dna sequence  sequence fail  quality control criteria     contain  stretch   least  consecutive non“” nucleotides  “filtered sequence”   greater     total length   sequence  mask  ""low complexity sequence""  sequence  remove   analysis remain sequence  “good sequences” remove host sequence  align sequence  reference genome use bwamem   durbin   megablast  sequence map  ""host"" genomic sequence  remove   analysis  sequence preprocessing  obtain high quality unique nonhost sequence  identification  candidate viral sequence  high quality unique sequence  step   subject  blastn alignment   virusonly nucleotide database  detect   share nucleotide sequence similarity  know viruses remain sequence   align use blastx   virusonly protein database  detect viruses share protein sequence similarity  know viruses sequence    significant hit  either step  “candidate viral sequences”    classify  bacteriophage  eukaryotic viral sequence base   taxonomy identity   best blast hit sequence classify  ""phage""  pool   single file  remove   analysis taxonomy information  bacteriophage sequence  output  correspond file  false positive removal  candidate eukaryotic viral sequence  step   sequentially query   ncbi bacteria reference genomes use bwamem  database use megablast evalue cutoff  blastn evalue cutoff   ncbi  database use blastx evalue cutoff  sequence  significant hit  classify  eukaryotic virus phage  nonviral   hit host mouse fungal bacterial   base   taxonomy identity   best blast hit   sequence align    virus   sequence derive  another organism type  bacteria  fungi among  top  best hit   evalue   cutoff   classify  “ambiguous”  eukaryotic viral sequence   classify  viral families base   taxonomy    best hit sequence without  significant hit  blastx  ncbi  database  place   “unassigned” category  report   find  final output   sample   single file summarize   eukaryotic viruses identify   dataset rank  increase sequence similarity    closely relate viruses similar output  bacteriophage sequence  also generate  virusseekerdiscovery workflow  vsdiscovery pipeline  control   master perl script virusseekerdiscoverypl  input   pipeline        vsvirome  sequence preprocessing  preprocessing  sequence file  similar  vsvirome except   follow minor change first host sequence  filter prior   initial deduplication use cdhit subsequently  twostep assembly process  implement   cdhit process  longest read   top  longest read   cdhit sequence cluster  extract read  assemble use newbler   life sciences branford  singleton  outliers    first step assemblies  extract  use   input   second step assembly use newbler   contigs   first step assemblies  extract  use   input   second step assembly use phrap    contigs singletons  outliers   second step assemblies  extract  cdhit  use   remove redundant sequence repeatmasker  apply  describe   identification  candidate viral sequence false positive removal  report   find  describe  except   follow minor change sequence   query   ncbi bacteria reference genomes  avoid removal  contigs  share similarity  bacteria    small region  genome annotation  phylogenetic analysis orfs  predict  annotate use artemis rutherford    multiple sequence alignments  perform  muscle edgar  phylogenetic analysis  perform use  maximum likelihood method  mega7 program kumar      bootstrap replicate  percentage  replicate tree    associate taxa cluster together   bootstrap test  show next   branch  value greater    show  viruses analyze  sequence accession number use  analyse  viruses analyze include  follow genera species  strain bunyaviridae bunyamwera genus bunyamwera virus bunv np_047211  crosse virus lacv aaa62607 schmallenberg virus sbv agu16231 severe fever  thrombocytopenia syndrome virus sftsv ban58179 pacui virus pacv ain55741 ngari virus nriv afy23376 abbey lake orthobunyavirus abbunv aia08883 batai virus batv afy52608 phlebovirus genus heartland virus hrtv afp33395 rift valley fever virus rvfv yp_003848704 toscana virus tosv caa48478 uukuniemi virus uukv baa01590 severe fever  thrombocytopenia syndrome virus sftsv ban58179 silver water virus silv aiu95032 soybean cyst nematode associate uukuniemi virus scpv aef56734 hantavirus genus hantaan virus htnv saaremaa virus saav cac85165 dobravabelgrade virus dobv np_942555 recently propose gouleakovirus genus gouleako virus golv aej38175 cumuto virus cumv ahh60917 unclassified ssrna negativestrand viruses botrytis cinerea negativestranded rna virus  bcnv yp_009182153 macrophomina phaseolina negativestranded rna virus  mpnsrv1 ald89106 picobirnaviridae dromedary picobirnavirus dromedary pbv aiy31294 dromedary picobirnavirus dromedary pbv aiy31286 fox fecal picobirnavirus fox pbv aib06801 fox picobirnavirus fox pbv agk45545 human picobirnavirus gahpbv aag53584 gii human picobirnavirus chn hpbv aag53583  human picobirnavirus hpbv yp_239361 human picobirnavirus vs6600008 hpbv aig71990 human picobirnavirus hpbv ahx00958 human picobirnavirus hpbv baj53294 gii otarine picobirnavirus otarine pbv afj79071 picobirnavirus gipbvturkeyusamn turkey pbv ahz46150 picobirnavirus bovinerubvpind bovine pbv act64131 picobirnavirus monkeychn monkey pbv afk81928 porcine picobirnavirus porcine pbv ahi59999  nucleotide sequence accession number  follow sequence   deposit   ncbi database  correspond genbank accession number ky174981 camula virus  segment ky174982 pbv  rna  segment ky174983 rna  segment",2
52,Truffle,"Targeted virus detection in next-generation sequencing data using an automated e-probe based approach
NGS data preparation Using a protocol described by Burger and Maree (2015) dsRNA was extracted from the phloem tissue of 14 grapevines displaying typical grapevine leafroll disease symptoms and 4 asymptomatic rootstocks. Sequencing libraries were prepared using an adapted Illumina TruSeq Stranded Total RNA Library Prep Kit (Burger and Maree, 2015) and were sequenced on either an Illumina HiSeq, HiScanSQ or MiSeq instrument. Data were trimmed and quality filtered using Trimmomatic (Bolger et al., 2014). A head crop of 9 nts was performed and reads were trimmed at the 3′ end when the quality score was lower that 20 (slidingwindow-4, Q20). 4.2. De novo genome assembly-based virus detection Trimmed reads were assembled into contigs using CLC Genomics Workbench 8. The minimum contig length was set to 250 nts while automatic bubble-size and word-size detection was applied. To determine the viral status of the samples all contigs were first aligned using blastn from Blast+ (Camacho et al., 2009) against GenBank's nt database, using default parameters. Contigs, which could not be annotated with blastn, were further analysed using tblastx against the same nt database also using default parameters. Filtered reads were additionally submitted to VirFind (Ho and Tzanetakis, 2014), applying default parameters, to determine the viral status. 4.3. Truffle development Truffle is an interface developed in Python to detect virus sequences in NGS data through designing and implementing virus-specific e-probes. The bioinformatic pipeline, based on the TOFI-derived (Satya et al., 2008) pipeline called EDNA (Stobbe et al., 2013), is outlined in Fig. 1. Firstly, probes can be designed which are customised for a user's specific virus species of interest. For probe design the genome of the target virus is first compared to that of a closely related virus (Table 2) using NUCmer (-c 10, -l 10, -g 0, --noextend, --maxmatch, --nosimplify), which forms part of the MUMmer package (Delcher et al., 1999, Delcher et al., 2002, Kurtz et al., 2004), to identity homologous genomic regions. Unique target-specific regions, 20 nts and longer, are extracted and serve as candidate probes. After removing sequences containing homo-oligomers of more than 4 nts in length the candidate probes are aligned to NCBI's online GenBank nt database (word size 7, gap cost to open 5 and to extend 2, reward 1, penalty -3), removing all probes that hit any sequence other than the virus of interest. An alignment with an e-value of 1×10−3 or less is considered a hit. The remaining probes form the virus-specific e-probes. A decoy set of sequences is also created, which comprises of the reverse sequences of the e-probes. For the second application of Truffle, Blast+ (Camacho et al., 2009) is used determine the viral status of a sample. The probes and decoys are aligned, with blastn (-task blastn-short), against a database composed of the raw NGS data. A score is generated for each probe and decoy based on the number of hits, e-value and percentage of query coverage (Stobbe et al., 2013). Depending on the nature of the score-data one of the following statistical tests is performed to compare the sets of probe and decoy scores, the parametric student t-test (for normally distributed data with equal variance), the Welch's t-test (for normally distributed data with unequal variance) or the Wilcoxon Ranksum test (for data which are not normally distributed). Samples with a p-value smaller than or equal to 0.05 are considered to be positive for a specific virus, while samples with a p-value greater than of equal to 0.1 are considered to be negative (Stobbe et al., 2013). Samples rendering a p-value between these two margins are only suspected to be positive and indicated as such. 4.4. Grapevine virus probe design and implementation Truffle was used to design probes for viruses, which are known to infect grapevine (Table 2). The viruses consist of a list of grapevine-infecting viruses generated by Martelli (2014). Generally the reference genome for a particular virus species available in GenBank was used as target genome while the type member of the genus served as the near-neighbour genome. For Grapevine leafroll-associated virus 3, Grapevine fanleaf virus and Grapevine virus E the full genome sequences of local isolates, available on NCBI, were used as target genomes. In the absence of a full genome the largest available sequence was used. In instances where the target species was the type member another closely related virus was chosen as near-neighbour. The final probes were screened against the raw NGS datasets of the 18 grapevine samples to determine their virus profiles. 4.5. Target genome assessment Different e-probe sets were designed for divergent GLRaV-3, GVA and GVB variants. The results generated for the distinctive probe-sets for a species were then compared to determine the effect of intra-species genetic variation on the sensitivity of virus detection. 4.6. Read-mapping analysis Using CLC Genomics Workbench 8, filtered reads were mapped onto all detected viruses (length fraction=0.5; similarity fraction=0.9; Non-specific reads mapped randomly) and the percentage genome coverage determined.",VirusDetection,"target virus detection  nextgeneration sequence data use  automate eprobe base approach
ngs data preparation use  protocol describe  burger  maree  dsrna  extract   phloem tissue   grapevines display typical grapevine leafroll disease symptoms   asymptomatic rootstocks sequence libraries  prepare use  adapt illumina truseq strand total rna library prep kit burger  maree    sequence  either  illumina hiseq hiscansq  miseq instrument data  trim  quality filter use trimmomatic bolger     head crop   nts  perform  read  trim   ′ end   quality score  lower   slidingwindow q20   novo genome assemblybased virus detection trim read  assemble  contigs use clc genomics workbench   minimum contig length  set   nts  automatic bubblesize  wordsize detection  apply  determine  viral status   sample  contigs  first align use blastn  blast camacho     genbank'  database use default parameters contigs  could   annotate  blastn   analyse use tblastx     database also use default parameters filter read  additionally submit  virfind   tzanetakis  apply default parameters  determine  viral status  truffle development truffle   interface develop  python  detect virus sequence  ngs data  design  implement virusspecific eprobes  bioinformatic pipeline base   tofiderived satya    pipeline call edna stobbe     outline  fig  firstly probe   design   customise   user' specific virus species  interest  probe design  genome   target virus  first compare     closely relate virus table  use nucmer       noextend maxmatch nosimplify  form part   mummer package delcher    delcher    kurtz     identity homologous genomic regions unique targetspecific regions  nts  longer  extract  serve  candidate probe  remove sequence contain homooligomers     nts  length  candidate probe  align  ncbi' online genbank  database word size  gap cost  open    extend  reward  penalty  remove  probe  hit  sequence    virus  interest  alignment   evalue    less  consider  hit  remain probe form  virusspecific eprobes  decoy set  sequence  also create  comprise   reverse sequence   eprobes   second application  truffle blast camacho     use determine  viral status   sample  probe  decoy  align  blastn task blastnshort   database compose   raw ngs data  score  generate   probe  decoy base   number  hit evalue  percentage  query coverage stobbe    depend   nature   scoredata one   follow statistical test  perform  compare  set  probe  decoy score  parametric student ttest  normally distribute data  equal variance  welch' ttest  normally distribute data  unequal variance   wilcoxon ranksum test  data    normally distribute sample   pvalue smaller   equal    consider   positive   specific virus  sample   pvalue greater   equal    consider   negative stobbe    sample render  pvalue   two margins   suspect   positive  indicate    grapevine virus probe design  implementation truffle  use  design probe  viruses   know  infect grapevine table   viruses consist   list  grapevineinfecting viruses generate  martelli  generally  reference genome   particular virus species available  genbank  use  target genome   type member   genus serve   nearneighbour genome  grapevine leafrollassociated virus  grapevine fanleaf virus  grapevine virus   full genome sequence  local isolate available  ncbi  use  target genomes   absence   full genome  largest available sequence  use  instance   target species   type member another closely relate virus  choose  nearneighbour  final probe  screen   raw ngs datasets    grapevine sample  determine  virus profile  target genome assessment different eprobe set  design  divergent glrav gva  gvb variants  result generate   distinctive probesets   species   compare  determine  effect  intraspecies genetic variation   sensitivity  virus detection  readmapping analysis use clc genomics workbench  filter read  map onto  detect viruses length fraction similarity fraction nonspecific read map randomly   percentage genome coverage determine",2
53,PhiSpy,"PhiSpy: a novel algorithm for finding prophages in bacterial genomes that combines similarity- and composition-based strategies
Data collection All bacterial genomes used in this analysis were retrieved from the Phage Annotation Tools and Methods server (Phantome server: http://www.phantome.org). As of March 2010, the server contained 547 complete bacterial genomes (at most 20 contigs) of which only 41 bacterial genomes (Supplemental Table S1) had 190 manually annotated prophages. All other lytic and lysogenic phage genomes were also collected from the Phantome server. Data analysis PhiSpy publicly available at http://phispy.sourceforge.net/ was written in python and C++. It has four steps (Supplemental Figure S1 is a flow chart of each step). Each step is described below. Calculation of different characteristics The first step calculates different parameters for the whole genome. The calculation of these parameters depends on a group of genes rather than a single gene. Therefore, for a complete genome, these parameters were computed using a sliding window of n genes. The average number of genes of the 190 known prophages is 39; so a window size of 40 genes was considered. The parameters are as follows: Customized AT and GC skew. The customized AT/GC skew was calculated by modifying the cumulative skew calculation (17,18). For a group of consecutive genes, the average skew of A, C, G and T were measured where n is the number of genes, Ai is the number of A nucleotide in the ith gene and so on. The customized AT and GC skews (described under ‘Results’ section) were developed and were calculated as follows: This customized version combines AT/GC and compensates for local deviations in the composition due to, for example, strand bias. Difference in median protein length. The median (M) of the lengths of all the proteins in a bacterial genome was calculated. For a group of proteins in a given window, the median protein length (m) was calculated and the difference in median length was computed as (M − m). Transcription strand orientation. For a given window size, the genes were partitioned in such a way so that all consecutive genes in a particular partition pointed in the same direction. The sum of the number of genes in the two largest partitions was taken for the window to maximize the number of consecutive genes in the same direction. Abundance of phage words. A ‘word’ is defined as a set of 12 consecutive base pairs. Each gene was split into 12 bp long non-overlapping words (four consecutive amino acids each). A unique ‘phage word library’ was built based on the 41 bacterial genomes that have well-annotated prophages. The library was constructed as follows: Bacterial words, B = {the ‘words’ of all bacterial genes of those 41 genomes but not including genes in prophages} Phage words, P = {the ‘words’ of all prophage genes of those 41 genomes} Unique ‘phage word library’ = P – B To measure the abundance of phage ‘words’, Shannon’s index (19,20) and the frequency of the presence of phage words were calculated. Shannon’s index was calculated by the following formula: where pi is the frequency of those words which are present in the ‘phage word library’. The frequency of words (F) of a window was calculated by dividing the number of available phage words with the total number of words. For a given window, the abundance of phage words is F/H. Homology. In a window of 40 genes, if there are at least 10 genes whose functional description is in phage subsystems (i.e. phage functional categories in the Phantome SEED database: http://www.phantome.org/PhageSeed/seedviewer.cgi), the window was considered a prophage window otherwise it was considered a bacterial window. Classification algorithm The second step of PhiSpy is to classify a window as a bacterial or a prophage window using random forests (21). A random forest is a classification algorithm that consists of multiple independent decision trees. The random forest requires a training set with multiple variables to build the forest of decision trees. In this case, there were five parameters whose values vary among distantly related genomes. If the similarities between two genomes were evolutionary significant, then they were considered as closely related genomes; otherwise, they were considered as distantly related genomes [the SEED API was used to determine relatedness (22)]. Therefore, for every group of closely related genomes, a different training set was constructed. Training/test set. In the Phantome server, there were 547 complete bacterial genomes that had 20 contigs or fewer (as of March 2010). From these 547 bacterial genomes, 19 groups of closely related genomes were constructed, where each group has at least one genome from the set of 41 bacterial genomes with annotated prophages. These 19 groups included 114 out of 547 bacterial genomes. For each group, one genome with manually annotated prophages was used as the training set for the rest of the genomes of that group (Table 1 and Supplemental Table S2). The genomes that did not belong to any group and had no manually annotated prophages were tested using a universal generic training set (constructed in the same way described above but using all 41 bacterial genomes). The parameter ‘abundance of phage word’ was ignored in the universal generic training set (Table 2). The statistical software program, R (http://www.r-project.org), was used to implement the random forest (23). The random forest produces a rank for each window of the whole genome that suggests whether the window consists of bacterial or phage genes. Processing the final rank for each gene The third step of PhiSpy provides a prediction status—either 0 (for non prophage genes) or 1 (for prophage genes) for each gene in the genome. If the window size was n, each gene contributed to 1 to n windows. Therefore, the final rank of a particular gene was measured by taking the average rank of the window in which the gene participated. The prophage prediction status was calculated from the final rank. If the final rank was greater than half of the maximum rank of any gene in the genome, then the gene was considered as a phage gene; otherwise, it was considered as a bacterial gene. Evaluation of the prediction The final step is to define the att sites for the predicted prophages and the overall evaluation of the prophages. When phages integrate into their hosts’ genome, they are usually bounded by two att sites—a short repeated sequence that flanks the insertion site. To find this insertion site, for each predicted prophage region (considered an initial prediction), the following steps were followed. After identifying the att sites, the next step is verifying the att sites. If the att sites lie inside the initial prediction, the number of phage-like proteins was counted for the two gaps (between attL and the start of the initial prediction and between attR and the end of the initial prediction). If the function of one-quarter of the genes in those two gaps belongs to phage subsystems, the initial prediction was considered as the final prediction otherwise the region covered by att sites was considered as the final prediction. If the att sites were outside the initial prediction, the same procedure was followed. Extending the predicted region up to 2000 bp on both sides. Identifying all duplicate short DNA sequences in that region. Finding the repeated pair that has minimum distance (<1000 bp) from either integrase/recombinase or tRNA/tmRNA genes or both. If there are multiple repeated pairs, the pair that covers the largest region was considered as the potential att sites. If no integrase/recombinase or tRNA/tmRNA genes were found, then the initially predicted region was considered. After verifying the att sites, the predicted prophages were evaluated by checking the function of all proteins in that region. If there are more than five proteins whose functions belong to the phage subsystems or are unknown and the number of phage-like/unknown proteins is at least half of the total number of proteins in the predicted region, then the region was considered as a potential prophage. However, if a group of proteins, whose functions belong to the phage subsystems, was not considered in the classification step as a probable prophage, then this region was also considered as a potential prophage. Calculation of false positives and false negatives The manually curated phage subsystems were used to evaluate the accuracy of the approach. A two-step program was designed to automatically calculate the error rate of the prophage prediction (for those genomes which have no information about prophages in their original genome analysis paper). In the first step of the program, true positives (TP) and false positives (FP) were predicted. If the predicted region consists of at least six phage proteins or 50% of the proteins within the predicted region belong to phage subsystems or are unknown, the predicted region was considered a TP prophage otherwise the region was considered a FP and not a prophage (those limits were determined by empirically). Prophages considered as TPs were divided into two groups: (i) known prophages—if the region contains phage-like proteins; we considered that it would be identified by similarity based approaches and therefore denoted it as a ‘known’ and (ii) undefined prophages—if the region has no phage-like protein; thus this would unlikely to be called a prophage. In the second step of the program, a region was considered as a false negative (FN) if there were at least six consecutive genes, whose functions belonged to the phage subsystems and the region was unidentified as a potential prophage. However, hypothetical proteins were ignored in this case, because the presence of several hypothetical proteins was not sufficient to predict a region as a prophage region.",VirusDetection,"phispy  novel algorithm  find prophages  bacterial genomes  combine similarity  compositionbased strategies
data collection  bacterial genomes use   analysis  retrieve   phage annotation tool  methods server phantome server    march   server contain  complete bacterial genomes    contigs     bacterial genomes supplemental table    manually annotate prophages   lytic  lysogenic phage genomes  also collect   phantome server data analysis phispy publicly available    write  python     four step supplemental figure    flow chart   step  step  describe  calculation  different characteristics  first step calculate different parameters   whole genome  calculation   parameters depend   group  genes rather   single gene therefore   complete genome  parameters  compute use  slide window   genes  average number  genes    know prophages     window size   genes  consider  parameters   follow customize    skew  customize atgc skew  calculate  modify  cumulative skew calculation    group  consecutive genes  average skew        measure     number  genes    number   nucleotide   ith gene     customize    skew describe  result section  develop   calculate  follow  customize version combine atgc  compensate  local deviations   composition due   example strand bias difference  median protein length  median    lengths    proteins   bacterial genome  calculate   group  proteins   give window  median protein length   calculate   difference  median length  compute     transcription strand orientation   give window size  genes  partition    way    consecutive genes   particular partition point    direction  sum   number  genes   two largest partition  take   window  maximize  number  consecutive genes    direction abundance  phage word  word  define   set   consecutive base pair  gene  split    long nonoverlapping word four consecutive amino acids   unique phage word library  build base    bacterial genomes   wellannotated prophages  library  construct  follow bacterial word   { word   bacterial genes    genomes   include genes  prophages} phage word   { word   prophage genes    genomes} unique phage word library      measure  abundance  phage word shannons index    frequency   presence  phage word  calculate shannons index  calculate   follow formula     frequency   word   present   phage word library  frequency  word    window  calculate  divide  number  available phage word   total number  word   give window  abundance  phage word   homology   window   genes     least  genes whose functional description   phage subsystems  phage functional categories   phantome seed database   window  consider  prophage window otherwise   consider  bacterial window classification algorithm  second step  phispy   classify  window   bacterial   prophage window use random forest   random forest   classification algorithm  consist  multiple independent decision tree  random forest require  train set  multiple variables  build  forest  decision tree   case   five parameters whose value vary among distantly relate genomes   similarities  two genomes  evolutionary significant    consider  closely relate genomes otherwise   consider  distantly relate genomes  seed api  use  determine relatedness  therefore  every group  closely relate genomes  different train set  construct trainingtest set   phantome server    complete bacterial genomes    contigs  fewer   march     bacterial genomes  group  closely relate genomes  construct   group   least one genome   set   bacterial genomes  annotate prophages   group include     bacterial genomes   group one genome  manually annotate prophages  use   train set   rest   genomes   group table   supplemental table   genomes    belong   group    manually annotate prophages  test use  universal generic train set construct    way describe   use   bacterial genomes  parameter abundance  phage word  ignore   universal generic train set table   statistical software program    use  implement  random forest   random forest produce  rank   window   whole genome  suggest whether  window consist  bacterial  phage genes process  final rank   gene  third step  phispy provide  prediction status—either   non prophage genes    prophage genes   gene   genome   window size    gene contribute     windows therefore  final rank   particular gene  measure  take  average rank   window    gene participate  prophage prediction status  calculate   final rank   final rank  greater  half   maximum rank   gene   genome   gene  consider   phage gene otherwise   consider   bacterial gene evaluation   prediction  final step   define  att sit   predict prophages   overall evaluation   prophages  phages integrate   host genome   usually bound  two att sites— short repeat sequence  flank  insertion site  find  insertion site   predict prophage region consider  initial prediction  follow step  follow  identify  att sit  next step  verify  att sit   att sit lie inside  initial prediction  number  phagelike proteins  count   two gap  attl   start   initial prediction   attr   end   initial prediction   function  onequarter   genes   two gap belong  phage subsystems  initial prediction  consider   final prediction otherwise  region cover  att sit  consider   final prediction   att sit  outside  initial prediction   procedure  follow extend  predict region       side identify  duplicate short dna sequence   region find  repeat pair   minimum distance    either integraserecombinase  trnatmrna genes      multiple repeat pair  pair  cover  largest region  consider   potential att sit   integraserecombinase  trnatmrna genes  find   initially predict region  consider  verify  att sit  predict prophages  evaluate  check  function   proteins   region      five proteins whose function belong   phage subsystems   unknown   number  phagelikeunknown proteins   least half   total number  proteins   predict region   region  consider   potential prophage however   group  proteins whose function belong   phage subsystems   consider   classification step   probable prophage   region  also consider   potential prophage calculation  false positives  false negative  manually curated phage subsystems  use  evaluate  accuracy   approach  twostep program  design  automatically calculate  error rate   prophage prediction   genomes    information  prophages   original genome analysis paper   first step   program true positives   false positives   predict   predict region consist   least six phage proteins     proteins within  predict region belong  phage subsystems   unknown  predict region  consider   prophage otherwise  region  consider      prophage  limit  determine  empirically prophages consider  tps  divide  two group  know prophages—  region contain phagelike proteins  consider   would  identify  similarity base approach  therefore denote    know   undefined prophages—  region   phagelike protein thus  would unlikely   call  prophage   second step   program  region  consider   false negative      least six consecutive genes whose function belong   phage subsystems   region  unidentified   potential prophage however hypothetical proteins  ignore   case   presence  several hypothetical proteins   sufficient  predict  region   prophage region",2
54,PHAST,"PHAST: a fast phage search tool
PHAST is an integrated search and annotation tool that combines genome-scale ORF prediction and translation (via GLIMMER), protein identification (via BLAST matching and annotation by homology), phage sequence identification (via BLAST matching to a phage-specific sequence database), tRNA identification, attachment site recognition and gene clustering density measurements using density-based spatial clustering of applications with noise (DBSCAN) ( 17 ) and sequence annotation text mining. In addition to these basic operations, PHAST also evaluates the completeness of the putative prophage, tabulates data on the phage or phage-like features and renders the data into several colorful graphs and charts. Details about the databases, algorithms and implementation are given below. Creation of custom prophage and bacterial sequence databases PHAST's prophage sequence database consists of a custom collection of phage and prophage protein sequences from two sources. One is the National Center for Biotechnology Information (NCBI) phage database that includes 46 407 proteins from 598 phage genomes. The other source is from the prophage database ( 12 ), which consists of 159 prophage regions and 9061 proteins not found in the NCBI phage database. Since many of the prophage proteins in the prophage database are actually bacterial proteins and some have only been identified computationally, we only selected those prophage proteins that have been associated with a clear phage function. This set includes a total of 379 phage protease, integrase and structural proteins. This PHAST phage library is used to identify putative phage proteins in the query genome via BLASTP ( 13 ) searches. In addition to a custom, self-updating phage sequence library, PHAST also maintains a bacterial sequence library consisting of 1300 non-redundant bacterial genomes/proteomes from all major eubacterial and archaebacterial phyla. This bacterial sequence library contains more than four million annotated or partially annotated protein sequences. Relative to the full GenBank protein sequence library (100+ million sequences), this bacterial-specific library is 25× smaller. This means that PHAST's genome annotation step (see below) can be accomplished 25× faster. Genome annotation and comparison PHAST accepts both raw DNA sequence and GenBank annotated genomes. If given a raw genomic sequence (FASTA format), PHAST identifies all ORFs using GLIMMER 3.02 ( 14 ). This ORF identification step takes about 45 s for an average bacterial genome of 5.0 Mb. The translated ORFs are then rapidly annotated via BLAST using PHAST's non-redundant bacterial protein library (∼2–3 min/genome). Because tRNA and tmRNA sites provide valuable information for identifying the attachment sites, they are calculated using the programs tRNAscan-SE ( 15 ) and ARAGORN ( 16 ). If an input (GenBank formatted) file is provided with complete protein and tRNA information, these steps are skipped. Phage or phage-like proteins are then identified by performing a BLAST search against PHAST's local phage/prophage sequence database along with specific keywords searches to facilitate further refinement and identification. Matched phage or phage-like sequences with BLAST e -values less than 10 −4 are saved as hits and their positions tracked for subsequent evaluation for local phage density by DBSCAN ( 17 ). Identification of prophage regions and prediction of their completeness Prophages can be considered as clusters of phage-like genes within a bacterial genome. The primary challenge (after phage-like genes have been identified) is to determine if these genes are sufficiently well clustered or proximal to each other to be considered prophage candidates. Although there are a few reported clustering methods for identifying phage gene clusters ( 9–11 ), we found the general DBSCAN algorithm performs just as well, likely because the identification of clusters of prophage genes is not a particularly difficult task. DBSCAN takes two parameters: the cluster size n and a distance e . The parameter n defines the minimal number of phage-like genes required to form a prophage cluster and e is the maximal spatial distance between two neighbor genes within the same cluster. In our case, the spatial distance between two genes is just the number of nucleotides between them. In other words, n can be considered as the minimal prophage size and e is the protein density within the prophage region. Empirically, we set n to be 6, since prophages generally have more than five proteins. The value of e was set to 3000 based on assessments from a small number of identified prophages in ProphageDB ( 12 ). We found that using a moderately different e -value will generally not change the prediction sensitivity. If PHAST's input file is an annotated GenBank file, an additional text scan is performed to identify prophages that may not have been found by clustering. This secondary (moving window) scan looks for specific phage-related keywords in the GenBank protein name field of the input file, such as ‘protease’, ‘integrase’ and ‘tail fiber’. If 6 or more proteins associated with these keywords are found within a window of 60 proteins, the region is considered as a putative prophage region even if an insufficient number of phage-like genes were found by DBSCAN within this region. Finally, if the identified prophage contains an integrase, potential phage attachment sites (one for each integrase in tandem prophages) are then identified by scanning the region for short nucleotide repeats (12–80 bases) ( 18 ). After all prophage regions have been detected, a completeness score is assigned to each identified prophage. Three potential scenarios are considered: (i) the region only contains genes/proteins of a known phage; (ii) >50% of the genes/proteins in the region are related to a known phage and (iii) <50% of the genes/proteins in the region are related to a known phage. In scenario (i), the region automatically has a completeness score of 150 (the maximum). In scenario (ii) and (iii), the region's completeness score is calculated as the sum of the scores corresponding to the region's size and number of genes. If it is found that the region is related to a known phage, both scores are calculated using the size and number of matched genes of the related phage, otherwise they are calculated using the average size (30 kb) and average number of genes (40) of typical phages. The total score in scenario (iii) also counts the number of identified ‘cornerstone’ genes as well as the density of phage-like genes in the region. ‘Cornerstone genes’ are genes encoding proteins involved in phage structure, DNA regulation, insertion and lysis ( 1 ). Table 1 shows the details of PHAST's completeness score calculation. A prophage region is considered to be incomplete if its completeness score is less than 60, questionable if the score is between 60 and 90, and intact if the score is above 90. Program and web server characteristics PHAST's search, annotation and DBSCAN clustering software were written using a combination of C and Java. PHAST's web interface was implemented using a standard CGI framework. PHAST's interactive Google-Map style graphics were built using Adobe's Flash Builder. PHAST also supports remote scripting using a URL API (this is described under PHAST's ‘Instructions’ link) and it maintains a large, hyperlinked database of pre-computed bacterial genomes for rapid prophage identification among known/well-studied genomes (see PHAST's ‘Databases’ link). A screenshot montage of PHAST's output is given in Figure 1 . The web application is platform independent and has been tested successfully on Internet Explorer 8.0, Mozilla Firefox 3.0 and Safari 4.0. However, in order to view the Flash output the user must have Adobe Flash Player installed. This is freely available at http://www.adobe.com/products/flashplayer . For the most up to date instructions of how to use the server please read the online help page at http://phast.wishartlab.com/how_to_use.html . Performance evaluation In order to compare PHAST's performance with other programs, we used a collection of hand-annotated prophages from 54 bacterial genomes ( 1 , 10 ) as our ‘gold standard’ reference control. PHAST was evaluated using both GenBank annotated sequences (i.e. bacterial genomes with manually or semi-automatically annotated genomes) as well as raw DNA sequence files. The performance was measured using both sensitivity [TP/(TP + FN)] and positive predictive value or PPV [TP/(TP + FP)]. Using this 54 genome data set PHAST achieved, a sensitivity (Sn) of 85.4% and a PPV of 94.2% when evaluated using GenBank annotated files. When using raw DNA sequence and its own ORF finding and genome annotation tools, PHAST achieved a sensitivity of 79.4% and a PPV of 86.5% for the same 54 genomes. PHAST's performance using the same annotated GenBank data was superior to Prophinder (Sn 77.5%, PPV 93.6%), Prophage Finder (Sn 92.1%, PPV 52.1%) and Phage_Finder (Sn 68.5%, PPV 94.3%). PHAST's performance using raw DNA sequence data does not quite match that of the pre-annotated data, but its combined sensitivity/positive predictive value is still comparable to Prophinder and superior to both Prophage Finder and Phage_Finder. Detailed comparisons for both the GenBank and raw sequence inputs for all 54 genomes can be found in PHAST's documentation page ( http://phast.wishartlab.com/documentation.html ). PHAST's improved performance does not necessarily indicate Prophinder, Prophage Finder or Phage_Finder's phage finding algorithms are inferior to PHAST's algorithm. Rather, some of the performance gain appears to be due to PHAST's implementation of a newer, larger phage sequence library and perhaps a better exploitation of keyword annotations. A further challenge with evaluating any kind of prophage identification software is that there is no ‘absolute’ or ‘gold’ standard. Careful manual annotation by phage experts is certainly a high standard, but it is more than likely that some prophages in the 54 evaluation genomes were not identified, having decayed or mutated too much for them to appear in the Casjens reference list ( 1 ). In other words, some of the false positive predictions may in fact be true positives. Indeed, through manual inspection of PHAST's results we found a number of ‘dense’ positive BLAST hits to phage proteins in several genomes, but these were not labeled as prophages in the Casjens reference list. Instead of ‘false positives’, we believe that they should be considered as prophage-related regions that have not been previously reported in the literature. In addition to evaluating PHAST's prophage identification performance, we also evaluated its speed. Given that PHAST accepts two kinds of file input (raw FASTA DNA sequence and GenBank formatted files), we assessed its performance for both kinds of input files. When given raw genomic sequence, PHAST must run GLIMMER as well as several gene/protein identification programs. Using the raw Escherichia coli O157:H7 genome sequence only (GenBank accession NC_002655), PHAST completed its prophage identification in just over 4 min. When tested on the same input file, Prophage Finder returned results after 20 min. However, it is important to note that Prophage Finder does not annotate bacterial genes, its output is very ‘crude’ and its combined Sn/PPV score is significantly worse than PHAST's ( Table 2 ). Using the GenBank annotated E. coli O157:H7 file, PHAST completed its prophage identification in 140 s. Using the same annotated NC_002655 file for the Prophinder ( 10 ) web server took 33 min, while using a local copy of Phage_Finder ( 9 ) running on a 2.1 GHz Pentium PC with 12 Gb RAM, the same file took 93 min. These data suggest that PHAST is between 5 and 40 times faster than existing prophage finding programs. A more complete feature and performance comparison between PHAST and other existing prophage finding tools is given in Table 2 . Limitations PHAST is not without some limitations. First, like all other database-driven annotation systems, PHAST obviously performs poorly at identifying novel phages, whose genes/proteins are not closely related to any record in the PHAST database. In this regard, the appearance of large numbers of proximal proteins with unknown function could be a good indication of a novel phage. Second, the DBSCAN algorithm used by PHAST assumes an even density of phage-like hits in every prophage genomic sequence, which is not generally true in practice. Consequently, a highly uneven distribution of phage-like genes could potentially fool the DBSCAN algorithm. Finally, PHAST will occasionally ‘split’ larger prophages into a number of smaller prophages due to a paucity of BLAST hits.",VirusDetection,"phast  fast phage search tool
phast   integrate search  annotation tool  combine genomescale orf prediction  translation via glimmer protein identification via blast match  annotation  homology phage sequence identification via blast match   phagespecific sequence database trna identification attachment site recognition  gene cluster density measurements use densitybased spatial cluster  applications  noise dbscan     sequence annotation text mine  addition   basic operations phast also evaluate  completeness   putative prophage tabulate data   phage  phagelike feature  render  data  several colorful graph  chart detail   databases algorithms  implementation  give  creation  custom prophage  bacterial sequence databases phast' prophage sequence database consist   custom collection  phage  prophage protein sequence  two source one   national center  biotechnology information ncbi phage database  include   proteins   phage genomes   source    prophage database     consist   prophage regions   proteins  find   ncbi phage database since many   prophage proteins   prophage database  actually bacterial proteins      identify computationally   select  prophage proteins    associate   clear phage function  set include  total   phage protease integrase  structural proteins  phast phage library  use  identify putative phage proteins   query genome via blastp    search  addition   custom selfupdating phage sequence library phast also maintain  bacterial sequence library consist   nonredundant bacterial genomesproteomes   major eubacterial  archaebacterial phyla  bacterial sequence library contain   four million annotate  partially annotate protein sequence relative   full genbank protein sequence library  million sequence  bacterialspecific library   smaller  mean  phast' genome annotation step see    accomplish  faster genome annotation  comparison phast accept  raw dna sequence  genbank annotate genomes  give  raw genomic sequence fasta format phast identify  orfs use glimmer      orf identification step take      average bacterial genome     translate orfs   rapidly annotate via blast use phast' nonredundant bacterial protein library  mingenome  trna  tmrna sit provide valuable information  identify  attachment sit   calculate use  program trnascanse     aragorn      input genbank format file  provide  complete protein  trna information  step  skip phage  phagelike proteins   identify  perform  blast search  phast' local phageprophage sequence database along  specific keywords search  facilitate  refinement  identification match phage  phagelike sequence  blast  value less     save  hit   position track  subsequent evaluation  local phage density  dbscan    identification  prophage regions  prediction   completeness prophages   consider  cluster  phagelike genes within  bacterial genome  primary challenge  phagelike genes   identify   determine   genes  sufficiently well cluster  proximal      consider prophage candidates although     report cluster methods  identify phage gene cluster     find  general dbscan algorithm perform   well likely   identification  cluster  prophage genes    particularly difficult task dbscan take two parameters  cluster size    distance    parameter  define  minimal number  phagelike genes require  form  prophage cluster     maximal spatial distance  two neighbor genes within   cluster   case  spatial distance  two genes    number  nucleotides     word    consider   minimal prophage size     protein density within  prophage region empirically  set     since prophages generally    five proteins  value    set   base  assessments   small number  identify prophages  prophagedb     find  use  moderately different  value  generally  change  prediction sensitivity  phast' input file   annotate genbank file  additional text scan  perform  identify prophages  may    find  cluster  secondary move window scan look  specific phagerelated keywords   genbank protein name field   input file   protease integrase  tail fiber     proteins associate   keywords  find within  window   proteins  region  consider   putative prophage region even   insufficient number  phagelike genes  find  dbscan within  region finally   identify prophage contain  integrase potential phage attachment sit one   integrase  tandem prophages   identify  scan  region  short nucleotide repeat  base      prophage regions   detect  completeness score  assign   identify prophage three potential scenarios  consider   region  contain genesproteins   know phage     genesproteins   region  relate   know phage  iii    genesproteins   region  relate   know phage  scenario   region automatically   completeness score    maximum  scenario   iii  region' completeness score  calculate   sum   score correspond   region' size  number  genes    find   region  relate   know phage  score  calculate use  size  number  match genes   relate phage otherwise   calculate use  average size    average number  genes   typical phages  total score  scenario iii also count  number  identify cornerstone genes  well   density  phagelike genes   region cornerstone genes  genes encode proteins involve  phage structure dna regulation insertion  lysis    table  show  detail  phast' completeness score calculation  prophage region  consider   incomplete   completeness score  less   questionable   score       intact   score    program  web server characteristics phast' search annotation  dbscan cluster software  write use  combination    java phast' web interface  implement use  standard cgi framework phast' interactive googlemap style graphics  build use adobe' flash builder phast also support remote script use  url api   describe  phast' instructions link   maintain  large hyperlinked database  precomputed bacterial genomes  rapid prophage identification among knownwellstudied genomes see phast' databases link  screenshot montage  phast' output  give  figure    web application  platform independent    test successfully  internet explorer  mozilla firefox   safari  however  order  view  flash output  user must  adobe flash player instal   freely available         date instructions    use  server please read  online help page    performance evaluation  order  compare phast' performance   program  use  collection  handannotated prophages   bacterial genomes        gold standard reference control phast  evaluate use  genbank annotate sequence  bacterial genomes  manually  semiautomatically annotate genomes  well  raw dna sequence file  performance  measure use  sensitivity tptp    positive predictive value  ppv tptp   use   genome data set phast achieve  sensitivity      ppv    evaluate use genbank annotate file  use raw dna sequence    orf find  genome annotation tool phast achieve  sensitivity     ppv       genomes phast' performance use   annotate genbank data  superior  prophinder   ppv  prophage finder   ppv   phage_finder   ppv  phast' performance use raw dna sequence data   quite match    preannotated data   combine sensitivitypositive predictive value  still comparable  prophinder  superior   prophage finder  phage_finder detail comparisons    genbank  raw sequence input    genomes   find  phast' documentation page    phast' improve performance   necessarily indicate prophinder prophage finder  phage_finder' phage find algorithms  inferior  phast' algorithm rather    performance gain appear   due  phast' implementation   newer larger phage sequence library  perhaps  better exploitation  keyword annotations   challenge  evaluate  kind  prophage identification software      absolute  gold standard careful manual annotation  phage experts  certainly  high standard      likely   prophages    evaluation genomes   identify  decay  mutate  much    appear   casjens reference list      word    false positive predictions may  fact  true positives indeed  manual inspection  phast' result  find  number  dense positive blast hit  phage proteins  several genomes     label  prophages   casjens reference list instead  false positives  believe     consider  prophagerelated regions     previously report   literature  addition  evaluate phast' prophage identification performance  also evaluate  speed give  phast accept two kinds  file input raw fasta dna sequence  genbank format file  assess  performance   kinds  input file  give raw genomic sequence phast must run glimmer  well  several geneprotein identification program use  raw escherichia coli o157h7 genome sequence  genbank accession nc_002655 phast complete  prophage identification     min  test    input file prophage finder return result   min however   important  note  prophage finder   annotate bacterial genes  output   crude   combine snppv score  significantly worse  phast'  table   use  genbank annotate  coli o157h7 file phast complete  prophage identification    use   annotate nc_002655 file   prophinder    web server take  min  use  local copy  phage_finder    run    ghz pentium     ram   file take  min  data suggest  phast      time faster  exist prophage find program   complete feature  performance comparison  phast   exist prophage find tool  give  table   limitations phast   without  limitations first like   databasedriven annotation systems phast obviously perform poorly  identify novel phages whose genesproteins   closely relate   record   phast database   regard  appearance  large number  proximal proteins  unknown function could   good indication   novel phage second  dbscan algorithm use  phast assume  even density  phagelike hit  every prophage genomic sequence    generally true  practice consequently  highly uneven distribution  phagelike genes could potentially fool  dbscan algorithm finally phast  occasionally split larger prophages   number  smaller prophages due   paucity  blast hit",2
55,PhageFinder,"Phage_Finder: automated identification and classification of prophage regions in complete bacterial genome sequences
System and software requirements Phage_Finder was written in PERL (http://www.perl.org) and tested using PERL version 5.8.5+ on Linux and Mac OS X 10.3/10.4 operating systems, but should work in most Unix environments if the following helper programs are installed and functional: NCBI BLASTALL (13) or WUBLAST 2.0 (http://blast.wustl.edu) (14) for BLAST searching, HMMSEARCH (15) to find HMM matches, tRNAscan-SE (18) to find the location of tRNAs, Aragorn (19) to locate tmRNA sequences, and FASTA33 (16), MUMMER (17), or BLASTN (13) to find att sites. Phage_Finder.pl utilizes the Math::Round PERL module that was written by Geoffrey Rommel to round numbers by defined multiples and is freely available (http://search.cpan.org/~grommel/Math-Round-0.05/Round.pm). Input requirements If running the included Phage_Finder.sh BASH script, the input requirements are as follows: name of file containing the protein sequences of the bacterial genome to be searched in FASTA format, name of BLAST-formatted phage protein sequences, the name of the file containing the DNA sequence of the entire bacterial genome to be searched (not the coding sequences), and the Phage_Finder information file (tab-delimited: contig_ID, size_of_contig, feat_name, end5, end3, annotation) or a GenBank .ptt file. If invoking Phage_Finder.pl directly, then the following tab-delimited files are required for full functionality: NCBI or WU BLASTP data, HMMSEARCH data, tRNAscan-SE data, Aragorn data and either a Phage_Finder information file or GenBank .ptt file. If the HMM data is not provided then, the search for att sites will not be performed. If the data from tRNAscan-SE/Aragorn is not provided, then the Phage_Finder.pl will not associate any putative target-site duplications with tRNA/tmRNA genes. Identification of prophage regions One of the original intentions of Phage_Finder was to have a program that can distinguish between largely intact, possibly functional prophages versus small regions or clusters of prophage remnants and other mobile elements. It takes advantage of several features of functional prophages to filter out unwanted fragmented regions. Since functional temperate phages integrate as linear molecules in a size range of 18–150 kb, good candidate prophage regions should have clusters of phage-like genes in this size range. Functional phages also have a large fraction of hypothetical or conserved hypothetical proteins. These stretches of phage-like and unknown genes are consecutive, not broken up by operons of house-keeping genes, although an occasional metabolic enzyme can be encoded on a phage. Tailed phages will have a conserved late gene operon that is responsible for packaging and head morphogenesis (21–23). This conserved region includes a small and large terminase subunit to recognize pac or cos sites and to cleave phage genome concatemers for packaging of the phage genome into capsids (24); a portal protein to form a hole for passage of the phage genome during packaging and release (25); a prohead protease to generate mature capsids (23,26); and the major capsid protein that forms the bacteriophage capsid or head (23). A functional prophage region will also lack ribosomal RNA sequences. The boundaries of functional prophages that integrated into specific locations can be determined by locating a site-specific recombinase at one of the ends of the phage region. Phages can integrate into tRNA/tmRNA genes, other conserved genes or intragenic regions. Since many phages and genetic elements tend to have an affinity for tRNA genes as the target for integration, any tRNA/tmRNA gene present within the putative phage region is checked first as a target for integration. The integrase can integrate into the anticodon-loop, the T-loop, or the 3′ end of the tRNA/tmRNA gene (27). The phage genome will contain sequence near the integrase gene that is homologous to the 3′ part of its target to avoid inactivating the gene after insertion (28). Following integration, the target gene will be a fusion with the 5′ end being of bacterial origin and the 3′ end of phage origin and the original bacterial-derived 3′ end on the other side of the inserted phage genome. By searching the other end of the putative phage region with the sequence of the tRNA/tmRNA gene, including extra sequence in case of miscalculation of the boundaries, one can identify what looks like a target-site duplication, the sequence of the replaced 3′ end of the tRNA/tmRNA gene. This homologous sequence, flanking the genome of the integrated phage genome, is referred to as the core attachment site (attcore) and the two half sites are attL (the phage-derived sequence) and attR (the original bacterial sequence). Sequences that are 3′ of the tRNA/tmRNA gene can also be part of the attcore (27). Phage_Finder overview The first analysis that Phage_Finder does is to count the number of valid BLASTP phage matches within a user-defined window size, sliding by a user-defined step size until the size of the genome is reached. I have determined that a window size of 10 000 bp and a step size of 5000 bp are optimal settings for defining clusters of phage hits with the least amount of noise. These setting are the default window and step size settings. The center of each prophage region is then defined as the window with the greatest number of phage database matches in a region that begins with a window having at least the user-defined number of hits per window (default is four hits per window). If at least one region is found within the minimum number of hits per window, then the 5′ and 3′ boundaries of each region is roughly determined. Beginning with the previously defined center of each region, Phage_Finder.pl slides gene by gene toward the 5′ and 3′ ends of the region, making a decision about inclusion. The decision to include a gene within a particular phage region is made in the following order: (i) if the protein has a phage HMM hit, then include; (ii) if it has a phage database BLASTP valid match, then include; (iii) if the gene is a tRNA or tmRNA, then include; (iv) if either of the next three genes are tRNAs or tmRNAs, then include; (v) if the gene has annotation that has been observed in known phages and there are at least three valid BLASTP phage database matches in the current window, then include (this ‘ok annotation’ is described below); (vi) if the gene has annotation that has been observed in known phages and there are at least two valid BLASTP phage database matches in the next window, then include; (vii) if there are at least three valid BLASTP phage database matches in the next window, then include; lastly, (viii) if there are at least two valid BLASTP matches in the current step and the current gene is before the matching gene, then include every gene up to the gene with the database match. If a putative prophage region contained valid HMM matches to XerC/D (TIGR02224 and TIGR02225) or integron (TIGR02249) integrases, then these regions would be excluded from further consideration. Before further defining the boundaries of each putative prophage region, the program attempts to define whether the regions are type prophage or type bacteriocin and whether regions can be further classified as Mu-like, retron phage R73-like, P2-like or P4-like. Degenerate regions are determined after analysis of putative attachment sites. A region is defined as type prophage if: (i) it has a core HMM match or (ii) it has a lytic HMM match and a tail HMM match and an integrase HMM match. If the region has a lytic HMM match and a tail HMM match, but no integrase HMM match, then the region is defined as type bacteriocin, which is analogous to the phage-like bacteriocins (pyocins) in P.aeruginosa or monocins in L.monocytogenes (11). The R-type and F-type pyocins are probably the best studied, being defined phenotypically and genetically (29). These pyocins encode headless phage tails, regulatory proteins and lysis proteins for the production and release of phage tails that are specific to closely related Pseudomonads, resulting in destruction of the target cell through membrane disruption (30). The region is sub classified as Mu-like if there are proteins within the region that match the following Mu-specific HMMs: PF02316 (Mu DNA-binding domain), PF02914 (Bacteriophage Mu transposase), PF06074 [labeled as protein of unknown function (DUF935), but matches the Mu portal, gp29] and PF07030 [phage conserved hypothetical protein (DUF1320), matching Mu gp36]. PF06074 and PF07030 were determined to be Mu-specific by searching the phage database with these models and via mapping to single-linkage clusters of the phage database searched against itself by BLASTP (data not shown). Only Mu-like phages were hit by these HMMs. A combination of specific HMM matches and region length were used to distinguish between the retron phage R73, P2 and P4. The retron phage R73 is a P4-like cryptic prophage from a clinical Escherichia coli isolate, containing a retroelement (31). Bacteriophage P4 is a satellite phage that can use the head and tail genes of coliphage P2 to package inself into infectious viral particles (32). All three of these phages can be identified as having HMM hits to PF04606 (Phage transcriptional activator, Ogr/Delta) and TIGR01613 (phage/plasmid primase, P4 family, C-terminal domain) and an integrase match. The region is classified as retron R73-like if there is a match to PF00078 [Reverse transcriptase (RNA-dependent DNA polymerase)]. If the length of the region is >25 000 bp, then it is considered P2-like and P4-like if the size is <15 000 bp. Provided that a file containing the DNA sequence of the genome and HMM or tRNA/tmRNA data was provided and the region is not Mu-like, a search for putative phage attachment sites is conducted. The user can specify BLASTN, FASTA33 or MUMMER to do the nucleotide similarity searches. BLASTN and FASTA33 have the advantage in that imperfect direct repeats can be identified, while MUMMER only looks for exact matches. BLASTN is the default nucleotide similarity search tool because it appears to do a better job of finding more significant matches. Only the top two matches are processed. If tRNA/tmRNA gene(s) are within a putative prophage region, the sequence of the inmost tRNA gene is used as the query to search the remaining 20% of the region plus 15 000 additional nucleotides for a similar direct repeat. If a direct repeat is identified, then Phage_Finder attempts to extend the region of homology by searching against the tRNA/tmRNA sequence plus 200 additional nucleotides at the 3′ end of the tRNA/tmRNA gene. If no homologies are found, the program will identify the outermost integrase gene. The sequence beginning 1 nt outside the outermost end of the integrase gene and extending outside of the phage region by 400 nt is used as the query to search the remaining 20% of the region plus 15 000 additional nucleotides for a similar direct repeat. If a putative att site is identified, the coordinates are used to identify a putative target gene. Analysis of the distance between integrase and att sites of known phages was conducted to determine that 400 nt are sufficient sequence to include the att site. The coordinates of tRNA/tmRNA genes are used to determine whether a putative att site targeted a tRNA/tmRNA gene, in case the incorrect tRNA was chosen from a series of multiple tRNAs. The final step is to format the data for printing a summary to the terminal and multiple output files to disk. If there are more contigs/assemblies to analyze, the program begins by finding clusters of phage database BLASTP matches on the next contig. This is repeated until there are no further contigs to search. Input Phage_Finder begins by checking for the two required data files: NCBI or WUBLAST BLASTP tab-delimited data where the query bacterial genome protein sequences were searched against a BLAST-formatted database of bacteriophage protein sequences and the Phage_Finder information file or GenBank .ptt file (described above). If both files are given and present, then the data from the Phage_Finder information file or GenBank .ptt file is processed, otherwise, the program terminates with a help menu. A flowchart of Phage_Finder logic is presented in Figure 1. If provided, data from tRNAscan-SE and Aragorn are processed and stored for future use. Other files, that change infrequently, are stored in the Phage_Finder home directory and are hard-coded into the program. A list of satisfactory gene annotations is read from a file so the program can differentiate between ‘house-keeping’ genes and those genes that have been previously associated with functional phages or can be associated with phages (i.e. hypothetical proteins, conserved proteins and regulatory proteins). This list of ‘ok annotation’ was generated by parsing the annotation associated with every phage sequence in the phage database used in BLAST searches, removing spaces and making non-redundant. This also allows those genes that are actually phage-derived, but not matching anything in the current, limited phage database, to be included in a phage region. If orthologs of every phage protein were already in the database, this step would not be necessary. Because orthologs of some proteins that are found in the genomes of functional phages are also present in bacterial genomes in non-phage regions, matches to these proteins are not specific to phage regions and must be excluded from analysis. Examples of proteins that fall into this category are transposases (non-Mu-like), ABC transporters, ribonucleotide reductase and certain other enzymes commonly encoded in the genomes of lytic phages. The accession numbers of these proteins are stored in the phage_exclude.lst file, which is used by Phage_Finder.pl to exclude these protein database matches from analysis. A list of core phage HMMs as well as lists of HMMs that are specific for phage lysis genes and tail proteins are read in from separate files (Table 1). Matches to these HMMs as well as the integrase HMMs (PF00239, PF00589, or PF02899) are used to characterize the putative prophage regions as either prophage or bacteriocin. Matches to other specific phage HMMs and the length of the region are used to distinguish between Mu-like regions, retron phage R73, P2 and P4. Tab-delimited data from BLASTP searches against a database of bacteriophage protein sequences is read in and processed. Data from NCBI BLASTP option −m 8 or WUBLASTP that has been processed with the BTAB BLAST output parser (33) are acceptable formats. Only valid matches are considered for further analysis. A valid match is the top or best match whose subject accession number is not in the exclude list and has an E-value less than or equal to the specified cut-off (default is 10−6). The results of phage-specific HMM searches are then processed. Protein sequences are searched with HMMSEARCH against 441 total models [295 glocal-mode (built with hmmls mode) and 146 fragment-mode (built with hmmfs mode)]. For an explanation of the glocal/hmmls and fragment/hmmfs alignment modes, please refer to the HMMER User's Guide (http://hmmer.wustl.edu/). Eight Pfam fragment-mode models were removed due to frequent matches to non-phage proteins (PF05442, PF07352, PF05012, PF06992, PF06806, PF07068, PF05037 and PF07230). A valid HMM match is recorded if the total score is greater than the noise cut-off (for glocal-mode) or is greater than the trusted cut-off for the fragment-mode). Unfortunately, due to a lack of sequence diversity that was included in the HMM seed for several glocal-mode model Pfams, several reasonable matches were not used by Phage_Finder.pl because the total score was between the noise and trusted cut-offs. There were five Pfam glocal-mode models (PF00589, PF02316, PF02914, PF06074 and PF07030) where the noise cut-off had to be set to a lower value within the Phage_Finder.pl program to increase the number of valid matches to these models. This was not the case for the fragment-mode models, where only scores above the trusted cut-off appear reliable. Output Phage_Finder will generate at least eight different filetypes as output if a phage-like region is identified. These include the following: (i) a log file that gives useful information about how Phage_Finder processed the data and a summary of the findings; (ii) a file that can be imported into XGRAPH (http://www.xgraph.org/), which plots the number of phage matches to the database per window and step size; (iii) a tab-delimited report file that shows (coordinate incremented by the step size, # hits per window, and the feat_name or locus name of the hits); (iv) a file containing the 5′ end of each gene, tRNA or att site within each region, the name of the feature and the annotation/database match/HMM match as well as the G + C% content of each region, a best guess for the type of region and the coordinates of each region with or without att site adjustments. There are three different names for this file, depending on the size of the regions (1–10 000, 10 001–18 000 and >18 001 bp); (v) a tab-delimited file containing (contig_id, size of the genome, G + C% content of the genome, 5′ end of the phage region, 3′ end of the phage region, size of region in bp, label (small, medium, large), region type (prophage, integrated element, degenerate), sequence of attR, sequence of attL, name of integration target, G + C% of region, 5′ feat_name or locus name, 3′ feat_name or locus name, # integrase HMM hits, # core_HMM hits, # above noise core_HMM hits, # lytic gene HMM hits, # tail HMM hits, # Mu HMM hits, orientation of the prophage based on orientation of the target or the position of the integrase, the distance from att site to integrase, and the number of genes in the region; (vi) a file in FASTA format containing the DNA sequence of the phage region; (vii) a file in FASTA format containing the DNA sequence of each gene within the phage region; and (viii) a file in FASTA format containing the protein sequence of each gene within the phage region. Calculation of distance from BSR The BSR has been used to compare three genomes at a time (34). This approach can be expanded to compare n number of genomes by computing the average of BSRs between each genome. For the purpose of tree building, a Phylip-style distance matrix was required (35,36). BLASTP was used to identify bidirectional matching protein sequences as described previously (37). The BSR was calculated on all protein matches that met the prerequisites as described previously (37). Those proteins that failed to meet the prerequisites were given a BSR value of zero. Because the Phylip-style distance matrix uses a different numerical scale than the BSR, a simple calculation (Dab = −99.999999 × BSR +99.999999) was used to convert the BSR (1 = exact match, 0 = no match) into a phylip distance (0 = exact match, 99.999999 = no match). To ensure that Dab equals Dba for all the proteins between two genomes (a and b), the following calculation was used to compute the mean of the distance between genomes a and b: D = (∑ Dab  i +∑ Dba  i )/N⁠, where N is the total number of proteins in the subject and query genomes a and b. This number, D, for every genome pairwise combination was formatted as a Phylip-style distance matrix file that was used as input for the Phylip NEIGHBOR program.",VirusDetection,"phage_finder automate identification  classification  prophage regions  complete bacterial genome sequences
system  software requirements phage_finder  write  perl   test use perl version   linux  mac    operate systems   work   unix environments   follow helper program  instal  functional ncbi blastall   wublast     blast search hmmsearch   find hmm match trnascanse   find  location  trnas aragorn   locate tmrna sequence  fasta33  mummer   blastn   find att sit phage_finderpl utilize  mathround perl module   write  geoffrey rommel  round number  define multiples   freely available ~grommelmathroundroundpm input requirements  run  include phage_findersh bash script  input requirements   follow name  file contain  protein sequence   bacterial genome   search  fasta format name  blastformatted phage protein sequence  name   file contain  dna sequence   entire bacterial genome   search   cod sequence   phage_finder information file tabdelimited contig_id size_of_contig feat_name end5 end3 annotation   genbank ptt file  invoke phage_finderpl directly   follow tabdelimited file  require  full functionality ncbi   blastp data hmmsearch data trnascanse data aragorn data  either  phage_finder information file  genbank ptt file   hmm data   provide   search  att sit    perform   data  trnascansearagorn   provide   phage_finderpl   associate  putative targetsite duplications  trnatmrna genes identification  prophage regions one   original intentions  phage_finder     program   distinguish  largely intact possibly functional prophages versus small regions  cluster  prophage remnants   mobile elements  take advantage  several feature  functional prophages  filter  unwanted fragment regions since functional temperate phages integrate  linear molecules   size range    good candidate prophage regions   cluster  phagelike genes   size range functional phages also   large fraction  hypothetical  conserve hypothetical proteins  stretch  phagelike  unknown genes  consecutive  break   operons  housekeep genes although  occasional metabolic enzyme   encode   phage tail phages    conserve late gene operon   responsible  package  head morphogenesis   conserve region include  small  large terminase subunit  recognize pac  cos sit   cleave phage genome concatemers  package   phage genome  capsids   portal protein  form  hole  passage   phage genome  package  release   prohead protease  generate mature capsids    major capsid protein  form  bacteriophage capsid  head   functional prophage region  also lack ribosomal rna sequence  boundaries  functional prophages  integrate  specific locations   determine  locate  sitespecific recombinase  one   end   phage region phages  integrate  trnatmrna genes  conserve genes  intragenic regions since many phages  genetic elements tend    affinity  trna genes   target  integration  trnatmrna gene present within  putative phage region  check first   target  integration  integrase  integrate   anticodonloop  tloop   ′ end   trnatmrna gene   phage genome  contain sequence near  integrase gene   homologous   ′ part   target  avoid inactivate  gene  insertion  follow integration  target gene    fusion   ′ end   bacterial origin   ′ end  phage origin   original bacterialderived ′ end    side   insert phage genome  search   end   putative phage region   sequence   trnatmrna gene include extra sequence  case  miscalculation   boundaries one  identify  look like  targetsite duplication  sequence   replace ′ end   trnatmrna gene  homologous sequence flank  genome   integrate phage genome  refer    core attachment site attcore   two half sit  attl  phagederived sequence  attr  original bacterial sequence sequence   ′   trnatmrna gene  also  part   attcore  phage_finder overview  first analysis  phage_finder    count  number  valid blastp phage match within  userdefined window size slide   userdefined step size   size   genome  reach   determine   window size       step size     optimal settings  define cluster  phage hit   least amount  noise  set   default window  step size settings  center   prophage region   define   window   greatest number  phage database match   region  begin   window   least  userdefined number  hit per window default  four hit per window   least one region  find within  minimum number  hit per window   ′  ′ boundaries   region  roughly determine begin   previously define center   region phage_finderpl slide gene  gene toward  ′  ′ end   region make  decision  inclusion  decision  include  gene within  particular phage region  make   follow order    protein   phage hmm hit  include      phage database blastp valid match  include iii   gene   trna  tmrna  include   either   next three genes  trnas  tmrnas  include    gene  annotation    observe  know phages     least three valid blastp phage database match   current window  include   annotation  describe     gene  annotation    observe  know phages     least two valid blastp phage database match   next window  include vii     least three valid blastp phage database match   next window  include lastly viii     least two valid blastp match   current step   current gene    match gene  include every gene    gene   database match   putative prophage region contain valid hmm match  xercd tigr02224  tigr02225  integron tigr02249 integrases   regions would  exclude   consideration   define  boundaries   putative prophage region  program attempt  define whether  regions  type prophage  type bacteriocin  whether regions    classify  mulike retron phage r73like p2like  p4like degenerate regions  determine  analysis  putative attachment sit  region  define  type prophage      core hmm match      lytic hmm match   tail hmm match   integrase hmm match   region   lytic hmm match   tail hmm match   integrase hmm match   region  define  type bacteriocin   analogous   phagelike bacteriocins pyocins  paeruginosa  monocins  lmonocytogenes   rtype  ftype pyocins  probably  best study  define phenotypically  genetically   pyocins encode headless phage tail regulatory proteins  lysis proteins   production  release  phage tail   specific  closely relate pseudomonads result  destruction   target cell  membrane disruption   region  sub classify  mulike    proteins within  region  match  follow muspecific hmms pf02316  dnabinding domain pf02914 bacteriophage  transposase pf06074 label  protein  unknown function duf935  match   portal gp29  pf07030 phage conserve hypothetical protein duf1320 match  gp36 pf06074  pf07030  determine   muspecific  search  phage database   model  via map  singlelinkage cluster   phage database search    blastp data  show  mulike phages  hit   hmms  combination  specific hmm match  region length  use  distinguish   retron phage r73     retron phage r73   p4like cryptic prophage   clinical escherichia coli isolate contain  retroelement  bacteriophage    satellite phage   use  head  tail genes  coliphage   package inself  infectious viral particles   three   phages   identify   hmm hit  pf04606 phage transcriptional activator ogrdelta  tigr01613 phageplasmid primase  family cterminal domain   integrase match  region  classify  retron r73like     match  pf00078 reverse transcriptase rnadependent dna polymerase   length   region        consider p2like  p4like   size     provide   file contain  dna sequence   genome  hmm  trnatmrna data  provide   region   mulike  search  putative phage attachment sit  conduct  user  specify blastn fasta33  mummer    nucleotide similarity search blastn  fasta33   advantage   imperfect direct repeat   identify  mummer  look  exact match blastn   default nucleotide similarity search tool   appear    better job  find  significant match   top two match  process  trnatmrna genes  within  putative prophage region  sequence   inmost trna gene  use   query  search  remain    region plus   additional nucleotides   similar direct repeat   direct repeat  identify  phage_finder attempt  extend  region  homology  search   trnatmrna sequence plus  additional nucleotides   ′ end   trnatmrna gene   homologies  find  program  identify  outermost integrase gene  sequence begin   outside  outermost end   integrase gene  extend outside   phage region     use   query  search  remain    region plus   additional nucleotides   similar direct repeat   putative att site  identify  coordinate  use  identify  putative target gene analysis   distance  integrase  att sit  know phages  conduct  determine     sufficient sequence  include  att site  coordinate  trnatmrna genes  use  determine whether  putative att site target  trnatmrna gene  case  incorrect trna  choose   series  multiple trnas  final step   format  data  print  summary   terminal  multiple output file  disk     contigsassemblies  analyze  program begin  find cluster  phage database blastp match   next contig   repeat      contigs  search input phage_finder begin  check   two require data file ncbi  wublast blastp tabdelimited data   query bacterial genome protein sequence  search   blastformatted database  bacteriophage protein sequence   phage_finder information file  genbank ptt file describe    file  give  present   data   phage_finder information file  genbank ptt file  process otherwise  program terminate   help menu  flowchart  phage_finder logic  present  figure   provide data  trnascanse  aragorn  process  store  future use  file  change infrequently  store   phage_finder home directory   hardcoded   program  list  satisfactory gene annotations  read   file   program  differentiate  housekeep genes   genes    previously associate  functional phages    associate  phages  hypothetical proteins conserve proteins  regulatory proteins  list   annotation  generate  parse  annotation associate  every phage sequence   phage database use  blast search remove space  make nonredundant  also allow  genes   actually phagederived   match anything   current limit phage database   include   phage region  orthologs  every phage protein  already   database  step would   necessary  orthologs   proteins   find   genomes  functional phages  also present  bacterial genomes  nonphage regions match   proteins   specific  phage regions  must  exclude  analysis examples  proteins  fall   category  transposases nonmulike abc transporters ribonucleotide reductase  certain  enzymes commonly encode   genomes  lytic phages  accession number   proteins  store   phage_excludelst file   use  phage_finderpl  exclude  protein database match  analysis  list  core phage hmms  well  list  hmms   specific  phage lysis genes  tail proteins  read   separate file table  match   hmms  well   integrase hmms pf00239 pf00589  pf02899  use  characterize  putative prophage regions  either prophage  bacteriocin match   specific phage hmms   length   region  use  distinguish  mulike regions retron phage r73    tabdelimited data  blastp search   database  bacteriophage protein sequence  read   process data  ncbi blastp option    wublastp    process   btab blast output parser   acceptable format  valid match  consider   analysis  valid match   top  best match whose subject accession number     exclude list    evalue less   equal   specify cutoff default    result  phagespecific hmm search   process protein sequence  search  hmmsearch   total model  glocalmode build  hmmls mode   fragmentmode build  hmmfs mode   explanation   glocalhmmls  fragmenthmmfs alignment modes please refer   hmmer user' guide  eight pfam fragmentmode model  remove due  frequent match  nonphage proteins pf05442 pf07352 pf05012 pf06992 pf06806 pf07068 pf05037  pf07230  valid hmm match  record   total score  greater   noise cutoff  glocalmode   greater   trust cutoff   fragmentmode unfortunately due   lack  sequence diversity   include   hmm seed  several glocalmode model pfams several reasonable match   use  phage_finderpl   total score    noise  trust cutoffs   five pfam glocalmode model pf00589 pf02316 pf02914 pf06074  pf07030   noise cutoff    set   lower value within  phage_finderpl program  increase  number  valid match   model     case   fragmentmode model   score   trust cutoff appear reliable output phage_finder  generate  least eight different filetypes  output   phagelike region  identify  include  follow   log file  give useful information   phage_finder process  data   summary   find   file    import  xgraph   plot  number  phage match   database per window  step size iii  tabdelimited report file  show coordinate incremented   step size # hit per window   feat_name  locus name   hit   file contain  ′ end   gene trna  att site within  region  name   feature   annotationdatabase matchhmm match  well      content   region  best guess   type  region   coordinate   region   without att site adjustments   three different name   file depend   size   regions            tabdelimited file contain contig_id size   genome    content   genome ′ end   phage region ′ end   phage region size  region   label small medium large region type prophage integrate element degenerate sequence  attr sequence  attl name  integration target     region ′ feat_name  locus name ′ feat_name  locus name # integrase hmm hit # core_hmm hit #  noise core_hmm hit # lytic gene hmm hit # tail hmm hit #  hmm hit orientation   prophage base  orientation   target   position   integrase  distance  att site  integrase   number  genes   region   file  fasta format contain  dna sequence   phage region vii  file  fasta format contain  dna sequence   gene within  phage region  viii  file  fasta format contain  protein sequence   gene within  phage region calculation  distance  bsr  bsr   use  compare three genomes   time   approach   expand  compare  number  genomes  compute  average  bsrs   genome   purpose  tree build  phylipstyle distance matrix  require  blastp  use  identify bidirectional match protein sequence  describe previously   bsr  calculate   protein match  meet  prerequisites  describe previously   proteins  fail  meet  prerequisites  give  bsr value  zero   phylipstyle distance matrix use  different numerical scale   bsr  simple calculation dab    bsr   use  convert  bsr   exact match    match   phylip distance   exact match    match  ensure  dab equal dba    proteins  two genomes     follow calculation  use  compute  mean   distance  genomes      ∑ dab   ∑ dba   ⁠     total number  proteins   subject  query genomes     number   every genome pairwise combination  format   phylipstyle distance matrix file   use  input   phylip neighbor program",2
56,VERSE,"VERSE: a novel approach to detect virus integration in host genomes through reference genome customization
Next generation sequencing data We used WGS of 13 hepatocellular carcinomas (HCCs), RNA-seq of 4 HCC cell lines, and TS of 2 Merkel cell carcinomas to evaluate VERSE (Table 1). All these samples are publicly available and were validated to harbor virus integration sites. Paired-end WGS (2 × 90 bp) of the 13 HCCs was performed on an Illumina HiSeq 2000 sequencer as described in [7]. Average coverage of these samples ranged from 31.7× to 121.2× (Table S1 in Additional file 1). The HBV integration sites identified in these samples were validated using PCR and Sanger resequencing [7]. In total, 22 integration events were validated in these tumor samples. Several samples harbored virus integration sites that were very close to each other. For example, the two HBV insertion sites in sample 145 T, chr19: 30303492 and chr19: 30303498, were only 6 bp away (Table S1 in Additional file 1). The discrimination of virus integration sites within this short distance is quite beyond the capability of current detection tools. Because VERSE applies a 10 bp cutoff to filter out low-confidence detections, we regarded two virus insertion loci as one if the distance between them was less than 10 bp. This gave us a final set of 20 virus integration sites for these samples. Whole transcriptomes of the four HCC cell lines were subjected to sequencing library preparation using an Illumina TruSeq RNA Sample Preparation Kit as reported in the original publication [11]. Sequencing was performed on an Illumina HiSeq 2000 platform, generating paired-end reads of length 101 bp with an average insertion size of 300 bp (Table S2 in Additional file 1). On average, 127 million reads were obtained per sample. Eleven chimeric HBV-human transcripts were detected in these samples using ViralFusionSeq [27] and validated using Sanger resequencing. For the two Merkel cell carcinomas, virus genomes were captured from formalin-fixed, paraffin-embedded tissues and enriched using PCR-generated capture probes [31]. Targeted paired-end sequencing was performed on an Illumina GAIIx platform (2 × 100 bp). In total, 3.9 and 5.0 million reads were produced for the two samples, respectively. The viral integration sites in the tumor genomes were detected using both BreakDancer [32] and SLOPE [33]. To validate the identified virus insertion events, primers were designed using Vector NTI suite (Invitrogen). For a detailed validation protocol, interested readers are referred to [31]. Besides the data from the real tumors and cancer cell lines, we also simulated WGS of human chromosome 1 using the profile-based Illumina pair-end Read Simulator (pIRS) [34]. We plugged a mutated copy of the HPV-16 virus reference genome (GI:310698439) into chromosome 1 of UCSC hg19 to create a new reference with which to run the command 'simulate' in pIRS to generate paired-end sequencing reads (insert size: 200 bp; read length: 2 × 75 bp; average coverage: 30×). Additionally, to mimic real data, we inserted single nucleotide polymorphisms (SNPs), small insertions and deletions (indels), and structural variants (SVs) into the data. We let the frequency of SNPs be 10 times higher than that of indels and the frequency of SVs 10 times less than that of indels. The simulation data are freely available at [30]. VERSE pipeline Figure 1 illustrates the VERSE pipeline, which overall follows a four-step procedure: (a) read subtraction, (b) virus genome customization, (c) host genome customization, and (d) virus integration site detection. Read subtraction The purpose of the read subtraction step is to collect viral reads, that is, the reads characteristic of the viruses infecting a host. In this step, VERSE uses the alignment tool Bowtie 2 [35] to map raw sequencing reads to the reference genome of the host species under study. Bowtie 2 is run in its sensitive end-to-end mode in order to achieve high alignment speed. Read pairs with one or both ends unmapped to the host genome are garnered. These reads are called viral reads for simplicity, although not all of them are related to the viruses. VERSE exploits primarily the viral reads in order to detect virus integrations. Virus genome customization In this step, VERSE utilizes ICORN [36], a tool for correcting errors in small genomes, to customize virus reference genomes. Specifically, VERSE runs ICORN to map viral reads to the virus reference genomes and to identify SNPs and indels from the mapped reads. Only high-quality consensus SNPs and indels are used to modify the virus reference genomes. ICORN compares the coverage of the mapped reads at each base before and after the modifications. The corrections that reduced the coverage are rejected. The whole process, from read alignment and variant detection to base correction, is run iteratively. Typically, six iterations suffice to correct majority errors in a small reference genome [36]. Host genome customization Customization of large genomes is time-consuming. For instance, to use short reads to modify the UCSC hg19, it can take ICORN over a week to complete at the Vanderbilt Advanced Computing Center for Research and Education (ACCRE) [37]. To speed up VERSE, we extract genomic regions from host genomes that are likely to harbor virus integration sites. We run ICORN only on the identified regions. Specifically, we first combine the reference genome of the host species under study with the consensus virus genome created in the previous step (designated as a separate pseudo-chromosome, chrVirus). Next, we use BWA [38] to align the viral reads to the resulting new reference. With the alignment file created, VERSE runs SVDetect [39], a software tool that uses anomalously mapped read pairs to localize genomic rearrangements, to call inter-chromosomal SVs that involve both the host genome and chrVirus. The host genomic regions that potentially harbor virus integration sites are then derived from the mapped positions of the reads of the SVs (Figure 1c(ii)). These genomic regions are typically <10,000 bp in length, significantly smaller than the host genome. Next, VERSE designates each region characterized from the host genome as a separate pseudo-chromosome. After concatenating them together, VERSE recruits reads mapped to these regions from the Bowtie 2-aligned file created in step (a). Then, following the same procedure as step (b), VERSE runs ICORN to iteratively align reads to these pseudo-chromosomes, call SNPs and indels from read alignment, and then correct the reference with the called SNPs and indels. The final outputs of this step are therefore customized host genomic regions, each of which corresponds to a potential virus integration event. By modifying only these small regions, we are able to reduce the computation time from over a week on a large reference genome to a few hours. Virus integration detection In this step, VERSE concatenates the host genomic regions recruited in the previous step with the consensus virus genome to create an analysis-ready reference genome. VERSE runs BWA to map the viral reads to this new reference and then utilizes CREST [40] to detect inter-chromosomal SVs. CREST is an algorithm that exploits soft-clipped reads, the reads with partial alignments to the reference genomes, for SV identification. The breakpoints of the SVs that involve both the virus and host genomes, if there are any, are then reported as virus integration sites. Result classification and filtering As demonstrated above, VERSE combines two complementary tools, SVDetect [39] and CREST [40], to customize reference genomes and detect virus integration sites. SVDetect uses spanning reads, that is, paired-end reads with one end mapped to the host genome and another aligned to the virus genome, to characterize virus integration loci. It is fast but not able to discern integration breakpoints accurately. In contrast to SVDetect, CREST utilizes soft-clipped reads, which are potentially split reads that harbor virus integration breakpoints within themselves. CREST is prone to miss true-positive loci due to the difficulty to map split reads. But it is able to determine virus integration sites at single-base resolution. By combining SVDetect and CREST, VERSE balances computational efficiency and detection accuracy. To measure the confidence of a predicted position relative to the real virus integration site, and based on the output of CREST, VERSE categorizes a prediction into one of two classes: (a) high confidence - if there are sufficient soft-clipped reads to support an integration locus so that CREST is able to detect it; and (b) low confidence - CREST fails to detect it for the lack of high-quality soft-clipped reads. For a high-confidence prediction, CREST’s output is used directly as a putative virus integration site. For a low-confidence one, however, VERSE predicts its position based on the soft-clipped reads that cover it. In particular, VERSE derives the boundaries of the region that potentially harbor the integration site from the output of SVDetect. Next, VERSE sorts the loci within the boundaries in the descending order of the number of soft-clipped reads that are aligned to them. The one covered with the most soft-clipped reads is then used as an estimate of the real integration locus. To discard the possible false-positives, VERSE requires the distance between two adjacent low-confidence virus integration sites to be at least 10 bp. The drawback of the use of this stringent cutoff is that VERSE could mistakenly discard a real integration event if it is within 10 bp of another. Input of VERSE The input of VERSE includes NGS reads (in FASTQ format) sequenced from a host, a reference host genome (in FASTA format), and a reference virus genome (FASTA format). The entire pipeline of VERSE, from the initial read subtraction step to virus integration detection to result classification and filtering, is fully automated. The output of each step is used automatically as the input for the next step of the pipeline. VERSE provides an argument sensitivity_level to allow users to designate the number of iterations of reference genome customization. Because the majority of errors in a reference gnome can be corrected using one or two rounds of ICORN iteration, when evaluating VERSE on the human tumors and cell lines in the section below, we let sensitivity_level = 1 for simplicity and to save time. We encourage users to tune its value in their applications so as to adjust VERSE’s detection sensitivity. Another input argument of VERSE is flank_region_size, which defines the size of the flanking regions upstream and downstream of a genomic region under study. In our experiments presented in the section below, flank_region_size was set to the default value of 4,000. This means VERSE will search both the upstream 4,000 bp and downstream 4,000 bp regions flanking a genomic segment predicted by SVDetect to harbor a candidate virus integration site. By allowing VERSE to examine the flanking regions, we reduce the chance to miss virus insertion sites therein. As mentioned above, the source code of VERSE is publicly available, through the open source software package VirusFinder 2 [30]. As the core module of VirusFinder 2, VERSE is utilized by VirusFinder 2 to characterize virus integration loci (in its sensitive detection mode; see user’s manual at",VirusDetection,"verse  novel approach  detect virus integration  host genomes  reference genome customization
next generation sequence data  use wgs   hepatocellular carcinomas hccs rnaseq   hcc cell line     merkel cell carcinomas  evaluate verse table    sample  publicly available   validate  harbor virus integration sit pairedend wgs        hccs  perform   illumina hiseq  sequencer  describe   average coverage   sample range     table   additional file   hbv integration sit identify   sample  validate use pcr  sanger resequencing   total  integration events  validate   tumor sample several sample harbor virus integration sit    close     example  two hbv insertion sit  sample   chr19   chr19      away table   additional file   discrimination  virus integration sit within  short distance  quite beyond  capability  current detection tool  verse apply    cutoff  filter  lowconfidence detections  regard two virus insertion loci  one   distance    less     give   final set   virus integration sit   sample whole transcriptomes   four hcc cell line  subject  sequence library preparation use  illumina truseq rna sample preparation kit  report   original publication  sequence  perform   illumina hiseq  platform generate pairedend read  length     average insertion size    table   additional file   average  million read  obtain per sample eleven chimeric hbvhuman transcripts  detect   sample use viralfusionseq   validate use sanger resequencing   two merkel cell carcinomas virus genomes  capture  formalinfixed paraffinembedded tissue  enrich use pcrgenerated capture probe  target pairedend sequence  perform   illumina gaiix platform      total    million read  produce   two sample respectively  viral integration sit   tumor genomes  detect use  breakdancer   slope   validate  identify virus insertion events primers  design use vector nti suite invitrogen   detail validation protocol interest readers  refer   besides  data   real tumors  cancer cell line  also simulate wgs  human chromosome  use  profilebased illumina pairend read simulator pirs   plug  mutate copy   hpv virus reference genome   chromosome   ucsc hg19  create  new reference    run  command imulate'  pirs  generate pairedend sequence read insert size   read length     average coverage  additionally  mimic real data  insert single nucleotide polymorphisms snps small insertions  deletions indels  structural variants svs   data  let  frequency  snps   time higher    indels   frequency  svs  time less    indels  simulation data  freely available   verse pipeline figure  illustrate  verse pipeline  overall follow  fourstep procedure  read subtraction  virus genome customization  host genome customization   virus integration site detection read subtraction  purpose   read subtraction step   collect viral read    read characteristic   viruses infect  host   step verse use  alignment tool bowtie    map raw sequence read   reference genome   host species  study bowtie   run   sensitive endtoend mode  order  achieve high alignment speed read pair  one   end unmapped   host genome  garner  read  call viral read  simplicity although      relate   viruses verse exploit primarily  viral read  order  detect virus integrations virus genome customization   step verse utilize icorn   tool  correct errors  small genomes  customize virus reference genomes specifically verse run icorn  map viral read   virus reference genomes   identify snps  indels   map read  highquality consensus snps  indels  use  modify  virus reference genomes icorn compare  coverage   map read   base     modifications  corrections  reduce  coverage  reject  whole process  read alignment  variant detection  base correction  run iteratively typically six iterations suffice  correct majority errors   small reference genome  host genome customization customization  large genomes  timeconsuming  instance  use short read  modify  ucsc hg19   take icorn   week  complete   vanderbilt advance compute center  research  education accre   speed  verse  extract genomic regions  host genomes   likely  harbor virus integration sit  run icorn    identify regions specifically  first combine  reference genome   host species  study   consensus virus genome create   previous step designate   separate pseudochromosome chrvirus next  use bwa   align  viral read   result new reference   alignment file create verse run svdetect   software tool  use anomalously map read pair  localize genomic rearrangements  call interchromosomal svs  involve   host genome  chrvirus  host genomic regions  potentially harbor virus integration sit   derive   map position   read   svs figure 1cii  genomic regions  typically    length significantly smaller   host genome next verse designate  region characterize   host genome   separate pseudochromosome  concatenate  together verse recruit read map   regions   bowtie align file create  step   follow   procedure  step  verse run icorn  iteratively align read   pseudochromosomes call snps  indels  read alignment   correct  reference   call snps  indels  final output   step  therefore customize host genomic regions    correspond   potential virus integration event  modify   small regions   able  reduce  computation time    week   large reference genome    hours virus integration detection   step verse concatenate  host genomic regions recruit   previous step   consensus virus genome  create  analysisready reference genome verse run bwa  map  viral read   new reference   utilize crest   detect interchromosomal svs crest   algorithm  exploit softclipped read  read  partial alignments   reference genomes   identification  breakpoints   svs  involve   virus  host genomes       report  virus integration sit result classification  filter  demonstrate  verse combine two complementary tool svdetect   crest   customize reference genomes  detect virus integration sit svdetect use span read   pairedend read  one end map   host genome  another align   virus genome  characterize virus integration loci   fast   able  discern integration breakpoints accurately  contrast  svdetect crest utilize softclipped read   potentially split read  harbor virus integration breakpoints within  crest  prone  miss truepositive loci due   difficulty  map split read    able  determine virus integration sit  singlebase resolution  combine svdetect  crest verse balance computational efficiency  detection accuracy  measure  confidence   predict position relative   real virus integration site  base   output  crest verse categorize  prediction  one  two class  high confidence     sufficient softclipped read  support  integration locus   crest  able  detect    low confidence  crest fail  detect    lack  highquality softclipped read   highconfidence prediction crest output  use directly   putative virus integration site   lowconfidence one however verse predict  position base   softclipped read  cover   particular verse derive  boundaries   region  potentially harbor  integration site   output  svdetect next verse sort  loci within  boundaries   descend order   number  softclipped read   align    one cover    softclipped read   use   estimate   real integration locus  discard  possible falsepositives verse require  distance  two adjacent lowconfidence virus integration sit    least    drawback   use   stringent cutoff   verse could mistakenly discard  real integration event    within    another input  verse  input  verse include ngs read  fastq format sequence   host  reference host genome  fasta format   reference virus genome fasta format  entire pipeline  verse   initial read subtraction step  virus integration detection  result classification  filter  fully automate  output   step  use automatically   input   next step   pipeline verse provide  argument sensitivity_level  allow users  designate  number  iterations  reference genome customization   majority  errors   reference gnome   correct use one  two round  icorn iteration  evaluate verse   human tumors  cell line   section   let sensitivity_level    simplicity   save time  encourage users  tune  value   applications    adjust verse detection sensitivity another input argument  verse  flank_region_size  define  size   flank regions upstream  downstream   genomic region  study   experiment present   section  flank_region_size  set   default value    mean verse  search   upstream    downstream   regions flank  genomic segment predict  svdetect  harbor  candidate virus integration site  allow verse  examine  flank regions  reduce  chance  miss virus insertion sit therein  mention   source code  verse  publicly available   open source software package virusfinder     core module  virusfinder  verse  utilize  virusfinder   characterize virus integration loci   sensitive detection mode see users manual ",2
57,Pathosphere,"Pathosphere.org: pathogen detection and characterization through a web-based, open source informatics platform
Pathogen isolate sample preparation Isolates sample preparation Samples 712 and 808 containing LuJo virus were prepared from human isolates [10]. RNA was extracted from the cerebrospinal fluid and serum of a liver transplant recipient. After digestion with DNase I to eliminate human chromosomal DNA, RNA preparations were amplified by means of reverse-transcriptase PCR (RT-PCR) with the use of random primers [11, 12]. Amplification products were pooled and sequenced with the use of the 454 Genome Sequencer FLX platform (Roche, Branford, CT), but DNA fragmentation was omitted. The Zaria bat coronavirus samples 819 and 820 (and the negative control 806) were obtained from the GI tract of bats that tested positive (and negative for the control) for coronavirus by PCR [13]. Sample 28 containing GBV-D was obtained from bat serum [14] and prepared as detailed previously. The isolated RNA for both coronavirus and GBV-D samples was converted to cDNA and the library was prepared similarly to the LuJo virus isolates detailed above. The bat parvovirus sample, 1164, was obtained from the spleen of parvovirus PCR-positive bats (like those discovered in [15, 16]), and DNA was isolated and the prepared libraries were sequenced on the 454 FLX (Roche, Branford, CT). Samples containing MERS-CoV (1500, 1501) [17] were prepared as previously described [18]. Viral cDNA was made using random primer RT-PCR from nasal swabs of camels. Further PCR amplifications were made using overlapping PCR primers spanning 2.0–2.5 kb fragments of MERS-CoV [19]. These amplification products were pooled and sequenced on the Ion Torrent PGM platform. The human serum spiked samples containing Y. pestis, F. tularensis, and B. anthracis, ​B. mallei, and B. psuedomallei were prepared for sequencing as described previously [20, 21] and sequenced on 454 FLX (Roche, Branford, CT), Ion Torrent PGM (Life Technologies, Grand Island, NY), and Illumina MiSeq platforms (Illumina, San Diego, CA). SRA information for each sample analyzed here are available through the NBCI BioProject # PRJNA276557. ECBC pipeline The pipeline described below was designed to integrate a wide range of analytical tools into a single automated process NGS data is first run through quality control trimming using standard metrics as the default but allowing for user trimming flexibility. Two preprocessing tools are currently available; Columbia University’s Preprocessing Procedure (CUPP) and a taxonomic analysis based on NCBI taxonomy results. CUPP was developed to reduce the complexity and total size of a NGS dataset. In this procedure, all the reads in the sample are compared using bowtie2 [22] to map reads against the CUPP database and then remove host reads from the analysis. The host databases for CUPP include Anopheles gambiae (mosquito), Danio rerio (zebra fish), Gallus gallus (chicken), Homo sapiens rRNA (human), Homo sapiens chromosome (human), Mus_musculus (rodent), Sus scrofa (pig), mitochondrion genome, and Xenopus laevis (frog) . The taxonomy analysis provides a lowest common ancestor for each read, thus providing a general description of bacterial, viral, and eukaryotic constituents in the sample. These procedures, CUPP and taxonomy analysis, can be used individually or serially as part of an analysis request (Fig. 1). These tools, and the code used to implement them into the analytical pipeline, are available as open-source software at (http://sourceforge.net/projects/pathosphere/?source=directory). The iterative analysis is designed to identify pathogens without assumptions about the sample identity or complexity. To fulfill this goal, a process has been constructed to perform a subtractive approach in searching for possible multiple pathogens or multiple chromosomal elements in a single sample. First, the genomic data uploaded to the system, or reads retained after the pre-processing manipulations, are processed through a de novo assembly. In the case of 454 data, the reads are assembled using the GS Newbler (Roche) program [23]. For Illumina data, the reads are assembled with velvet [24]. The de novo assembly produces longer contiguous lengths (contigs) of genomic sequences. A database search step then compares the contigs with genome sequences in the NCBI nt database to identify high quality matches. Each query (from a de novo assembled contig) results in a series of hits which are ranked by BLAST bit score. The resultant top hit per query is cumulatively ranked using bit score compared to the other top hits. The topmost ranked NCBI database genome sequence in the cumulative ranked list is selected as the nearest neighbor (NN) sequence for the iteration. In the next step, the taxonomical neighbors of this NN in the NCBI nucleotide database are collected according to the following procedure: if NN is ranked as subspecies, or its direct taxonomical parent is ranked as subspecies, all the database records belonging to the same NCBI taxonomic subspecies sub-tree are collected; if the total count of the collected records is less than 20 (default value, can be reset by user), then the species sub-tree the NN belongs to is searched and the additional database records that belong to this sub-tree (and that also appear in the rank hit list) are collected. After the NN's neighbor genomes are collected, all the input reads for this iteration are mapped to each of those genome sequences by reference mapping. In the final step of the first iteration, all the input reads used for de novo assembly are reference mapped to the NN reference, and the unmapped reads are extracted and used as input to the next iteration. For 454 data, the reads are referenced mapped using the GS Newbler (Roche) program [23]. For Illumina data, the reads are reference mapped with the Bowtie2 program [25]. In the next iteration, the steps described above are repeated. The iterative analysis allows multiple chromosomes, plasmids, or inserted genomic elements to be identified and reported to the user for directed, manual analysis. USAMRIID-WRAIR pipeline The USAMRIID-WRAIR pipeline was designed to be modular and thus give it flexibility to integrate new software as it becomes available, replacing older versions for reasons such as speed and sensitivity. Acceptable input formats include SFF, fastq single or paired-end, and compressed gzipped files. Step1 first decompresses the file and/or converts the file into fastq format if an SFF file is the starting input. The converted fastq or paired-end fastqs are processed for host removal using Bowtie2 [25]. The first iteration uses the host genome of choice for read removal followed by the host transcriptome. Once host reads are removed, adaptors are trimmed and reads go through quality filtering using cutadapt [26] and prinseq-lite [27]. Reads are assembled into contigs using the de novo assembler Ray Meta [28], followed by a contig assembly using Cap3 [29] to ensure the longest possible contigs. Identification of contigs and single reads (singletons) is achieved through an iterative BLAST search using the NCBI nt database. Iterative BLAST 1 uses the contigs as the query and starts with a megablast followed by a discontiguous megablast. Only the contigs that do not get identified in the megablast go on to the dc-megablast. Iterative BLAST 2 is essentially the same except that the singletons are used as the input. These BLAST searching schemes ensure that highly homologous sequences (megablast: word size of 28) are matched appropriately, and that less homologous sequences (discontiguous megablast: word size 12) are identified within the dataset. The outputs are divided into contig and read reports. The output reports resemble a top blast output with the addition of reads that aligned to each contig. Taxonomy is assigned using names and nodes files from NCBI. Architecture and web implementation Pathosphere is a practical implementation and reference design for scalable, secure web services for genomics processing. There are two main parts of the Pathosphere system. The first is a cloud-based web interface provided by custom applets running inside of Liferay (http://www.liferay.com/). The second part of the system consists of any number of backend processing computers or clusters. This architecture separates the web interface, user collaboration tools, and result display mechanisms from the systems that actually process the data through pipelines. In this way, the pipeline design, construction, execution, along with any hardware configuration, is completely independent from the server providing the user interface. This allows for unlimited flexibility in the types of pipelines being integrated into the Pathosphere system. The cloud-based front end web server has relatively low system requirements, since this portion of the system only stores data and results, allows submission of jobs, and provides collaboration tools. This design keeps the computationally intensive processing tasks off of this server. Currently, as jobs are submitted, they are processed serially, although a more sophisticated job management system could be implemented. The current Pathosphere front end server resides on a single, mid-level server, but this portion of the system could be easily scaled up on more powerful servers if the user load were to increase in the future. Like the front end web server, the backend servers in the Pathosphere architecture can also exist anywhere in the world with a network connection. These backend servers can range from single machines to large computational clusters, depending on the types of algorithms being processed. The pipelines described in this paper are set up to run on a computing cluster consisting of 14 blade servers, several supporting servers, and over 40 TB of shared storage. Similar to the front end, the backend processing needs are built to be expandable to cloud based services [9] when user load increases. Security features Communication between the client and web server is via https, using TLS v1.0 or higher. The Public Key Infrastructure (PKI) certificate is a StartCom signed RSA 4096 bit key. This ensures secure communication between the client and the webserver. Individual users are authenticated using usernames and passwords. The only information stored about a run is its sample name and title. The user should not enter identifiable patient information in these fields, as the system is not intented to store confidential patient data. Only the data uploaded by a specific user is visible to that user, unless it is explicitly shared with another user. In order to join a community, a user must have permission from the group owner. The web server, mail server, and cluster all have network access restricted by external firewalls that limit access to only the expected network communication. The only access to the backend computing cluster is via a Secure Shell (SSH) connection, with a PKI key, ensuring that the data remains secure in transit. Data is not encrypted while stored on the computing cluster, but the cluster is located in a secure location on a military installation.",VirusDetection,"pathosphereorg pathogen detection  characterization   webbased open source informatics platform
pathogen isolate sample preparation isolate sample preparation sample    contain lujo virus  prepare  human isolate  rna  extract   cerebrospinal fluid  serum   liver transplant recipient  digestion  dnase   eliminate human chromosomal dna rna preparations  amplify  mean  reversetranscriptase pcr rtpcr   use  random primers   amplification products  pool  sequence   use    genome sequencer flx platform roche branford   dna fragmentation  omit  zaria bat coronavirus sample      negative control   obtain    tract  bat  test positive  negative   control  coronavirus  pcr  sample  contain gbvd  obtain  bat serum   prepare  detail previously  isolate rna   coronavirus  gbvd sample  convert  cdna   library  prepare similarly   lujo virus isolate detail   bat parvovirus sample   obtain   spleen  parvovirus pcrpositive bat like  discover     dna  isolate   prepare libraries  sequence    flx roche branford  sample contain merscov     prepare  previously describe  viral cdna  make use random primer rtpcr  nasal swab  camels  pcr amplifications  make use overlap pcr primers span   fragment  merscov   amplification products  pool  sequence   ion torrent pgm platform  human serum spike sample contain  pestis  tularensis   anthracis ​ mallei   psuedomallei  prepare  sequence  describe previously    sequence   flx roche branford  ion torrent pgm life technologies grand island   illumina miseq platforms illumina san diego  sra information   sample analyze   available   nbci bioproject # prjna276557 ecbc pipeline  pipeline describe   design  integrate  wide range  analytical tool   single automate process ngs data  first run  quality control trim use standard metrics   default  allow  user trim flexibility two preprocessing tool  currently available columbia universitys preprocessing procedure cupp   taxonomic analysis base  ncbi taxonomy result cupp  develop  reduce  complexity  total size   ngs dataset   procedure   read   sample  compare use bowtie2   map read   cupp database   remove host read   analysis  host databases  cupp include anopheles gambiae mosquito danio rerio zebra fish gallus gallus chicken homo sapiens rrna human homo sapiens chromosome human mus_musculus rodent sus scrofa pig mitochondrion genome  xenopus laevis frog   taxonomy analysis provide  lowest common ancestor   read thus provide  general description  bacterial viral  eukaryotic constituents   sample  procedures cupp  taxonomy analysis   use individually  serially  part   analysis request fig   tool   code use  implement    analytical pipeline  available  opensource software    iterative analysis  design  identify pathogens without assumptions   sample identity  complexity  fulfill  goal  process   construct  perform  subtractive approach  search  possible multiple pathogens  multiple chromosomal elements   single sample first  genomic data upload   system  read retain   preprocessing manipulations  process    novo assembly   case   data  read  assemble use   newbler roche program   illumina data  read  assemble  velvet    novo assembly produce longer contiguous lengths contigs  genomic sequence  database search step  compare  contigs  genome sequence   ncbi  database  identify high quality match  query    novo assemble contig result   series  hit   rank  blast bite score  resultant top hit per query  cumulatively rank use bite score compare    top hit  topmost rank ncbi database genome sequence   cumulative rank list  select   nearest neighbor  sequence   iteration   next step  taxonomical neighbor      ncbi nucleotide database  collect accord   follow procedure    rank  subspecies   direct taxonomical parent  rank  subspecies   database record belong    ncbi taxonomic subspecies subtree  collect   total count   collect record  less   default value   reset  user   species subtree   belong   search   additional database record  belong   subtree   also appear   rank hit list  collect   ' neighbor genomes  collect   input read   iteration  map     genome sequence  reference map   final step   first iteration   input read use   novo assembly  reference map    reference   unmapped read  extract  use  input   next iteration   data  read  reference map use   newbler roche program   illumina data  read  reference map   bowtie2 program    next iteration  step describe   repeat  iterative analysis allow multiple chromosomes plasmids  insert genomic elements   identify  report   user  direct manual analysis usamriidwrair pipeline  usamriidwrair pipeline  design   modular  thus give  flexibility  integrate new software   become available replace older versions  reason   speed  sensitivity acceptable input format include sff fastq single  pairedend  compress gzipped file step1 first decompress  file andor convert  file  fastq format   sff file   start input  convert fastq  pairedend fastqs  process  host removal use bowtie2   first iteration use  host genome  choice  read removal follow   host transcriptome  host read  remove adaptors  trim  read   quality filter use cutadapt   prinseqlite  read  assemble  contigs use   novo assembler ray meta  follow   contig assembly use cap3   ensure  longest possible contigs identification  contigs  single read singletons  achieve   iterative blast search use  ncbi  database iterative blast  use  contigs   query  start   megablast follow   discontiguous megablast   contigs    get identify   megablast     dcmegablast iterative blast   essentially   except   singletons  use   input  blast search scheme ensure  highly homologous sequence megablast word size    match appropriately   less homologous sequence discontiguous megablast word size   identify within  dataset  output  divide  contig  read report  output report resemble  top blast output   addition  read  align   contig taxonomy  assign use name  nod file  ncbi architecture  web implementation pathosphere   practical implementation  reference design  scalable secure web service  genomics process   two main part   pathosphere system  first   cloudbased web interface provide  custom applets run inside  liferay   second part   system consist   number  backend process computers  cluster  architecture separate  web interface user collaboration tool  result display mechanisms   systems  actually process  data  pipelines   way  pipeline design construction execution along   hardware configuration  completely independent   server provide  user interface  allow  unlimited flexibility   type  pipelines  integrate   pathosphere system  cloudbased front end web server  relatively low system requirements since  portion   system  store data  result allow submission  job  provide collaboration tool  design keep  computationally intensive process task    server currently  job  submit   process serially although   sophisticate job management system could  implement  current pathosphere front end server reside   single midlevel server   portion   system could  easily scale    powerful servers   user load   increase   future like  front end web server  backend servers   pathosphere architecture  also exist anywhere   world   network connection  backend servers  range  single machine  large computational cluster depend   type  algorithms  process  pipelines describe   paper  set   run   compute cluster consist   blade servers several support servers      share storage similar   front end  backend process need  build   expandable  cloud base service   user load increase security feature communication   client  web server  via https use tls   higher  public key infrastructure pki certificate   startcom sign rsa  bite key  ensure secure communication   client   webserver individual users  authenticate use usernames  passwords   information store   run   sample name  title  user   enter identifiable patient information   field   system   intented  store confidential patient data   data upload   specific user  visible   user unless   explicitly share  another user  order  join  community  user must  permission   group owner  web server mail server  cluster   network access restrict  external firewalls  limit access    expect network communication   access   backend compute cluster  via  secure shell ssh connection   pki key ensure   data remain secure  transit data   encrypt  store   compute cluster   cluster  locate   secure location   military installation",2
58,Decontaminer,"DecontaMiner: A Pipeline for the Detection and Analysis of Contaminating Sequences in Human NGS Sequencing Data
The DecontaMiner pipeline is a suite composed of several command-line tools wrapped together to identify, through digital subtraction, non-human nucleotide sequences generated by high-throughput sequencing of RNA or DNA samples. It is mainly written using Bash scripting and the Perl language. It requires in input the BAM files or the raw fastQ files containing the unmapped reads (i.e., all the reads discarded during the alignment on the human reference genome) if any. A schematic view of the pipeline is shown in Fig. 1. All the files that have to be submitted to DecontaMiner can be collected in the same directory, and its path given as input. The entire pipeline can be subdivided into three main phases. The first phase involves the filtering and file format conversion steps, needed to remove low quality reads and to obtain reads in fasta-format files, ready to be aligned to the genome databases. More in detail, DecontaMiner wraps in its pipeline two of the most used toolkits, Samtools [13] and Bedtools [14] used for the format conversions, and FastX [15] for the quality filtering. The filtering is mainly based on two parameters set by the user, namely the Phred quality threshold and the minimum percentage of bases within that threshold. DecontaMiner works both on paired- and single-end experiments, a parameter that must be specified by the user. The conversion steps allow to sort the reads and switch from bam to fastq and then to fasta formats. Once terminated the conversion phase, the mapping module can start. In the case of RNA-seq data, it is crucial to remove the ribosomal RNA (rRNA). Indeed, rRNA represents up to 90 % of the total RNA. Although the wet lab procedures provide an rRNA removal step, often this procedure is not totally satisfactory, due to high number of rRNA copies. We downloaded the fasta sequences of human ribosomal RNA (28S, 18S, 5S, 5.8S and mitochondrial 12S, 16S) from NCBI website. The rRNA alignment is performed using the SortmeRNA tool [16], which is a software designed to this aim. All the reads that do not map to the human rRNA will undergo mapping to bacteria, viruses, and fungi genome databases (NCBI nt) using the MegaBLAST [17] algorithm. The rRNA alignment reliability is evaluated using the E-value score. This threshold can be either set by the user or left at the default SortMeRna value. The user can specify also the alignment length and number of allowed mismatches/gaps when aligning to contaminating genomes. A scheme of the DecontaMiner pipeline. On the right, in blue are the input files, and in red the tools used to process the data. In the central part, as a flux, the processing steps are described. On the left, the parameters that can be set for each step are indicated in green. Several tab-delimited files and one matrix are the pipeline outputs. All the discarded reads are also provided, as well as all the different file formats generated (fastQ, FASTA, etc.). The matrix, containing all the samples, can be easily used to create a bar plot The BLAST outputs, in table format, are then submitted to the third and last phase, that involves the collection and extraction of information from the local alignments. This module, mainly composed of Perl scripts, is executed accordingly to some user-specified parameters specifying the filtering and collecting options. In particular, the filtering is based on the threshold number of total reads successfully mapped and on the minimum threshold of reads mapped to a single organism. Instead, the collecting options involve the choice of organizing the results according either to genus or to species names. DecontaMiner stores the output reads into three main files: unaligned, ambiguous, and aligned. The “unaligned” file contains the reads that do not satisfy the filtering parameters (i.e., length of alignment, number of allowed gaps, and mismatches). The ambiguous reads are those that map to different Genera or, in case of paired-end reads, those having mates mapping to different genera. Ambiguous reads mapping to more than one Genus might derive from ortholog sequences. Since Reads matching all the filtering criteria are stored into the “aligned” file. The results are available in a tabular format, one for each sample, containing the names of the detected organisms and the relative reads count. Furthermore, DecontaMiner generates a matrix that can be easily used to create a barplot or other types of diagrams in which all the data are collected together. Lastly, the summary statistics about the number of matched/filtered/discarded reads and organisms are generated and stored into tabular textual files.",VirusDetection,"decontaminer  pipeline   detection  analysis  contaminate sequence  human ngs sequence data
 decontaminer pipeline   suite compose  several commandline tool wrap together  identify  digital subtraction nonhuman nucleotide sequence generate  highthroughput sequence  rna  dna sample   mainly write use bash script   perl language  require  input  bam file   raw fastq file contain  unmapped read    read discard   alignment   human reference genome    schematic view   pipeline  show  fig    file     submit  decontaminer   collect    directory   path give  input  entire pipeline   subdivide  three main phase  first phase involve  filter  file format conversion step need  remove low quality read   obtain read  fastaformat file ready   align   genome databases   detail decontaminer wrap   pipeline two    use toolkits samtools   bedtools  use   format conversions  fastx    quality filter  filter  mainly base  two parameters set   user namely  phred quality threshold   minimum percentage  base within  threshold decontaminer work   pair  singleend experiment  parameter  must  specify   user  conversion step allow  sort  read  switch  bam  fastq    fasta format  terminate  conversion phase  map module  start   case  rnaseq data   crucial  remove  ribosomal rna rrna indeed rrna represent       total rna although  wet lab procedures provide  rrna removal step often  procedure   totally satisfactory due  high number  rrna copy  download  fasta sequence  human ribosomal rna      mitochondrial    ncbi website  rrna alignment  perform use  sortmerna tool     software design   aim   read    map   human rrna  undergo map  bacteria viruses  fungi genome databases ncbi  use  megablast  algorithm  rrna alignment reliability  evaluate use  evalue score  threshold   either set   user  leave   default sortmerna value  user  specify also  alignment length  number  allow mismatchesgaps  align  contaminate genomes  scheme   decontaminer pipeline   right  blue   input file   red  tool use  process  data   central part   flux  process step  describe   leave  parameters    set   step  indicate  green several tabdelimited file  one matrix   pipeline output   discard read  also provide  well    different file format generate fastq fasta etc  matrix contain   sample   easily use  create  bar plot  blast output  table format   submit   third  last phase  involve  collection  extraction  information   local alignments  module mainly compose  perl script  execute accordingly   userspecified parameters specify  filter  collect options  particular  filter  base   threshold number  total read successfully map    minimum threshold  read map   single organism instead  collect options involve  choice  organize  result accord either  genus   species name decontaminer store  output read  three main file unaligned ambiguous  align  “unaligned” file contain  read    satisfy  filter parameters  length  alignment number  allow gap  mismatch  ambiguous read    map  different genera   case  pairedend read   mat map  different genera ambiguous read map    one genus might derive  ortholog sequence since read match   filter criteria  store   “aligned” file  result  available   tabular format one   sample contain  name   detect organisms   relative read count furthermore decontaminer generate  matrix    easily use  create  barplot   type  diagram     data  collect together lastly  summary statistics   number  matchedfiltereddiscarded read  organisms  generate  store  tabular textual file",2
59,ART-DECO,"ART-DeCo: easy tool for detection and characterization of cross-contamination of DNA samples in diagnostic next-generation sequencing analysis
Library preparation and sequencing Library preparation was performed manually with SureSelect QXT kit on a home-made 384 kb gene panel (Agilent Technologies). The first step of this preparation consisted of dilution of the patients’ gDNA in a 4 × 8 plate (4 columns of standard 96-wells plate). gDNA was then fragmented and adaptor-tagged. The library was purified using Agencourt AMPure XP beads (Beckman Coulter), amplified and re-purified. Samples were hybridized to the capture probes, and then captured using streptavidin-coated beads (Dynabeads MyOne Streptavidin T1, Life Technologies). Libraries were amplified to add barcodes and purified using beads. Libraries were then pooled and sequenced with a NextSeq 500® sequencer (Illumina®). Nomenclature For each SNP, a wild-type homozygous sample is indicated as Ref/Ref, whereas a homozygous sample for the alternative allele is indicated as Alt/Alt. Heterozygous samples are indicated as Ref/Alt. Rationale This strategy is based on the detection of SNPs presenting unexpected AR for constitutional analyses, i.e. distortions from 0 (homozygous wild-type Ref/Ref), 1 (homozygous alternate Alt/Alt) and 0.5 (heterozygous Ref/Alt) (Fig. 1). For each SNP, the AR of heterozygous samples should be 0.5, but it actually fluctuates around this value, e.g. due to differences in mapping quality between reads with or without mismatches. The heterozygous range was defined as [0.25–75]. Note that this [0.25–0.75] range could be restrained and adapted to the heterozygous distribution if this distribution is known. However, it is not needful. Indeed, homozygous SNPs with 50% maximum contamination level result in AR values fluctuating around 0.25 and around 0.75, whereas lower contamination levels result in AR values <0.25 or >0.75 (Supplementary Table 1, Supplementary Fig. 1). Consequently, the [0.25–75] range allows accurate discrimination between poorly called heterozygous SNPs and sample contamination. For each SNP, the AR of homozygous samples is theoretically 0 or 1, but in practice is slightly different because of the background noise generated by polymerase, sequencing and alignment errors, index hopping or incomplete trimming of the adaptors (see below AR extraction) [2–4]. Background noise is generally low in Illumina® sequencing and expected values are usually observed. As the minimum depth for the SNPs under study was set at 200× (see below SNP selection), the background noise was set at 0.5%, in order to tolerate at least one read as default background. The expected AR intervals of homozygous SNPs used were therefore [0–0.005 [and] 0.995–1] for Ref/Ref and Alt/Alt genotypes, respectively. Consequently, unexpected ratios were situated in the [0.005–0.25 [and] 0.75–0.995] ranges (Fig. 1). AR extraction After trimming adaptors by Cutadapt using default parameters [5], reads were aligned via Bowtie2 allowing up to one mismatch in the 22 bp-long seed and reporting only unique alignments [6]. Reads with mapping quality less than 20 were filtered out. Variant calling software was not used, as we wanted to report any frequencies within a focused list of SNPs. The Depth Of Coverage function from the Genome Analysis Toolkit (GATK) was used [7], together with additional statistical analysis detailed below to report ARs of SNPs. To ensure analysis of high-quality data, only base quality ≥20 were considered for determination of the depth of coverage of the selected polymorphisms. SNP selection For each sample, the AR distribution of an SNP selection was computed by an algorithm in order to detect, identify and quantify the contaminant (Supplementary Fig. 2). SNP selection only retained informative polymorphisms with a typical trimodal AR distribution. Among the 628 SNPs, with an European population frequency in the range 0.1–99.9% from the 1000 Genomes database and present in our 60-gene panel, those with recurrent high background noise (e.g. close to homopolymer stretches) were excluded (see “optimization stepˮ section below). Similarly, polymorphisms within paralogous genes were excluded to avoid misinterpretation of AR spoiled by expected misalignment. A total of 547 polymorphisms were then able to be analyzed. Only SNPs with at least 200× coverage were taken into account to allow detection of low contamination. Homozygous SNPs for the same allelic version throughout the samples were non-informative and could not be used for analysis. Detection of contamination: “worst-case scenario” screening test The first step of identification of contamination consisted of a screening test for each sample of the run, based on estimation of the “worst-case scenario” (WCS) percentage of contamination. This screening test is independent of background noise and identifies samples possibly contaminated above a certain cutoff, defined as 1% in the present study. Following the optimization step (see below), the WCS calculation was defined as: WCS = max(r × 2; (1 – a) × 2); with r = median of the highest 2% of ARs of Ref/Ref SNPs and a = median of the lowest 2% of ARs of Alt/Alt SNPs. The main advantage of the WCS test is to rapidly rule out any contamination when it is negative. However, it has a low specificity and a positive WCS test must be confirmed by identification of the contaminant, as the worst scenario is never certain. Identification of the contaminant Contamination was suspected when the WCS percentage of contamination was ≥1%. The second step consisted of identification of the contaminant in order to confirm the contamination. This identification was based on the SNPs of the contaminated sample (i.e. its genotype) compared to the genotypes of the other samples of the run. Only homozygous SNPs of the contaminated sample were used, as heterozygous SNPs exhibited excessive variability of AR values to allow reliable identification of small variations corresponding to low-level contamination. Only SNPs with AR <0.25 or >0.75 were used, corresponding to homozygous SNPs (Ref/Ref or Alt/Alt), including contaminated (<0.25 or >0.75) or non-contaminated SNPs (<0.005 or >0. 995, i.e. background noise). To identify a putative contaminant, the percentage of SNPs compatible with contamination of one sample (A) by another sample (B) was calculated according to the number of homozygous SNPs satisfying the compatibility conditions listed in Table 1. In other words, for homozygous SNPs with expected AR values, the contaminant had to have the same genotype, while for SNPs with unexpected AR values, the contaminant had to have a different genotype. The suspected contaminant was therefore identified by its genotype. For each sample, the other samples from the same run were tested, scored and ranked as putative contaminants and the sample with the highest percentage was considered to be a putative contaminant. To determine whether the putative contaminant actually contaminated the sample under study, two conditions were then required. Firstly, the percentage of SNPs compatible with contamination of sample A by sample B had to be higher than the percentage of SNPs of sample A, compatible with absence of contamination; otherwise sample B could simply be genetically similar to sample A. The percentage of SNPs compatible with absence of contamination was the percentage of SNPs with a normal AR <0.005 or >0.995 among the total number of homozygous SNP (i.e. with AR < 0.25 or AR > 0.75) (Supplementary Table 2). Secondly, the putative contaminant had to be significantly more compatible with the contaminated sample than the other samples of the run (Fisher’s exact test with Bonferroni correction for multi-testing, limit of significance 0.05). Quantification of contamination As the WCS contamination is only a rough, overestimated value, a refined percentage is calculated following identification of the contaminant and according to its genotype. The contamination percentage of a sample by its contaminant is expressed as the median of the values obtained for calculation of contamination rate for each SNP All samples were collected for diagnostic and genetic counselling purposes. Appropriate individual written consent for genetic analysis was obtained from all the participating patients or their legal guardians.",VirusDetection,"artdeco easy tool  detection  characterization  crosscontamination  dna sample  diagnostic nextgeneration sequence analysis
library preparation  sequence library preparation  perform manually  sureselect qxt kit   homemade   gene panel agilent technologies  first step   preparation consist  dilution   patients gdna      plate  columns  standard well plate gdna   fragment  adaptortagged  library  purify use agencourt ampure  bead beckman coulter amplify  repurified sample  hybridize   capture probe   capture use streptavidincoated bead dynabeads myone streptavidin  life technologies libraries  amplify  add barcodes  purify use bead libraries   pool  sequence   nextseq ® sequencer illumina® nomenclature   snp  wildtype homozygous sample  indicate  refref whereas  homozygous sample   alternative allele  indicate  altalt heterozygous sample  indicate  refalt rationale  strategy  base   detection  snps present unexpected   constitutional analyse  distortions   homozygous wildtype refref  homozygous alternate altalt   heterozygous refalt fig    snp    heterozygous sample      actually fluctuate around  value  due  differences  map quality  read   without mismatch  heterozygous range  define   note    range could  restrain  adapt   heterozygous distribution   distribution  know however    needful indeed homozygous snps   maximum contamination level result   value fluctuate around   around  whereas lower contamination level result   value    supplementary table  supplementary fig  consequently   range allow accurate discrimination  poorly call heterozygous snps  sample contamination   snp    homozygous sample  theoretically      practice  slightly different    background noise generate  polymerase sequence  alignment errors index hop  incomplete trim   adaptors see   extraction  background noise  generally low  illumina® sequence  expect value  usually observe   minimum depth   snps  study  set   see  snp selection  background noise  set    order  tolerate  least one read  default background  expect  intervals  homozygous snps use  therefore     refref  altalt genotypes respectively consequently unexpected ratios  situate      range fig   extraction  trim adaptors  cutadapt use default parameters  read  align via bowtie2 allow   one mismatch    bplong seed  report  unique alignments  read  map quality less    filter  variant call software   use   want  report  frequencies within  focus list  snps  depth  coverage function   genome analysis toolkit gatk  use  together  additional statistical analysis detail   report ars  snps  ensure analysis  highquality data  base quality ≥  consider  determination   depth  coverage   select polymorphisms snp selection   sample   distribution   snp selection  compute   algorithm  order  detect identify  quantify  contaminant supplementary fig  snp selection  retain informative polymorphisms   typical trimodal  distribution among   snps   european population frequency   range     genomes database  present   gene panel   recurrent high background noise  close  homopolymer stretch  exclude see “optimization stepˮ section  similarly polymorphisms within paralogous genes  exclude  avoid misinterpretation   spoil  expect misalignment  total   polymorphisms   able   analyze  snps   least  coverage  take  account  allow detection  low contamination homozygous snps    allelic version throughout  sample  noninformative  could   use  analysis detection  contamination “worstcase scenario” screen test  first step  identification  contamination consist   screen test   sample   run base  estimation   “worstcase scenario” wcs percentage  contamination  screen test  independent  background noise  identify sample possibly contaminate   certain cutoff define     present study follow  optimization step see   wcs calculation  define  wcs  maxr           median   highest   ars  refref snps    median   lowest   ars  altalt snps  main advantage   wcs test   rapidly rule   contamination    negative however    low specificity   positive wcs test must  confirm  identification   contaminant   worst scenario  never certain identification   contaminant contamination  suspect   wcs percentage  contamination  ≥  second step consist  identification   contaminant  order  confirm  contamination  identification  base   snps   contaminate sample   genotype compare   genotypes    sample   run  homozygous snps   contaminate sample  use  heterozygous snps exhibit excessive variability   value  allow reliable identification  small variations correspond  lowlevel contamination  snps       use correspond  homozygous snps refref  altalt include contaminate     noncontaminated snps      background noise  identify  putative contaminant  percentage  snps compatible  contamination  one sample   another sample   calculate accord   number  homozygous snps satisfy  compatibility condition list  table    word  homozygous snps  expect  value  contaminant      genotype   snps  unexpected  value  contaminant     different genotype  suspect contaminant  therefore identify   genotype   sample   sample    run  test score  rank  putative contaminants   sample   highest percentage  consider    putative contaminant  determine whether  putative contaminant actually contaminate  sample  study two condition   require firstly  percentage  snps compatible  contamination  sample   sample     higher   percentage  snps  sample  compatible  absence  contamination otherwise sample  could simply  genetically similar  sample   percentage  snps compatible  absence  contamination   percentage  snps   normal     among  total number  homozygous snp          supplementary table  secondly  putative contaminant    significantly  compatible   contaminate sample    sample   run fishers exact test  bonferroni correction  multitesting limit  significance  quantification  contamination   wcs contamination    rough overestimate value  refine percentage  calculate follow identification   contaminant  accord   genotype  contamination percentage   sample   contaminant  express   median   value obtain  calculation  contamination rate   snp  sample  collect  diagnostic  genetic counsel purpose appropriate individual write consent  genetic analysis  obtain    participate patients   legal guardians",2
60,PaPrBaG,"PaPrBaG: A machine learning approach for the detection of novel pathogens from NGS data
summarises the individual steps of PaPrBaG. The supervised machine learning setup comprises a training and a prediction workflow. The entire set of HP and non-HP bacterial species is divided into non-overlapping training and test sets. Subsequently, selected genomes from all species are fragmented into reads (see subsection Benchmark), from which a range of sequence features are extracted (subsection Features). The training sequence features together with the associated phenotype labels compose the training database, on which the random forest algorithm trains a pathogenicity classifier (subsection Machine Learning). In turn, this classifier predicts the pathogenic potential for each read in the test set. Based on these raw results various analysis steps can be performed. This section further provides a summary of the different benchmark approaches (subsection Benchmark), evaluation strategies (subsection Metrics) and metagenome configuration (subsection Metagenomic example) used in the Results section. Machine Learning A random forest classifier34 was trained using the below-described features and (genome) pathogenicity labels for each read in the training data set. We chose this classifier type because it combines high accuracy, fast prediction speed and the capability to deal with noisy data35,36. Among the different implementations of the random forest algorithm available, we opted for ranger37,38 since it is one of the fastest - implemented in C++ — and can handle large data sets. We used probability forests, which return the fraction of votes for each class. This can also be interpreted as the prediction probability. We therefore refer to the prediction probability of the HP class as the pathogenic potential of a read. As another advantage, the random forest approach has only few tuneable parameters. In this context, we found it sufficient to train 100 trees per forest, as more trees do not lead to better predictions (see Supplementary Fig. S1). Fluctuations play some role for small tree numbers between 20 and 50 trees per forest. Forests with 100 trees or more, however, yield very similar per-read predictions and nearly identical per-genome predictions, justifying our parameter setting. We further adjusted the minimum size for terminal nodes. High numbers can result in impure terminal nodes and smaller trees. Changing it from 1 (default and our choice here) to 10 had no effect, while sizes above 1,000 led to overfitting. Changing any further parameters had no substantial effect, respectively. The trained random forest objects are available on github. Features For the machine learning task, a set of informative features must be extracted from the read sequences. We implemented a number of different feature types to capture the information content present in a sequencing read. Genomic features Different features were extracted from DNA sequences, all of which based on k-mer occurrence patterns. Since we analyse read data, strand information is not available. Therefore, we cast all features symmetrically so that the occurrences of a word and its reverse-complement were considered jointly, a common strategy in related methods39,40. A first features type is the relative k-mer frequency. We found that including monomers, dimers, trimers and tetramers led to good results, but higher values of k did not lead to further improvement. The occurrences of longer k-mers are less likely to overlap among highly divergent sequences. Conversely, the consideration of a large number of uninformative long k-mers can compromise prediction performance. However, as focussing on selected longer sequence motifs can still be beneficial for classification, we also recorded the frequencies of the 100 most abundant 8-mers in an independent set of bacterial genomes, scanning both strands and allowing for one mismatch. Spaced words were introduced for the alignment of dissimilar sequences41,42. Thus, their incorporation is useful in the context of novel species discovery. Spaced words denote the occurrence of all k-mers interrupted by (l-k) spacers in a word of length l. For this analysis we searched for all symmetric 4-mers in a spaced word of length 6. Protein features Bacterial genomes are known to be densely packed with proteins43. Since protein sequences are evolutionarily more conserved than DNA sequences, peptide features can provide additional valuable information. A read might (partially) cover a protein sequence, but the correct reading frame is unknown. However, longer DNA sequences tend to contain frequent stop codons in the anti-sense frames by chance. Therefore, as a simple heuristic, we generally used the frame and strand with the fewest number of stop codons. This frame was translated into a peptide sequence and several types of features were extracted: codon frequencies, relative monopeptide and dipeptide frequencies, amino acid properties and Amino Acid Index (AAIndex)44,45,46 statistics. The amino acid property features consist of the relative frequencies of tiny, small, aliphatic, aromatic, non-polar, polar, charged, basic and acidic residues47. Finally, the AAIndex assigns scores for diverse properties (often based on peptide secondary structure) to each residue. From 544 indices, we selected the 32 with the lowest pairwise correlation. Features were obtained by computing the product of the amino acid frequencies and their associated index scores. In total, we included 948 features in the classification workflow. Feature importance As measured by both the permutation and Gini tests, the most important features come from the DNA monomer, dimer and trimer feature groups (see Supplementary Table S1). Among the 100 most important features, the tetramer, codon frequency, AAIndex and spaced words groups are also prevalent. We estimated the importance of the different groups by searching for the highest scoring member of each group. The resulting order of group importance was trimer, monomer, dimer, tetramer, spaced words, AAIndex score, codon frequencies, monopeptides, DNA motifs, amino acid properties and dipeptides. Particularly the last five groups were of minor importance for the classification task. Benchmarking Cross-validation strategy To evaluate the developed classifier, we performed a five-fold cross-validation study. In this, we randomly distributed the HP and non-HP genomes into five equally-sized, non-overlapping parts, respectively. Further, only a single, randomly selected strain (genome) per species was considered. The significant variation in the number of strains per species (1 to over 200), which is mainly due to a pathogen study bias, would have otherwise translated into largely skewed training databases. Apart from markedly reducing label imbalance (the ratio decreases from 16 to 7), this also reduced the training data size. Further, the described approach reflects the scope of this work, which is predicting phenotypes on the species level. Still, for comparison, we included all strains of each training species in a separate benchmark study (see Results). E. coli played a unique role in that it possesses a large number of known HP and non-HP strains. In this study, we considered the HP strains only. Since the aim was to provide species-level predictions, the rationale was to be more sensitive towards pathogens. Read simulation For both the training and test data sets, we simulated 250 bp long Illumina reads. To that end we used the Mason read simulator with the default Illumina error model48. The number of reads sampled per genome differed for the training and test sets. In training binary classifiers, it is generally advantageous to show the learner an equal number of examples for both classes. Therefore, despite an HP to non-HP ratio of about 7:1, we decided to sample the same total number of reads per class, 106. This represents a trade-off between genome coverage and training data size. An increase to 107 reads did not substantially improve the prediction results. Further, the number of reads per genome in each class was chosen such that each genome had the same coverage, i.e. proportional to the size of the genome. Conversely, for the test data sets, we chose to sample up to a coverage of approximately 1 for each genome. The read simulation was repeated for each fold. Comparison with other methods We further compared the performance of PaPrBaG with a range of other tools, most of which were originally developed for taxonomic classification. First, we used Bowtie2, one of the commonly used read mappers combining speed and accuracy49. Further, we considered Pathoscope217 as a dedicated pipeline for pathogen identification. More sensitive mapping is expected from BLAST13, which is still widely used in NGS pipelines. As a method integrating both alignment- and composition-based characteristics, we further assessed Kraken, which has emerged as one of the primary taxonomic classification tools20. Finally, we considered NBC as a composition-based machine learning method22,23. Unlike similar approaches, it allows the construction of a custom training database. We evaluated the performance of these tools using the PaPrBaG training and test genome sets, again using five-fold cross-validation. Bowtie2 For read mapping, we used Bowtie2 (v2.2.4) in the very-sensitive configuration, which is highly tolerant towards mismatches and gaps. We obtained the 50 top alignments of each read. Parsing the resulting SAM file, we matched the best-scoring mapping of each read with the label database. When more than one alignment shared the best score, we chose a match to an HP over a match to a non-HP. For unmapped reads, no prediction could be made. Additionally, we repeated this mapping workflow for a larger reference genome set including all strains of the training species. Pathoscope2 Pathoscope2 (v2.0.6) works as a post-mapping filter. Hence, we ran its ‘Identification’ module on the SAM file produced by Bowtie2. The resulting filtered SAM file was analysed as above to obtain label predictions. Also this analysis was repeated using the all-strain reference set. Kraken We provided Kraken (v0.10.5) with the training genome sequences, from which it builds a database based on clade-specific 31-mers. Based on this, the tool tries to classify each read taxonomically. The resulting species were matched to the label database via their NCBI taxonomy ID. If that failed, the species name (obtained via Kraken’s translation module) sometimes still produced a match. When Kraken’s prediction was not at species resolution, no prediction was made. Further, since matching 31-mers to divergent sequences may prove challenging, we repeated the entire analysis using 16-mers (Kraken-16). Note that the former shares characteristics of an alignment-based method, while the latter can be considered composition-based. BLAST We ran NCBI BLASTN (v2.2.28) with the option ‘-task dc-megaBLAST’, which is tailored for inter-species comparisons. Additionally, we chose an E-value threshold of 10. From the resulting BLAST output, the highest-scoring target was matched to the label database. Naïve Bayes Classifier We created a set of NBC (v1.0) training databases with word length 15 and then scored all test read sets against all training databases. For each read, we selected the highest-scoring hit and matched the species name to the label database. Since classification with NBC took very long, we had to use parallel threads. Evaluation metrics Majority prediction rule All tested methods yield predictions for individual reads, but ultimately we were interested in a single, integrated prediction for each genome. An individual read matching to an HP genome cannot, by itself, be deemed significant, given that also non-HP genomes will contain stretches similar to HP genomes. In PaPrBaG, we therefore average over all read-based prediction probabilities. If this value exceeds 0.5, the sample is classified as pathogenic. Likewise, for the other methods, we assign the class with the higher number of reads. This evaluation metric will henceforth be referred to as the majority prediction rule. Minimum detection threshold The majority prediction rule allows for a simple estimation of the pathogenic potential of a sample; however, it ignores uncertainty due to missing predictions. Therefore, we implemented a complementary metric, the minimum detection threshold. Here, a user can define the minimum fraction of reads that should be required for a confident phenotype prediction. As before, for a given test genome, we collect the read-based evidence. Then, a phenotype is considered for prediction only if the number of reads supporting it exceeds the minimum detection threshold. If both phenotypes are supported, the better-supported one determines the prediction. Further, no prediction is made when neither phenotype is sufficiently supported. For both phenotypes, we assessed the fraction of correct predictions, which corresponds to the true positive rate (TPR) and true negative rate (TNR), respectively. We then summarised the performance using informedness, also known as Youden’s J statistic, which is a joint measure of specificity and sensitivity50. Formally, informedness is defined as I = TPR + TNR − 1 and ranges from −1 (only wrong predictions) to 1 (only correct predictions). Consensus filter Individual approaches may yield heterogeneous predictions, which makes it attractive to combine them to enhance prediction confidence. We therefore define a consensus filter as follows. First, we evaluate which predictions coincide between two methods. We then keep only the consensus subset for further performance evaluation. Prediction certainty Each prediction made by the majority prediction rule is associated with uncertainty. We defined the prediction certainty as |μ − 0.5| × 2, where μ denotes the majority prediction as discussed above. The result is a relative certainty value that always ranges from 0 (maximally uncertain) to 1 (maximally certain). Note that this value does not reflect the predicted class. We further normalised the certainty of each predictor by the highest certainty it reports for any genome. This is not a necessary step but aids visualisation. Metagenomic example To illustrate the application of PaPrBaG in a metagenomic context, we simulated an artificial set of reads belonging to a set of known non-HPs (present in a given training fold) and a single unknown HP (not present in that fold). As HP we chose Gardnerella vaginalis, currently the only known representative of its genus. Since it is known to invade the female urogenital system, we then selected a range of non-HPs that are known to colonise the same habitat in healthy humans according to the HMP. These were Lactobacillus salivarius, Lactobacillus reuteri, Lactobacillus rhamnosus, Bifidobacterium breve and Haemophilus parainfluenzae, all of which found in the relevant training fold. Our further strategy resembled that used in ref. 20. Mimicking a clinical sample, where non-HPs (e.g. commensals) will generally vastly outnumber the given HP, we sampled 200,000 reads for each of the latter but only 6,604 reads (equivalent to a coverage of 1) for the former. In this controlled scenario we can identify the number of true positives (G. vaginalis reads mapped to another HP), false negatives (G. vaginalis reads mapped to a non-HP), and false positives (non-HP reads mapped to an HP).",VirusDetection,"paprbag  machine learn approach   detection  novel pathogens  ngs data
summarises  individual step  paprbag  supervise machine learn setup comprise  train   prediction workflow  entire set    nonhp bacterial species  divide  nonoverlapping train  test set subsequently select genomes   species  fragment  read see subsection benchmark    range  sequence feature  extract subsection feature  train sequence feature together   associate phenotype label compose  train database    random forest algorithm train  pathogenicity classifier subsection machine learn  turn  classifier predict  pathogenic potential   read   test set base   raw result various analysis step   perform  section  provide  summary   different benchmark approach subsection benchmark evaluation strategies subsection metrics  metagenome configuration subsection metagenomic example use   result section machine learn  random forest classifier34  train use  belowdescribed feature  genome pathogenicity label   read   train data set  choose  classifier type   combine high accuracy fast prediction speed   capability  deal  noisy data35 among  different implementations   random forest algorithm available  opt  ranger37 since   one   fastest  implement   —   handle large data set  use probability forest  return  fraction  vote   class   also  interpret   prediction probability  therefore refer   prediction probability    class   pathogenic potential   read  another advantage  random forest approach    tuneable parameters   context  find  sufficient  train  tree per forest   tree   lead  better predictions see supplementary fig  fluctuations play  role  small tree number     tree per forest forest   tree   however yield  similar perread predictions  nearly identical pergenome predictions justify  parameter set   adjust  minimum size  terminal nod high number  result  impure terminal nod  smaller tree change    default   choice      effect  size   lead  overfitting change   parameters   substantial effect respectively  train random forest object  available  github feature   machine learn task  set  informative feature must  extract   read sequence  implement  number  different feature type  capture  information content present   sequence read genomic feature different feature  extract  dna sequence    base  kmer occurrence pattern since  analyse read data strand information   available therefore  cast  feature symmetrically    occurrences   word   reversecomplement  consider jointly  common strategy  relate methods39  first feature type   relative kmer frequency  find  include monomers dimers trimers  tetramers lead  good result  higher value     lead   improvement  occurrences  longer kmers  less likely  overlap among highly divergent sequence conversely  consideration   large number  uninformative long kmers  compromise prediction performance however  focus  select longer sequence motifs  still  beneficial  classification  also record  frequencies     abundant mers   independent set  bacterial genomes scan  strand  allow  one mismatch space word  introduce   alignment  dissimilar sequences41 thus  incorporation  useful   context  novel species discovery space word denote  occurrence   kmers interrupt   spacers   word  length    analysis  search   symmetric mers   space word  length  protein feature bacterial genomes  know   densely pack  proteins43 since protein sequence  evolutionarily  conserve  dna sequence peptide feature  provide additional valuable information  read might partially cover  protein sequence   correct read frame  unknown however longer dna sequence tend  contain frequent stop codons   antisense frame  chance therefore   simple heuristic  generally use  frame  strand   fewest number  stop codons  frame  translate   peptide sequence  several type  feature  extract codon frequencies relative monopeptide  dipeptide frequencies amino acid properties  amino acid index aaindex statistics  amino acid property feature consist   relative frequencies  tiny small aliphatic aromatic nonpolar polar charge basic  acidic residues47 finally  aaindex assign score  diverse properties often base  peptide secondary structure   residue   indices  select     lowest pairwise correlation feature  obtain  compute  product   amino acid frequencies   associate index score  total  include  feature   classification workflow feature importance  measure    permutation  gini test   important feature come   dna monomer dimer  trimer feature group see supplementary table  among    important feature  tetramer codon frequency aaindex  space word group  also prevalent  estimate  importance   different group  search   highest score member   group  result order  group importance  trimer monomer dimer tetramer space word aaindex score codon frequencies monopeptides dna motifs amino acid properties  dipeptides particularly  last five group   minor importance   classification task benchmarking crossvalidation strategy  evaluate  develop classifier  perform  fivefold crossvalidation study    randomly distribute    nonhp genomes  five equallysized nonoverlapping part respectively    single randomly select strain genome per species  consider  significant variation   number  strain per species       mainly due   pathogen study bias would  otherwise translate  largely skew train databases apart  markedly reduce label imbalance  ratio decrease      also reduce  train data size   describe approach reflect  scope   work   predict phenotypes   species level still  comparison  include  strain   train species   separate benchmark study see result  coli play  unique role    possess  large number  know   nonhp strain   study  consider   strain  since  aim   provide specieslevel predictions  rationale     sensitive towards pathogens read simulation    train  test data set  simulate   long illumina read   end  use  mason read simulator   default illumina error model48  number  read sample per genome differ   train  test set  train binary classifiers   generally advantageous  show  learner  equal number  examples   class therefore despite    nonhp ratio     decide  sample   total number  read per class   represent  tradeoff  genome coverage  train data size  increase   read   substantially improve  prediction result   number  read per genome   class  choose    genome    coverage  proportional   size   genome conversely   test data set  choose  sample    coverage  approximately    genome  read simulation  repeat   fold comparison   methods   compare  performance  paprbag   range   tool     originally develop  taxonomic classification first  use bowtie2 one   commonly use read mappers combine speed  accuracy49   consider pathoscope217   dedicate pipeline  pathogen identification  sensitive map  expect  blast13   still widely use  ngs pipelines   method integrate  alignment  compositionbased characteristics   assess kraken   emerge  one   primary taxonomic classification tools20 finally  consider nbc   compositionbased machine learn method22 unlike similar approach  allow  construction   custom train database  evaluate  performance   tool use  paprbag train  test genome set  use fivefold crossvalidation bowtie2  read map  use bowtie2    verysensitive configuration   highly tolerant towards mismatch  gap  obtain   top alignments   read parse  result sam file  match  bestscoring map   read   label database    one alignment share  best score  choose  match      match   nonhp  unmapped read  prediction could  make additionally  repeat  map workflow   larger reference genome set include  strain   train species pathoscope2 pathoscope2  work   postmapping filter hence  run  identification module   sam file produce  bowtie2  result filter sam file  analyse    obtain label predictions also  analysis  repeat use  allstrain reference set kraken  provide kraken    train genome sequence    build  database base  cladespecific mers base    tool try  classify  read taxonomically  result species  match   label database via  ncbi taxonomy    fail  species name obtain via krakens translation module sometimes still produce  match  krakens prediction    species resolution  prediction  make  since match mers  divergent sequence may prove challenge  repeat  entire analysis use mers kraken note   former share characteristics   alignmentbased method   latter   consider compositionbased blast  run ncbi blastn    option task dcmegablast   tailor  interspecies comparisons additionally  choose  evalue threshold     result blast output  highestscoring target  match   label database naïve bay classifier  create  set  nbc  train databases  word length    score  test read set   train databases   read  select  highestscoring hit  match  species name   label database since classification  nbc take  long    use parallel thread evaluation metrics majority prediction rule  test methods yield predictions  individual read  ultimately   interest   single integrate prediction   genome  individual read match    genome cannot    deem significant give  also nonhp genomes  contain stretch similar   genomes  paprbag  therefore average   readbased prediction probabilities   value exceed   sample  classify  pathogenic likewise    methods  assign  class   higher number  read  evaluation metric  henceforth  refer    majority prediction rule minimum detection threshold  majority prediction rule allow   simple estimation   pathogenic potential   sample however  ignore uncertainty due  miss predictions therefore  implement  complementary metric  minimum detection threshold   user  define  minimum fraction  read    require   confident phenotype prediction     give test genome  collect  readbased evidence   phenotype  consider  prediction    number  read support  exceed  minimum detection threshold   phenotypes  support  bettersupported one determine  prediction   prediction  make  neither phenotype  sufficiently support   phenotypes  assess  fraction  correct predictions  correspond   true positive rate tpr  true negative rate tnr respectively   summarise  performance use informedness also know  youdens  statistic    joint measure  specificity  sensitivity50 formally informedness  define    tpr  tnr    range    wrong predictions    correct predictions consensus filter individual approach may yield heterogeneous predictions  make  attractive  combine   enhance prediction confidence  therefore define  consensus filter  follow first  evaluate  predictions coincide  two methods   keep   consensus subset   performance evaluation prediction certainty  prediction make   majority prediction rule  associate  uncertainty  define  prediction certainty         denote  majority prediction  discuss   result   relative certainty value  always range   maximally uncertain   maximally certain note   value   reflect  predict class   normalise  certainty   predictor   highest certainty  report   genome     necessary step  aid visualisation metagenomic example  illustrate  application  paprbag   metagenomic context  simulate  artificial set  read belong   set  know nonhps present   give train fold   single unknown   present   fold    choose gardnerella vaginalis currently   know representative   genus since   know  invade  female urogenital system   select  range  nonhps   know  colonise   habitat  healthy humans accord   hmp   lactobacillus salivarius lactobacillus reuteri lactobacillus rhamnosus bifidobacterium breve  haemophilus parainfluenzae    find   relevant train fold   strategy resemble  use  ref  mimic  clinical sample  nonhps  commensals  generally vastly outnumber  give   sample  read     latter    read equivalent   coverage     former   control scenario   identify  number  true positives  vaginalis read map  another  false negative  vaginalis read map   nonhp  false positives nonhp read map   ",2
61,VirVarSeq,"VirVarSeq: a low-frequency virus variant detection pipeline for Illumina sequencing using adaptive base-calling accuracy filtering
Several samples were sequenced using Illumina’s genome analyzer (GA) IIx according to manufacturing protocols (see Supplementary Material ). The VirVarSeq pipeline proceeds as follows: The sequenced reads are aligned against a reference sequence using the Burrows–Wheeler aligner tool ( Li and Durbin, 2009 ). Based on this alignment, a consensus sequence is defined. A realignment is performed against this consensus. This strategy of iterative mapping will increase coverage, especially in samples where the consensus strongly deviates from the reference (see Supplementary Fig. S1 ). After alignment, Q-cpileup is executed, which consists of a three-step analysis: In the first step, the Qs of the codons in the reading frame of interest are retrieved. Next, the threshold is determined dependent on the quality of the run. Finally, the filtered codon table is constructed. The VirVarSeq pipeline, which runs from fastq to the filtered codon table, is available at http://sourceforge.net/projects/virtools/?source=directory together with a user’s guide (see Supplementary Material 2 ). All reads containing indel errors are removed before running Q-cpileup. It is hereby assumed that indels will result in non-viable virus. In some rare occasions, however, there might be an insertion mutation at the codon level, which can be investigated in a separate analysis (see indel Table Supplementary Material ). Below, the different steps from Q-cpileup will be explained in more detail. 2.1 Quality of codons A pileup of read bases is generated using the alignments to a consensus sequence. In analogy with mpileup of samtools, for which the base-pair information at each reference position is described, cpileup describes the codon information at each amino acid position of the reference genome. For each position in the reference genome, the different codons are reported together with one Q for each codon. This requires that the Qs of the three nucleotides within a codon have to be summarized. A comparative analysis of different summarizations revealed that the weakest link, i.e. minimum Q of the three nucleotides in the codon, provided the best separation between low- and high-quality codons (see Supplementary Fig. S2 ). This minimum Q represents the codon’s nucleotide with the highest probability of being a sequencing error. A codon table is built based on the pileup where for each codon position of the reference the different codons within a sample are reported together with their frequency ( Table 1 ). The minimum Qs of the codons at a particular position are averaged to give a rough idea about the overall quality. However, the individual minimum Qs of the codons themselves is used in subsequent analyses. 2.2 Q-intersection threshold The distribution of the minimum Qs was checked and compared for one particular sample sequenced in two different runs and three different lanes reaching an average coverage of around 30 000 ( Fig. 1 ). The shape of the distributions can be approximated by a mixture distribution with three truncated normal components (see Supplementary Material for model selection and goodness of fit in Supplementary Fig. S3 ). Truncation is performed at the lower and upper ends of the Q range. The first mode represents a point probability at Q 2, which is the lowest Illumina Q. This is due to an artifact created by Illumina’s base caller. Read ends with a segment of mostly low quality (≤Q15) are given a Q of 2. The second component distribution is a distribution of low Qs, reflecting the sequencing error distribution. Finally, the highest mode, close to 40, originates from a distribution of reliable calls. Note that the mixture of three normal components for the errors and reliable calls should be considered only as a working assumption and that neither trimming nor filtering of the data is required before fitting the mixture models. The expectation maximization (EM) algorithm of McLachlan ( McLachlan and Jones, 1988 ) will be applied for fitting these normal mixture models. We have written an R-wrapper to run the original Fortran code of McLachlan, which is embedded in the pipeline VirVarSeq. The EM algorithm was initialized by setting the three modes at 2, 10 and 35 and the variances at 0.8 for the point probability and 40 for the other two distributions. The marginal error probability, the sum of mixing proportion of the distributions at 2 and 10, was set to 15%. The bulk of Qs was high, indicating a majority of reliable calls in the dataset (green distribution in Fig. 1 ). At the other end, a clear point probability at the Q of 2 was seen. The red distribution in Figure 1 corresponds to low-quality codons that are likely to be sequencing errors. There are several criteria to define a threshold for filtering the low-quality codons and for distinguishing between errors and reliable calls. An approach is chosen that is adaptive and robust. The intersection point of the two component distributions was used and is referred to as the Q-intersection threshold (QIT), which is indicated with vertical dashed lines in Figure 1 . The distribution of the minimum Qs of the codons and hence the QIT varies between different runs for the same sample, confirming the need for an adaptive filtering strategy. 2.3 Filtering of codon tables Once the QIT is determined, an updated codon table can be constructed. All codons with a minimum Q below the threshold will be filtered from the analysis. The influence of trimming is negligible, as it mainly affects low-quality nucleotides, which are removed by the filter. The three-step analysis returns a codon table with different variants and their frequencies at each codon position of the reference, which is robust to sequencing errors. Distribution of the minimum Qs of the codons present in an HCV sample, which was sequenced twice in the same run (run 1) but in different lanes (lanes 1 and 2), and which was sequenced yet another time in another run (run 2). The black line shows the overall fit of the mixture distribution, which consists of the blue, the red and the green component distributions. The blue and the red distributions correspond to codons that likely result from sequencing errors, and the green distribution represents reliable calls. The QIT is indicated with a vertical black dashed line  Notes . The different codons observed in a sample are counted at each codon position of the reference genome and their frequencies are calculated using the coverage at that particular position. The Qs are summarized by averaging the minimum Qs of the codons. Position: amino acid position of the reference. Ref: codon of the reference genome at a particular position. Codon: codon present in a sample at a particular position. Count: the number of times a particular codon occurs in the cpileup at a particular position. Coverage: the number of reads that fully cover a particular codon position. Frequency: Count/Coverage. Mean: Average of the minimum Qs of a particular codon at a particular position.",VirusDetection,"virvarseq  lowfrequency virus variant detection pipeline  illumina sequence use adaptive basecalling accuracy filtering
several sample  sequence use illuminas genome analyzer  iix accord  manufacture protocols see supplementary material   virvarseq pipeline proceed  follow  sequence read  align   reference sequence use  burrowswheeler aligner tool    durbin   base   alignment  consensus sequence  define  realignment  perform   consensus  strategy  iterative map  increase coverage especially  sample   consensus strongly deviate   reference see supplementary fig    alignment qcpileup  execute  consist   threestep analysis   first step     codons   read frame  interest  retrieve next  threshold  determine dependent   quality   run finally  filter codon table  construct  virvarseq pipeline  run  fastq   filter codon table  available   together   users guide see supplementary material    read contain indel errors  remove  run qcpileup   hereby assume  indels  result  nonviable virus   rare occasion however  might   insertion mutation   codon level    investigate   separate analysis see indel table supplementary material    different step  qcpileup   explain   detail  quality  codons  pileup  read base  generate use  alignments   consensus sequence  analogy  mpileup  samtools    basepair information   reference position  describe cpileup describe  codon information   amino acid position   reference genome   position   reference genome  different codons  report together  one    codon  require      three nucleotides within  codon    summarize  comparative analysis  different summarizations reveal   weakest link  minimum    three nucleotides   codon provide  best separation  low  highquality codons see supplementary fig    minimum  represent  codons nucleotide   highest probability    sequence error  codon table  build base   pileup    codon position   reference  different codons within  sample  report together   frequency  table    minimum    codons   particular position  average  give  rough idea   overall quality however  individual minimum    codons   use  subsequent analyse  qintersection threshold  distribution   minimum   check  compare  one particular sample sequence  two different run  three different lanes reach  average coverage  around    fig    shape   distributions   approximate   mixture distribution  three truncate normal components see supplementary material  model selection  goodness  fit  supplementary fig   truncation  perform   lower  upper end    range  first mode represent  point probability       lowest illumina    due   artifact create  illuminas base caller read end   segment  mostly low quality ≤q15  give      second component distribution   distribution  low  reflect  sequence error distribution finally  highest mode close   originate   distribution  reliable call note   mixture  three normal components   errors  reliable call   consider    work assumption   neither trim  filter   data  require  fit  mixture model  expectation maximization  algorithm  mclachlan  mclachlan  jones     apply  fit  normal mixture model   write  rwrapper  run  original fortran code  mclachlan   embed   pipeline virvarseq   algorithm  initialize  set  three modes        variances     point probability      two distributions  marginal error probability  sum  mix proportion   distributions      set    bulk    high indicate  majority  reliable call   dataset green distribution  fig      end  clear point probability       see  red distribution  figure  correspond  lowquality codons   likely   sequence errors   several criteria  define  threshold  filter  lowquality codons   distinguish  errors  reliable call  approach  choose   adaptive  robust  intersection point   two component distributions  use   refer    qintersection threshold qit   indicate  vertical dash line  figure    distribution   minimum    codons  hence  qit vary  different run    sample confirm  need   adaptive filter strategy  filter  codon table   qit  determine  update codon table   construct  codons   minimum    threshold   filter   analysis  influence  trim  negligible   mainly affect lowquality nucleotides   remove   filter  threestep analysis return  codon table  different variants   frequencies   codon position   reference   robust  sequence errors distribution   minimum    codons present   hcv sample   sequence twice    run run    different lanes lanes       sequence yet another time  another run run   black line show  overall fit   mixture distribution  consist   blue  red   green component distributions  blue   red distributions correspond  codons  likely result  sequence errors   green distribution represent reliable call  qit  indicate   vertical black dash line  note   different codons observe   sample  count   codon position   reference genome   frequencies  calculate use  coverage   particular position    summarize  average  minimum    codons position amino acid position   reference ref codon   reference genome   particular position codon codon present   sample   particular position count  number  time  particular codon occur   cpileup   particular position coverage  number  read  fully cover  particular codon position frequency countcoverage mean average   minimum    particular codon   particular position",2
62,Giant Virus Finder,"The ""Giant Virus Finder"" discovers an abundance of giant viruses in the Antarctic dry valleys
We believe that the method, presented here, is a general workflow: it could also be applied for identifying other sets of taxa, not only giant viruses. The steps of the general workflow are as follows: 1. Identify the set X of genomes to be searched for (in our application example X is the set of genomes for the giant viruses); 2. Apply subsequence-search for the sequences in X in the target metagenomic shotgun sequence database Y (in our example Y is one of the 16 metagenomes from [8]); 3. Verify the specificity of the hits: the whole fragments in the metagenomic dataset, containing the highest-scored alignments, are aligned to the sequences of a large nucleotide database. Suppose that the top scored hit has score z. If all the hits with scores greater than 0.8×z are from the set X, ACCEPT, otherwise REJECT the hit (in our example, the hits are aligned to the sequences of the Nucleotide Collection (nt) of the NCBI; and a hit is accepted only if every sequence in the top-scored 20% belong to set X that is, to the giant virus list). A 10% cut-off is applied as a default value in the MEGAN phylogenetic analysis tool [12] for a similar decision. We found this number is too low for our purposes so we set a more stringent value of 20%. Users can simply change this threshold if they require. The Giant Virus Toplist In the workflow described above, we need a list, X, of the genomes and sequences of the organisms we are searching for. Defining what is a giant virus and what is not, is a difficult question. We would not like to use potentially questionable and disputed phylogenetic information in this definition; rather we have simply constructed a list of viruses with viral genomes or partial genomes (if there is no complete genome deposited) larger than 300 kbp, as is detailed in http://pitgroup.org/giant-virus-toplist/. Reference genome data are taken from the ftp.ncbi.nlm.nih.gov/genomes/Viruses/all.fna.tar.gz file from the NCBI Genome FTP. Note that the length of distinct genome sequences (segments) belonging to a single genome are summed. Other sequences are added from the NCBI Nucleotide database using the search term: “Viruses”[Organism] AND 300000:10000000[Sequence Length] NOT “Bacteria”[Organism] NOT “Archaea”[Organism]. The list of the viruses found is also given in Table S2 in the supporting material, together with the sequence accession numbers applied in this work. The inspiration for the Giant Virus Toplist came from http://www.giantvirus.org/top.html. Our toplist is more up-to-date and contains not only the full-, but also partial genomes. However, we are well aware that this Toplist is not a single perfect list, and it needs to be modified with the course of time because of novel discoveries and genome depositions. Therefore, we have made it possible to change the list in the “Giant Virus Finder”, in order to suit the specific needs of the users. Sequence alignments The metagenomic data from the article [8] is deposited in the MG-RAST archive: http://metagenomics.anl.gov/metagenomics.cgi?page=MetagenomeProject\&project=2997. We downloaded and converted the files into fastq formats. Next, with the stand-alone BLAST distribution [1] downloadable makeblastdb program we created 16 BLAST databases for each of the 16 metagenomes. In Phase 1 (Figure 2) we used the stand alone UNIX blastn program with the default Megablast algorithm and changed the word-size from 28 to 16 and e-value from 10 to 0.01. All the other parameters and the scores and penalties were retained at default (for blastn). Next, in Phase 2, the hits with an E-value better than 0.01 were collected from each alignment, and were aligned using blastn, with word-size of 7, against the whole Nucleotide Collection (nt) of NCBI. Supposing that the top scored hit was score z, if all the hits with scores greater than 0.8×z are from the The Giant Virus Toplist, we accepted the hit, otherwise we rejected it. The summary of the results of the two-phase search process with the highest scored giant viruses is given in Supplementary Table S3, and their counts in Figure 1. All the files created by the workflow are given at http://pitgroup.org/public/giant-virus-finder/Giants-in-16Soil-metagenomes/. The advantage of the two-phase method Using a straightforward one phase method (i.e. simply blastn searching all reads against the nt database with the word-size=7 option) would require about 1080 years (about 0,084 h/read) in a machine using a single CPU core. Selecting 9,829 candidate reads from all 112,674,624 reads from the 16 metagenomes in Phase 1 reduced the running time to about 34 days in a single-core machine.",VirusDetection," ""giant virus finder"" discover  abundance  giant viruses   antarctic dry valleys
 believe   method present    general workflow  could also  apply  identify  set  taxa   giant viruses  step   general workflow   follow  identify  set   genomes   search    application example    set  genomes   giant viruses  apply subsequencesearch   sequence     target metagenomic shotgun sequence database    example   one    metagenomes    verify  specificity   hit  whole fragment   metagenomic dataset contain  highestscored alignments  align   sequence   large nucleotide database suppose   top score hit  score     hit  score greater      set  accept otherwise reject  hit   example  hit  align   sequence   nucleotide collection    ncbi   hit  accept   every sequence   topscored  belong  set      giant virus list   cutoff  apply   default value   megan phylogenetic analysis tool    similar decision  find  number   low   purpose   set   stringent value   users  simply change  threshold   require  giant virus toplist   workflow describe   need  list    genomes  sequence   organisms   search  define    giant virus       difficult question  would  like  use potentially questionable  dispute phylogenetic information   definition rather   simply construct  list  viruses  viral genomes  partial genomes     complete genome deposit larger   kbp   detail   reference genome data  take   ftpncbinlmnihgovgenomesvirusesallfnatargz file   ncbi genome ftp note   length  distinct genome sequence segment belong   single genome  sum  sequence  add   ncbi nucleotide database use  search term “viruses”organism  sequence length  “bacteria”organism  “archaea”organism  list   viruses find  also give  table    support material together   sequence accession number apply   work  inspiration   giant virus toplist come    toplist   uptodate  contain    full  also partial genomes however   well aware   toplist    single perfect list   need   modify   course  time   novel discoveries  genome depositions therefore   make  possible  change  list   “giant virus finder”  order  suit  specific need   users sequence alignments  metagenomic data   article   deposit   mgrast archive   download  convert  file  fastq format next   standalone blast distribution  downloadable makeblastdb program  create  blast databases      metagenomes  phase  figure   use  stand alone unix blastn program   default megablast algorithm  change  wordsize      evalue        parameters   score  penalties  retain  default  blastn next  phase   hit   evalue better    collect   alignment   align use blastn  wordsize     whole nucleotide collection   ncbi suppose   top score hit  score     hit  score greater       giant virus toplist  accept  hit otherwise  reject   summary   result   twophase search process   highest score giant viruses  give  supplementary table    count  figure    file create   workflow  give    advantage   twophase method use  straightforward one phase method  simply blastn search  read    database   wordsize option would require   years   hread   machine use  single cpu core select  candidate read    read    metagenomes  phase  reduce  run time    days   singlecore machine",2
63,IMSA,"IMSA: Integrated Metagenomic Sequence Analysis for Identification of Exogenous Reads in a Host Genomic Background
IMSA uses an Action File to filter non-host reads and align to universal database IMSA uses an action file to guide the filtering and alignment steps. A typical action file to filter a large read set against the human genome might be: quality bowtie human doDivide = True blat human | -fastMap blat human blast human maxEval-1e-15 | -word_size 24 blast human maxEval-1e-8 blast nt maxEval-1e-5 | —max_target_seqs 200 For each step there are additional parameter options which are described in the user manual. Quality filtering by default removes all reads with more than 15 bases with a quality less than 15. The user can define alternate quality metrics or omit this step from the action file. For example, “quality 10 20” would remove reads where more than 10 bases had a quality score less than 20. For alignment actions, each line in the action file specifies two steps. First, alignment is performed with the specified alignment program, such as bowtie or BLAST. Next, the read set is filtered to remove reads with a hit in the alignment results. Parameters for filtering are before the pipe (“|”) whereas anything after the pipe is sent directly to the alignment program. Some parameters, such as “maxEval” for blast can be used for both filtering and alignment and will automatically be sent to the alignment program. By default paired ends are treated as individual reads for maximum sensitivity, but this behavior can be modified. Databases for the alignment programs are defined in the IMSA configuration file so the databases used and their location in the computer system can be easily modified. Similarly, the configuration file can specify an ooc file for use in the blat alignment. IMSA can run and filter bowtie, blat and blastn (new NCBI version) alignments. Filtering the read set using the bowtie results can be memory intensive so an option to divide the file into pieces can be used to reduce the memory footprint. Blat and Blast alignments can be performed on a SUN Grid cluster or run straight, without a cluster. The final step in the action file is the alignment to the universal database. The input to this alignment is the host-filtered read set. For this step, the blast results are used in subsequent analysis, rather than simply being used to filter the read set. For host filtering blast steps, the default is to set the max_target_seqs parameters to 5 to short circuit searches for reads that map multiple places. In the alignment to the universal database, seen above on the last line of the action file, this parameter is set higher so all matches for each read are found. Taxonomy scores are calculated based on universal database alignment and visualized with TaxMaps Once IMSA has completed the filtering action file, the next step is to process the universal database alignment results to yield the IMSA reports. First, the blast results are processed to identify the best alignment for each read. If a read hits many sequences in the universal database, only the alignment with the highest score is kept. If a read aligns to multiple sequences with equally high scores, all of the targets are reported. However, IMSA assigns a score to each target indicating whether and how much the sequence read is split. For example, a read that aligns to a single sequence in the universal database is given a score of 1.0. If a read hits two sequences in the universal database with equal scores, both alignments get a score of 0.5. A read hitting three sequences will get a score of 0.333 for each hit, etc. This treatment allows ties to be kept, but the score assigned to each node is lower for non-unique reads that likely represent conserved regions, and higher for reads that are unique to that taxonomic node. The default behavior is to keep all the ties scored in this way, but IMSA provides the functionality to filter reads with scores below a given threshold. Though this reduces sensitivity, it can be informative to only look at reads with a unique best hit in the universal database. Next, the taxonomy of the scored blast result file is calculated. For this, the species of each hit in the universal database is determined from the gi. For this step, IMSA assumes that the universal database is the NCBI nucleotide transcript database (nt)[19]. If the universal database is another database with titles in a different format, the user will can customize the portion of the code that extracts a gi from the fasta title and translates it into a species. Once the targets are determined for each hit, IMSA then retrieves the entire taxonomic record for those targets from NCBI. IMSA uses the best read alignment scores assigned to each target to calculate the score for each species, genus, family and division with an aligned read in the sequence dataset. The taxonomy of universal database results is listed in taxonomy report text files. In addition, TaxMap bubble diagrams can be generated for species, genus, family and division. IMSA generates text files in a format ready to be interpreted into a diagram using the GraphViz open source graphing software. These data can be used to for downstream analysis to characterize the metagenome of the sample or to identify potential pathogens. IMSA includes additional tools for subsequent investigation. The python script getFastaForTaxonomy takes a list of taxonomy IDs (at any level) along with the fasta file of filtered reads and the blast alignment to the universal database to create a fasta file of all the reads aligning to the taxonomic IDs. The script speciesToClusterTable can take a set of species, genus, family, or division files and create a table of results suitable for input into Cluster and TreeView for visualization of the frequency in each sample of a larger study",VirusDetection,"imsa integrate metagenomic sequence analysis  identification  exogenous read   host genomic background
imsa use  action file  filter nonhost read  align  universal database imsa use  action file  guide  filter  alignment step  typical action file  filter  large read set   human genome might  quality bowtie human dodivide  true blat human  fastmap blat human blast human maxeval1e  word_size  blast human maxeval1e blast  maxeval1e  —max_target_seqs    step   additional parameter options   describe   user manual quality filter  default remove  read     base   quality less    user  define alternate quality metrics  omit  step   action file  example “quality  ” would remove read     base   quality score less    alignment action  line   action file specify two step first alignment  perform   specify alignment program   bowtie  blast next  read set  filter  remove read   hit   alignment result parameters  filter    pipe “” whereas anything   pipe  send directly   alignment program  parameters   “maxeval”  blast   use   filter  alignment   automatically  send   alignment program  default pair end  treat  individual read  maximum sensitivity   behavior   modify databases   alignment program  define   imsa configuration file   databases use   location   computer system   easily modify similarly  configuration file  specify  ooc file  use   blat alignment imsa  run  filter bowtie blat  blastn new ncbi version alignments filter  read set use  bowtie result   memory intensive   option  divide  file  piece   use  reduce  memory footprint blat  blast alignments   perform   sun grid cluster  run straight without  cluster  final step   action file   alignment   universal database  input   alignment   hostfiltered read set   step  blast result  use  subsequent analysis rather  simply  use  filter  read set  host filter blast step  default   set  max_target_seqs parameters    short circuit search  read  map multiple place   alignment   universal database see    last line   action file  parameter  set higher   match   read  find taxonomy score  calculate base  universal database alignment  visualize  taxmaps  imsa  complete  filter action file  next step   process  universal database alignment result  yield  imsa report first  blast result  process  identify  best alignment   read   read hit many sequence   universal database   alignment   highest score  keep   read align  multiple sequence  equally high score    target  report however imsa assign  score   target indicate whether   much  sequence read  split  example  read  align   single sequence   universal database  give  score     read hit two sequence   universal database  equal score  alignments get  score    read hit three sequence  get  score     hit etc  treatment allow tie   keep   score assign   node  lower  nonunique read  likely represent conserve regions  higher  read   unique   taxonomic node  default behavior   keep   tie score   way  imsa provide  functionality  filter read  score   give threshold though  reduce sensitivity    informative   look  read   unique best hit   universal database next  taxonomy   score blast result file  calculate    species   hit   universal database  determine      step imsa assume   universal database   ncbi nucleotide transcript database    universal database  another database  title   different format  user   customize  portion   code  extract     fasta title  translate    species   target  determine   hit imsa  retrieve  entire taxonomic record   target  ncbi imsa use  best read alignment score assign   target  calculate  score   species genus family  division   align read   sequence dataset  taxonomy  universal database result  list  taxonomy report text file  addition taxmap bubble diagram   generate  species genus family  division imsa generate text file   format ready   interpret   diagram use  graphviz open source graph software  data   use   downstream analysis  characterize  metagenome   sample   identify potential pathogens imsa include additional tool  subsequent investigation  python script getfastafortaxonomy take  list  taxonomy ids   level along   fasta file  filter read   blast alignment   universal database  create  fasta file    read align   taxonomic ids  script speciestoclustertable  take  set  species genus family  division file  create  table  result suitable  input  cluster  treeview  visualization   frequency   sample   larger study",2
64,DeepVirFinder,"Identifying viruses from metagenomic data using deep learning
viruses and prokaryotic genomes used for training, validation and testing We collected 2,314 RefSeq of viruses infecting prokar- yotes (bacteria and archaea) from NCBI (https://www. ncbi.nlm.nih.gov/genome/browse). The dataset was parti- tioned into three parts based on the dates when the genomes were discovered. We used the genomes discovered before January 2014 for training, those between January 2014 and May 2015 for validation, and those after May 2015 for testing. The partitioning of the dataset not only avoids the overlaps between the training, validation, and test datasets, but also helps to evaluate the methods ability for predicting future new viruses based on the previously discovered viruses. We previously used the data before May 2015 in Ren et al. [5]. For this study, we updated the dataset to include new viruses after May 2015 and it was natural to use them as the test data. Since sequences in real metagenomics data are of various lengths ranging from hundreds to thousands of base pairs, we fragmented the genomes into non- overlapping sequences of different sizes, L = 150, 300, 500, 1000, and 3000 bp. We then built models for sequences of each size, respectively. In particular, the models for 150 and 300 bp were designed for the next generation sequencing technology, which commonly generates sequences of those fixed lengths. Table 1 shows the numbers of sequences in different sizes that were used for training, validation, and test. The dataset is paired with the same number of prokaryotic sequences, fragmented from 38,234 RefSeq and partitioned by the exact same dates. The objective function is to minimize the binary cross- entropy loss between the predicted score Yfinal and the true labels (0 for prokaryotic sequences, and 1 for virus sequences). The training dataset is iteratively fed into the model in batches of size 150. One iteration of finishing feding batches of the whole training dataset is called one epoch. The parameters in the neural networks were updated through back-propagation using Adam optimiza- tion algorithm for stochastic gradient descent with learning rate 0.001 [61]. Dropout regularization of rate 0.1 are applied after the max pooling layer, and after the fully connected layer, to reduce overfitting in neural networks by randomly dropping out a few dimensions. This convolutional neural network has three critical hyper parameters, the length of motifs (or filters), the number of motifs, and the number of epochs for training. The first two determine the complexity of the model, and the third one controls the balance between overfitting and underfitting. To find the best parameter settings, we trained the model with different combinations of the three parameters using the data before January 2014, and evaluated the model performance using AUROC on the validation dataset. Collection of metavirome datasets To achieve high prediction accuracy, a deep learning algorithm needs a large amount of training data. Though a large number of training sequences were obtained from RefSeq, there is a potential to enlarge the training dataset by including viral sequences from metavirome sequen- cing data. Metavirome sequencing targets at sequencing mainly viruses by removing prokaryotic cells in samples using the physical 0.22 μm filters. Metavirome sequen- cing does not rely on culturing viruses in the lab, so it is able to capture both cultivated and uncultivated viruses, representing the true viral diversity. A few studies have used this technique to extract viruses and sequenced viral genomes in human gut and ocean samples [1,2,62,63]. Normal et al. sequenced virome in the human gut sample from IBD patients using Illumina sequencing technology [1]. Reyes et al. studied viruses in fecal samples from Malawian twins with Severe Acute Malnutrition (SAM) using Roche 454 sequencing technology [2]. Minot et al. and Kim et al. investigated virome in healthy human gut using Roche 454 [11,62]. For marine virome, the Tara Ocean Virome project collected the largest number of virome samples from both surface- and deep-ocean sites over the world [63]. We collected the metavirome samples from those studies and aimed to add more viral diversity, especially adding viruses not- or under-represented in RefSeq, to the training data. We were careful in quality control of the samples because it is likely that the sample can be contaminated by prokaryotic DNA, since the physical filters may not exclude small sized prokaryotic cells. The details of preparation of metavirome data and quality control can be found in Supplementary Materials and Supplementary Table S3. Up to 1.3 million of sequences were generated from the metavirome data, and they were combined with sequences derived from viral RefSeq before May 2015 for training. The same number of prokaryotic sequences were paired with the viral sequences in the enlarged dataset for training. The new model was evaluated and compared with the original model trained based on RefSeq only, using the test sequences from RefSeq after May 2015. Simulation of metagenomic datasets To assess the performance of DeepVirFinder trained previously using RefSeq on predicting viruses in metagenomic samples, we generated sythetic metage- nomic samples based on organism abundance profiles derived from a real human gut metagenomic sample (accession ID SRR061166, Platform: Illumina). Given a total budget of base pairs in a sample, the number of base pairs in contigs sampled from each genome was computed proportionally to the abundance profile. For each reference genome, contigs were sampled randomly and independently from the genome, where the contig length follows the same distribution as that in a real human metagenomics dataset for CRC patients (Fig. 4A), until the number of base pairs reaches the total budget. The details can be found in Supplementary Materials. We constructed metagenomic samples with different viral fractions and evaluated DeepVirFinder on each of them. In metagenomic sequencing experiments, there are two major types of genome sampling strategies. One is referred to as cellular metagenomes in which all the genetic materials, including bacteria, archaea, and viruses, are sampled and sequenced. Another type of data is metavirome where cellular organisms like bacteria are filtered out first before sequencing and mostly viruses are sequenced. To mimic the different viral fractions in real metagenomic data, the abundance profile was rescaled to make samples of three different viral fractions 10%, 50%, and 90%, while keeping the relative abundance within viruses and that within hosts the same. Viral analysis of human gut metagenomics from patients with colorectal cancer Human gut metagenomics samples from patients with CRC and the control group were downloaded from European Nucleotide Archive (ENA) database (see the website:www.ebi.ac.uk/ena) with accession number ERP005534. Samples from 53 cancer patients and 61 normal individuals were randomly split into 2/3 for training and 1/3 for testing. The patient ID and the disease status can be found in the Supplementary Materials. The metagenomics samples from training were combined and cross-assembled. To guarantee high accuracies in the downstream analysis including virus contig identification and contig binning, we filtered contigs smaller than 500 bp. DeepVirFinder was then applied to predict viral contigs in the remaining dataset. To control the false discovery rate, the predicted p-value for each contig was converted to a q-value. The q-value is an estimation of the proportion of false prediction if the prediction is made at the level of the corresponding p-value. Contigs were sorted by q-values from the smallest to the largest, and the contigs having q-values < 0.01 were predicted as viruses. The viral contigs predicted by DeepVirFinder were then grouped into contig bins, and the abundance of contig bins was derived based on the read mapping results. To study the association between the viruses and the cancer status, we built a logistic regression classifier with Lasso penalty to predict the CRC status based on the bin abundance on training data, and evaluated the performance on test data. The details can be found in the Supplementary Materials.",VirusIdentification,"identify viruses  metagenomic data use deep learning
viruses  prokaryotic genomes use  train validation  test  collect  refseq  viruses infect prokar yotes bacteria  archaea  ncbi  ncbinlmnihgovgenomebrowse  dataset  parti tioned  three part base   date   genomes  discover  use  genomes discover  january   train   january   may   validation    may   test  partition   dataset   avoid  overlap   train validation  test datasets  also help  evaluate  methods ability  predict future new viruses base   previously discover viruses  previously use  data  may   ren      study  update  dataset  include new viruses  may     natural  use    test data since sequence  real metagenomics data   various lengths range  hundreds  thousands  base pair  fragment  genomes  non overlap sequence  different size            build model  sequence   size respectively  particular  model       design   next generation sequence technology  commonly generate sequence   fix lengths table  show  number  sequence  different size   use  train validation  test  dataset  pair    number  prokaryotic sequence fragment   refseq  partition   exact  date  objective function   minimize  binary cross entropy loss   predict score yfinal   true label   prokaryotic sequence    virus sequence  train dataset  iteratively feed   model  batch  size  one iteration  finish feding batch   whole train dataset  call one epoch  parameters   neural network  update  backpropagation use adam optimiza tion algorithm  stochastic gradient descent  learn rate   dropout regularization  rate   apply   max pool layer    fully connect layer  reduce overfitting  neural network  randomly drop    dimension  convolutional neural network  three critical hyper parameters  length  motifs  filter  number  motifs   number  epochs  train  first two determine  complexity   model   third one control  balance  overfitting  underfitting  find  best parameter settings  train  model  different combinations   three parameters use  data  january   evaluate  model performance use auroc   validation dataset collection  metavirome datasets  achieve high prediction accuracy  deep learn algorithm need  large amount  train data though  large number  train sequence  obtain  refseq    potential  enlarge  train dataset  include viral sequence  metavirome sequen cing data metavirome sequence target  sequence mainly viruses  remove prokaryotic cells  sample use  physical   filter metavirome sequen cing   rely  culture viruses   lab    able  capture  cultivate  uncultivated viruses represent  true viral diversity   study  use  technique  extract viruses  sequence viral genomes  human gut  ocean sample  normal   sequence virome   human gut sample  ibd patients use illumina sequence technology  reyes   study viruses  fecal sample  malawian twin  severe acute malnutrition sam use roche  sequence technology  minot    kim   investigate virome  healthy human gut use roche    marine virome  tara ocean virome project collect  largest number  virome sample   surface  deepocean sit   world   collect  metavirome sample   study  aim  add  viral diversity especially add viruses   underrepresented  refseq   train data   careful  quality control   sample    likely   sample   contaminate  prokaryotic dna since  physical filter may  exclude small size prokaryotic cells  detail  preparation  metavirome data  quality control   find  supplementary materials  supplementary table     million  sequence  generate   metavirome data    combine  sequence derive  viral refseq  may   train   number  prokaryotic sequence  pair   viral sequence   enlarge dataset  train  new model  evaluate  compare   original model train base  refseq  use  test sequence  refseq  may  simulation  metagenomic datasets  assess  performance  deepvirfinder train previously use refseq  predict viruses  metagenomic sample  generate sythetic metage nomic sample base  organism abundance profile derive   real human gut metagenomic sample accession  srr061166 platform illumina give  total budget  base pair   sample  number  base pair  contigs sample   genome  compute proportionally   abundance profile   reference genome contigs  sample randomly  independently   genome   contig length follow   distribution     real human metagenomics dataset  crc patients fig    number  base pair reach  total budget  detail   find  supplementary materials  construct metagenomic sample  different viral fraction  evaluate deepvirfinder      metagenomic sequence experiment   two major type  genome sample strategies one  refer   cellular metagenomes     genetic materials include bacteria archaea  viruses  sample  sequence another type  data  metavirome  cellular organisms like bacteria  filter  first  sequence  mostly viruses  sequence  mimic  different viral fraction  real metagenomic data  abundance profile  rescale  make sample  three different viral fraction      keep  relative abundance within viruses   within host   viral analysis  human gut metagenomics  patients  colorectal cancer human gut metagenomics sample  patients  crc   control group  download  european nucleotide archive ena database see  websitewwwebiacukena  accession number erp005534 sample   cancer patients   normal individuals  randomly split    train    test  patient    disease status   find   supplementary materials  metagenomics sample  train  combine  crossassembled  guarantee high accuracies   downstream analysis include virus contig identification  contig bin  filter contigs smaller    deepvirfinder   apply  predict viral contigs   remain dataset  control  false discovery rate  predict pvalue   contig  convert   qvalue  qvalue   estimation   proportion  false prediction   prediction  make   level   correspond pvalue contigs  sort  qvalues   smallest   largest   contigs  qvalues    predict  viruses  viral contigs predict  deepvirfinder   group  contig bin   abundance  contig bin  derive base   read map result  study  association   viruses   cancer status  build  logistic regression classifier  lasso penalty  predict  crc status base   bin abundance  train data  evaluate  performance  test data  detail   find   supplementary materials",3
65,ViraMiner,"ViraMiner: Deep learning on raw DNA sequences for identifying viral genomes in human samples
Samples and sequencing types The metagenomic sequences in this work were generated using Next Generation Sequencing platforms such as the NextSeq, Miseq, and HiSeq (Illumina) as described by the manufacturer. The dataset was derived from human samples belonging to different patient groups that have been described in detail in [6–8, 30–32]. The goal of those analyses was to detect viral genomes or other microorganisms in diseased individuals or in their matched control subjects. Bioinformatics All the sequencing experiments were processed and analyzed using a benchmarked bioinformatics workflow [33]. To summarize, we start analysis with quality checking where reads are filtered based on their Phred quality score. Afterwards, quality checked reads that align to human, bacterial, phage and vector DNA with >95% identity over 75% of their length are subtracted from further analysis using BWA-MEM [34]. The reads that are left are normalized and then assembled using the IDBA-UD [35], Trinity [36], SOAPdenovo, and SOAPdenovo-Trans [37] assemblers. The assembled contigs are then subjected to taxonomic classification using BLAST. The code of the pipeline is available on GitHub (https://github.com/NGSeq/ViraPipe and https://github.com/NIASC/VirusMeta). Data processing and labeling The training dataset included 19 different NGS experiments that were analyzed and labeled by PCJ-BLAST [38] after applying the de novo genome assembly algorithms. Parameters for PCJ-BLAST were as follows: type of algorithm = Blastn; nucleotide match reward = 1; nucleotide mismatch penalty = 1; cost to open a gap = 0; cost to extend a gap = 2; e-value ≤ e−4. All assembled contigs that were labeled by this bioinformatics workflow were merged to train several machine learning algorithms. To extract input dataset for Convolutional Neural Networks, labeled sequences were divided into equal pieces (300bp and 500bp), each one of them was labeled as the original sequence and remaining nucleotides at the end of the contigs were removed from the further analysis. For example, according to this processing 650 bp viral contig would produce two equal 300bp viral sequences and the last 50 nucleotides would be removed from the input dataset. Initially, we generated two training datasets based on sequence lengths: first with 300bp and the second with 500bp. After noticing that 300bp contigs produced significantly better results we continued working only on this dataset and dropped the other. Furthermore, all contigs that contained even one “N” letter (reference to any nucleotide) were removed from the further analysis. The most common genes found in the viral sequences in our metagenomic datasets are listed in S1 Table. The most common viral classes are listed in S2 Table. For computing baselines, we also counted k-mers in the processed dataset. Given that extracting k-mers becomes more and more computationally expensive as k increases, we conducted distributed and parallel computing of k-mers by using Hadoop (https://hadoop.apache.org) and Spark [39]. The Spark code (available under https://github.com/NIASC/ViraPipeV2) outputs 4k × l matrix where l is number of rows from an input dataset. Machine learning methods Convolutional neural networks (CNNs). Convolutional neural networks (CNNs) are a type of feed-forward neural networks [40, 41]. In addition to fully-connected layers that have all-to-all connections, CNNs by definition also contain convolutional layers. Convolutional layers have partial connectivity and use the same parameters repeatedly [40]. In supervised learning settings, CNNs learn to perform a task by observing input-output pairs. The learning is achieved by minimizing the error (between model’s output and the known true output) via gradient descent [40, 42]. CNNs are most widely used in image processing [40, 43], but have been successfully applied in various fields [44, 45], including analysis of biological sequences such as DNA and amino acid sequences [46–48]. The convolutional layers treat their inputs as multidimensional arrays—an image is treated as a 2D array with 3 color channels, while a DNA sequence is seen as a 1D array with one channel per possible nucleotide value. In the present work we use sequences of length 300 and consider 5 possible values (ATGCN) at each position, corresponding to a 1D sequence of length 300 with 5 channels (as depicted in Fig 1). A convolutional layer is characterized by a set of learnable filters that are convolved along the input DNA sequence (or along the width and height of an image). Filters are smaller than the input and can be applied at multiple locations along the input. At each position where the filter is applied, a dot product between the weights of the filter and the local input values is computed. The dot products resulting from applying the same filter to different positions along the sequence form a feature map. A convolutional layer usually applies many filters and hence its output is a set of feature maps, one per filter. On this output, further computations can be performed such as applying an activation function, pooling or adding further layers. The parameter values in the filters, like all other learnable parameters in the model, are optimized by the gradient descent algorithm. CNN architectures for predicting the viral nature of sequences. In this work we use a convolutional neural network called ViraMiner to predict whether a given DNA sequence is of a viral origin or not. This model contains two convolutional branches that contribute different types of information to the final fully-connected decision-making layer This model is partly based on the DeepVirFinder (DVF) architecture by Ren et al [24]. The DVF model is a Convolutional Neural Network (CNN) that applies the max operator on the outputs of each convolutional filter (on each feature map). Only the one maximal activation value per filter is passed on to the next layers. All other information, such as how often a good match was found, is lost. This architecture is illustrated in Fig 6c. In this work we suggest that a convolutional layer followed by average operator instead of max operator provides important information about the frequency of patterns, information that is otherwise lost. In such a case, while losing information about the maximal activation (best match), we gain information about frequency—the average cannot be high if only a few good matches were found. In previous work the authors of DVF and the authors of the current article have shown that methods based on pattern frequency (k-mer counts, relative synonymous codon usage) are effective in separating viral samples from non-viral ones [24, 28]. Using convolution + average is a natural extension to these pattern counting-based models. This results in the architecture in Fig 6b. To clarify, we do not claim that using convolution+max is without merit, it simply captures a different type of information. In fact, in our ViraMiner model we combine feature maps processed by averaging and by max operators. This allows the model to base its decisions on both pattern-matching and pattern-frequency information. The ViraMiner detailed architecture is shown in Fig 6a where Frequency Branch and Pattern Branch refer to the architectures in Fig 6b and 6c, without the input and output layers. Training. It is important to note that Pattern and Frequency models can be used as separate models and trained independently. In the first step of our training scheme we train these two models, then remove the output layers and use the middle layers as Frequency and Pattern branches in the full ViraMiner model. In the second step we optimize only the parameters in the final layer of the ViraMiner architecture, leaving the weight and bias values in the branches unchanged, exactly as they were in the independent models. Notice that in this last step of learning, values of only roughly two thousand parameters are changed. This step-wise training procedure helps reduce overfitting compared to simpler approaches. The simplest alternative training strategy consists in randomly initializing and optimizing all the parameters (both branches and the final layer) at the same time. This standard end-to-end approach allows the model to freely use all of its capacity to minimize the error in training data points. However, too much capacity might lead to lower generalization ability. Indeed, with this training procedure ViraMiner model strongly overfits and performs significantly worse on unseen data. Fine-tuning is another alternative training procedure where the branches are initialized with pre-trained values, but the values are not frozen. This means that when training the full ViraMiner model both the final layer and the branches are optimized. The re-optimization of pre-trained weights gives the name fine-tuning. Similarly to end-to-end training we see overfitting in this intermediate training procedure. Therefore, the models in the Results section are obtained with step-wise training procedure, rather than end-to-end or fine-tuning. Hyperparameter search and testing. We merged and shuffled the data originating from 19 metagenomics sequencing experiments and divided it into training (80%), validation (10%) and test sets (10%). We then performed an extensive hyperparameter search for the Pattern and Frequency models separately. For both architectures, hundreds of variants were trained using the 80% of data in training set. The performance of these models was measured by AUROC on validation set. The models were trained until validation AUROC had not increased for 6 consecutive epochs, but not longer than 30 epochs. After each epoch the model was saved only if validation AUROC had increased. Adam optimizer [49] and batch size 128 were used in all cases. The initial hyperparameter search scanned the following parameters: Filter size: in range from 4 to 40 with step of 4. Learning rate: 0.01, 0.001 and 0.0001. Each value was tried with and without learning rate decay, which multiplies learning rate by 0.5 after each 5 epochs. Layer sizes (applies both to number of filters in convolutional layers and to number of nodes in fully connected layer): 1000, 1200 or 1500 Dropout probability: 0.1 or 0.5 In a second stage of hyperparameter search, each possible filter size value (with step of 1) was scanned in the region of interest determined by initial scan (8 to 16 for Frequency model; 6 to 14 for Pattern model). For other hyperparameters, some values were left out from the second stage as they clearly worsened the performance. When the extensive hyperparameter scan was completed, the best Pattern model and best Frequency model (as per validation AUROC) were selected and used to initialize the branches in the ViraMiner architecture. We then trained the ViraMiner model with different learning rates, learning rate decays and with/without fine-tuning the branches and again selected the best model according to validation AUROC. Having selected best models according to validation performance, we finally evaluated the best Pattern, best Frequency and best ViraMiner model on the test set which had been left unused up until this point. Baselines. Using 80/10/10 split, we also trained two types of baseline models. First of all, using k-mer counts extracted from the contigs as input, we trained Random Forest models with 1000 trees (all other parameters as default in scikit-learn Python package [50]) for k in range from 3 to 7. In the Results section, we report each model’s test AUROC. Secondly, we also trained a Random Forest model directly on the 300bp sequences, without any feature extraction. For that we one-hot encoded the sequences and flattened them, resulting in 1200 inputs, 4 per each position in the sequence. Notice that such model is not position invariant—a shift of the sequence by just one base pair would completely change the output. Simulated data. Simulated pair-ended reads for this study were generated with ART simulator [29]. The reference file for the simulation included 30 thousand genomes from GenBank consisting 2870 viral and 27 130 cellular organisms (mammals, plants, vertebrates and bacterias). Parameters for the simulator were as follows: -ss MSv1 -p -l 150 -f 20 -m 200 -s 10. The simulation produced appproximately 13 million paired reads which in turn were assembled into 46 498 contigs by using IDBA-UD assembly [35]. Afterwards, we applied the same data processing and labeling pipeline as described for the 19 metagenomic datasets.",VirusIdentification,"viraminer deep learn  raw dna sequence  identify viral genomes  human samples
samples  sequence type  metagenomic sequence   work  generate use next generation sequence platforms    nextseq miseq  hiseq illumina  describe   manufacturer  dataset  derive  human sample belong  different patient group    describe  detail     goal   analyse   detect viral genomes   microorganisms  diseased individuals    match control subject bioinformatics   sequence experiment  process  analyze use  benchmarked bioinformatics workflow   summarize  start analysis  quality check  read  filter base   phred quality score afterwards quality check read  align  human bacterial phage  vector dna   identity     length  subtract   analysis use bwamem   read   leave  normalize   assemble use  idbaud  trinity  soapdenovo  soapdenovotrans  assemblers  assemble contigs   subject  taxonomic classification use blast  code   pipeline  available  github    data process  label  train dataset include  different ngs experiment   analyze  label  pcjblast   apply   novo genome assembly algorithms parameters  pcjblast   follow type  algorithm  blastn nucleotide match reward   nucleotide mismatch penalty   cost  open  gap   cost  extend  gap   evalue ≤   assemble contigs   label   bioinformatics workflow  merge  train several machine learn algorithms  extract input dataset  convolutional neural network label sequence  divide  equal piece 300bp  500bp  one    label   original sequence  remain nucleotides   end   contigs  remove    analysis  example accord   process   viral contig would produce two equal 300bp viral sequence   last  nucleotides would  remove   input dataset initially  generate two train datasets base  sequence lengths first  300bp   second  500bp  notice  300bp contigs produce significantly better result  continue work    dataset  drop   furthermore  contigs  contain even one “” letter reference   nucleotide  remove    analysis   common genes find   viral sequence   metagenomic datasets  list   table   common viral class  list   table  compute baselines  also count kmers   process dataset give  extract kmers become    computationally expensive   increase  conduct distribute  parallel compute  kmers  use hadoop   spark   spark code available   output    matrix    number  row   input dataset machine learn methods convolutional neural network cnns convolutional neural network cnns   type  feedforward neural network    addition  fullyconnected layer   alltoall connections cnns  definition also contain convolutional layer convolutional layer  partial connectivity  use   parameters repeatedly   supervise learn settings cnns learn  perform  task  observe inputoutput pair  learn  achieve  minimize  error  model output   know true output via gradient descent   cnns   widely use  image process      successfully apply  various field   include analysis  biological sequence   dna  amino acid sequence   convolutional layer treat  input  multidimensional arrays— image  treat    array   color channel   dna sequence  see    array  one channel per possible nucleotide value   present work  use sequence  length   consider  possible value atgcn   position correspond    sequence  length    channel  depict  fig   convolutional layer  characterize   set  learnable filter   convolve along  input dna sequence  along  width  height   image filter  smaller   input    apply  multiple locations along  input   position   filter  apply  dot product   weight   filter   local input value  compute  dot products result  apply   filter  different position along  sequence form  feature map  convolutional layer usually apply many filter  hence  output   set  feature map one per filter   output  computations   perform   apply  activation function pool  add  layer  parameter value   filter like   learnable parameters   model  optimize   gradient descent algorithm cnn architectures  predict  viral nature  sequence   work  use  convolutional neural network call viraminer  predict whether  give dna sequence    viral origin    model contain two convolutional branch  contribute different type  information   final fullyconnected decisionmaking layer  model  partly base   deepvirfinder dvf architecture  ren     dvf model   convolutional neural network cnn  apply  max operator   output   convolutional filter   feature map   one maximal activation value per filter  pass    next layer   information    often  good match  find  lose  architecture  illustrate  fig    work  suggest   convolutional layer follow  average operator instead  max operator provide important information   frequency  pattern information   otherwise lose    case  lose information   maximal activation best match  gain information  frequency— average cannot  high     good match  find  previous work  author  dvf   author   current article  show  methods base  pattern frequency kmer count relative synonymous codon usage  effective  separate viral sample  nonviral ones   use convolution  average   natural extension   pattern countingbased model  result   architecture  fig   clarify    claim  use convolutionmax  without merit  simply capture  different type  information  fact   viraminer model  combine feature map process  average   max operators  allow  model  base  decisions   patternmatching  patternfrequency information  viraminer detail architecture  show  fig   frequency branch  pattern branch refer   architectures  fig    without  input  output layer train   important  note  pattern  frequency model   use  separate model  train independently   first step   train scheme  train  two model  remove  output layer  use  middle layer  frequency  pattern branch   full viraminer model   second step  optimize   parameters   final layer   viraminer architecture leave  weight  bias value   branch unchanged exactly      independent model notice    last step  learn value   roughly two thousand parameters  change  stepwise train procedure help reduce overfitting compare  simpler approach  simplest alternative train strategy consist  randomly initialize  optimize   parameters  branch   final layer    time  standard endtoend approach allow  model  freely use    capacity  minimize  error  train data point however  much capacity might lead  lower generalization ability indeed   train procedure viraminer model strongly overfits  perform significantly worse  unseen data finetuning  another alternative train procedure   branch  initialize  pretrained value   value   freeze  mean   train  full viraminer model   final layer   branch  optimize  reoptimization  pretrained weight give  name finetuning similarly  endtoend train  see overfitting   intermediate train procedure therefore  model   result section  obtain  stepwise train procedure rather  endtoend  finetuning hyperparameter search  test  merge  shuffle  data originate   metagenomics sequence experiment  divide   train  validation   test set    perform  extensive hyperparameter search   pattern  frequency model separately   architectures hundreds  variants  train use    data  train set  performance   model  measure  auroc  validation set  model  train  validation auroc   increase   consecutive epochs   longer   epochs   epoch  model  save   validation auroc  increase adam optimizer   batch size   use   case  initial hyperparameter search scan  follow parameters filter size  range      step   learn rate      value  try   without learn rate decay  multiply learn rate      epochs layer size apply   number  filter  convolutional layer   number  nod  fully connect layer     dropout probability      second stage  hyperparameter search  possible filter size value  step    scan   region  interest determine  initial scan     frequency model     pattern model   hyperparameters  value  leave    second stage   clearly worsen  performance   extensive hyperparameter scan  complete  best pattern model  best frequency model  per validation auroc  select  use  initialize  branch   viraminer architecture   train  viraminer model  different learn rat learn rate decay  withwithout finetuning  branch   select  best model accord  validation auroc  select best model accord  validation performance  finally evaluate  best pattern best frequency  best viraminer model   test set    leave unused    point baselines use  split  also train two type  baseline model first   use kmer count extract   contigs  input  train random forest model   tree   parameters  default  scikitlearn python package     range       result section  report  model test auroc secondly  also train  random forest model directly   300bp sequence without  feature extraction    onehot encode  sequence  flatten  result   input  per  position   sequence notice   model   position invariant— shift   sequence   one base pair would completely change  output simulate data simulate pairended read   study  generate  art simulator   reference file   simulation include  thousand genomes  genbank consist  viral    cellular organisms mammals plant vertebrates  bacterias parameters   simulator   follow  msv1           simulation produce appproximately  million pair read   turn  assemble    contigs  use idbaud assembly  afterwards  apply   data process  label pipeline  describe    metagenomic datasets",3
66,VirFinder,"VirFinder: a novel k-mer based tool for identifying viral sequences from assembled metagenomic data
Viruses and prokaryotic host genomes used for training and validation We collected 1562 virus RefSeq genomes infecting prokaryotes and 31,986 prokaryotic host RefSeq genomes from NCBI in May 2015. The NCBI accession numbers of the RefSeq sequences are provided in the Additional file 2: Table S2. To mimic fragmented metagenomic sequences, for a given length L = 500, 1000, 3000, 5000, and 10000 bp, viruses were split into non-overlapping fragments of length L and the same number of non-overlapping fragments of length L were randomly subsampled from the prokaryotic genomes. Fragments were generated for virus genomes discovered before 1 January 2014 and after 1 January 2014 and were separately used as training and testing sets, respectively (Table 1). To generate evaluation datasets containing 10, 50, and 90% viral contigs, the number of viral contigs was set as in Table 1 and was combined with 9 times more, equal numbers, or 9-fold less randomly sampled host contigs, respectively. Highly represented host phyla (Actinobacteria, Cyanobacteria, Firmicutes, Proteobacteria) and genera (Mycobacterium, Escherichia, Pseudomonas, Staphylococcus, Bacillus, Vibrio, and Streptococcus) were selected for the analyses where viruses infecting these taxa were excluded from the training of VirFinder. For evaluation of the different trained VirFinder models, equal numbers of contigs of the excluded viruses and all other viruses were selected and then combined with randomly selected host contigs such that total virus and host contigs were equal in number. For the analysis of VirFinder trained with 14,722 prokaryotic genomes with or without proviruses removed, these genomes were downloaded from the database cited in [6]. Likewise, the positions of proviruses predicted by VirSorter in these 14,722 genomes were obtained from the published data of [6] and were used to remove theses sequence from their corresponding host genomes. The k-mer based machine learning prediction model For a fragment sequence S, let N(w) be the number of occurrences of the word w = w 1 w 2 … w k and its complimentary word 𝐰⎯⎯⎯⎯, 𝑤𝑖∈≡{𝐴,𝐶,𝐺,𝑇},𝑖=1,2,…,𝑘. For simplicity, we use word w to refer to the word patterns w and its compliment 𝐰⎯⎯⎯⎯. We defined the sequence signatures as the normalized word frequencies, 𝑉(𝐰)=𝑁(𝐰)∑𝑤𝑁(𝐰),𝐰∈𝑘. Duplicated word pairs were removed as in [6, 39, 69]. For example, for k = 4, only the 136 unique word pattern pairs are used. Based on sequence signatures, a binary classifier for identifying viral sequences was built. The classifier was trained using the training data and then was evaluated using the testing data. Given a training dataset composed of the same number of viral sequences and host sequences, we first used t statistic to test for each word w if the mean word frequency in viral sequences was significantly different from that in host sequences. Note that V(w) were subjected to the unit sum constraint ∑𝐰𝑉(𝐰)=1. To overcome the problem of multicollinearity, we excluded the word with the highest p value (the least significant word). Then based on the selected words, we used the logistic regression model to build a binary classifier. We added a lasso regularization to make the model flexible and let the data choose the model with the highest accuracy. Let S 1, S 2, …, S n be n sequences. Let Y i  = 1 if S i comes from viral sequences and Y i  = 0 if it is from host sequences, and V i (w) is the sequence signatures of S i , i = 1, …, n. Then we model, log⎛⎝⎜⎜⎜𝑃(𝑌𝑖=1∣∣∣𝑉𝑖(𝐰))1−𝑃(𝑌𝑖=1∣∣∣𝑉𝑖(𝐰))⎞⎠⎟⎟⎟=∑𝐰∈𝑘𝛽(𝐰)𝑉𝑖(𝐰)+𝛽0, or in other words, 𝑃(𝑌𝑖=1∣∣∣𝑉𝑖(𝐰))=exp(∑𝐰∈𝑘𝛽(𝐰)𝑉𝑖(𝐰)+𝛽0)1+exp(∑𝐰∈𝑘𝛽(𝐰)𝑉𝑖(𝐰)+𝛽0). Thus, the objective function is −1n∑𝑖=1𝑛log𝑙(𝑌𝑖∣∣∣𝑉𝑖(𝐰),𝛽(𝐰), 𝛽0)+𝜆∑𝑤∈𝑘|𝛽(𝐰)|, where l is the likelihood, β is estimated by minimizing the objective function. We chose the parameter λ to have the highest AUROC using 10-fold cross validation on the training data. The R package “glmnet” was used for the model training and testing [41]. ROCs were plotted using R package “ROCR” [70] and AUC scores, and its confidence interval were computed using R package “pROC” [71]. In real metagenomic experiments, the assembled contigs are of various lengths. In order to compare scores from different prediction models, for each query contig, a p value was computed by comparing the score with the null distribution, that is, the distribution of scores of the testing host contigs. The p value was computed as the fraction of testing host contigs that have greater scores than the score of the query sequence. To estimate the false discovery rate (the proportion of predictions that are hosts), we used R package “qvalue” [37, 38] to estimate false discovery rates based on the p values. Then each query contig was associated with a false discovery rate, also known as the q value. The contigs were sorted by q values from the smallest to the largest. Given a threshold, the contigs with q values smaller than the threshold were predicted as viral sequences, and the largest q value among the predicted contigs gave the estimation of the false discovery rate for the prediction. Simulation studies on metagenomes Metagenomic samples were simulated based on species abundance profiles derived from a real human gut metagenomic sample (accession ID SRR061166, Platform: Illumina) from the Human Microbiome Project (HMP) [72], commonly used for metagenomic data analysis [73,74,75,76]. Following a similar simulation procedure as in [77], we first mapped reads from sample SRR061166 using bwa-0.7.15 [78] to 1562 virus and 2698 host complete genome sequences downloaded from NCBI RefSeq to generate abundance profiles. The reads from the sample were first mapped to viral genomes and then the remaining unmapped reads were mapped to the host complete genomes using the command of “bwa mem”. 10% of reads mapped to viral genomes, consistent with the range previously reported for human gut metagenomes (4–17% viral) [7]. The abundance profiles are provided in the Additional file 3: Table S3. Then we used NeSSM [33] to simulate metagenomic samples with pair-end short reads of length 150 bp in an Illumina MiSeq setting mode based on the abundance profiles. Ten and 20 million read samples were generated at 3 different mixtures of virus and prokaryotic sequences. The relative abundance among viruses and among hosts were kept the same and the virus and hosts reads were mixed to make 10 (the native level in sample SRR061166 determined from mapping), 50, and 90% viral samples. metaSPAdes [34, 35] was used to de-novo assemble the simulated metagenome samples, using the command “spades.py –meta”. Only contigs ≥500 bp were used for the downstream analysis. To obtain the true labels of the assembled contigs, reads in the simulated data were mapped to the set of contigs using “bwa mem”. A contig was labeled as a viral contig if it was assembled from reads only from viral genomes; similarly, a contig was labeled as a host contig if it was assembled from reads from host genomes. A contig was labeled as chimeric if it was assembled from a mixture of virus and host reads. To validate the prediction, we plotted ROC (receiver operating characteristic) curves at different ranges of contig lengths, 500–1000 bp, 1000–3000 bp and ≥3000 bp. The ROC curves were based on the predictions of viral contigs from genomes sequenced after 1 January 2014 paired with the same number of randomly sampled host contigs. VirSorter settings VirSorter was run in the “Viromes” mode on the same sets of evaluation sequences as used for VirFinder. VirSorter reported predicted viral sequences in three categories: I for “most confident” predictions, II for “likely” predictions, and III for “possible” predictions. Assembly and analysis of human gut metagenomic samples from liver cirrhosis study The data from Qin et al. [36] contains two independent datasets each of which contains Illumina 2 × 100 bp paired read metagenomes of stool samples from both healthy individuals and liver cirrhosis patients, all of Han Chinese origin. These metagenomes were downloaded from the European Nucleotide Archive, accession number ERP005860. The first dataset, referred to as the “training set”, has 78 samples comprised of 40 samples from 31 healthy patients and 38 samples from 25 liver cirrhosis patients. The second dataset, referring to as the “testing set”, has 230 samples comprised of 103 samples from 83 healthy patients and 127 samples from 98 liver cirrhosis patients. Megahit [79] was used to cross-assemble the 78 sample training dataset using the default settings since the 230 sample dataset was too large for assembly. COCACOLA [39] was used to separately cluster viral contigs predicted by VirFinder and VirSorter based on sequence tetranucleotide frequencies and contig coverages normalized by contig length and number of mapped reads in samples. Contig coverages (RPKMs) were determined by mapping sample reads with Bowtie2 [80] using the default settings and were averaged for each bin. Averaged bin RPKMs were used to train a classification model to classify the disease status (0 for healthy and 1 for liver cirrhosis). A logistic regression model with lasso regularization was used in order to enhance the prediction accuracy and interpretability. Thus, a subset of viral bins was chosen to achieve the best prediction accuracy. To assess the classification model, the average RPKM of bins in the second dataset with 230 samples were used to test the classification model, and ROC curves were used for evaluation. Two-way hierarchical clustering was performed using the average RPKM coverages of the 116 VirFinder contig bins using all 78 training set samples and 78 samples randomly selected from the 230 sample testing dataset. Distances were computed using Euclidean distance and were clustered with complete linkage method in R. Blast analyses were used to assess if predicted viral contigs assembled from the cirrhosis study samples had similarity to previously reported sequences. Blastn and blastp searches were performed with default settings against NCBI’s non-redundant nucleotide (nt) and protein (nr) databases from August 2016. Protein sequences were predicted for each contig using Prodigal [81, 82] with the “meta” procedure (−p meta). The best hits for each contig (nucleotide) or each predicted protein on the contigs were retained. Resulting proteins in the nr databases were called as viral if they came from a virus (recorded in their taxonomy) or had one of the following terms in their definition lines: virus, phage, capsid, tail, head, or terminase. Proteins were also searched against Pfam domains via the webserver at http://pfam.xfam.org/. Resulting domains were considered viral if their description contained one of the following terms: virus, phage, capsid, tail, head, tape, terminase, Gpnn (where n are digits), “podo”, or “sipho”.",VirusIdentification,"virfinder  novel kmer base tool  identify viral sequence  assemble metagenomic data
viruses  prokaryotic host genomes use  train  validation  collect  virus refseq genomes infect prokaryotes   prokaryotic host refseq genomes  ncbi  may   ncbi accession number   refseq sequence  provide   additional file  table   mimic fragment metagenomic sequence   give length          viruses  split  nonoverlapping fragment  length     number  nonoverlapping fragment  length   randomly subsampled   prokaryotic genomes fragment  generate  virus genomes discover   january     january    separately use  train  test set respectively table   generate evaluation datasets contain     viral contigs  number  viral contigs  set   table    combine   time  equal number  fold less randomly sample host contigs respectively highly represent host phyla actinobacteria cyanobacteria firmicutes proteobacteria  genera mycobacterium escherichia pseudomonas staphylococcus bacillus vibrio  streptococcus  select   analyse  viruses infect  taxa  exclude   train  virfinder  evaluation   different train virfinder model equal number  contigs   exclude viruses    viruses  select   combine  randomly select host contigs   total virus  host contigs  equal  number   analysis  virfinder train   prokaryotic genomes   without proviruses remove  genomes  download   database cite   likewise  position  proviruses predict  virsorter    genomes  obtain   publish data     use  remove theses sequence   correspond host genomes  kmer base machine learn prediction model   fragment sequence  let    number  occurrences   word       …     complimentary word ⎯⎯⎯⎯ ∈≡{𝐴𝐶𝐺𝑇}…  simplicity  use word   refer   word pattern    compliment ⎯⎯⎯⎯  define  sequence signatures   normalize word frequencies 𝑉𝐰𝑁𝐰∑𝑤𝑁𝐰𝐰∈ duplicate word pair  remove       example        unique word pattern pair  use base  sequence signatures  binary classifier  identify viral sequence  build  classifier  train use  train data    evaluate use  test data give  train dataset compose    number  viral sequence  host sequence  first use  statistic  test   word    mean word frequency  viral sequence  significantly different    host sequence note    subject   unit sum constraint ∑𝐰𝑉𝐰  overcome  problem  multicollinearity  exclude  word   highest  value  least significant word  base   select word  use  logistic regression model  build  binary classifier  add  lasso regularization  make  model flexible  let  data choose  model   highest accuracy let     …     sequence let         come  viral sequence           host sequence       sequence signatures        …    model log⎛⎝⎜⎜⎜𝑃𝑌𝑖∣∣∣𝑉𝑖𝐰𝑃𝑌𝑖∣∣∣𝑉𝑖𝐰⎞⎠⎟⎟⎟∑∈𝑘𝛽𝐰𝑉𝑖𝐰𝛽0    word 𝑃𝑌𝑖∣∣∣𝑉𝑖𝐰exp∑∈𝑘𝛽𝐰𝑉𝑖𝐰𝛽0exp∑∈𝑘𝛽𝐰𝑉𝑖𝐰𝛽0 thus  objective function  ∑𝑖1𝑛log𝑙𝑌𝑖∣∣∣𝑉𝑖𝐰𝛽𝐰 𝛽0𝜆∑∈𝑘𝛽𝐰     likelihood   estimate  minimize  objective function  choose  parameter     highest auroc use fold cross validation   train data   package “glmnet”  use   model train  test  rocs  plot use  package “rocr”   auc score   confidence interval  compute use  package “proc”   real metagenomic experiment  assemble contigs   various lengths  order  compare score  different prediction model   query contig   value  compute  compare  score   null distribution    distribution  score   test host contigs   value  compute   fraction  test host contigs   greater score   score   query sequence  estimate  false discovery rate  proportion  predictions   host  use  package “qvalue”    estimate false discovery rat base    value   query contig  associate   false discovery rate also know    value  contigs  sort   value   smallest   largest give  threshold  contigs   value smaller   threshold  predict  viral sequence   largest  value among  predict contigs give  estimation   false discovery rate   prediction simulation study  metagenomes metagenomic sample  simulate base  species abundance profile derive   real human gut metagenomic sample accession  srr061166 platform illumina   human microbiome project hmp  commonly use  metagenomic data analysis  follow  similar simulation procedure     first map read  sample srr061166 use bwa    virus   host complete genome sequence download  ncbi refseq  generate abundance profile  read   sample  first map  viral genomes    remain unmapped read  map   host complete genomes use  command  “bwa mem”   read map  viral genomes consistent   range previously report  human gut metagenomes  viral   abundance profile  provide   additional file  table    use nessm   simulate metagenomic sample  pairend short read  length     illumina miseq set mode base   abundance profile ten   million read sample  generate   different mixtures  virus  prokaryotic sequence  relative abundance among viruses  among host  keep     virus  host read  mix  make   native level  sample srr061166 determine  map    viral sample metaspades    use  denovo assemble  simulate metagenome sample use  command “spadespy meta”  contigs ≥   use   downstream analysis  obtain  true label   assemble contigs read   simulate data  map   set  contigs use “bwa mem”  contig  label   viral contig    assemble  read   viral genomes similarly  contig  label   host contig    assemble  read  host genomes  contig  label  chimeric    assemble   mixture  virus  host read  validate  prediction  plot roc receiver operate characteristic curve  different range  contig lengths      ≥   roc curve  base   predictions  viral contigs  genomes sequence   january  pair    number  randomly sample host contigs virsorter settings virsorter  run   “viromes” mode    set  evaluation sequence  use  virfinder virsorter report predict viral sequence  three categories   “ confident” predictions   “likely” predictions  iii  “possible” predictions assembly  analysis  human gut metagenomic sample  liver cirrhosis study  data  qin    contain two independent datasets    contain illumina     pair read metagenomes  stool sample   healthy individuals  liver cirrhosis patients   han chinese origin  metagenomes  download   european nucleotide archive accession number erp005860  first dataset refer    “training set”   sample comprise   sample   healthy patients   sample   liver cirrhosis patients  second dataset refer    “testing set”   sample comprise   sample   healthy patients   sample   liver cirrhosis patients megahit   use  crossassemble   sample train dataset use  default settings since   sample dataset   large  assembly cocacola   use  separately cluster viral contigs predict  virfinder  virsorter base  sequence tetranucleotide frequencies  contig coverages normalize  contig length  number  map read  sample contig coverages rpkms  determine  map sample read  bowtie2  use  default settings   average   bin average bin rpkms  use  train  classification model  classify  disease status   healthy    liver cirrhosis  logistic regression model  lasso regularization  use  order  enhance  prediction accuracy  interpretability thus  subset  viral bin  choose  achieve  best prediction accuracy  assess  classification model  average rpkm  bin   second dataset   sample  use  test  classification model  roc curve  use  evaluation twoway hierarchical cluster  perform use  average rpkm coverages    virfinder contig bin use   train set sample   sample randomly select    sample test dataset distance  compute use euclidean distance   cluster  complete linkage method   blast analyse  use  assess  predict viral contigs assemble   cirrhosis study sample  similarity  previously report sequence blastn  blastp search  perform  default settings  ncbis nonredundant nucleotide   protein  databases  august  protein sequence  predict   contig use prodigal     “meta” procedure  meta  best hit   contig nucleotide   predict protein   contigs  retain result proteins    databases  call  viral   come   virus record   taxonomy   one   follow term   definition line virus phage capsid tail head  terminase proteins  also search  pfam domains via  webserver   result domains  consider viral   description contain one   follow term virus phage capsid tail head tape terminase gpnn    digits “podo”  “sipho”",3
67,VirNet,"VirNet: Deep attention model for viral reads identification
Building training and testing dataset We divided viruses, bacteria and archaea genomes from RefSeq database randomly into a train and test genomes with 80% of total base pairs in training. Table I shows the number of genomes we used in training and testing. We processed all available viral genomes until Nov. 1st, 2017 and a sample from prokaryotic genomes due to the huge number of available prokaryotic genomes. Then, we converted the viral genomes into non-overlapping fragments of different lengths n = {100, 500, 1000, 3000}. We generated an approximate number of non-overlapping fragments of prokaryotic genomes with the same lengths randomly as well. Generating simulated virome and microbiome Grinder [4] is an open-source tool commonly used for generating a simulate amplicon and shotgun metagenomic datasets from reference genomes. We generated two metagenomic data of virome and microbiome of 1M reads and fragment length 100bp using Grinder with our reference test genomes to simulate shotgun metagenomic sequences in order to verify the ability of our tool to detect viral reads in metagenomic data instead of generated fragments from the reference genomes. The virome data has 75% of viral reads while microbiome has 25%. C. Deep Learning Model Recurrent neural networks (RNN), long short-term memory (LSTM) [8] and gated recurrent neural networks (GRU) [6] can model complex sequences and have been used for sequence modeling problems. Our deep learning model is implemented as an attentional encoder network (Figure 1a). An input sequence x = (x1, … , xm) and calculates a forward sequence of hidden states (h1→,…,hm−→). The hidden states hj→ are averaged to obtain the attention vector hj representing the context vector from the input sequence. Embedding layer maps discrete input words to dense vectors for computational efficiency before feeding this sequence to LSTM Layers. The attentional network could learn how to extract suitable features from the raw data and can attend to previous DNA nucleotide within the same input sequence. The attentional neural model was trained with the DNA nucleotide bases with fragments with different lengths. The model will predict in a binary output format whether this fragment is viral or non-viral. The top-performing model (Figure 1b) consists of an input embedding layer of size 128 mapping input DNA nucleotide tokens into an embedding space, that is fed to an LSTM layer. The forward sequence hj→ is then averaged together to create an attentional vector representing token context within the same fragment. A dropout layer was added after the attentional layer to avoid overfitting over the input data. The input sequence is divided into 5 grams sized tokens these tokens are then treated as a single word (Figure 1a). This single token is mapped as a point in the embedding space created during training the neural model. During training, all parameters are optimized jointly using Adam to maximize the conditional probability of tokens found together to predict if an input sequence is viral or not. D. Hyperparameters optimization We selected the grid search technique in order to find the most suitable parameters. The grid search is considered a traditional technique for hyperparameters optimization and it brute force different combinations. We ran several experiments on 20% of our training set for 500 bp. Then, we divided it into training, validation, and testing set with the following percentages 70%, 10%, and 20%. These experiments were designed to find the best parameters for the number of recurrent layers, the embedding size for each layer and the input sub-words (ngram). Our reported results (Table III) show that the best parameters setup is for 2 layers, 128 embedding size, and 5 ngram. ",VirusIdentification,"virnet deep attention model  viral read identification
building train  test dataset  divide viruses bacteria  archaea genomes  refseq database randomly   train  test genomes    total base pair  train table  show  number  genomes  use  train  test  process  available viral genomes  nov 1st    sample  prokaryotic genomes due   huge number  available prokaryotic genomes   convert  viral genomes  nonoverlapping fragment  different lengths   {   }  generate  approximate number  nonoverlapping fragment  prokaryotic genomes    lengths randomly  well generate simulate virome  microbiome grinder    opensource tool commonly use  generate  simulate amplicon  shotgun metagenomic datasets  reference genomes  generate two metagenomic data  virome  microbiome   read  fragment length 100bp use grinder   reference test genomes  simulate shotgun metagenomic sequence  order  verify  ability   tool  detect viral read  metagenomic data instead  generate fragment   reference genomes  virome data    viral read  microbiome    deep learn model recurrent neural network rnn long shortterm memory lstm   gate recurrent neural network gru   model complex sequence    use  sequence model problems  deep learn model  implement   attentional encoder network figure   input sequence    …    calculate  forward sequence  hide state →…→  hide state →  average  obtain  attention vector  represent  context vector   input sequence embed layer map discrete input word  dense vectors  computational efficiency  feed  sequence  lstm layer  attentional network could learn   extract suitable feature   raw data   attend  previous dna nucleotide within   input sequence  attentional neural model  train   dna nucleotide base  fragment  different lengths  model  predict   binary output format whether  fragment  viral  nonviral  topperforming model figure  consist   input embed layer  size  map input dna nucleotide tokens   embed space   feed   lstm layer  forward sequence →   average together  create  attentional vector represent token context within   fragment  dropout layer  add   attentional layer  avoid overfitting   input data  input sequence  divide   grams size tokens  tokens   treat   single word figure   single token  map   point   embed space create  train  neural model  train  parameters  optimize jointly use adam  maximize  conditional probability  tokens find together  predict   input sequence  viral    hyperparameters optimization  select  grid search technique  order  find   suitable parameters  grid search  consider  traditional technique  hyperparameters optimization   brute force different combinations  run several experiment     train set      divide   train validation  test set   follow percentages      experiment  design  find  best parameters   number  recurrent layer  embed size   layer   input subwords ngram  report result table iii show   best parameters setup    layer  embed size   ngram ",3
68,VirSorter,"VirSorter: mining viral signal from microbial genomic data
Building reference databases for bacterial and archaeal viruses Two reference databases of viral protein sequences were built for VirSorter and are available in the iPlant Discovery Environment (Data/Community_Data/iVirus/VirSorter/Database). The first includes 114,297 proteins from viruses infecting bacteria or archaea in RefSeqVirus genomes (as of January 2014), hereafter named “RefSeqABVir.” Protein clusters (PCs) were defined using MCL clustering (Enright, Van Dongen & Ouzounis, 2002) of these proteins (inflation 2.0) based on their reciprocal blastp comparisons (threshold of 50 on bit score and 10−03 on E-value). The 9,735 PCs with at least 3 sequences were used to define a profile database searchable with HMMER3 tools (Eddy, 2011). The remaining 34,668 unclustered sequences were formatted for a blastp search. All PCs that did not contain any sequences from Caudovirales and unclustered sequences from viruses other than Caudovirales were marked as “Non-Caudovirales.” The RefSeqABVir database was then augmented by virome sequences sampled from freshwater, seawater, and human gut, lung and saliva, resulting in an extended version of the reference database (hereafter named “Viromes”) which includes both virome and RefSeqABVir sequences. This combined reference dataset should help to detect new viruses for which no cultivated reference sequence is available. When only raw reads were available, viromes were assembled using Newbler (threshold of 98% identity on 35bp). The resulting contigs were then checked for the presence of cellular genome sequences, and only the 68 viromes for which no 16S rRNA genes were retained (see Table S1 for a complete list of these viromes). Contigs assembled from these 68 viromes were then manually inspected (through annotations generated by Metavir; Roux et al., 2014a) and revealed no identifiable cellular genome sequences (i.e., no sequence contained more than 2 genes that matched a cellular genome and were not found in any known virus). A total of 146,521 complete predicted proteins from this quality-controlled dataset were then clustered with the 114,297 proteins from RefSeqABVir, leading to 15,673 clusters with 3 sequences or more, and 88,052 unclustered sequences. PCs from the combined Viromes database were used to create a profile database searchable with HMMER3, and the 34,338 unclustered sequences from RefseqABVir were formatted for BLAST search (unclustered sequences from viromes were not added to the database to prevent the inclusion of contaminating sequences). Within these databases, viral “hallmark” genes were defined though a text-searching script looking for “major capsid protein,” “portal,” “terminase large subunit,” “spike,” “tail,” “virion formation” or “coat” annotations. After a manual curation step removing genes with more general annotation such as “protease” or “chaperone,” 826 PCs or single genes were identified as “viral hallmark genes.” This latter point meant removing domains also matching “protease” or “chaperone” domains and was conducted to minimize false positives for our viral hallmark genes category by extra-cautiously avoiding PCs that might include domains that could derive from either both viruses or microbes. VirSorter sequence pre-processing VirSorter was inspired by previous algorithms and tools developed to detect prophages (viral sequences integrated in cellular genomes), especially Prophinder (Lima-Mendez et al., 2008). For each (set of) genome(s) and/or contig(s) (for draft genomes) provided as raw nucleotide sequences, the initial stages of VirSorter include a detection of circular sequences (i.e., sequences with matching ends likely representing circular templates; Roux et al., 2014a), gene prediction on each sequence with MetageneAnnotator (Noguchi, Taniguchi & Itoh, 2008), and selection of all sequences with more than 2 genes predicted. VirSorter also removes all poor-quality predicted protein sequences (predicted protein sequences with more than 50 consecutive X, F, A, K or P residues) likely originating from gene prediction across low-complexity or poorly defined genome regions (e.g., “bridges” between contigs generated during scaffolding) and yielding false-positive matches when compared to protein domain databases. Predicted protein sequences are then compared to PFAM (v27) and to the viral database selected by the user (either RefSeqABVir or Viromes) with hmmsearch (Eddy, 2011) and blastp (Altschul et al., 1997) and each gene is affiliated to its most significant hit based on alignment score. Thresholds for significant hits are as follows: minimum score of 40 and maximum E-value of 10−05 for hmmsearch, and minimum score of 50 and maximum E-value of 10−03 for blastp. VirSorter metrics computation Following the sequence pre-processing, viral regions are detected through computation of multiple metrics using sliding windows. The metrics used are (i) presence of viral hallmark genes (Koonin, Senkevich & Dolja, 2006; Roux et al., 2014b), (ii) enrichment in viral-like genes (i.e., genes with best hit against the viral reference database, either RefSeqABVir or Viromes), (iii) depletion in PFAM affiliated genes, (iv) enrichment in uncharacterized genes (i.e., predicted genes with no hits either in PFAM or the viral reference database), (v) enrichment in short genes (genes with a size within the 10% shorter genes of the genome), and (vi) depletion in strand switching (i.e., change of coding strand between two consecutive genes). For all the enrichment and depletion metrics, a score comparable to the one of Prophinder was used (Lima-Mendez et al., 2008). First, a global value for each metric is estimated for the whole genome set (global rate of viral-like genes, global rate of PFAM-affiliated genes, etc). Then, for each window, the number of observed events (e.g. number of viral-like genes) is compared to an expected number deduced from the global value of the metric (modeled with a binomial law). A p-value is computed, reflecting the probability of observing n events or more (for enrichment) or n events or fewer (for depletion) at random, thus corresponding to a risk of generating false positives. These p-values are multiplied by the total number of comparisons (here the total number of sliding windows observed on a sequence), and a negative logarithmic transformation (−log10) defines the associated significance score, again as in the Prophinder algorithm (Lima-Mendez et al., 2008). For the detection of viral-like genes enrichment, two different values are computed for each dataset: one based on genes in the entire database (RefSeqABVir or Viromes), and another based on non-Caudovirales genes only. Indeed, Caudovirales genomes represent 81% of RefSeqABVir, and the remaining viral families usually have only a handful of reference genomes. The global rate of viral-like genes in cellular genomes is thus usually one order of magnitude lower when considering only non-Caudovirales genes (viral-like genes ratio across the bacterial and archaeal class for which complete genomes are available at NCBI RefSeq and WGS ranges from 4.8 to 16%, with an average of 10.6%, whereas the ratio of non-Caudovirales genes in these same genomes ranges from 0.01 to 1.4%, with an average of 0.16%). Hence, the same number of genes in a region would be considered as non-significant when matching Caudovirales (compared to the global rate of Caudovirales-like genes in the whole genomes), but would be significant when only composed of non-Caudovirales genes. Sequence metrics summary Each metric is computed using sliding windows from 10 to 100 genes wide, starting at every gene along the sequence, and all scores greater than 2 are stored. Local maxima of significance score are then searched and the associated set of genes is defined as a putative viral region. These different predictions (based on the metrics above) are then merged when overlapping (extending the regions to include all predicted windows), leading to a list of putative viral regions associated with a (set of) metric(s). These regions are classified into three categories: (i) category 1 (“most confident” predictions) regions have significant enrichment in viral-like genes or non-Caudovirales genes on the whole region and at least one hallmark viral gene detected; (ii) category 2 (“likely” predictions) regions have either enrichment in viral-like or non-Caudovirales genes, or a viral hallmark gene detected, associated with at least one other metric (depletion in PFAM affiliation, enrichment in uncharacterized genes, enrichment in short genes, depletions in strand switch); and (iii) category 3 (“possible” predictions) regions have neither a viral hallmark gene nor enrichment in viral-like or non-Caudovirales genes, but display at least two of the other metrics with at least one significance score greater than 4. Finally, if a predicted region spans more than 80% of predicted genes on a contig, the entire contig is considered viral Next, higher confidence predictions are used to refine the sequence space search. Specifically, sequences from all open reading frames from category 1 predictions that do not match a viral protein cluster are clustered and added to the reference database (RefSeqABVir or Viromes depending on the initial user choice). This updated database is then used in another round of search by VirSorter. This iteration where category 1 sequences are used to refine the searches is continued until no new genes are added to the database. Once no new genes are added, the final VirSorter output is provided to the user and includes nucleotide sequences of all predicted viral sequences in fasta files, an automatic annotation of each prediction in genbank file format, and a summary table displaying for each prediction the associated category and significance scores of all metrics. By providing the predictions and the underlying significance scoring, users can evaluate each prediction and apply custom thresholds on significance scores through a simple text-parsing script, even for large-scale datasets. VirSorter is available as an application (App) in the iPlant discovery environment (https://de.iplantcollaborative.org/de/) under Apps/Experimental/iVirus (see Fig. S1 for a step-by-step guide of VirSorter app on iPlant). This application allows users to search any set of contigs for viral sequences using either the RefSeqABVir or the Viromes database. The reference values of VirSorter metrics will be evaluated on the complete set of input sequences, hence mixed datasets should be sorted (when possible) by type of bacteria or archaea in order to get the most accurate result possible. In addition to these reference databases, the VirSorter App on iPlant allows users to input their own reference viral genome sequence already assembled or to-be assembled using iPlant Apps prior to analysis with VirSorter. Assembled sequences are processed as follows: (i) genes are predicted with MetaGeneAnnotator (Noguchi, Taniguchi & Itoh, 2008), (ii) predicted proteins are clustered with sequences from the user-selected database (either RefSeqABVir or Viromes), and (iii) unclustered proteins are added to the “unclustered” pool. VirSorter scripts are also available through the github repository https://github.com/simroux/VirSorter.git. Comparison of VirSorter with other prophage predictors We first evaluated VirSorter results against the manually curated prophages from (Casjens, 2003). Each genome was processed with VirSorter, PhiSpy (Akhter, Aziz & Edwards, 2012), Phage_Finder (Fouts, 2006) and PHAST (Zhou et al., 2011). For each tool, a prophage was considered as “detected” when a prediction covered more than 75% of the known prophage. For a more detailed example case of prophage detection in a complete bacterial genome including both prophages and genomic islands, the same tools were applied to the manually annotated Pseudomonas aeruginosa LES B58 genome (Winstanley et al., 2009). VirSorter was then compared with the same prophage detection tools on the set of simulated SAGs. In that case, a viral sequence was considered as detected if predicted as completely viral or as a prophage. All the additional detections were manually checked to verify if the region was indeed viral (originating from a prophage in one of the microbial genomes rather than from a viral genome) or a false positive. The same approach was used for the simulated microbial and viral metagenomes results. For each set of predictions, two metrics are computed. First, the Recall value corresponds to the number of viral sequences correctly predicted divided by the total number of known viral sequences in the dataset, and reflects the ability of the tool to find every known viral sequence in the dataset. Second, the Precision value is computed as the total number of viral sequences correctly predicted divided by the total number of viral sequences predicted, and indicates how accurate the tool is in its identification of viral signal. Simulation of draft genomes and metagenomes A total of 10 Single-cell amplified genomes, 10 microbial metagenomes and 10 viral metagenomes were simulated with NeSSM (Jia et al., 2013). Microbial genomes were randomly picked within the bacterial and archaeal genomes available in RefSeq and WGS (as of January 2014). Viral genomes were picked within the most recently submitted genomes (since June 2014), thus are not in VirSorter reference database. Simulated inputs for each genome group (viral and microbial) followed a power-law distribution of abundances within the microbial and viral communities. The proportion of viral reads varied from 5 to 20% for microbial metagenome, and from 75 to 99% for viral metagenomes (Tables S4 and S5). For each simulated dataset, 100bp paired-end reads similar to those obtained with HiSeq Illumina were generated (100,000 for SAGs, 1,000,000 for metagenomes), QC’d with fastq_quality_trimmer with a threshold of 30 (part of the fastx_toolkit, http://hannonlab.cshl.edu/fastx_toolkit/), and assembled with Idba_ud (Peng et al., 2012). To identify viral sequences in the assemblies, the resulting contigs were compared to the viral genomes with nucmer (Delcher, Salzberg & Phillippy, 2003), and all sequences matching one of the viral genomes at 97% nucleotide identity or more were considered as viral. All simulated contigs and composition table (i.e., relative abundance of each genome in the simulated dataset) are available in the iPlant Discovery Environment alongisde VirSorter results for each of these simulated datasets (/iplant/home/shared/imicrobe/VirSorter/Benchmark_datasets and Benchmark_results respectively).",VirusIdentification,"virsorter mine viral signal  microbial genomic data
building reference databases  bacterial  archaeal viruses two reference databases  viral protein sequence  build  virsorter   available   iplant discovery environment datacommunity_dataivirusvirsorterdatabase  first include  proteins  viruses infect bacteria  archaea  refseqvirus genomes   january  hereafter name “refseqabvir” protein cluster pcs  define use mcl cluster enright van dongen  ouzounis    proteins inflation  base   reciprocal blastp comparisons threshold    bite score    evalue   pcs   least  sequence  use  define  profile database searchable  hmmer3 tool eddy   remain  unclustered sequence  format   blastp search  pcs    contain  sequence  caudovirales  unclustered sequence  viruses   caudovirales  mark  “noncaudovirales”  refseqabvir database   augment  virome sequence sample  freshwater seawater  human gut lung  saliva result   extend version   reference database hereafter name “viromes”  include  virome  refseqabvir sequence  combine reference dataset  help  detect new viruses    cultivate reference sequence  available   raw read  available viromes  assemble use newbler threshold   identity  35bp  result contigs   check   presence  cellular genome sequence     viromes     rrna genes  retain see table    complete list   viromes contigs assemble    viromes   manually inspect  annotations generate  metavir roux     reveal  identifiable cellular genome sequence   sequence contain    genes  match  cellular genome    find   know virus  total   complete predict proteins   qualitycontrolled dataset   cluster    proteins  refseqabvir lead   cluster   sequence     unclustered sequence pcs   combine viromes database  use  create  profile database searchable  hmmer3    unclustered sequence  refseqabvir  format  blast search unclustered sequence  viromes   add   database  prevent  inclusion  contaminate sequence within  databases viral “hallmark” genes  define though  textsearching script look  “major capsid protein” “portal” “terminase large subunit” “spike” “tail” “virion formation”  “coat” annotations   manual curation step remove genes   general annotation   “protease”  “chaperone”  pcs  single genes  identify  “viral hallmark genes”  latter point mean remove domains also match “protease”  “chaperone” domains   conduct  minimize false positives   viral hallmark genes category  extracautiously avoid pcs  might include domains  could derive  either  viruses  microbes virsorter sequence preprocessing virsorter  inspire  previous algorithms  tool develop  detect prophages viral sequence integrate  cellular genomes especially prophinder limamendez      set  genomes andor contigs  draft genomes provide  raw nucleotide sequence  initial stag  virsorter include  detection  circular sequence  sequence  match end likely represent circular templates roux    gene prediction   sequence  metageneannotator noguchi taniguchi  itoh   selection   sequence     genes predict virsorter also remove  poorquality predict protein sequence predict protein sequence     consecutive       residues likely originate  gene prediction across lowcomplexity  poorly define genome regions  “bridges”  contigs generate  scaffold  yield falsepositive match  compare  protein domain databases predict protein sequence   compare  pfam v27    viral database select   user either refseqabvir  viromes  hmmsearch eddy   blastp altschul      gene  affiliate    significant hit base  alignment score thresholds  significant hit   follow minimum score    maximum evalue    hmmsearch  minimum score    maximum evalue    blastp virsorter metrics computation follow  sequence preprocessing viral regions  detect  computation  multiple metrics use slide windows  metrics use   presence  viral hallmark genes koonin senkevich  dolja  roux     enrichment  virallike genes  genes  best hit   viral reference database either refseqabvir  viromes iii depletion  pfam affiliate genes  enrichment  uncharacterized genes  predict genes   hit either  pfam   viral reference database  enrichment  short genes genes   size within   shorter genes   genome   depletion  strand switch  change  cod strand  two consecutive genes    enrichment  depletion metrics  score comparable   one  prophinder  use limamendez    first  global value   metric  estimate   whole genome set global rate  virallike genes global rate  pfamaffiliated genes etc    window  number  observe events  number  virallike genes  compare   expect number deduce   global value   metric model   binomial law  pvalue  compute reflect  probability  observe  events    enrichment   events  fewer  depletion  random thus correspond   risk  generate false positives  pvalues  multiply   total number  comparisons   total number  slide windows observe   sequence   negative logarithmic transformation log10 define  associate significance score     prophinder algorithm limamendez      detection  virallike genes enrichment two different value  compute   dataset one base  genes   entire database refseqabvir  viromes  another base  noncaudovirales genes  indeed caudovirales genomes represent   refseqabvir   remain viral families usually    handful  reference genomes  global rate  virallike genes  cellular genomes  thus usually one order  magnitude lower  consider  noncaudovirales genes virallike genes ratio across  bacterial  archaeal class   complete genomes  available  ncbi refseq  wgs range       average   whereas  ratio  noncaudovirales genes    genomes range       average   hence   number  genes   region would  consider  nonsignificant  match caudovirales compare   global rate  caudoviraleslike genes   whole genomes  would  significant   compose  noncaudovirales genes sequence metrics summary  metric  compute use slide windows     genes wide start  every gene along  sequence   score greater    store local maxima  significance score   search   associate set  genes  define   putative viral region  different predictions base   metrics    merge  overlap extend  regions  include  predict windows lead   list  putative viral regions associate   set  metrics  regions  classify  three categories  category  “ confident” predictions regions  significant enrichment  virallike genes  noncaudovirales genes   whole region   least one hallmark viral gene detect  category  “likely” predictions regions  either enrichment  virallike  noncaudovirales genes   viral hallmark gene detect associate   least one  metric depletion  pfam affiliation enrichment  uncharacterized genes enrichment  short genes depletions  strand switch  iii category  “possible” predictions regions  neither  viral hallmark gene  enrichment  virallike  noncaudovirales genes  display  least two    metrics   least one significance score greater   finally   predict region span     predict genes   contig  entire contig  consider viral next higher confidence predictions  use  refine  sequence space search specifically sequence   open read frame  category  predictions    match  viral protein cluster  cluster  add   reference database refseqabvir  viromes depend   initial user choice  update database   use  another round  search  virsorter  iteration  category  sequence  use  refine  search  continue   new genes  add   database   new genes  add  final virsorter output  provide   user  include nucleotide sequence   predict viral sequence  fasta file  automatic annotation   prediction  genbank file format   summary table display   prediction  associate category  significance score   metrics  provide  predictions   underlie significance score users  evaluate  prediction  apply custom thresholds  significance score   simple textparsing script even  largescale datasets virsorter  available   application app   iplant discovery environment   appsexperimentalivirus see fig    stepbystep guide  virsorter app  iplant  application allow users  search  set  contigs  viral sequence use either  refseqabvir   viromes database  reference value  virsorter metrics   evaluate   complete set  input sequence hence mix datasets   sort  possible  type  bacteria  archaea  order  get   accurate result possible  addition   reference databases  virsorter app  iplant allow users  input   reference viral genome sequence already assemble  tobe assemble use iplant apps prior  analysis  virsorter assemble sequence  process  follow  genes  predict  metageneannotator noguchi taniguchi  itoh   predict proteins  cluster  sequence   userselected database either refseqabvir  viromes  iii unclustered proteins  add   “unclustered” pool virsorter script  also available   github repository  comparison  virsorter   prophage predictors  first evaluate virsorter result   manually curated prophages  casjens   genome  process  virsorter phispy akhter aziz  edwards  phage_finder fouts   phast zhou      tool  prophage  consider  “detected”   prediction cover      know prophage    detail example case  prophage detection   complete bacterial genome include  prophages  genomic islands   tool  apply   manually annotate pseudomonas aeruginosa les b58 genome winstanley    virsorter   compare    prophage detection tool   set  simulate sag   case  viral sequence  consider  detect  predict  completely viral    prophage   additional detections  manually check  verify   region  indeed viral originate   prophage  one   microbial genomes rather    viral genome   false positive   approach  use   simulate microbial  viral metagenomes result   set  predictions two metrics  compute first  recall value correspond   number  viral sequence correctly predict divide   total number  know viral sequence   dataset  reflect  ability   tool  find every know viral sequence   dataset second  precision value  compute   total number  viral sequence correctly predict divide   total number  viral sequence predict  indicate  accurate  tool    identification  viral signal simulation  draft genomes  metagenomes  total   singlecell amplify genomes  microbial metagenomes   viral metagenomes  simulate  nessm jia    microbial genomes  randomly pick within  bacterial  archaeal genomes available  refseq  wgs   january  viral genomes  pick within   recently submit genomes since june  thus    virsorter reference database simulate input   genome group viral  microbial follow  powerlaw distribution  abundances within  microbial  viral communities  proportion  viral read vary      microbial metagenome       viral metagenomes table      simulate dataset 100bp pairedend read similar   obtain  hiseq illumina  generate   sag   metagenomes qcd  fastq_quality_trimmer   threshold   part   fastx_toolkit   assemble  idba_ud peng     identify viral sequence   assemblies  result contigs  compare   viral genomes  nucmer delcher salzberg  phillippy    sequence match one   viral genomes   nucleotide identity    consider  viral  simulate contigs  composition table  relative abundance   genome   simulate dataset  available   iplant discovery environment alongisde virsorter result     simulate datasets iplanthomesharedimicrobevirsorterbenchmark_datasets  benchmark_results respectively",3
69,VirusDetect,"VirusDetect: An automated pipeline for efficient virus discovery using deep sequencing of small RNAs
Implementation of VirusDetect The VirusDetect package is implemented in Perl. BWA (Li and Durbin, 2009) is employed to align siRNA reads to the reference virus sequences, host sequences or assembled virus contigs. For reference-guided assembly, SAMtools (Li et al., 2009) is used to process BWA alignments and generate per-position alignment information in pileup format, which is used to guide the construction of virus contigs. De novo assembly of viral siRNAs is performed using Velvet (Zerbino and Birney, 2008). VirusDetect uses the BLAST program (Altschul et al., 1990) to compare assembled contigs against virus reference nucleotide and protein sequence databases. Perl modular Bio::Graphics (Stajich et al., 2002) is used to generate the track images according to the BLAST result of virus contigs. 4.2. Curation and classification of GenBank virus sequence database Virus sequences downloaded from GenBank were classified into eight different host kingdoms: vertebrate, invertebrate, plant, protozoa, algae, fungus, bacteria, and archaea, based on the virus taxonomy information provided by the International Committee on Taxonomy of Viruses (ICTV; http://www.ictvonline.org/) using a perl script, which is included in the VirusDetect package. Virus sequences that were not classified in ICTV were then manually classified according to the descriptions of the virus sequences and their high sequence similarity to those classified viruses. Virus sequences classified into each of the eight host kingdoms were further processed to remove redundant sequences with sequence identity cutoff of 95%, 97% and 100%, respectively, using cd-hit (Li and Godzik, 2006). The corresponding protein sequences of each virus were also extracted. All the classified virus nucleotide and protein sequences are available at the VirusDetect website. 4.3. Plant material, RNA extraction and sRNA library construction and sequencing For potato sRNA sequencing and standard virus indexing, two copies of 16 in vitro accessions were selected from the germplasm collection at the CIP (Table 1). One copy of in vitro plants was processed for standard virus indexing and the other set was processed for sRNA sequencing. Total RNA was extracted from 1 g of fresh leaf tissue using the CTAB method. The quantity of the total RNA was determined using a NanoDrop analyzer (Thermo Fisher Scientific, USA) and the quality was checked by agarose gel electrophoresis. sRNAs of 20–30 nt were purified after excising the band from a 3.5% agarose gel. To demonstrate the efficiency of VirusDetect in identifying and assembling genomes of novel viruses from sRNA sequences, a leaf sample of an unspecified weed species with yellowing and mosaic-like symptoms was collected from a tomato field in Barbacena, Brazil in February 2013. Total RNA from this sample was purified using TRIzol reagents following the manufacturer's instructions (Invitrogen, USA). After quantification in a NanoDrop (Thermo Fisher Scientific, USA), RNA molecules, ranging from 18 to 28 nt, were excised from a polyacrylamide gel and extracted. sRNA libraries of the weed and potato samples were constructed following the protocol described in Chen et al., 2012, Chen et al., 2012 and sequenced on an Illumina HiSeq 2500 system with the 50-bp single-end mode. 4.4. sRNA read processing Raw sRNA reads were first processed to identify the 3′ adaptor sequences using an in-house perl script, which is included in the VirusDetect package. Briefly, the adaptor sequences were identified in the sRNA reads if the first eleven nucleotides of the adaptor could match the sRNA reads with at most one mismatch. sRNA reads with no adaptor sequences identified were discarded and the remaining reads were then processed to trim the 3′ adaptor sequences. The processed reads that were in low quality (containing ambiguous bases) or shorter than 15 nt were excluded from downstream analysis. The cleaned sRNA reads were used for virus discovery using VirusDetect. The non-redundant (95%) plant virus database was used as the reference. The potato genome (Xu et al., 2011) was used as the reference for host sRNA subtraction for the potato sRNA datasets. 4.5. Standard virus indexing of potato Indexing of the 16 potato accessions was performed by virology service unit at the CIP following an ISO/IEC 17025 accredited procedure which includes the following tests: i) NASH for Potato spindle tuber viroid (PSTVd) and PVT from in vitro and greenhouse-grown plants, respectively, ii) DAS-ELISA from three replicates of greenhouse grown plants with antibodies for PVX, PLRV, PVS, APMMV, Potato virus Y (PVY), Potato yellowing virus (PYV), Arracacha virus B-Oca strain (AVB-O), and Andean potato mottle virus (APMoV), and iii) mechanical inoculation and symptom evaluation of eleven biological indicator plants: Nicotiana benthamiana, N. clevelandii×N. bigelovii, N. debneyii, N. glutinosa, N. tabacum ""White Burley"", Chenopodium murale, C. quinoa, Datura stramonium, Gomphrena globosa, Solanum lycopersicum ""Rutgers"" and Physalis floridana. In addition graft inoculation of D. stramonium was also performed. 4.6. Validation of known and novel plant viruses detected by VirusDetect Potato viruses were confirmed by RT-PCR using virus specific primers for PVX, PVS, PVT, PLRV, and APMMV (Supplementary Table 6). Same total RNA used for sRNA library construction was used for PCR. For cDNA synthesis 1 μg of total RNA and 250 ng/μl of random hexamer primers were used in a total reaction mix of 20 μl using 200 U of M-MLV reverse transcriptase (Invitrogen). After incubation at 37 °C for 50 min, the reaction was diluted 1:10 with nuclease free water and 5 μl was used for PCR. The PCR was performed using GoTaq® DNA Polymerase (Promega) in a volume of 20 μl with a final primer concentration of 0.5 µM each. The PCR protocol consisted of 5 min initial denaturation at 94 °C, followed by 35 cycles of 94 °C for 30 s, 50–60 °C for 30 s and 72 °C between 30s to 1 min, with final extension at 72 °C for 10 min. PCR products were visualized on 1% agarose gels stained with GelRed (Biotium). PCR fragments amplified for each isolate were purified using the High Pure PCR product purification kit (Roche), then cloned into pGEM-T easy vector (Promega) following standard procedures and transformed into Escherichia coli (DH 5α). Samples were sent to Macrogen (Korea) for Sanger sequencing. To validate the genome sequence of the novel virus (BWVY) assembled by VirusDetect, a series of 16 primer pairs (Supplementary Table 7) were designed to amplify overlapping fragments covering the entire virus genome. Amplicons generated using the above primer pairs in reverse transcription-PCR (RT-PCR) were sequenced directly or cloned using a TOPO TA cloning kit (Invitrogen) and sequenced using the Sanger technology by the Functional Biosciences, Inc. (Madison, WI). 4.7. RNA-Seq data processing Raw RNA-Seq dataset of P. domestica described in Rodamilans et al. (2014) was downloaded from NCBI SRA database under accession SRP041925. Adaptor and low quality sequences were removed using Trimmomatic (Bolger et al., 2014). The remaining high quality reads were aligned to the rRNA database (Quast et al., 2013) using bowtie (Langmead et al., 2009) allowing up to 3 mismatches. The unaligned reads were used for virus discovery using VirusDetect.",VirusIdentification,"virusdetect  automate pipeline  efficient virus discovery use deep sequence  small rnas
implementation  virusdetect  virusdetect package  implement  perl bwa   durbin   employ  align sirna read   reference virus sequence host sequence  assemble virus contigs  referenceguided assembly samtools      use  process bwa alignments  generate perposition alignment information  pileup format   use  guide  construction  virus contigs  novo assembly  viral sirnas  perform use velvet zerbino  birney  virusdetect use  blast program altschul     compare assemble contigs  virus reference nucleotide  protein sequence databases perl modular biographics stajich     use  generate  track image accord   blast result  virus contigs  curation  classification  genbank virus sequence database virus sequence download  genbank  classify  eight different host kingdoms vertebrate invertebrate plant protozoa algae fungus bacteria  archaea base   virus taxonomy information provide   international committee  taxonomy  viruses ictv  use  perl script   include   virusdetect package virus sequence    classify  ictv   manually classify accord   descriptions   virus sequence   high sequence similarity   classify viruses virus sequence classify     eight host kingdoms   process  remove redundant sequence  sequence identity cutoff      respectively use cdhit   godzik   correspond protein sequence   virus  also extract   classify virus nucleotide  protein sequence  available   virusdetect website  plant material rna extraction  srna library construction  sequence  potato srna sequence  standard virus index two copy    vitro accession  select   germplasm collection   cip table  one copy   vitro plant  process  standard virus index    set  process  srna sequence total rna  extract     fresh leaf tissue use  ctab method  quantity   total rna  determine use  nanodrop analyzer thermo fisher scientific usa   quality  check  agarose gel electrophoresis srnas     purify  excise  band    agarose gel  demonstrate  efficiency  virusdetect  identify  assemble genomes  novel viruses  srna sequence  leaf sample   unspecified weed species  yellow  mosaiclike symptoms  collect   tomato field  barbacena brazil  february  total rna   sample  purify use trizol reagents follow  manufacturer' instructions invitrogen usa  quantification   nanodrop thermo fisher scientific usa rna molecules range       excise   polyacrylamide gel  extract srna libraries   weed  potato sample  construct follow  protocol describe  chen    chen     sequence   illumina hiseq  system    singleend mode  srna read process raw srna read  first process  identify  ′ adaptor sequence use  inhouse perl script   include   virusdetect package briefly  adaptor sequence  identify   srna read   first eleven nucleotides   adaptor could match  srna read    one mismatch srna read   adaptor sequence identify  discard   remain read   process  trim  ′ adaptor sequence  process read    low quality contain ambiguous base  shorter     exclude  downstream analysis  clean srna read  use  virus discovery use virusdetect  nonredundant  plant virus database  use   reference  potato genome      use   reference  host srna subtraction   potato srna datasets  standard virus index  potato index    potato accession  perform  virology service unit   cip follow  isoiec  accredit procedure  include  follow test  nash  potato spindle tuber viroid pstvd  pvt   vitro  greenhousegrown plant respectively  daselisa  three replicate  greenhouse grow plant  antibodies  pvx plrv pvs apmmv potato virus  pvy potato yellow virus pyv arracacha virus boca strain avbo  andean potato mottle virus apmov  iii mechanical inoculation  symptom evaluation  eleven biological indicator plant nicotiana benthamiana  clevelandiin bigelovii  debneyii  glutinosa  tabacum ""white burley"" chenopodium murale  quinoa datura stramonium gomphrena globosa solanum lycopersicum ""rutgers""  physalis floridana  addition graft inoculation   stramonium  also perform  validation  know  novel plant viruses detect  virusdetect potato viruses  confirm  rtpcr use virus specific primers  pvx pvs pvt plrv  apmmv supplementary table   total rna use  srna library construction  use  pcr  cdna synthesis    total rna   ngμl  random hexamer primers  use   total reaction mix    use    mmlv reverse transcriptase invitrogen  incubation   °   min  reaction  dilute   nuclease free water     use  pcr  pcr  perform use gotaq® dna polymerase promega   volume      final primer concentration      pcr protocol consist   min initial denaturation   ° follow   cycle   °     °      °     min  final extension   °   min pcr products  visualize   agarose gel stain  gelred biotium pcr fragment amplify   isolate  purify use  high pure pcr product purification kit roche  clone  pgemt easy vector promega follow standard procedures  transform  escherichia coli   sample  send  macrogen korea  sanger sequence  validate  genome sequence   novel virus bwvy assemble  virusdetect  series   primer pair supplementary table   design  amplify overlap fragment cover  entire virus genome amplicons generate use   primer pair  reverse transcriptionpcr rtpcr  sequence directly  clone use  topo  clone kit invitrogen  sequence use  sanger technology   functional biosciences inc madison   rnaseq data process raw rnaseq dataset   domestica describe  rodamilans     download  ncbi sra database  accession srp041925 adaptor  low quality sequence  remove use trimmomatic bolger     remain high quality read  align   rrna database quast    use bowtie langmead    allow    mismatch  unaligned read  use  virus discovery use virusdetect",3
70,VIP,"VIP: an integrated pipeline for metagenomics of virus identification and discovery
The VIP is comprised of a series of Shell, Python, and Perl scripts in Linux and incorporates several open-source tools. VIP has a set of fixed external software and database dependencies and user-defined custom parameters. The pipeline accepts cross-platform results generated from 454, ion torrent and Illumina with a variety of formats such as FastQ, FastA, SAM and BAM alignment formats. Reads are handled by concatenating the files into a single file for streamlined analysis. Data import and quality control Raw NGS short read data can be imported by trans-formatted into FastQ format using PICARD (http://picard.sourceforge.net). VIP will determine the encoding version of the input data. This is necessary to make sure those differences in the way the quality scores were generated from different sequencing platforms are properly taken into consideration during preprocessing. VIP can also accept FastA format raw NGS data. The quality control step, however, will only perform sequence-based strategies, such as the complexity and length as main factors. Generally, the quality control step is comprised of trimming low-quality and adapter reads, removing low-complexity sequences using the DUST algorithm and retaining reads of trimmed length >20 bp using PRINSEQ31. In fast mode, Bowtie2 alignments are first performed against the host DB followed by removing the host-related sequences. The remaining reads are subject to ViPR/IRD nucleotide DB. In sense mode, the initial alignment against host DB and bacteria DB is followed by subtraction of reads mapped. Background-subtracted reads are then subject to a virus subset of NCBI nt DB. Extensive classification and coverage map Previous reports suggested that viruses have the potential to mutate rapidly or jump between species. Reads from mutation region of these viruses might be unclassified or classified into other species or strains. In order to avoid the misclassifications caused by mutations, we applied a two-steps computational alignment strategy for classification in VIP (Fig. 4). In the first step, all matched reads will be assigned to a specific gene identifier (GI) after nucleotide/amino acid alignment. These reads are therefore taxonomically classified to genus level by lookup of matched GI from the NCBI taxonomy database by SQL (Structured Query Language). i.e According to the GI, the scientific names for each reference records, which are composed of the species, genus and family information, are achieved and appended to the alignment results. Secondly, reads classified under genus are automatically mapped to the most likely reference genome as follows. Abundance of reference sequences that are selected during nucleotide alignment corresponding to that genus are calculated and sorted. Here we hypothesized the genome coverage percentage was alongside with the sequencing depth for specific reference sequences. In other words the higher abundance of a genome suggested the higher possibility to recover its genome. All the reference sequences with the following key words are kept: (1) complete genomes; (2) complete sequences; or (3) complete cds. Assigned reads are directly mapped to all nucleotide reference sequences selected using optimal BLASTn at reward/penalty score (1/−1). The optimal score strategy is most suitable for sequences with low conserved ratio32. For each genus, coverage map(s) for the reference sequence(s) were then generated. Phylogenetic analysis and multiple k-mer based de novo assembly The construction of a phylogenetic tree allows us to visualize the underlying genealogy between the contiguous sequences and reference sequences. In order to perform a phylogenetic analysis of candidate viruses in a certain viral genus, a backbone with high quality and wide spectrum is indispensable. For a genus, sequences with Refseq standard sunder that genus and the reference sequence which is used to generate the coverage map are selected to carry out multiple sequence alignment to build a backbone using MAFFT The de novo assembly step benefits from the classification method in VIP for significant reduction of complexity of reads. Still de novo assemblies from virus samples, especially RNA viruses, into a genome sequence is challenging due to extremely uneven read depth distribution caused by amplification bias in the inevitable reverse transcription and PCR amplification process during library preparations. We present the Multiple-k method in which various k-mer lengths are used for de novo assembly with Velvet-Oases34,35,36. In case that sparse reads do not overlap sufficiently to permit de novo assembly into longer contiguous sequences, assigned reads and contigs are retained if they are the most appropriate empirical >1.5 × longer than the average length of the candidate reads. Finally the largest contig after de novo assembly is added into the backbone to generate phylogenetic tree by unweighted pair-group method with arithmetic means (UPGMA) and visualized by Environment for Tree Exploration37. Reference database A 3.8 gigabase (Gb) human nucleotide database (human DB) was constructed from a combination of human genomic DNA, unlocalized DNA (GRCh38/hg38), ribosomal RNA (rRNA, RefSeq), RNA (RefSeq), and mitochondrial DNA (RefSeq) sequences in NCBI as of July of 2015. The viral nucleotide databases in fast mode were constructed from a combination of sequences in ViPR/IRD as of July of 2015. The viral nucleotide DB in sense mode consisted of 87071 entries was constructed by collection of all Refseq viral complete genomes and their neighbor genomes. The neighbor genomes were the complete viral nucleotide sequences which were non-RefSeq recorded from DDBJ, EMBL and GenBank. The viral protein databases were constructed from NCBI Refseq DB. The bacterial DB in sense mode was constructed from the collections of unique genome segments at species level within GOTTCHA38 package. Hardware VIP is tested on a common desktop PC with a 3.4 GHz Intel Core i7-4770 and 16 GB of RAM (running Ubuntu 14.04 LTS). Minimum hardware requirements for running VIP include a 4 GB of RAM, PC running Ubuntu 14.04 LTS (preferred). VIP and its external dependencies require about 500 MB of disk space. Reference data requires about 70 GB of disk space. During VIP runtime, up to 10 × the size of the input file may be needed as additional temporary storage.",VirusIdentification,"vip  integrate pipeline  metagenomics  virus identification  discovery
 vip  comprise   series  shell python  perl script  linux  incorporate several opensource tool vip   set  fix external software  database dependencies  userdefined custom parameters  pipeline accept crossplatform result generate   ion torrent  illumina   variety  format   fastq fasta sam  bam alignment format read  handle  concatenate  file   single file  streamline analysis data import  quality control raw ngs short read data   import  transformatted  fastq format use picard  vip  determine  encode version   input data   necessary  make sure  differences   way  quality score  generate  different sequence platforms  properly take  consideration  preprocessing vip  also accept fasta format raw ngs data  quality control step however   perform sequencebased strategies    complexity  length  main factor generally  quality control step  comprise  trim lowquality  adapter read remove lowcomplexity sequence use  dust algorithm  retain read  trim length   use prinseq31  fast mode bowtie2 alignments  first perform   host  follow  remove  hostrelated sequence  remain read  subject  viprird nucleotide   sense mode  initial alignment  host   bacteria   follow  subtraction  read map backgroundsubtracted read   subject   virus subset  ncbi   extensive classification  coverage map previous report suggest  viruses   potential  mutate rapidly  jump  species read  mutation region   viruses might  unclassified  classify   species  strain  order  avoid  misclassifications cause  mutations  apply  twosteps computational alignment strategy  classification  vip fig    first step  match read   assign   specific gene identifier   nucleotideamino acid alignment  read  therefore taxonomically classify  genus level  lookup  match    ncbi taxonomy database  sql structure query language  accord     scientific name   reference record   compose   species genus  family information  achieve  append   alignment result secondly read classify  genus  automatically map    likely reference genome  follow abundance  reference sequence   select  nucleotide alignment correspond   genus  calculate  sort   hypothesize  genome coverage percentage  alongside   sequence depth  specific reference sequence   word  higher abundance   genome suggest  higher possibility  recover  genome   reference sequence   follow key word  keep  complete genomes  complete sequence   complete cds assign read  directly map   nucleotide reference sequence select use optimal blastn  rewardpenalty score   optimal score strategy   suitable  sequence  low conserve ratio32   genus coverage map   reference sequence   generate phylogenetic analysis  multiple kmer base  novo assembly  construction   phylogenetic tree allow   visualize  underlie genealogy   contiguous sequence  reference sequence  order  perform  phylogenetic analysis  candidate viruses   certain viral genus  backbone  high quality  wide spectrum  indispensable   genus sequence  refseq standard sunder  genus   reference sequence   use  generate  coverage map  select  carry  multiple sequence alignment  build  backbone use mafft   novo assembly step benefit   classification method  vip  significant reduction  complexity  read still  novo assemblies  virus sample especially rna viruses   genome sequence  challenge due  extremely uneven read depth distribution cause  amplification bias   inevitable reverse transcription  pcr amplification process  library preparations  present  multiplek method   various kmer lengths  use   novo assembly  velvetoases34  case  sparse read   overlap sufficiently  permit  novo assembly  longer contiguous sequence assign read  contigs  retain      appropriate empirical   longer   average length   candidate read finally  largest contig   novo assembly  add   backbone  generate phylogenetic tree  unweighted pairgroup method  arithmetic mean upgma  visualize  environment  tree exploration37 reference database   gigabase  human nucleotide database human   construct   combination  human genomic dna unlocalized dna grch38hg38 ribosomal rna rrna refseq rna refseq  mitochondrial dna refseq sequence  ncbi   july    viral nucleotide databases  fast mode  construct   combination  sequence  viprird   july    viral nucleotide   sense mode consist   entries  construct  collection   refseq viral complete genomes   neighbor genomes  neighbor genomes   complete viral nucleotide sequence   nonrefseq record  ddbj embl  genbank  viral protein databases  construct  ncbi refseq   bacterial   sense mode  construct   collections  unique genome segment  species level within gottcha38 package hardware vip  test   common desktop     ghz intel core      ram run ubuntu  lts minimum hardware requirements  run vip include     ram  run ubuntu  lts prefer vip   external dependencies require     disk space reference data require     disk space  vip runtime      size   input file may  need  additional temporary storage",3
71,VSDToolkit,"An internet-based bioinformatics toolkit for plant biosecurity diagnosis and surveillance of viruses and viroids
Sample collection, RNA extraction and NGS sequencing Imported plants and positive control samples were grown in quarantine glasshouse facilities until sample collection. Plants were grown under natural lighting with a daytime temperature of approximately 22 °C. For each plant sample one or more leaves were collected prior to RNA extraction. Total RNA and/or small RNA enriched fraction (<200 bp) were extracted from approximately 10 mg of tissue using the mirVana microRNA isolation kit (Ambion, LifeTechnologies) following manufactures instructions. Collected samples were stored at -80 °C within quarantine facilities until shipped to the Beijing Genomics Institute (BGI, Hong Kong). Libraries were prepared using the TruSeq Small RNA Sample Prep Kit (Illumina) and sequenced with 50 bp single-end (SE) reads deep sequencing of collected small RNA samples (small RNA-Seq) on an Illumina HiSeq2000. We sequenced 21 quarantined plant samples (Additional file 1). Small RNA-Seq datasets has been submitted to the Short Read Archive (SRA) under the BioProject PRJNA325594. Selection of small RNA assembler and scaffolding tools We compared Velvet [16], SPAdes [17], ABySS [18] and SOAPdenovo [19] assemblers using twelve selected small RNA-Seq samples collected from distinct plant species generated in this study (Additional file 1). We tested de novo assembly using individual kmer lengths of 15 (K15), 17 (K17), 19 (K19) and 21 (K21) as well as combined kmer sets of 15,17,19 (K15-17-19) and 15,17,19,21 (K15-17-19-21). Assembled contigs were further scaffolded using CAP3 using optimised parameters for short overlaps (-o 16, -p 90, -i 30, -j 31, -s 300) [20]. Additionally, merging and scaffolding of contigs produced by two or all three assemblers were also evaluated. Assembly statistics were calculated using the Quality Assessment Tool for Genome Assemblies (QUAST) [21]. Overview of the automated viral diagnosis and surveillance toolkit The viruses and viroids surveillance and diagnosis (VSD) bioinformatics toolkit was developed utilising Yabi [15], an open source internet-based analytical environment that allows for the customisation of tools and scripts into analysis workflows [22]. Yabi has five tabs, namely, ‘Jobs’, ‘Design’, ‘Files’, ‘Account’ and ‘Admin’ tabs, where the later is only visible to a person or group responsible for the maintenance and further customisation of the Yabi platform [15]. The ‘Jobs’ tab allows visualising and downloading results from prior jobs. The ‘Design’ tab enables re-use of existing optimised workflows, design of modified versions of existing workflows, and the construction of new analysis workflows. The ‘Files’ tab present files and directories of all available backend resources (i.e. HPC and/or cloud instances) to the user [15]. The ‘Account’ tab enables a user to easily modify their password information to their Yabi account. The ‘Admin’ tab facilitates the management and addition of new computational tools into the Yabi environment. New features of the Yabi platform include: i) save and share workflows; ii) fetch data from public repositories; iii) submission of processed data to specialised databases such as National or International Patient Registries; and iv) enables ‘bioinformatics on demand’ analyses through the deployment of cloud instance at the beginning of a computational workflow and its obliteration at the final step of the data processing and analysis workflow. The VSD toolkit has three versions of the ‘virus and viroid detection’ workflows (Fig. 1), with users able to choose from three subsets of small RNA read lengths (21–25 nt, 21–22 nt, or 24 nt length reads). Existing automated workflows can be reused or modified and saved (Additional file 2). Additional workflows such as the ‘detecting novel viroids’ and ‘mapping reads onto a reference genome’ are also available, and can be run as a separate job, or added to the ‘virus and viroid detection’ workflows Virus and viroid detection workflow Files of small RNA reads in fastq format (gzipped files are accepted) are first uploaded through the ‘Files’ tab in Yabi. Files may be uploaded directly from a personal computer or transferred to a Yabi directory from another high performance computing storage location. Once the file is uploaded, users then navigate to the ‘Design’ tab, where they choose the saved workflow of interest (21–25 nt, 21–22 nt or 24 nt length reads). Users are also able to build their own workflows, by simply dragging and dropping tools into the workflow area. The first stage of the workflow is the ‘Select file’ tool. The fastq file of interest is then selected. If an adapter trimming step needs to be performed, users can add in the ‘fastx_clipper’ tool (http://hannonlab.cshl.edu/fastx_toolkit/) to the workflow, and perform quality control checks using the ‘fastQC’ tool [23]. Reads then undergo quality trimming through the content dependent read trimming tool ConDeTri (version 2.2), which trims and removes reads with low quality scores [24]. Minimum read length is set to 18 nt. Reads of the desired read length are then extracted through the ‘Extract_reads_21–25 nt’ , ‘Extract_reads_21–22 nt’ , or ‘Extract_reads_24 nt’ tools with de novo assemblies of contigs performed with SPAdes (version 3.5.0) with kmer sizes set to 15,17 and 19 [17]. Overlapping SPAdes contigs are then merged with CAP3 (version date 08/06/13) [20]. Contigs greater than or equal to 40 nt are then extracted using an in-house python script (‘Extract and rename contigs’ tool), and BLAST searched against databases generated from all plant, virus and viroid sequences populated by the entrez search query’s for viruses “txid10239 [orgn]”, not cellular organisms “txid131567 [orgn]”, viroids “txid12884 [orgn]” and plants “txid3193 [orgn]”. For BLASTN, the task is set to BLASTN short, and for both BLASTN and BLASTX, the maximum number of aligned sequences (-max_target_seqs) is set to 5 and the expected value (-evalue) set to 1e−10. BLASTN and BLASTX results are written out in a customised tabular format and extracted through an in-house script (‘Extract BLAST hits’ tool) into ‘plant’ or ‘virus and viroid’ BLAST output files. Contigs are also extracted into fasta output files through an in-house script (‘Extract contigs’) into subsets with a BLAST hit to plant or virus and viroid sequences, and contigs with no BLAST hits. The BLAST output files are then parsed through an in-house script, which produces several output files in csv format including all blast results in tabular format (header added), blast results with alignment lengths >39 nt, and a summary file which reports the Genbank ID of the virus or plant hit, the name of the plant or virus, the number of contig hits, the average percent sequence identity of the hit to the virus, the alignment length, the length of the virus or plant sequence, and the percentage coverage by contigs of a virus or plant sequence. These statistics are calculated using the Bioconductor’s GenomicRanges package (version 1.18.1). Detecting novel viroids workflow The output file ‘no_hits_contigs.fasta’ from the ‘Extract contigs’ tool represent sequences with no BLASTN and BLASTX sequence similarity to viral and plant sequences. These sequences are further filtered to extract contigs with lengths between 200–460 b that are typical for viroids. An in-house script is then utilised to evaluate the sequence similarity and overlap of both 5′-end and 3′-end of selected sequences. Sequences with overlapping ends with 100% sequence similarity are reported as putative circular viroid candidates. Users are recommended to further inspect identified candidate circular sequences. For example evaluate sequence similarity to non-coding RNA databases such as Rfam [25] and miRBase [26], which are not part of the VSD toolkit. Viroids from the Pospiviroidae (e.g. Grapevine yellow speckle viroid 1) and some from the Avsunviroidae (e.g. Avocado sunblotch viroid) form hairpin-like RNA secondary structures [27]. Thus, such confirmation can be evaluated in filtered candidate novel viroid circular sequences using RNAfold [28]. Furthermore, the expression of candidate sequences passing all filtering steps can be evaluated in multiple tissues of the infected plant and/or its progeny to validate i) the de novo assembled circular sequence, and ii) provide independent evidence of its expression in multiple tissues and/or individuals. Mapping small RNAs onto reference genome workflow Mapping of viRNAs onto identified viral genomes from similarity searches typically provide a broader coverage of the viral pathogen sequence as compared to de novo assembled contigs. Quality trimmed reads or the subset of reads (21–25 nt, 21–22 nt, or 24 nt length reads) can be mapped against a reference genome of choice (fasta file must also be uploaded by users) through bowtie2 [29]. Optionally, the SAMtools suite (Fig. 1) [30] can be used to sort and index aligned reads. The resulting alignment file (in sam or bam format) can then be downloaded and viewed using the java standalone tool MISIS [14]. Unique features of the VSD toolkit The major unique feature of our VSD toolkit as compared to VirFind [11], is the ability to exclusively use 21–22 nt small RNA reads for the de novo assembly of viral sequences. Assembly of viral sequences with this set of reads directly reflect the plant endogenous antiviral response mediated by Dicer4 and Dicer2 [31]. Additionally, we provide an assembly pipeline that uses 24 nt small RNAs overlapping the expected size for endogenous heterochromatin and transposon derived siRNAs [32]. This 24 nt pipeline identifies viral sequences potentially integrated in the host genome, particularly if they are not detected using 21–22 nt pipeline. Finally, we provide a 21–25 nt pipeline for users to compare their outputs against other published work that typically use a broad range of small RNAs and/or compare with the results from the targeted 21–22 nt and 24 nt pipelines. Another unique feature of the VSD toolkit is the ability to modify the parameter options for most of the individual steps in the workflow (Fig. 1). Additionally, the VSD toolkit uses optimised SPAdes de novo assembly settings that yield improved results as compared to other tested assemblers (See below). The similarity screening of viruses in VSD toolkit is run in parallel using both BLASTN and BLASTX [33] for all de novo assembled contigs against viruses and viroid sequences in the NT and NR databases (ftp://ftp.ncbi.nlm.nih.gov/blast/db/), respectively. The top five database hits for each de novo assembled contig are reported improving coverage of specific isolate/strain viral sequences and/or preventing false negative results when a top viral hit is annotated as “synthetic sequence”. VirFind runs BLAST screening in a staggered manner, reporting first nucleotide top hits against viral sequences, and then for contigs with negative BLASTN results, a BLASTX screening is conducted reporting the best hit [11]. Finally, the VSD toolkit also provides a list of potential viroid-like circular sequences with no sequence similarity to any nucleotide sequence in public databases. Deployment of the toolkit During optimisation and testing of the bioinformatics toolkit, the workflows were run on a dynamic SGE cluster located on Amazon Web Services (AWS) Elastic Compute Cloud (EC2), which allows compute nodes (29.4 GB RAM) to be easily added or removed as required. Yabi and the bioinformatics toolkit may be deployed on a variety of high performance computing resources.",VirusIdentification," internetbased bioinformatics toolkit  plant biosecurity diagnosis  surveillance  viruses  viroids
sample collection rna extraction  ngs sequence import plant  positive control sample  grow  quarantine glasshouse facilities  sample collection plant  grow  natural light   daytime temperature  approximately  °   plant sample one   leave  collect prior  rna extraction total rna andor small rna enrich fraction    extract  approximately    tissue use  mirvana microrna isolation kit ambion lifetechnologies follow manufacture instructions collect sample  store   ° within quarantine facilities  ship   beijing genomics institute bgi hong kong libraries  prepare use  truseq small rna sample prep kit illumina  sequence    singleend  read deep sequence  collect small rna sample small rnaseq   illumina hiseq2000  sequence  quarantine plant sample additional file  small rnaseq datasets   submit   short read archive sra   bioproject prjna325594 selection  small rna assembler  scaffold tool  compare velvet  spade  aby   soapdenovo  assemblers use twelve select small rnaseq sample collect  distinct plant species generate   study additional file   test  novo assembly use individual kmer lengths   k15  k17  k19   k21  well  combine kmer set   k15   k15 assemble contigs   scaffold use cap3 use optimise parameters  short overlap            additionally merge  scaffold  contigs produce  two   three assemblers  also evaluate assembly statistics  calculate use  quality assessment tool  genome assemblies quast  overview   automate viral diagnosis  surveillance toolkit  viruses  viroids surveillance  diagnosis vsd bioinformatics toolkit  develop utilise yabi   open source internetbased analytical environment  allow   customisation  tool  script  analysis workflows  yabi  five tabs namely job design file account  admin tabs   later   visible   person  group responsible   maintenance   customisation   yabi platform   job tab allow visualise  download result  prior job  design tab enable reuse  exist optimise workflows design  modify versions  exist workflows   construction  new analysis workflows  file tab present file  directories   available backend resources  hpc andor cloud instance   user   account tab enable  user  easily modify  password information   yabi account  admin tab facilitate  management  addition  new computational tool   yabi environment new feature   yabi platform include  save  share workflows  fetch data  public repositories iii submission  process data  specialise databases   national  international patient registries   enable bioinformatics  demand analyse   deployment  cloud instance   begin   computational workflow   obliteration   final step   data process  analysis workflow  vsd toolkit  three versions   virus  viroid detection workflows fig   users able  choose  three subsets  small rna read lengths        length read exist automate workflows   reuse  modify  save additional file  additional workflows    detect novel viroids  map read onto  reference genome  also available    run   separate job  add   virus  viroid detection workflows virus  viroid detection workflow file  small rna read  fastq format gzipped file  accept  first upload   file tab  yabi file may  upload directly   personal computer  transfer   yabi directory  another high performance compute storage location   file  upload users  navigate   design tab   choose  save workflow  interest        length read users  also able  build   workflows  simply drag  drop tool   workflow area  first stage   workflow   select file tool  fastq file  interest   select   adapter trim step need   perform users  add   fastx_clipper tool    workflow  perform quality control check use  fastqc tool  read  undergo quality trim   content dependent read trim tool condetri version   trim  remove read  low quality score  minimum read length  set    read   desire read length   extract   extract_reads_21   extract_reads_21    extract_reads_24  tool   novo assemblies  contigs perform  spade version   kmer size set      overlap spade contigs   merge  cap3 version date   contigs greater   equal      extract use  inhouse python script extract  rename contigs tool  blast search  databases generate   plant virus  viroid sequence populate   entrez search query  viruses “txid10239 orgn”  cellular organisms “txid131567 orgn” viroids “txid12884 orgn”  plant “txid3193 orgn”  blastn  task  set  blastn short    blastn  blastx  maximum number  align sequence max_target_seqs  set     expect value evalue set   blastn  blastx result  write    customise tabular format  extract   inhouse script extract blast hit tool  plant  virus  viroid blast output file contigs  also extract  fasta output file   inhouse script extract contigs  subsets   blast hit  plant  virus  viroid sequence  contigs   blast hit  blast output file   parse   inhouse script  produce several output file  csv format include  blast result  tabular format header add blast result  alignment lengths     summary file  report  genbank    virus  plant hit  name   plant  virus  number  contig hit  average percent sequence identity   hit   virus  alignment length  length   virus  plant sequence   percentage coverage  contigs   virus  plant sequence  statistics  calculate use  bioconductors genomicranges package version  detect novel viroids workflow  output file no_hits_contigsfasta   extract contigs tool represent sequence   blastn  blastx sequence similarity  viral  plant sequence  sequence   filter  extract contigs  lengths      typical  viroids  inhouse script   utilise  evaluate  sequence similarity  overlap   ′end  ′end  select sequence sequence  overlap end   sequence similarity  report  putative circular viroid candidates users  recommend   inspect identify candidate circular sequence  example evaluate sequence similarity  noncoding rna databases   rfam   mirbase     part   vsd toolkit viroids   pospiviroidae  grapevine yellow speckle viroid      avsunviroidae  avocado sunblotch viroid form hairpinlike rna secondary structure  thus  confirmation   evaluate  filter candidate novel viroid circular sequence use rnafold  furthermore  expression  candidate sequence pass  filter step   evaluate  multiple tissue   infect plant andor  progeny  validate    novo assemble circular sequence   provide independent evidence   expression  multiple tissue andor individuals map small rnas onto reference genome workflow map  virnas onto identify viral genomes  similarity search typically provide  broader coverage   viral pathogen sequence  compare   novo assemble contigs quality trim read   subset  read        length read   map   reference genome  choice fasta file must also  upload  users  bowtie2  optionally  samtools suite fig     use  sort  index align read  result alignment file  sam  bam format    download  view use  java standalone tool misis  unique feature   vsd toolkit  major unique feature   vsd toolkit  compare  virfind    ability  exclusively use   small rna read    novo assembly  viral sequence assembly  viral sequence   set  read directly reflect  plant endogenous antiviral response mediate  dicer4  dicer2  additionally  provide  assembly pipeline  use   small rnas overlap  expect size  endogenous heterochromatin  transposon derive sirnas     pipeline identify viral sequence potentially integrate   host genome particularly     detect use   pipeline finally  provide    pipeline  users  compare  output   publish work  typically use  broad range  small rnas andor compare   result   target      pipelines another unique feature   vsd toolkit   ability  modify  parameter options     individual step   workflow fig  additionally  vsd toolkit use optimise spade  novo assembly settings  yield improve result  compare   test assemblers see   similarity screen  viruses  vsd toolkit  run  parallel use  blastn  blastx     novo assemble contigs  viruses  viroid sequence      databases ftpftpncbinlmnihgovblastdb respectively  top five database hit    novo assemble contig  report improve coverage  specific isolatestrain viral sequence andor prevent false negative result   top viral hit  annotate  “synthetic sequence” virfind run blast screen   stagger manner report first nucleotide top hit  viral sequence    contigs  negative blastn result  blastx screen  conduct report  best hit  finally  vsd toolkit also provide  list  potential viroidlike circular sequence   sequence similarity   nucleotide sequence  public databases deployment   toolkit  optimisation  test   bioinformatics toolkit  workflows  run   dynamic sge cluster locate  amazon web service aws elastic compute cloud ec2  allow compute nod   ram   easily add  remove  require yabi   bioinformatics toolkit may  deploy   variety  high performance compute resources",3
72,VirusHunter,"Identification of novel viruses using VirusHunter--an automated data analysis pipeline
Virus Strain Salanga virus (strain AnB 904a) was originally isolated from a rodent (Aethomys medicatus) collected in September 1971 at Salanga, Central African Republic. The Heramatsu virus (strain KY-663) used in this study was originally isolated from the blood of a bat (Myotis macrodactylus) collected in a mine at Heramatsu, Kagoshima, Japan in July 18th, 1965. Transmission Electron Microscopy For ultrastructural analysis in ultrathin sections, infected Vero or BHK cells were fixed for at least 1 h in a mixture of 2.5% formaldehyde prepared from paraformaldehyde powder, and 0.1% glutaraldehyde in 0.05 M cacodylate buffer pH 7.3 to which 0.03% picric acid and 0.03% CaCl2 were added. The monolayers were washed in 0.1 M cacodylate buffer, cells were scraped off and processed further as a pellet. The pellets were post-fixed in 1% OsO4 in 0.1 M cacodylate buffer pH 7.3 for 1 h, washed with distilled water and en bloc stained with 2% aqueous uranyl acetate for 20 min at 60°C. The pellets were dehydrated in ethanol, processed through propylene oxide and embedded in Poly/Bed 812 (Polysciences, Warrington, PA). Ultrathin sections were cut on Leica EM UC7 ultramicrotome (Leica Microsystems, Buffalo Grove, IL), stained with lead citrate and examined in a Philips 201 transmission electron microscope at 60 kV. Antigens and Immune Reagents Antigens used in serologic tests were infected newborn mouse brains prepared by the sucrose/acetone extraction method [27]. Specific hyperimmune mouse ascitic fluids were prepared at the WRCEVA against each of the viruses used in this study. The immunization schedule consisted of four intraperitoneal injections given at weekly intervals. Immunogens were 10% suspensions of homogenized infected mouse brain in PBS, mixed with Freund’s adjuvant just prior to inoculation. Sarcoma 180 cells were also given intraperitoneally with the final immunization in order to induce ascites formation. All animal experiments were carried out under an animal protocol approved by the University of Texas Medical Branch IACUC committee. Serologic Tests Hemagglutination inhibition (HI) tests were done in microtiter plates, as previously described [28], using four units of antigen. The HI titer was read after overnight incubation of antigen and antibody at 4°C. Complement-fixation (CF) tests were performed by the microtiter technique [28], using two units of guinea pig complement and overnight incubation of the antigen and antibody at 4°C. The CF titers were recorded as the highest dilutions giving 3+ or 4+ fixation of complement on a scale of 0 to 4+. Preparation and Sequencing of Viral DNA and RNA Total nucleic acid was extracted from Heramatsu virus infected BHK cell culture supernatant and cell lysate independently with a Qiagen DNeasy kit according to the manufacturer’s instructions. DNA and RNA were extracted from Salanga virus infected culture of Vero E6 cells cell separately. Both cell lines were obtained from the American Type Culture Collection (Manassas, VA). Nucleic acid from each sample was reverse-transcribed to enable subsequent detection of both RNA and DNA viruses and was amplified as previously described [29,30]. Amplification products were pooled, adaptor-ligated and sequenced at the Washington University Genome Sequencing Center on the 454 GS-FLX platform (454 Life Sciences). Sequences were then trimmed to remove primer B sequences prior to assembly using the Newbler program (454 Life Sciences, Branford, CT). Next Generation Sequencing Data Analysis Because the nucleic acid used for sequencing contained a mixture of host cell DNA and virus RNA, we used a customized informatics pipeline VirusHunter to identify viral sequences. The VirusHunter pipeline is controlled by a master Perl script VirusHunter.pl, which executes each step in the pipeline. The input to the pipeline is a directory path. The directory holds sequencing data from different samples. Each sample has its own directory with a single file containing FASTA format sequence reads. The workflow of the pipeline is shown in Figure 2. Below is a brief description of each step. 1. Remove redundant sequences. Identical or nearly-identical sequences are frequently present in NGS data, either due to the sheer depth of NGS or because many of the pre-sequencing sample preparation methods involve PCR amplification. To reduce the computing cost of downstream analysis, CD-HIT [19] is first used to cluster similar sequences. The default parameters in VirusHunter are set to cluster sequences that share ≥ 95% identity over 95% of the sequence length. The longest sequence from each cluster is retained as the representative sequence and used for downstream analysis. These are the “unique sequences”. 2. Mask repetitive sequences and sequence quality control. Many eukaryotic genomes contain stretches of highly repetitive DNA sequences which cause problems in BLAST-based similarity searches and result in high rates of false-positive alignments. RepeatMasker (http://www.repeatmasker.org) is used to mask interspersed repeats and low complexity DNA sequences. A sequence fails the quality control criteria if it does not contain a stretch of at least 50 consecutive non-“N” nucleotides (i.e., “Filtered sequence”) or if greater than 40% of the total length of the sequence is masked (i.e., ""low complexity sequence""). These sequences are removed from further analysis. Remaining sequences are “good sequences”. 3. Filtering host sequences. Sequences are subjected to BLASTn [31] alignment against the appropriate host genome (default Human). BLASTn output files are parsed using BioPerl [32]. Any sequence that shares significant similarity (e value ≤ 1E-10) is classified as ""Host"" and removed from further analysis. Any desired sequence database can be used for filtering. For Salanga virus, which was a unique case wherein the virus was cultured in Vero (African Green Monkey) cells, but inoculated with infected newborn mouse brain homogenate, we used the reference Human genome from the 1000 Genomes Project as reference (ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/) for simplicity; for Heramatsu virus, which was cultured in the BHK (hamster) cell line we used the golden hamster Mesocricetus auratus genome as the reference (GenBank Assembly ID: GCA_000349665.1). 4. BLASTn against NCBI nt database. Sequences retained from the previous step are queried against the NCBI nt database using BLASTn. Sequences with significant hits (e value cutoff 1E-10) are broadly classified as human, mouse, fungal, bacterial, phage, viral or other based on the taxonomy identity of the best BLAST hit. The NCBI gi – taxid data for nucleotide sequences are uploaded to a MySQL database. The gi number of the best BLAST hit is used to query the database to obtain the taxonomy ID which is in turn used to retrieve the full taxonomy lineage using BioPerl. In some instances, one query has two or more hits with the same e value. If a sequence aligns to both a virus and a sequence derived from another organism type (e.g. bacteria or fungi) with the same e value it is classified as “ambiguous”. All eukaryotic viral sequences are further classified into viral families based on the taxonomy ID of the best hit. Sequences without hits progress to the next step. 5. BLASTx against NCBI nr database. Sequences retained from the previous step are queried against the NCBI nr database using BLASTx (e-value cutoff 1E-5). BLASTx output files are parsed and sequences are phylotyped as described in the previous step. Sequences without any significant hit are placed in the “unassigned” category. 6. Report of the Findings The final output of the VirusHunter pipeline is a single file summarizing all the viruses identified in each dataset in the input directory. The pipeline can be customized to generate similar outputs for other bacterial, fungal or parasitic sequences. VirusHunter is written in Perl. It uses shell scripting, the BioPerl library, MySQL database, CD-HIT, RepeatMasker and NCBI BLAST suite. The pipeline is fully automated and high-throughput. All components are organized in a hierarchical set of readily modifiable scripts, and multiple copies of the pipeline can be run in parallel. The pipeline is designed for running on a high performance computing cluster using GridEngine as the job scheduler. It can be easily modified to use other job management software. The pipeline can be easily customized, for example to replace the human genome database with that of a different host. Installation and configuration of VirusHunter requires basic knowledge of Perl, MySQL database and Linux system administration. Distribution and source code for VirusHunter 1.0 are available at (http://www.ibridgenetwork.org/wustl/virushunter). Assembly of the Viral Genome and Genome Annotation Sequences identified as viral as well as sequences that had no significant hit to any sequence in the NR and NT databases were assembled using Newbler (454 Life Sciences, Branford, CT) with default parameters. ORFs were predicted and annotated using Artemis [33]. Multiple sequence alignments were performed with ClustalW [34]. Phylogenetic analysis was performed using both neighbor-joining method and maximum parsimony method in MEGA5 program [35] with 1000 bootstrap replicates. Trees with the same topology were generated using both methods for all the data sets used in the study. Phylogenetic trees were visualized using TreeView [36]. Viruses Analyzed and Sequence Accession Numbers Used for Analyses. The viruses analyzed included the following genera, species, and strains: Bunyaviridae Phlebovirus Genus. Aguacate virus (AGUV, YP_004414703.1), Arbia virus (ARBV, AGA82737.1), Arumowot virus (AMTV, AEF30501.1), Durania virus (DURV, AEB70976.1), Leticia virus (AEL29649.1, also named Phlebovirus CoAr 171616), Odrenisrou virus (ODRV, AEL29670.1), Rift Valley fever virus (RVFV, YP_003848704.1), Rift Valley fever virus (RVFV_ABD51499.1, ABD51499.1), Salehabad virus (SALV, AGA82741.1), Toscana virus (TOSV, CAA48478.1), Uukuniemi virus (UUKV, BAA01590.1). Reoviridae Orbivirus Genus. African horsesickness virus serotype 1 (AHSV-1, CAP04840.1), African horsesickness virus serotype 9 (AHSV-9, AAC40586.1), Bluetongue virus Serotype 10 (BTV-10, YP_052968.1), Bluetongue virus Serotype 12 (BTV-12, AGJ83521.1), Chuzan virus (CHUV, BAA76549.1), Epizootic hemorrhagic disease virus serotype 1 / strain New Jersey (EHDV-1, YP_003240108.1), Epizootic hemorrhagic disease virus serotype 7 / strain CSIRO 775 (EHDV-7, CAN99553.1), Great Island virus (GIV, YP_003896058.1). St Croix River virus (SCRV, YP_052942.1), Yunnan orbivirus (YUOV, YP_443925.1).",VirusIdentification,"identification  novel viruses use virushunteran automate data analysis pipeline
virus strain salanga virus strain anb   originally isolate   rodent aethomys medicatus collect  september   salanga central african republic  heramatsu virus strain  use   study  originally isolate   blood   bat myotis macrodactylus collect   mine  heramatsu kagoshima japan  july 18th  transmission electron microscopy  ultrastructural analysis  ultrathin section infect vero  bhk cells  fix   least     mixture   formaldehyde prepare  paraformaldehyde powder   glutaraldehyde    cacodylate buffer      picric acid   cacl2  add  monolayers  wash    cacodylate buffer cells  scrap   process    pellet  pellets  postfixed   oso4    cacodylate buffer      wash  distil water   bloc stain   aqueous uranyl acetate   min  °  pellets  dehydrate  ethanol process  propylene oxide  embed  polybed  polysciences warrington  ultrathin section  cut  leica  uc7 ultramicrotome leica microsystems buffalo grove  stain  lead citrate  examine   philips  transmission electron microscope    antigens  immune reagents antigens use  serologic test  infect newborn mouse brain prepare   sucroseacetone extraction method  specific hyperimmune mouse ascitic fluids  prepare   wrceva     viruses use   study  immunization schedule consist  four intraperitoneal injections give  weekly intervals immunogens   suspensions  homogenize infect mouse brain  pbs mix  freunds adjuvant  prior  inoculation sarcoma  cells  also give intraperitoneally   final immunization  order  induce ascites formation  animal experiment  carry    animal protocol approve   university  texas medical branch iacuc committee serologic test hemagglutination inhibition  test    microtiter plat  previously describe  use four units  antigen   titer  read  overnight incubation  antigen  antibody  ° complementfixation  test  perform   microtiter technique  use two units  guinea pig complement  overnight incubation   antigen  antibody  °   titers  record   highest dilutions give    fixation  complement   scale     preparation  sequence  viral dna  rna total nucleic acid  extract  heramatsu virus infect bhk cell culture supernatant  cell lysate independently   qiagen dneasy kit accord   manufacturers instructions dna  rna  extract  salanga virus infect culture  vero  cells cell separately  cell line  obtain   american type culture collection manassas  nucleic acid   sample  reversetranscribed  enable subsequent detection   rna  dna viruses   amplify  previously describe  amplification products  pool adaptorligated  sequence   washington university genome sequence center    gsflx platform  life sciences sequence   trim  remove primer  sequence prior  assembly use  newbler program  life sciences branford  next generation sequence data analysis   nucleic acid use  sequence contain  mixture  host cell dna  virus rna  use  customize informatics pipeline virushunter  identify viral sequence  virushunter pipeline  control   master perl script virushunterpl  execute  step   pipeline  input   pipeline   directory path  directory hold sequence data  different sample  sample    directory   single file contain fasta format sequence read  workflow   pipeline  show  figure     brief description   step  remove redundant sequence identical  nearlyidentical sequence  frequently present  ngs data either due   sheer depth  ngs   many   presequencing sample preparation methods involve pcr amplification  reduce  compute cost  downstream analysis cdhit   first use  cluster similar sequence  default parameters  virushunter  set  cluster sequence  share ≥  identity     sequence length  longest sequence   cluster  retain   representative sequence  use  downstream analysis    “unique sequences”  mask repetitive sequence  sequence quality control many eukaryotic genomes contain stretch  highly repetitive dna sequence  cause problems  blastbased similarity search  result  high rat  falsepositive alignments repeatmasker   use  mask intersperse repeat  low complexity dna sequence  sequence fail  quality control criteria     contain  stretch   least  consecutive non“” nucleotides  “filtered sequence”   greater     total length   sequence  mask  ""low complexity sequence""  sequence  remove   analysis remain sequence  “good sequences”  filter host sequence sequence  subject  blastn  alignment   appropriate host genome default human blastn output file  parse use bioperl   sequence  share significant similarity  value ≤   classify  ""host""  remove   analysis  desire sequence database   use  filter  salanga virus    unique case wherein  virus  culture  vero african green monkey cells  inoculate  infect newborn mouse brain homogenate  use  reference human genome    genomes project  reference ftpftpncbinlmnihgov1000genomesftptechnicalreference  simplicity  heramatsu virus   culture   bhk hamster cell line  use  golden hamster mesocricetus auratus genome   reference genbank assembly  gca_000349665  blastn  ncbi  database sequence retain   previous step  query   ncbi  database use blastn sequence  significant hit  value cutoff   broadly classify  human mouse fungal bacterial phage viral   base   taxonomy identity   best blast hit  ncbi   taxid data  nucleotide sequence  upload   mysql database   number   best blast hit  use  query  database  obtain  taxonomy     turn use  retrieve  full taxonomy lineage use bioperl   instance one query  two   hit     value   sequence align    virus   sequence derive  another organism type  bacteria  fungi     value   classify  “ambiguous”  eukaryotic viral sequence   classify  viral families base   taxonomy    best hit sequence without hit progress   next step  blastx  ncbi  database sequence retain   previous step  query   ncbi  database use blastx evalue cutoff  blastx output file  parse  sequence  phylotyped  describe   previous step sequence without  significant hit  place   “unassigned” category  report   find  final output   virushunter pipeline   single file summarize   viruses identify   dataset   input directory  pipeline   customize  generate similar output   bacterial fungal  parasitic sequence virushunter  write  perl  use shell script  bioperl library mysql database cdhit repeatmasker  ncbi blast suite  pipeline  fully automate  highthroughput  components  organize   hierarchical set  readily modifiable script  multiple copy   pipeline   run  parallel  pipeline  design  run   high performance compute cluster use gridengine   job scheduler    easily modify  use  job management software  pipeline   easily customize  example  replace  human genome database     different host installation  configuration  virushunter require basic knowledge  perl mysql database  linux system administration distribution  source code  virushunter   available   assembly   viral genome  genome annotation sequence identify  viral  well  sequence    significant hit   sequence      databases  assemble use newbler  life sciences branford   default parameters orfs  predict  annotate use artemis  multiple sequence alignments  perform  clustalw  phylogenetic analysis  perform use  neighborjoining method  maximum parsimony method  mega5 program    bootstrap replicate tree    topology  generate use  methods    data set use   study phylogenetic tree  visualize use treeview  viruses analyze  sequence accession number use  analyse  viruses analyze include  follow genera species  strain bunyaviridae phlebovirus genus aguacate virus aguv yp_004414703 arbia virus arbv aga82737 arumowot virus amtv aef30501 durania virus durv aeb70976 leticia virus ael29649 also name phlebovirus coar  odrenisrou virus odrv ael29670 rift valley fever virus rvfv yp_003848704 rift valley fever virus rvfv_abd51499 abd51499 salehabad virus salv aga82741 toscana virus tosv caa48478 uukuniemi virus uukv baa01590 reoviridae orbivirus genus african horsesickness virus serotype  ahsv cap04840 african horsesickness virus serotype  ahsv aac40586 bluetongue virus serotype  btv yp_052968 bluetongue virus serotype  btv agj83521 chuzan virus chuv baa76549 epizootic hemorrhagic disease virus serotype   strain new jersey ehdv yp_003240108 epizootic hemorrhagic disease virus serotype   strain csiro  ehdv can99553 great island virus giv yp_003896058  croix river virus scrv yp_052942 yunnan orbivirus yuov yp_443925",3
73,Vipie,"Vipie: web pipeline for parallel characterization of viral populations from multiple NGS samples
Our pipeline processes de-multiplexed paired FASTQ files, the most typical product of metagenomics sequencing. Several steps are then performed in parallel for all samples: quality control (QC), de-novo assembly of putative genomic contigs, taxonomic classification of the assembled contigs and orphan singleton reads by performing Blast queries against a local custom virus database derived from Genbank, and finally remapping of the sequencing reads onto reference sequences identified by this taxonomic classification. Default analysis parameters can be easily modified (e.g. the QC stringency, or the de novo assembly algorithm). Depicted in Fig. 1, Vipie pipeline uses multi processor architecture with integration of PostgreSQL for performance and data management while providing secured interactive results and allowing web form parameters for QC, assembly and scoring. The individual parameters and its default values are listed in the user guide. Trimming and quality control are parameter based applying Galaxy project utilities [13, 14]. We have integrated leading de-novo assembly tools - Velvet [15], MetaVelvet [16], IDBA [17] and MEGAHIT (SOAPDENOVO) [18] and ABySS [19]; these methods and tools are further described and reviewed [5, 20,21,22]. Taxonomic identification is performed using BLAST [23] against a local NCBI database restricted to whole virus genomes. The final step of the parallel analysis remaps the raw reads using BWA [24] onto a list of best matches from the BLAST queries, and lists the count of original reads matching to each of these references. In cases where reads match equally well to multiple viruses, the score is divided among such best matches to express importantly the ambiguity in assignation of the motifs shared among viral taxa, and the uncertainty of the presently available classification. De-novo contigs and reads that do not match to any currently known virus, optionally filtered for human genome and known ribosomal DNA, can be retrieved for further analysis as this ‘dark matter’ of the virome presumably containing novel viruses. Our pipeline allows a direct export of these unmapped reads owing to three-step filtering strategy. Reads unmatched to known viruses are first deprived of sequences that match to ribosomal DNA of bacterial, archeal and fungal origin. This is performed by remapping the reads by the BWA program to databases of 16S, 23S and 5S rDNA (a copy of ftp.ncbi.nlm.nih.gov/genomes/TARGET, and a reduced database of 5S rDNA http://www.combio.pl/rrna/) [25]. The next step remaps the reduced set of reads to the human genome. This step yields the potential dark matter of the human genome, mixed with a small proportion of bacterial genomic DNA. Our pipeline does not filter out these bacterial genomic reads, as they may contain novel lysogenic (dormant) phages. VIPIE’s reference virus database was built from three sources and clustering the sequences to the 97% level of identity further reduced the complexity. First, all viruses were downloaded from the refseq database at the NCBI (https://ftp.ncbi.nih.gov/refseq/release/viral/), and reduced to 97% identity by using the CD-HIT program (https://github.com/weizhongli/cdhit/[26]). Then, all virus sequences labeled as “complete”, with the “txid10239” (superkingdom Viruses) in the “Orgn” field were retrieved from Genbank. The query retrieved approximately 80,000 sequences from the database, which were subsequently reduced to the 97% similarity by using the CD-HIT program. Finally, similarly to previous two databases, phages were merged and clustered from the European Bioinformatics Institute (EBI) repository (ftp.ebi.ac.uk/pub/databases/fastafiles/embl_genomes/genomes/Phage/). The web form, interface dialogs and results are programmed to HTML5 standards and using JavaScript and modern, open source JavaScript libraries (https://jquery.org, https://datatables.net) for browser compatibility. Biopython [27] is used for sequencing parsing and formatting. Parallel processing is achieved via python (https://www.python.org) subprocess module implementation and uses PostgreSQL (https://www.postgresql.org) schema for job tracking and results merging. Standard SMTP library is used for notification, hence the email registration requirement. Clustered heatmaps are implemented with R ggplot2 [28] while other summary and alpha diversity statistics are computed using custom python scripts. Population maps and read distribution count summary charts are created using highcharts.js (https://www.highcharts.com) and custom event handlers for interactivity.",VirusIdentification,"vipie web pipeline  parallel characterization  viral populations  multiple ngs samples
 pipeline process demultiplexed pair fastq file   typical product  metagenomics sequence several step   perform  parallel   sample quality control  denovo assembly  putative genomic contigs taxonomic classification   assemble contigs  orphan singleton read  perform blast query   local custom virus database derive  genbank  finally remapping   sequence read onto reference sequence identify   taxonomic classification default analysis parameters   easily modify    stringency    novo assembly algorithm depict  fig  vipie pipeline use multi processor architecture  integration  postgresql  performance  data management  provide secure interactive result  allow web form parameters   assembly  score  individual parameters   default value  list   user guide trim  quality control  parameter base apply galaxy project utilities     integrate lead denovo assembly tool  velvet  metavelvet  idba   megahit soapdenovo   aby   methods  tool   describe  review   taxonomic identification  perform use blast    local ncbi database restrict  whole virus genomes  final step   parallel analysis remaps  raw read use bwa  onto  list  best match   blast query  list  count  original read match     reference  case  read match equally well  multiple viruses  score  divide among  best match  express importantly  ambiguity  assignation   motifs share among viral taxa   uncertainty   presently available classification denovo contigs  read    match   currently know virus optionally filter  human genome  know ribosomal dna   retrieve   analysis   dark matter   virome presumably contain novel viruses  pipeline allow  direct export   unmapped read owe  threestep filter strategy read unmatched  know viruses  first deprive  sequence  match  ribosomal dna  bacterial archeal  fungal origin   perform  remapping  read   bwa program  databases      rdna  copy  ftpncbinlmnihgovgenomestarget   reduce database   rdna    next step remaps  reduce set  read   human genome  step yield  potential dark matter   human genome mix   small proportion  bacterial genomic dna  pipeline   filter   bacterial genomic read   may contain novel lysogenic dormant phages vipies reference virus database  build  three source  cluster  sequence    level  identity  reduce  complexity first  viruses  download   refseq database   ncbi   reduce   identity  use  cdhit program    virus sequence label  “complete”   “txid10239” superkingdom viruses   “orgn” field  retrieve  genbank  query retrieve approximately  sequence   database   subsequently reduce    similarity  use  cdhit program finally similarly  previous two databases phages  merge  cluster   european bioinformatics institute ebi repository ftpebiacukpubdatabasesfastafilesembl_genomesgenomesphage  web form interface dialogs  result  program  html5 standards  use javascript  modern open source javascript libraries    browser compatibility biopython   use  sequence parse  format parallel process  achieve via python  subprocess module implementation  use postgresql  schema  job track  result merge standard smtp library  use  notification hence  email registration requirement cluster heatmaps  implement   ggplot2    summary  alpha diversity statistics  compute use custom python script population map  read distribution count summary chart  create use highchartsjs   custom event handlers  interactivity",3
74,ViromeScan,"ViromeScan: a new tool for metagenomic viral community profiling
Workflow of the software Once downloaded, ViromeScan locally processes the metagenome for searching eukaryotic viral sequences. Input files should be single-end or paired-end reads in fastq format (for paired-end reads compressed files in gzip, bzip2 and zip formats are also accepted) retrieved from shotgun sequencing or RNA-seq. Depending on the research strategy, ViromeScan gives users the option to choose from a range of in-house built reference databases, including human DNA virus database, human DNA/RNA virus database, eukaryotic DNA virus database and eukaryotic DNA/RNA virus database. The human virus databases contain only viruses that have the human being as the natural host; on the other hand, the eukaryotic virus databases also include viruses for vertebrates, invertebrates, fungi, algae and plants, while excluding bacteriophages. All databases are based on the complete viral genomes available on the NCBI website [23]. The NCBI IDs of the viral genomes used to build the different databases are reported in Additional file 1. The schematic description of the procedures of analysis computed by ViromeScan is provided in Fig. 1. In detail, metagenomic reads are compared to the viral genomes of the selected database using bowtie2 [24]. This first step is a complete and accurate screening of the sequences to select candidate viral reads. To perform this procedure before filtering processes allows a considerable gain of time in the subsequent parts of the pipeline, due to the reduction of the dataset to less than 1 % of the total amount of metagenomic reads. Afterwards, a quality filtering step of the candidate viral reads has been implemented as described in the processing procedure of the Human Microbiome Project (HMP) [25]. In brief, sequences are trimmed for low quality score using a modified version of the script trimBWAstyle.pl that works directly from BAM files [26]. The script is utilized to trim bases off the ends of sequences, which show a quality value of two or lower. This threshold is taken to delete all the bases with an uncertain quality as defined by Illumina’s EAMMS (End Anchored Max Scoring Segments) filter. Additionally, reads trimmed to less than 60 bp are also removed. Since the sequences analyzed are whole-genome or RNA-seq products, it is plausible that the candidate viral reads contain a small percentage of human reads. For this reason, it is necessary to subject the sequences to the control for human contamination. As reported in the HMP procedures [27], Human Best Match Tagger (BMTagger) [28] is an efficient tool that discriminates among human, viral and microbial reads. First of all, BMTagger attempts to discriminate between human reads and the other reads by comparing the 18mers produced from the input file with those contained in the reference human database. If this process fails, an additional alignment procedure is performed to guarantee the detection of all matches with up to two errors. Human-filtered reads may also contain an amount of bacterial sequences, which need to be filtered out to avoid biases due to bacterial contamination. Bacterial reads are identified and masked using BMTagger, the same tool utilized for the human sequence removal procedure. In particular, in order to detect bacterial sequences, human-filtered reads are screened against the genomic DNA of a representative group of bacterial taxa that are known to be common in the human body niches. See Additional file 2 for the list of bacteria included in this process. Nevertheless, the user can customize the filtering procedure by replacing the bacterial database within the ViromeScan folder with the microbial sequences of interest, associated to environments other than the human body (e.g. microbiome associated with animals, soil or water). Finally, filtered reads are again compared to the viral genomes of the chosen hierarchical viral database using bowtie2 [24], allowing the definitive association of each virome sequence to a viral genome. For each sample analyzed, the total amount of counts is summarized in a table as number of hits and relative abundance. Additionally, graphs representing the abundances at family, genus and species level are provided, using the “graphics” and “base” R packages. Validation of the tool and comparison with other existing methods Five different mock communities each containing 20 human DNA viruses at different relative abundances were built and submitted to ViromeScan for its validation. The mock communities contained also human sequences and reads of other microorganisms to test the filtering steps of the pipeline. The simulated metagenomes were composed of sequences of 100 bp randomly generated from the chosen genomic DNAs by an in-house developed script. In order to compare the performance of ViromeScan with other existing tools, the same mock samples were analyzed using Metavir [22] and blastN [29]. In particular, in the Metavir pipeline, we determined the taxonomic composition using the number of best hits normalized by genome length through the GAAS metagenomic tool [30]. The genomes used to generate the five mock communities are reported in Additional file 3. Case study: using ViromeScan to profile the eukaryotic DNA virome across different human body sites Twenty metagenomic samples from HMP [27], belonging to four body sites, including stool, mid vagina, buccal mucosa and retroauricular crease, were used to illustrate the results that can be obtained by ViromeScan. The IDs of HMP samples are reported in Additional file 4. These metagenomes had been sequenced using the Illumina GAIIx platform with 101 bp paired-end reads. The entire metagenomic dataset was utilized to study the differences in the composition of the viral communities across different body sites. No ethics approval was required for any work performed in this study.",VirusIdentification,"viromescan  new tool  metagenomic viral community profiling
workflow   software  download viromescan locally process  metagenome  search eukaryotic viral sequence input file   singleend  pairedend read  fastq format  pairedend read compress file  gzip bzip2  zip format  also accept retrieve  shotgun sequence  rnaseq depend   research strategy viromescan give users  option  choose   range  inhouse build reference databases include human dna virus database human dnarna virus database eukaryotic dna virus database  eukaryotic dnarna virus database  human virus databases contain  viruses    human    natural host    hand  eukaryotic virus databases also include viruses  vertebrates invertebrates fungi algae  plant  exclude bacteriophages  databases  base   complete viral genomes available   ncbi website   ncbi ids   viral genomes use  build  different databases  report  additional file   schematic description   procedures  analysis compute  viromescan  provide  fig   detail metagenomic read  compare   viral genomes   select database use bowtie2   first step   complete  accurate screen   sequence  select candidate viral read  perform  procedure  filter process allow  considerable gain  time   subsequent part   pipeline due   reduction   dataset  less      total amount  metagenomic read afterwards  quality filter step   candidate viral read   implement  describe   process procedure   human microbiome project hmp   brief sequence  trim  low quality score use  modify version   script trimbwastylepl  work directly  bam file   script  utilize  trim base   end  sequence  show  quality value  two  lower  threshold  take  delete   base   uncertain quality  define  illuminas eamms end anchor max score segment filter additionally read trim  less     also remove since  sequence analyze  wholegenome  rnaseq products   plausible   candidate viral read contain  small percentage  human read   reason   necessary  subject  sequence   control  human contamination  report   hmp procedures  human best match tagger bmtagger    efficient tool  discriminate among human viral  microbial read first   bmtagger attempt  discriminate  human read    read  compare  18mers produce   input file   contain   reference human database   process fail  additional alignment procedure  perform  guarantee  detection   match    two errors humanfiltered read may also contain  amount  bacterial sequence  need   filter   avoid bias due  bacterial contamination bacterial read  identify  mask use bmtagger   tool utilize   human sequence removal procedure  particular  order  detect bacterial sequence humanfiltered read  screen   genomic dna   representative group  bacterial taxa   know   common   human body niches see additional file    list  bacteria include   process nevertheless  user  customize  filter procedure  replace  bacterial database within  viromescan folder   microbial sequence  interest associate  environments    human body  microbiome associate  animals soil  water finally filter read   compare   viral genomes   choose hierarchical viral database use bowtie2  allow  definitive association   virome sequence   viral genome   sample analyze  total amount  count  summarize   table  number  hit  relative abundance additionally graph represent  abundances  family genus  species level  provide use  “graphics”  “base”  package validation   tool  comparison   exist methods five different mock communities  contain  human dna viruses  different relative abundances  build  submit  viromescan   validation  mock communities contain also human sequence  read   microorganisms  test  filter step   pipeline  simulate metagenomes  compose  sequence    randomly generate   choose genomic dnas   inhouse develop script  order  compare  performance  viromescan   exist tool   mock sample  analyze use metavir   blastn   particular   metavir pipeline  determine  taxonomic composition use  number  best hit normalize  genome length   gaas metagenomic tool   genomes use  generate  five mock communities  report  additional file  case study use viromescan  profile  eukaryotic dna virome across different human body sit twenty metagenomic sample  hmp  belong  four body sit include stool mid vagina buccal mucosa  retroauricular crease  use  illustrate  result    obtain  viromescan  ids  hmp sample  report  additional file   metagenomes   sequence use  illumina gaiix platform    pairedend read  entire metagenomic dataset  utilize  study  differences   composition   viral communities across different body sit  ethics approval  require   work perform   study",3
75,Genome Detective,"Genome Detective: an automated system for virus identification from high-throughput sequencing data
Genome Detective accepts unprocessed paired-end or single reads generated by NGS platforms in FASTQ format and/or processed FASTA sequences. For FASTQ files, low-quality reads are filtered and adapters trimmed with Trimmomatic (Bolger et al., 2014). The quality of the reads is visualized using FastQC (Brown et al., 2017) before and after trimming. Candidate viral reads are identified using the protein-based alignment method DIAMOND (Buchfink et al., 2015). We used the viral subset of the Swissprot UniRef90 protein database, which contains representative clusters of proteins linked to taxonomy IDs, to improve sensitivity and speed. The Swissprot UniRef90 is constantly updated and at the time of the submission of this paper, the viral subset of this database contained 494 134 protein clusters. At the same time, also the NCBI RefSeq database is constantly updated, and at the time of the submission of this paper, the viral subset of this database contained 7560 unique taxonomic IDs. Genome Detective has an automated procedure to download new versions of the reference databases and the current version and the number of viral taxonomy IDs identified are shown on the interface. The speed and accuracy of Genome Detective was also improved by first sorting short reads into groups, or buckets. Our objective was to run a separate metagenomic de novo assembly in each bucket, so all reads of one virus species needed to be assigned to the same bucket. Each bucket is then identified using the taxonomy ID of the lowest common ancestor (LCA) of the hits identified by DIAMOND. However, some reads that represented the same viral species were assigned to buckets at different taxonomic ranks. We solved this problem by either distributing the reads from the node downwards, or collapsing them upwards, by comparing the number of reads identified at each node of the taxonomy tree versus in all descendant nodes. In addition, given that metagenomic studies are accelerating (reviewed in Rose et al., 2016), an increasing number of reference sequences are of novel viruses that have not yet been classified. This causes the LCA taxonomy ID to be unspecific for a number of Uniref clusters, and in the analysis of hits identified by DIAMOND. To avoid these problems, while retaining the sequence themselves, we excluded the taxonomic classification of these viruses in LCA algorithms. Once all of the reads have been sorted in buckets, each bucket is then de novo assembled separately using SPAdes (Bankevich et al., 2012) for single-ended reads or metaSPAdes (Bankevich et al., 2012) for paired-end reads. Blastx and Blastn are used to search for candidate reference sequences against the NCBI RefSeq virus database. Genome Detective combines the results for every detected contig at amino acid and nucleotide (nt) level with by calculating a total score that is a sum of the total nt score plus total amino acid score. We then chose the five best scoring references for each contig to be used during the alignment. The contigs for each individual species are joined using Advanced Genome Aligner (AGA) (Deforche, 2017), which is a new dynamic programing algorithm. AGA is designed to compute the optimal global alignment considering simultaneously the alignment of all annotated coding sequences of a reference genome. AGA builds further on the optimal alignment algorithms first proposed by Needleman–Wunsch (Smith and Waterman, 1981), Smith–Waterman (Smith and Waterman, 1981) and Gotoh (Gotoh, 1982), by expanding the induction state with additional state parameters. This makes alignments using AGA, and therefore Genome Detective, more sensitive and accurate as both nt and protein scores are taken into account in order to produce a consensus sequence from the de novo contigs. A report is generated, referring to the final contigs and consensus sequences, available as FASTA files. The report also contains detailed information on filtering, assemblage and consensus sequence. Web-based (using the JWt libraries) graphics are available for viral species, genome images, alignment viewer, nt and amino acid similarity measures and read counts. In addition, the user can produce a bam file with BWA (Li and Durbin, 2009) using the reference or de novo consensus sequence by selecting the detailed report (Supplementary Fig. S1) and access viral phylogenetic identification tools (de Oliveira et al., 2005) directly from the interface. ",VirusIdentification,"genome detective  automate system  virus identification  highthroughput sequence data
genome detective accept unprocessed pairedend  single read generate  ngs platforms  fastq format andor process fasta sequence  fastq file lowquality read  filter  adapters trim  trimmomatic bolger     quality   read  visualize use fastqc brown       trim candidate viral read  identify use  proteinbased alignment method diamond buchfink     use  viral subset   swissprot uniref90 protein database  contain representative cluster  proteins link  taxonomy ids  improve sensitivity  speed  swissprot uniref90  constantly update    time   submission   paper  viral subset   database contain   protein cluster    time also  ncbi refseq database  constantly update    time   submission   paper  viral subset   database contain  unique taxonomic ids genome detective   automate procedure  download new versions   reference databases   current version   number  viral taxonomy ids identify  show   interface  speed  accuracy  genome detective  also improve  first sort short read  group  bucket  objective   run  separate metagenomic  novo assembly   bucket   read  one virus species need   assign    bucket  bucket   identify use  taxonomy    lowest common ancestor lca   hit identify  diamond however  read  represent   viral species  assign  bucket  different taxonomic rank  solve  problem  either distribute  read   node downwards  collapse  upwards  compare  number  read identify   node   taxonomy tree versus   descendant nod  addition give  metagenomic study  accelerate review  rise     increase number  reference sequence   novel viruses    yet  classify  cause  lca taxonomy    unspecific   number  uniref cluster    analysis  hit identify  diamond  avoid  problems  retain  sequence   exclude  taxonomic classification   viruses  lca algorithms     read   sort  bucket  bucket    novo assemble separately use spade bankevich     singleended read  metaspades bankevich     pairedend read blastx  blastn  use  search  candidate reference sequence   ncbi refseq virus database genome detective combine  result  every detect contig  amino acid  nucleotide  level   calculate  total score    sum   total  score plus total amino acid score   choose  five best score reference   contig   use   alignment  contigs   individual species  join use advance genome aligner aga deforche     new dynamic program algorithm aga  design  compute  optimal global alignment consider simultaneously  alignment   annotate cod sequence   reference genome aga build    optimal alignment algorithms first propose  needlemanwunsch smith  waterman  smithwaterman smith  waterman   gotoh gotoh   expand  induction state  additional state parameters  make alignments use aga  therefore genome detective  sensitive  accurate     protein score  take  account  order  produce  consensus sequence    novo contigs  report  generate refer   final contigs  consensus sequence available  fasta file  report also contain detail information  filter assemblage  consensus sequence webbased use  jwt libraries graphics  available  viral species genome image alignment viewer   amino acid similarity measure  read count  addition  user  produce  bam file  bwa   durbin  use  reference   novo consensus sequence  select  detail report supplementary fig   access viral phylogenetic identification tool  oliveira    directly   interface ",3
76,VaPid,"VAPiD: a lightweight cross-platform viral annotation pipeline and identification tool to facilitate virus genome submissions to NCBI GenBank
VAPiD can be downloaded at https://github.com/rcs333/VAPiD. An installation guide, usage instructions, and test data can also be found at the above webpage. The invocation of VAPiD is shown in Fig. 1, users must provide a standard FASTA file containing all of the viral genomes they wish to annotate. Users also must provide a GenBank submission template (.sbt file) that includes author, publication, and project metadata. The GenBank submission template can be used for multiple viral sequences or submissions and is easily created at the NCBI Submission portal (https://submit.ncbi.nlm.nih.gov/genbank/template/submission/). An optional sample metadata file (.csv file) can be provided to VAPiD to expedite the process of incorporating sample metadata. This optional file can also be used to include any of the Source Modifiers supported by NCBI (https://www.ncbi.nlm.nih.gov/Sequin/modifiers.html). If no sample metadata file is provided, VAPiD will prompt the user to input the required sample metadata at runtime. Additionally, users can provide a specified reference from which to annotate all viruses in a run, as well as provide their own BLASTn database or force VAPiD to search NCBI’s NT database over the internet. The VAPiD pipeline is summarized in Fig. 2. The first step is finding the correct reference sequence. This is accomplished in three ways 1) using the provided reference database (default), 2) forcing VAPiD to execute an online BLASTn search of NCBI’s NT database, or 3) inputting the accession number of a single NCBI sequence to use as the reference. In the default case, NCBI’s BLAST+ tools are called from the command line to search against a reference database that is included with the VAPiD installation. This database was generated by downloading all complete viral genomes in NCBI on May 1, 2018. The best result from this search is passed as the reference into the next steps. If the online option for finding the reference is specified, VAPiD finds an appropriate reference sequence for each genome to be annotated by performing an online BLASTn search with a word size of 28 using BioPython’s NCBI WWW.qblast() function against the online NCBI NT database. The BLASTn output is parsed for the best scoring alignment among the top 15 results that contains “complete genome” in the reference definition line. If no complete genome is found in the top 15 BLASTn results, the top-scoring hit is used as the reference sequence. If a specific reference is provided, VAPiD simply downloads it directly from NCBI. After the correct reference is downloaded, gene locations are stripped from the reference and a pairwise nucleotide alignment between the reference and the submitted sequence is generated using MAFFT [21]. The relative locations of the genes on the reference sequence are then mapped onto the new sequence based off the alignment. This putative alignment only requires that start codons are in regions of high homology and does not rely on intergenic spacing or gene lengths. Gene names are taken from the annotated reference sequence GenBank entry. Spellchecking is performed using NCBI’s ESpell module. This module provides spellchecking of many biological strings including protein product names. An optional argument can be provided at execution that enables this step. The diverse array of methods viruses use to encode genes can present problems for any viral genome annotator. Ribosomal slippage allows viruses to produce two proteins from a single mRNA transcript by having the ribosome ‘slip’ one or two nucleotides along the mRNA transcript, thus changing the reading frame. Since ribosomal slippage is well conserved within viral species and complete reference genomes often list exactly where it occurs, custom code was used to strip the correct junction site and include it in the annotation. RNA editing is another process by which viruses can include multiple proteins in a single gene. In RNA editing, the RNA polymerase co-transcriptionally adds one or two nucleotides that are not on the template. These changes are specifically created during viral mRNA transcription and not during viral genome replication. RNA editing presents an annotation issue because the annotated protein sequence does not match the expected translated nucleotide sequence. To correctly annotate genes with RNA editing, VAPiD parses the reference genome viral species, detects the RNA editing locus, and mimics the RNA polymerase. VAPiD adds the correct number of non-templated nucleotides for the viral species and provides an alternative protein translation. This process is hard-coded for human parainfluenza 2–4, Nipah virus, Sendai virus, measles virus, and mumps virus. Although RNA editing occurs in Ebola virus, references for Ebola virus are annotated in the same way as ribosomal slippage, so code written for ribosomal slippage handles Ebola virus annotations. After ribosomal slippage and RNA editing are processed, files required for GenBank submission are generated with the provided author and sample metadata. VAPiD first generates the .fsa file, .tbl file, and optional .cmt file. Submission files for each viral genome are packaged into a separate folder for each sequence. VAPiD then runs tbl2asn on each folder using the provided GenBank submission template file (.sbt).",VirusIdentification,"vapid  lightweight crossplatform viral annotation pipeline  identification tool  facilitate virus genome submissions  ncbi genbank
vapid   download    installation guide usage instructions  test data  also  find    webpage  invocation  vapid  show  fig  users must provide  standard fasta file contain    viral genomes  wish  annotate users also must provide  genbank submission template sbt file  include author publication  project metadata  genbank submission template   use  multiple viral sequence  submissions   easily create   ncbi submission portal   optional sample metadata file csv file   provide  vapid  expedite  process  incorporate sample metadata  optional file  also  use  include    source modifiers support  ncbi    sample metadata file  provide vapid  prompt  user  input  require sample metadata  runtime additionally users  provide  specify reference    annotate  viruses   run  well  provide   blastn database  force vapid  search ncbis  database   internet  vapid pipeline  summarize  fig   first step  find  correct reference sequence   accomplish  three ways  use  provide reference database default  force vapid  execute  online blastn search  ncbis  database   inputting  accession number   single ncbi sequence  use   reference   default case ncbis blast tool  call   command line  search   reference database   include   vapid installation  database  generate  download  complete viral genomes  ncbi  may    best result   search  pass   reference   next step   online option  find  reference  specify vapid find  appropriate reference sequence   genome   annotate  perform  online blastn search   word size   use biopythons ncbi wwwqblast function   online ncbi  database  blastn output  parse   best score alignment among  top  result  contain “complete genome”   reference definition line   complete genome  find   top  blastn result  topscoring hit  use   reference sequence   specific reference  provide vapid simply download  directly  ncbi   correct reference  download gene locations  strip   reference   pairwise nucleotide alignment   reference   submit sequence  generate use mafft   relative locations   genes   reference sequence   map onto  new sequence base   alignment  putative alignment  require  start codons   regions  high homology    rely  intergenic space  gene lengths gene name  take   annotate reference sequence genbank entry spellchecking  perform use ncbis espell module  module provide spellchecking  many biological string include protein product name  optional argument   provide  execution  enable  step  diverse array  methods viruses use  encode genes  present problems   viral genome annotator ribosomal slippage allow viruses  produce two proteins   single mrna transcript    ribosome slip one  two nucleotides along  mrna transcript thus change  read frame since ribosomal slippage  well conserve within viral species  complete reference genomes often list exactly   occur custom code  use  strip  correct junction site  include    annotation rna edit  another process   viruses  include multiple proteins   single gene  rna edit  rna polymerase cotranscriptionally add one  two nucleotides      template  change  specifically create  viral mrna transcription    viral genome replication rna edit present  annotation issue   annotate protein sequence   match  expect translate nucleotide sequence  correctly annotate genes  rna edit vapid parse  reference genome viral species detect  rna edit locus  mimic  rna polymerase vapid add  correct number  nontemplated nucleotides   viral species  provide  alternative protein translation  process  hardcoded  human parainfluenza  nipah virus sendai virus measles virus  mumps virus although rna edit occur  ebola virus reference  ebola virus  annotate    way  ribosomal slippage  code write  ribosomal slippage handle ebola virus annotations  ribosomal slippage  rna edit  process file require  genbank submission  generate   provide author  sample metadata vapid first generate  fsa file tbl file  optional cmt file submission file   viral genome  package   separate folder   sequence vapid  run tbl2asn   folder use  provide genbank submission template file sbt",3
77,VirusSeq,"VirusSeq: software to identify viruses and their integration sites using next-generation sequencing of human cancer tissue
Mapping/Alignment The PE reads in FASTQ format are used as input. VirusSeq works with both whole-genome and whole-transcriptome sequencing data. The raw PE reads are aligned to the reference genome using MOSAIK (Hiller et al., 2008) alignment software, which implements both a hashing scheme and the Smith–Waterman algorithm to produce gapped optimal alignments. 2.2 Virus detection from NGS data VirusSeq starts with computational subtraction of human sequences by aligning raw PE reads from whole-genome/transcriptome sequencing to the human genome reference. Thus, a set of non-human sequences is effectively generated by subtracting the human sequences. In the second step, VirusSeq aligns the non-human sequences against a comprehensive database that includes all known viral sequences from Genome Information Broker for Viruses (http://gib-v.genes.nig.ac.jp/) and quantifies the virus representation by the overall count of mapped reads within a virus genome to determine the existence of viruses in human samples with an empirical cut-off. Any virus with an overall count of mapped reads below the cut-off is treated as non-existent. We used 1000 as the cut-off for the overall count of mapped reads within a virus genome; this cut-off should be applicable for both RNA-Seq data and whole-genome sequencing data with 30× coverage. This cut-off should be reduced by half or more for low-pass whole-genome sequencing data. 2.3 Identification of virus integration sites The genome sequences of viruses, which are well known in terms of cancer association and were detected in the previous step in our The Cancer Genome Atlas (TCGA) dataset, were concatenated into a single chromosome named chrVirus, with related annotation of each viral gene in refFlat format. A new hybrid reference genome named hg19Virus is built by combining hg19 and chrVirus (designated as chr25 in hg19Virus). All PE reads without computational subtraction are mapped to this reference (hg19Virus). If the PE reads are uniquely mapped with one end to one human chromosome and the other to chr25, the read pair is reported as a discordant read pair. All discordant reads are then annotated with human and viral genes defined in the curated refFlat file. VirusSeq then clusters the discordant read pairs that support the same integration (fusion) event (e.g., HBV-MLL4). VirusSeq implements a dynamic clustering procedure (details in Supplementary Notes) to accurately determine the boundary of the cluster, whose size is constrained by the insert size (fragment length) distribution. To remove outliers within a cluster, VirusSeq implements the robust ‘extreme studentized deviate’ multiple-outlier detection procedure (Rosner, 1983). Once outliers are detected within a cluster, the cluster boundary is reset by excluding the outlier reads. VirusSeq reports the fusion candidates by using both supporting pairs (at least four) and junction spanning reads (at least one) as the cut-offs. Meanwhile, an in silico sequence is generated using the consensus of reads within discordant read clusters for each fusion candidate to help the PCR primer design, which facilitates quick PCR validation. Go to:",VirusIdentification,"virusseq software  identify viruses   integration sit use nextgeneration sequence  human cancer tissue
mappingalignment   read  fastq format  use  input virusseq work   wholegenome  wholetranscriptome sequence data  raw  read  align   reference genome use mosaik hiller    alignment software  implement   hash scheme   smithwaterman algorithm  produce gap optimal alignments  virus detection  ngs data virusseq start  computational subtraction  human sequence  align raw  read  wholegenometranscriptome sequence   human genome reference thus  set  nonhuman sequence  effectively generate  subtract  human sequence   second step virusseq align  nonhuman sequence   comprehensive database  include  know viral sequence  genome information broker  viruses   quantify  virus representation   overall count  map read within  virus genome  determine  existence  viruses  human sample   empirical cutoff  virus   overall count  map read   cutoff  treat  nonexistent  use    cutoff   overall count  map read within  virus genome  cutoff   applicable   rnaseq data  wholegenome sequence data   coverage  cutoff   reduce  half    lowpass wholegenome sequence data  identification  virus integration sit  genome sequence  viruses   well know  term  cancer association   detect   previous step    cancer genome atlas tcga dataset  concatenate   single chromosome name chrvirus  relate annotation   viral gene  refflat format  new hybrid reference genome name hg19virus  build  combine hg19  chrvirus designate  chr25  hg19virus   read without computational subtraction  map   reference hg19virus    read  uniquely map  one end  one human chromosome     chr25  read pair  report   discordant read pair  discordant read   annotate  human  viral genes define   curated refflat file virusseq  cluster  discordant read pair  support   integration fusion event  hbvmll4 virusseq implement  dynamic cluster procedure detail  supplementary note  accurately determine  boundary   cluster whose size  constrain   insert size fragment length distribution  remove outliers within  cluster virusseq implement  robust extreme studentized deviate multipleoutlier detection procedure rosner   outliers  detect within  cluster  cluster boundary  reset  exclude  outlier read virusseq report  fusion candidates  use  support pair  least four  junction span read  least one   cutoffs meanwhile   silico sequence  generate use  consensus  read within discordant read cluster   fusion candidate  help  pcr primer design  facilitate quick pcr validation  ",3
78,PAIPline,"PAIPline: pathogen identification in metagenomic and clinical next generation sequencing samples
The main steps of PAIPline are data preprocessing, read assignment and result evaluation. Below, we describe these steps below along with the sample and data preparation. Preprocessing The workflow starts with the preprocessing of a set of raw reads in fastq format. Initially, the read input quality control is performed in three steps, the base quality control step, the sequence complexity control step and the length cut-off step. The base quality control follows a sliding window approach (by default, the window size is 20 and the minimum average quality is Q10). This is done to prevent misleading low-quality bases from contributing to the sensitive alignments which provide the basis for the sample constituents calculation later on. Subsequently, the remaining bases of the read are checked regarding their sequence complexity based on the SDUST algorithm which discards regions of low complexity and strongly biased composition from the reads (Morgulis et al. 2006). This prevents mathematically valid alignments of low biological significance which could possibly stem from regions such as naturally occurring repeat regions. Such regions can be found in different clades and species throughout the tree of life, thus the resulting alignments do not provide insight into the origin of a read. To our knowledge, this is the only pipeline where such a complexity filtering approach for reads is implemented. Reads are discarded if they are shorter than a minimum length cut-off (36 by default) after the previous trimming steps since such short alignments are extremely unlikely to be unambiguous. All preprocessing parameters can be changed by the user to account for their specific experimental requirements and data. 2.2 Read alignment and validation Following preprocessing, the reads are mapped to the chosen foreground and background databases using Bowtie 2 with its very sensitive mode (Langmead and Salzberg 2012; Langmead et al. 2009). Typical databases that can be used with PAIPline are viral, bacterial, fungi, amoebozoa or apicomplexa databases. These can be created using the database updater script described in the subsection Database preparation. Reads that map to the background are removed from the further analysis, whereas the remaining foreground aligned reads undergo BLAST validation by being queried with the blastn program against the complete NCBI nt database (Altschul et al. 1990; Camacho et al. 2009). This ensures that for every read all possible origin sequences are found, as long as these are known and included in the NCBI nt database. The user can control the parameters of the applied assignment methods according to his or her needs. 2.3 Result presentation After read assignment, PAIPline generates a result overview. It constructs a taxonomic tree including all OTUs hit by any number of reads and their respective ancestors up to the taxonomic root. Subsequently, this tree is checked for ambiguities by evaluating the hits on each taxonomic rank. A hit is deemed unique if it is only assigned to references within a single OTU. If a read hits several OTUs, this hit is assumed to be unambiguous if the identity to a reference within the best hit OTU is higher than every hit on any other OTU. The identity cut-off for this step can be configured by the user for any named taxonomic rank such as species, genus, family, etc. and reasonable default values are provided. If none of the hits qualify as sufficiently unambiguous, the hits are moved upwards in the tree and compared again on the next-higher rank. Therefore, PAIPline uses a modified lowest common ancestor (LCA) approach (Huson et al. 2007). At this point, all user-designated OLIs are marked for filtering purposes. Afterwards, the constructed taxonomic tree is transformed and saved in a csv file that allows easy parsing, filtering and visualization using third-party applications such as spreadsheet software. The resulting file contains all OTUs, their taxonomic lineage, as well as their respective unique, unambiguous and total hit counts. 2.4 Database preparation Because PAIPline needs databases containing foreground and background organism-associated sequences, we provide an auxiliary script that allows users to download and maintain a local copy of the NCBI nucleotide (nt) database as well as sub-databases of interest. The script downloads the nt database along with the taxonomic information provided by NCBI and re-annotates the contained sequences with their taxonomic lineage, keeping the original NCBI annotation. Afterwards, user-definable sub-databases of taxonomic clades relevant to a pathogen search, for example viruses, bacteria, fungi, apicomplexa and amoebozoaare created along with background databases for host organisms and artificial sequences. Finally, all newly created databases are indexed for use with the alignment tools applied in the workflow of PAIPline. This precomputation has to be done only once and is usable for all PAIPline runs afterwards. The database update script is available under https://gitlab.com/rki_bioinformatics/database-updater. 2.5 Benchmarking To assess the performance of PAIPline, it was benchmarked along a selected set of other previously published metagenomics tools. The benchmarking included Pathoscope 2.0 (Hong et al. 2014), Kraken (Wood and Salzberg 2014) and Sigma (Ahn et al. 2015) applied on four published biological samples (Kohl et al. 2015) and an artificial one. These tools were selected because they are well-known in the field of bioinformatics and are not inherently limited in terms of detectable pathogens by their approach (Forbes et al. 2017). For the evaluation, the precision P was calculated, which is defined as P = TP/(TP + FP), where TP are true positives and FP are false positives. Furthermore, the recall R, defined as R = TP/(TP + FN), where FN denotes false negatives, was evaluated. Lastly, the F-score F1 was determined, which is the harmonic mean between recall and precision F1 = 2/((1/R) + (1/P)) and is used to average the two during the evaluation. The datasets used in the benchmarking of PAIPline and the other metagenomic profiling tools were obtained in two ways and were selected to be representative for biological samples from different backgrounds, such as different lab or clinical samples. An artificial dataset was generated using pIRS with its default settings for 100 bp long Illumina reads (Hu et al. 2012). It was designed to test the pathogen detection capabilities of all employed tools against typical virus sequences with different degrees of similarity to human genome sequences. The resulting composition of the artificial sample can be seen in Table 1. The biological samples were acquired and sequenced as described previously (Kohl et al. 2015). One of these samples was obtained from a marmoset that died from a Sendai virus infection. The other sample used in that study was obtained by infecting fertilized chicken eggs with low doses of Vaccinia virus, an Orthoreovirus, an Influenza virus and a Sendai virus, respectively, to represent a metagenome containing various viruses. From that study the chicken DNA library, chicken RNA library, marmoset DNA library and marmoset RNA library were used for the benchmarking. All analyses were run on an on-site server with 24 cores of 2.2 GHz and 128 GB RAM running Ubuntu 14.04.3 LTS. All tools were run with their respective default parameters except for the number of threads, which was set to 8, where possible. The databases for PAIPline and Pathoscope were created from the NCBI nt database and both tools were provided sub-databases containing the respective background organisms. The database for Sigma was provided by running the included database creation script.",VirusIdentification,"paipline pathogen identification  metagenomic  clinical next generation sequence samples
 main step  paipline  data preprocessing read assignment  result evaluation   describe  step  along   sample  data preparation preprocessing  workflow start   preprocessing   set  raw read  fastq format initially  read input quality control  perform  three step  base quality control step  sequence complexity control step   length cutoff step  base quality control follow  slide window approach  default  window size     minimum average quality  q10     prevent mislead lowquality base  contribute   sensitive alignments  provide  basis   sample constituents calculation later  subsequently  remain base   read  check regard  sequence complexity base   sdust algorithm  discard regions  low complexity  strongly bias composition   read morgulis     prevent mathematically valid alignments  low biological significance  could possibly stem  regions   naturally occur repeat regions  regions   find  different clades  species throughout  tree  life thus  result alignments   provide insight   origin   read   knowledge     pipeline    complexity filter approach  read  implement read  discard    shorter   minimum length cutoff   default   previous trim step since  short alignments  extremely unlikely   unambiguous  preprocessing parameters   change   user  account   specific experimental requirements  data  read alignment  validation follow preprocessing  read  map   choose foreground  background databases use bowtie     sensitive mode langmead  salzberg  langmead    typical databases    use  paipline  viral bacterial fungi amoebozoa  apicomplexa databases    create use  database updater script describe   subsection database preparation read  map   background  remove    analysis whereas  remain foreground align read undergo blast validation   query   blastn program   complete ncbi  database altschul    camacho     ensure   every read  possible origin sequence  find  long    know  include   ncbi  database  user  control  parameters   apply assignment methods accord     need  result presentation  read assignment paipline generate  result overview  construct  taxonomic tree include  otus hit   number  read   respective ancestors    taxonomic root subsequently  tree  check  ambiguities  evaluate  hit   taxonomic rank  hit  deem unique     assign  reference within  single otu   read hit several otus  hit  assume   unambiguous   identity   reference within  best hit otu  higher  every hit    otu  identity cutoff   step   configure   user   name taxonomic rank   species genus family etc  reasonable default value  provide  none   hit qualify  sufficiently unambiguous  hit  move upwards   tree  compare    nexthigher rank therefore paipline use  modify lowest common ancestor lca approach huson      point  userdesignated olis  mark  filter purpose afterwards  construct taxonomic tree  transform  save   csv file  allow easy parse filter  visualization use thirdparty applications   spreadsheet software  result file contain  otus  taxonomic lineage  well   respective unique unambiguous  total hit count  database preparation  paipline need databases contain foreground  background organismassociated sequence  provide  auxiliary script  allow users  download  maintain  local copy   ncbi nucleotide  database  well  subdatabases  interest  script download   database along   taxonomic information provide  ncbi  reannotates  contain sequence   taxonomic lineage keep  original ncbi annotation afterwards userdefinable subdatabases  taxonomic clades relevant   pathogen search  example viruses bacteria fungi apicomplexa  amoebozoaare create along  background databases  host organisms  artificial sequence finally  newly create databases  index  use   alignment tool apply   workflow  paipline  precomputation         usable   paipline run afterwards  database update script  available    benchmarking  assess  performance  paipline   benchmarked along  select set   previously publish metagenomics tool  benchmarking include pathoscope  hong    kraken wood  salzberg   sigma ahn    apply  four publish biological sample kohl      artificial one  tool  select    wellknown   field  bioinformatics    inherently limit  term  detectable pathogens   approach forbes      evaluation  precision   calculate   define    tptp      true positives    false positives furthermore  recall  define    tptp     denote false negative  evaluate lastly  fscore   determine    harmonic mean  recall  precision        use  average  two   evaluation  datasets use   benchmarking  paipline    metagenomic profile tool  obtain  two ways   select   representative  biological sample  different background   different lab  clinical sample  artificial dataset  generate use pirs   default settings    long illumina read       design  test  pathogen detection capabilities   employ tool  typical virus sequence  different degrees  similarity  human genome sequence  result composition   artificial sample   see  table   biological sample  acquire  sequence  describe previously kohl    one   sample  obtain   marmoset  die   sendai virus infection   sample use   study  obtain  infect fertilize chicken egg  low dose  vaccinia virus  orthoreovirus  influenza virus   sendai virus respectively  represent  metagenome contain various viruses   study  chicken dna library chicken rna library marmoset dna library  marmoset rna library  use   benchmarking  analyse  run   onsite server   core   ghz    ram run ubuntu  lts  tool  run   respective default parameters except   number  thread   set    possible  databases  paipline  pathoscope  create   ncbi  database   tool  provide subdatabases contain  respective background organisms  database  sigma  provide  run  include database creation script",3
79,GATK PathSeq,"GATK PathSeq: a customizable computational tool for the discovery and identification of microbial sequences in libraries from eukaryotic hosts
PathSeq begins with removal of low quality, low complexity, host-derived and duplicate reads (Supplementary Fig. S1, Supplementary Material S1). Several methodological improvements have been made to these steps in order to improve performance. Apache Spark is used to process batches of sequencing reads asynchronously in memory, thus maximizing resource utilization and minimizing slow hard-disk operations between pipeline stages. To accelerate filtering of low-complexity sequences, the RepeatMasker step (http://www.repeatmasker.org/) has been replaced with the symmetric DUST algorithm (Morgulis et al., 2006). To rescue reads that are partially informative, the new version also incorporates trimming of low-quality bases, sequencing adapter artifacts and low-complexity sequences. The reads are then subjected to a fast k-mer search using a Bloom filter (Bloom, 1970) to detect short sequences from the host reference. Reads containing at least one host k-mer (k = 31) are removed. This step typically subtracts ∼90% of the host reads prior to performing sequence alignment, thus greatly reducing run time. The BLAST (Altschul et al., 1990) aligner has been replaced with the faster BWA-MEM aligner (Li, 2013), which is used to map the remaining reads to the host reference (Supplementary Table S1). PathSeq then aligns the remaining non-host reads to a reference of microbial genomes (viruses, bacteria, fungi, etc.; Supplementary Table S2) and classifies each read taxonomically. Alignment efficiency is again improved using BWA-MEM. Classification specificity has been increased by running the aligner in paired-end mode and requiring that both reads in a pair map to the same organism during classification. Finally, a report is generated containing microbial abundance estimates at each taxonomic level (e.g. species, genus, family, etc.).",VirusIdentification,"gatk pathseq  customizable computational tool   discovery  identification  microbial sequence  libraries  eukaryotic hosts
pathseq begin  removal  low quality low complexity hostderived  duplicate read supplementary fig  supplementary material  several methodological improvements   make   step  order  improve performance apache spark  use  process batch  sequence read asynchronously  memory thus maximize resource utilization  minimize slow harddisk operations  pipeline stag  accelerate filter  lowcomplexity sequence  repeatmasker step    replace   symmetric dust algorithm morgulis     rescue read   partially informative  new version also incorporate trim  lowquality base sequence adapter artifacts  lowcomplexity sequence  read   subject   fast kmer search use  bloom filter bloom   detect short sequence   host reference read contain  least one host kmer     remove  step typically subtract    host read prior  perform sequence alignment thus greatly reduce run time  blast altschul    aligner   replace   faster bwamem aligner     use  map  remain read   host reference supplementary table  pathseq  align  remain nonhost read   reference  microbial genomes viruses bacteria fungi etc supplementary table   classify  read taxonomically alignment efficiency   improve use bwamem classification specificity   increase  run  aligner  pairedend mode  require   read   pair map    organism  classification finally  report  generate contain microbial abundance estimate   taxonomic level  species genus family etc",3
80,RINS,"Rapid identification of non-human sequences in high-throughput sequencing datasets
The appeal of searching for pathogen sequences in high-throughput sequencing data has grown as massively parallel sequencing capabilities have developed (Shendure and Ji, 2008) and sequencing has identified pathogens such as the Merkel cell polyoma virus as a contributing factor in Merkel cell carcinoma (Feng et al., 2008). Algorithms such as PathSeq (Kostic et al., 2011) have emerged that apply computational subtraction to the task of pathogen detection. These algorithms are computationally intensive and thus still require cloud computing scale resources. Here, we present rapid identification of non-human sequences (RINS), an alternative to computational subtraction that can efficiently identify the presence of pathogens from a custom reference in high-throughput sequencing datasets. The speed and local computing-based nature of RINS makes it attractive for hypothesis-driven discovery of pathogens in large datasets. The accessibility of a workflow such as RINS opens the door for extensive pathogen discovery in a variety of contexts such as cancer and other difficult to treat diseases. RINS employs intersection analysis with a user provided reference set, as opposed to computational subtraction. The latter is a process that maps first to the reference organism's genome and then attempts to assign all unmapped reads to a non-reference organism. Because mapping algorithms require intense RAM to store these unmapped reads, computational subtraction is slow and requires cloud scale resources. While both methods ultimately filter through a reference organism (e.g. human) and look for pathogenic sequences, RINS first maps to a query dataset, thereby lowering the computational requirements. It can use any non-human reference set, including genomes of viruses, bacteria or other pathogens. This set is used as the template for the RINS initial search. The reference genome set included with the RINS package contains viruses of all known classes that infect a variety of organisms in order to offer the broadest template to identify pathogens. The workflow starts by generating non-overlapping 25 mers of each read that maximize the sensitivity of the alignment with Blat (Kent, 2002) against the reference genomes(s) provided, using an 80% match threshold (Fig. 1, RINS Step 1). This threshold was optimized for sensitivity and specificity performance on a randomly mutated test set. Step 2 of RINS removes duplicates that may have been generated by Step 1 alignment and filters the longest-associated read for each mapped 25 mer for complexity using a Lempel–Ziv–Welch (LZW) (Welch, 1984) compression ratio of 50% (RINS Step 2). The LZW compression method (Yozwiak et al., 2010) uses a dictionary-based approach to quantitate the complexity of a sequence by adding to the dictionary a new ‘word’ for every unique string of characters, eliminating repeat regions. These repeats are frequently found in both humans and microbes, making organismal origin difficult to pinpoint. This complexity ratio was optimized to minimize loss of non-human reads while further filtering the data for potentially confounding repetitive sequences. The filtered reads are then mapped against the human genome using Bowtie (Langmead et al., 2009) (RINS Step 3), and after this intersection, reads that mapped to the human genome are removed from the read set. Remaining reads, with their mate pairs (if the dataset is paired end) are assembled into contigs with the de novo assembler Trinity (Grabherr et al., 2011) (RINS Step 4). Using a local version of BLAST to classify contigs (RINS Step 5), those contigs with minimal homology to human sequences are then extended by mapping the original read set back to the contig. The process of identifying mate pairs and assembling the contig is repeated as before. This method of extension allows for reads that are part of the contig but were eliminated by other filtration methods to be reincorporated into the contig to increase the sensitivity and specificity of the results. RINS will then output a tab-delimited text file detailing the candidate contigs that have been generated, presented with the number of supporting reads and a BLAST e-value. Parameters used here are modifiable by the user if desired. RINS uses intersection (marked by asterisks), not subtraction, to identify non-human reads. The workflow intersects the reads in the dataset with a reference of non-human genomes of interest using Blat to align non-overlapping 25 mers for each read. Reads with >80% homology are aligned to the human genome and reads with >97% homology are removed from the read set. Remaining reads are complexity filtered with an LZW compression ratio of 0.50 and mate pairs for sufficiently complex reads are identified. This read set is then assembled into pathogen sequence contigs. Sensitivity of RINS was evaluated with a randomly mutated test set of viral genomes, which served as a stringent measuring system. In this test set, mutations occurred randomly throughout the genome without any conserved regions that are often found in nature. With this test set, it was shown that using the 25 mer reads promotes identification of mutated non-human genomes (Supplementary Fig. S1a). Specifically, at mutation rates >25%, mapping to the custom reference with these shorter read segments is significantly better at identifying genomes than with the full length reads (Supplementary Fig. S1b). This indicates known pathogens are identified with confidence, and genomes with >50% homology to the reference genomes can be extracted from the data using RINS. A positive control was used to test RINS accuracy and speed (Supplementary Table S1). Sequencing data from the CA-HPV-10 prostate cancer cell line (SRR073726) (Prensner et al., 2011) was analyzed with RINS and accurately retrieved only HPV serotype 18 (the transforming virus) with a 570 bp contig (Supplementary Tables S1 and S2). RINS took <2 h to perform this analysis on a dual core machine with 8 GB of RAM and a 2.93 GHz processor. Accuracy of RINS was further tested in RNA sequencing data from Sézary syndrome, SRA046736 (Lee et al., manuscript in preparation) where a contig with homology to vector constructs and HIV was generated (Supplementary Table S2). Using PCR amplification and Sanger sequencing (data not shown), the existence of this laboratory contaminant in the cDNA of the relevant sample was confirmed. Comparisons of RINS to the pathogen discovery algorithm PathSeq show similar performance, with better speed and lower cost for RINS. PathSeq sensitivity and specificity are derived from statistics presented by the authors in their work using an analogous test set of randomly mutated viral genomes. The sensitivity of PathSeq was 99.22% at a mutation rate of 0%, and drops to 0% at a mutation rate of 50% (Kostic et al., 2011, Supplementary Data and Supplementary Data). RINS has a similar sensitivity of 99.78% based upon test set read recovery at a mutation rate of 0%. The RINS sensitivity drops to 0% at any mutation rate >50%, though at 50% there is a minimal sensitivity of 0.5–1% (Supplementary Fig. S1). Specificity for the test set is 100% for PathSeq. RINS also has no false positives for the test set and no false positives were identified from the CA-HPV-10 data or the Sezary syndrome data, giving RINS a specificity of 100%. The additional rigor of PathSeq would confidently allow identification of novel pathogens, though the ability of RINS to identify reads with up to 50% divergence from the reference genome suggests this could also be feasible with RINS if the novel pathogen has at least 50% homology to one or more of the custom reference genomes. PathSeq requires 13 h of cloud computing time and costs to process 10 million reads, whereas RINS takes <2 h for the 13 million reads in SRR073726. Importantly, RINS scales up well and is able to complete the six high-throughput datasets from SRA046736 with an average read depth of 112 million reads in <4 h each at no additional cost beyond access to a computer. The lower cost and faster speed are significant for accessibility to researchers interested in either hypothesis driven queries of datasets or queries of many different datasets in a reasonable timeframe. RINS is optimized for mate-paired high-throughput sequencing data with reads at least 36 bp and up to 500 bp, and can be run on sequencing data from any species. Non-paired end sequencing data can also be used, though contig generation and extension will be less robust. As the read length and number of reads increases, the computational time required to complete RINS will increase as Blat and Trinity processing speeds will decrease. Included in the online package are 32 102 viral genomes of all classes curated by GenBank (Benson et al., 2008) and the International Committee on Taxonomy of Viruses (ICTV), retrieved through the National Center for Biotechnology Information (NCBI). Also provided are all scripts to run the processes, with options for user control of all default parameters. The user must provide other open source softwares referenced above.",VirusIdentification,"rapid identification  nonhuman sequence  highthroughput sequence datasets
 appeal  search  pathogen sequence  highthroughput sequence data  grow  massively parallel sequence capabilities  develop shendure     sequence  identify pathogens    merkel cell polyoma virus   contribute factor  merkel cell carcinoma feng    algorithms   pathseq kostic     emerge  apply computational subtraction   task  pathogen detection  algorithms  computationally intensive  thus still require cloud compute scale resources   present rapid identification  nonhuman sequence rins  alternative  computational subtraction   efficiently identify  presence  pathogens   custom reference  highthroughput sequence datasets  speed  local computingbased nature  rins make  attractive  hypothesisdriven discovery  pathogens  large datasets  accessibility   workflow   rins open  door  extensive pathogen discovery   variety  contexts   cancer   difficult  treat diseases rins employ intersection analysis   user provide reference set  oppose  computational subtraction  latter   process  map first   reference organism' genome   attempt  assign  unmapped read   nonreference organism  map algorithms require intense ram  store  unmapped read computational subtraction  slow  require cloud scale resources   methods ultimately filter   reference organism  human  look  pathogenic sequence rins first map   query dataset thereby lower  computational requirements   use  nonhuman reference set include genomes  viruses bacteria   pathogens  set  use   template   rins initial search  reference genome set include   rins package contain viruses   know class  infect  variety  organisms  order  offer  broadest template  identify pathogens  workflow start  generate nonoverlapping  mers   read  maximize  sensitivity   alignment  blat kent    reference genomess provide use   match threshold fig  rins step   threshold  optimize  sensitivity  specificity performance   randomly mutate test set step   rins remove duplicate  may   generate  step  alignment  filter  longestassociated read   map  mer  complexity use  lempelzivwelch lzw welch  compression ratio   rins step   lzw compression method yozwiak    use  dictionarybased approach  quantitate  complexity   sequence  add   dictionary  new word  every unique string  character eliminate repeat regions  repeat  frequently find   humans  microbes make organismal origin difficult  pinpoint  complexity ratio  optimize  minimize loss  nonhuman read   filter  data  potentially confound repetitive sequence  filter read   map   human genome use bowtie langmead    rins step     intersection read  map   human genome  remove   read set remain read   mate pair   dataset  pair end  assemble  contigs    novo assembler trinity grabherr    rins step  use  local version  blast  classify contigs rins step   contigs  minimal homology  human sequence   extend  map  original read set back   contig  process  identify mate pair  assemble  contig  repeat    method  extension allow  read   part   contig   eliminate   filtration methods   reincorporated   contig  increase  sensitivity  specificity   result rins   output  tabdelimited text file detail  candidate contigs    generate present   number  support read   blast evalue parameters use   modifiable   user  desire rins use intersection mark  asterisk  subtraction  identify nonhuman read  workflow intersect  read   dataset   reference  nonhuman genomes  interest use blat  align nonoverlapping  mers   read read   homology  align   human genome  read   homology  remove   read set remain read  complexity filter   lzw compression ratio    mate pair  sufficiently complex read  identify  read set   assemble  pathogen sequence contigs sensitivity  rins  evaluate   randomly mutate test set  viral genomes  serve   stringent measure system   test set mutations occur randomly throughout  genome without  conserve regions   often find  nature   test set   show  use   mer read promote identification  mutate nonhuman genomes supplementary fig s1a specifically  mutation rat  map   custom reference   shorter read segment  significantly better  identify genomes    full length read supplementary fig s1b  indicate know pathogens  identify  confidence  genomes   homology   reference genomes   extract   data use rins  positive control  use  test rins accuracy  speed supplementary table  sequence data   cahpv prostate cancer cell line srr073726 prensner     analyze  rins  accurately retrieve  hpv serotype   transform virus     contig supplementary table    rins take    perform  analysis   dual core machine     ram    ghz processor accuracy  rins   test  rna sequence data  sézary syndrome sra046736 lee   manuscript  preparation   contig  homology  vector construct  hiv  generate supplementary table  use pcr amplification  sanger sequence data  show  existence   laboratory contaminant   cdna   relevant sample  confirm comparisons  rins   pathogen discovery algorithm pathseq show similar performance  better speed  lower cost  rins pathseq sensitivity  specificity  derive  statistics present   author   work use  analogous test set  randomly mutate viral genomes  sensitivity  pathseq     mutation rate    drop     mutation rate   kostic    supplementary data  supplementary data rins   similar sensitivity   base upon test set read recovery   mutation rate    rins sensitivity drop     mutation rate  though      minimal sensitivity   supplementary fig  specificity   test set    pathseq rins also   false positives   test set   false positives  identify   cahpv data   sezary syndrome data give rins  specificity    additional rigor  pathseq would confidently allow identification  novel pathogens though  ability  rins  identify read     divergence   reference genome suggest  could also  feasible  rins   novel pathogen   least  homology  one     custom reference genomes pathseq require    cloud compute time  cost  process  million read whereas rins take      million read  srr073726 importantly rins scale  well   able  complete  six highthroughput datasets  sra046736   average read depth   million read       additional cost beyond access   computer  lower cost  faster speed  significant  accessibility  researchers interest  either hypothesis drive query  datasets  query  many different datasets   reasonable timeframe rins  optimize  matepaired highthroughput sequence data  read  least           run  sequence data   species nonpaired end sequence data  also  use though contig generation  extension   less robust   read length  number  read increase  computational time require  complete rins  increase  blat  trinity process speed  decrease include   online package    viral genomes   class curated  genbank benson      international committee  taxonomy  viruses ictv retrieve   national center  biotechnology information ncbi also provide   script  run  process  options  user control   default parameters  user must provide  open source softwares reference ",3
81,SURPI,"A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples
Clinical NGS data sets Details regarding clinical samples, approved research protocols for sample collection, and NGS library construction are provided in the Supplemental Methods. Hardware Minimum hardware requirements for running SURPI include a multicore server running Ubuntu 12.04 (preferred) with at least 60 GB of RAM. SURPI and its software dependencies require ∼1 GB of disk space. Reference data requires ∼1 TB of disk space. During SURPI runtime, up to 10× the size of the input FASTQ file may be needed as additional temporary storage. The specific hardware used here for SURPI testing and benchmarking are provided in the Supplemental Methods. Custom modifications to the SNAP nucleotide aligner SNAP is a new, hash-based nucleotide aligner developed for mapping of NGS data to reference genomes across a wide range of read lengths (50–10,000 bp) (Zaharia et al. 2011). Available at http://snap.cs.berkeley.edu, SNAP runs 10–100× faster than existing tools while maintaining comparable or higher accuracy (Fig. 1C). The aligner partially derives its speed from loading the entire indexed reference database into RAM. Since SNAP was originally designed only for human genome (hg19) mapping, we generated a custom build tailored for alignment to different reference databases containing thousands of similar and/or overlapping sequences, such as bacterial RefSeq (Pruitt et al. 2007). This modified SNAP build (v0.15) included options to improve alignment speed and efficiency by stopping at the first hit (“-f” parameter) and retaining hits that mapped to multiple locations (“-x” parameter). All of the SNAP alignments used by SURPI incorporate these two additional parameters. Reference databases A description of how the reference databases used by SURPI were generated is given in the Supplemental Methods. ROC curve analysis of in silico-generated query data sets ROC curve analysis (Zweig and Campbell 1993) was used to evaluate the ability of the various nucleotide aligners (SNAP, BWA, BT2, and BLASTn) to correctly classify a given set of in silico NGS reads when mapped against the human DB, bacterial DB, or viral nucleotide DB (Fig. 2A–E). Similarly, ROC curve analysis was used to compare RAPSearch and BLASTx performance when mapping translated nucleotide reads to the viral protein DB (Fig. 2F). Details on the construction of the in silico query nucleotide data sets and range of parameters used for the ROC curve analysis (Supplemental Fig. S4) are provided in the Supplemental Methods. The gold standard criterion used for a correctly classified read to any given database was that the in silico read had originated from that database. To generate the ROC curves, the true positive rate [TPR = TP/(TP + FN)], or sensitivity, was plotted against the false positive rate [FPR = FP/(FP + TN)], or 1-specificity. Youden’s index and the F1 score (harmonic mean) were applied as independent criteria to select an optimal cutoff point of diagnostic accuracy for the ROC curve (Akobeng 2007). In all instances, the cutoff point identified by Youden’s index and the F1 score were identical. Speed benchmarking for aligners Details of speed benchmarking for the various alignment algorithms are provided in the Supplemental Methods. ROC curve analysis of clinical query data sets The ROC curve analysis used NGS data sets corresponding to a stool sample from a child in Mexico with diarrhea (Yu et al. 2012), a nasal swab sample from a patient with acute respiratory illness (Greninger et al. 2010), and a serum sample from a patient in California with hantavirus pulmonary syndrome (Nunez et al. 2014), harboring RSV, influenza A(H1N1)pdm2009, and Sin Nombre virus, respectively (Supplemental Methods). Seven million unique preprocessed reads were selected from each data set, and human and bacterial reads were removed prior to ROC curve analysis by SNAP alignment to the human DB and bacterial DB, respectively, using an edit distance of 12. The gold standard criterion for a correct viral classification was BLASTn alignment against the target viral genome (obtained by Sanger sequencing) at an E-value cutoff of 10−8. SURPI pipeline The SURPI pipeline is comprised of a series of shell, Python, and Perl scripts in Linux and incorporates several open-source tools, including the SNAP and RAPSearch aligners. SURPI has a set of fixed external software and database dependencies (Supplemental Fig. S5) and user-defined custom parameters (Supplemental Methods). The pipeline accepts a raw FASTQ file as input and recognizes the presence of multiple barcodes used for indexing. Paired-end reads are handled by concatenating the files corresponding to the individual reads and their mate pairs into a single file for streamlined analysis. The preprocessing step consists of (1) trimming low-quality and adapter sequences using cutadapt (Martin 2011), retaining reads of trimmed length >50 bp, (2) removing low-complexity sequences using the DUST algorithm in PRINSEQ (Schmieder and Edwards 2011), and (3) normalizing read lengths for SNAP alignment by cropping reads of length >75 to 75 bp. In fast mode, SNAP alignments are first performed against the human DB followed by separate alignments of the human background-subtracted reads to bacterial and viral nucleotide DBs, whereas in comprehensive mode, the initial SNAP alignment against the human database is followed by sequential alignments to 29 indexed nt subdatabases. Following SNAP alignment, matched reads are taxonomically classified by lookup of matched GI/accession numbers from the NCBI taxonomy database. The taxonomic classification is then appended to the SAM (sequence alignment/map) file outputted by SNAP. In comprehensive mode, the SURPI pipeline continues to the de novo assembly step, which uses an empiric approach that is optimized for NGS metagenomics data (Supplemental Material). Duplicates at the level of cropped reads are first removed using GenomeTools (gt) SEQUNIQ (Gremme et al. 2013). The corresponding full-length reads are then de-multiplexed by barcode and analyzed using the Message Passing Interface (MPI)-based parallel version of the AbySS de novo assembler (AbYSS 1.3.5 release) (Simpson et al. 2009). Increased robustness of the de Brujin graph-based assembly is obtained by running AbySS multiple times at a kmer size of 34, using both the entire data set and individually partitioned sets of 100,000 reads as input. Output contig sequences of length greater than or equal to the read length are then combined into a single file and further analyzed using the OLC de novo assembler Minimo (Minimo v1.6 release) (Treangen et al. 2011) at default parameters. Contigs are retained if they are >1.75× the length of the original reads. Finally, the full-length unmatched reads, along with the final assembled contigs, are subjected to a protein homology search against the viral protein DB using RAPSearch at an E-value cutoff of 10−1. A user-defined option also allows for a protein homology search against the NCBI nr DB. Retrieved taxonomic information and sequences in FASTA format are appended to the RAPSearch output. To generate coverage maps, reads classified by SURPI as viral or bacterial are automatically mapped to the most likely reference genome present as follows. For each discrete viral or bacterial genus, assigned NGS reads are directly mapped to all nucleotide reference sequences corresponding to that genus at the species, strain, or substrain level using BLASTn at an E-value cutoff of 10−20. For each genus, a coverage map of the reference sequence with the highest percent coverage is generated, with priority given to reference sequences in the following order: (1) complete genomes; (2) complete sequences; or (3) partial sequences/individual genes. Detection of clinically relevant pathogens using SURPI The output of the SURPI pipeline includes a list of all classified reads annotated with their taxonomic assignment; a summary table of read counts stratified by family, genus, species, and accession number (Supplemental Methods); and a series of coverage maps for detected microbial genomes (Supplemental Fig. S2). Coverage maps shown in Figure 5 were edited using Microsoft Excel, as were pie charts derived from the summary tables (Supplemental Tables S6–S21). Sequences corresponding to bacteriophages were grouped together in a single category. Speed benchmarking for SURPI End-to-end processing times for the SURPI pipeline (Fig. 7; Supplemental Table S1) were measured using the elapsed wall-clock time and included the following individually timed steps: (1) preprocessing; (2) computational subtraction against the human DB; (3) SNAP alignment to the bacterial DB (fast mode); (4) SNAP alignment to the viral nucleotide DB (fast mode); (5) SNAP alignment to the complete NCBI nt DB (comprehensive mode); (6) de novo contig assembly (comprehensive mode); (7) RAPSearch viral protein homology search using translated nucleotide queries (comprehensive mode); and (8) overhead time, including file conversion, sequence retrieval, determination of read counts, and generation of summary tables and coverage maps. Processing time trend lines and regression R2 values were generated using Microsoft Excel.",VirusIdentification," cloudcompatible bioinformatics pipeline  ultrarapid pathogen identification  nextgeneration sequence  clinical samples
clinical ngs data set detail regard clinical sample approve research protocols  sample collection  ngs library construction  provide   supplemental methods hardware minimum hardware requirements  run surpi include  multicore server run ubuntu  prefer   least    ram surpi   software dependencies require    disk space reference data require    disk space  surpi runtime     size   input fastq file may  need  additional temporary storage  specific hardware use   surpi test  benchmarking  provide   supplemental methods custom modifications   snap nucleotide aligner snap   new hashbased nucleotide aligner develop  map  ngs data  reference genomes across  wide range  read lengths   zaharia    available   snap run  faster  exist tool  maintain comparable  higher accuracy fig   aligner partially derive  speed  load  entire index reference database  ram since snap  originally design   human genome hg19 map  generate  custom build tailor  alignment  different reference databases contain thousands  similar andor overlap sequence   bacterial refseq pruitt     modify snap build  include options  improve alignment speed  efficiency  stop   first hit “” parameter  retain hit  map  multiple locations “” parameter    snap alignments use  surpi incorporate  two additional parameters reference databases  description    reference databases use  surpi  generate  give   supplemental methods roc curve analysis   silicogenerated query data set roc curve analysis zweig  campbell   use  evaluate  ability   various nucleotide aligners snap bwa bt2  blastn  correctly classify  give set   silico ngs read  map   human  bacterial   viral nucleotide  fig 2ae similarly roc curve analysis  use  compare rapsearch  blastx performance  map translate nucleotide read   viral protein  fig  detail   construction    silico query nucleotide data set  range  parameters use   roc curve analysis supplemental fig   provide   supplemental methods  gold standard criterion use   correctly classify read   give database     silico read  originate   database  generate  roc curve  true positive rate tpr  tptp    sensitivity  plot   false positive rate fpr  fpfp    specificity youdens index    score harmonic mean  apply  independent criteria  select  optimal cutoff point  diagnostic accuracy   roc curve akobeng    instance  cutoff point identify  youdens index    score  identical speed benchmarking  aligners detail  speed benchmarking   various alignment algorithms  provide   supplemental methods roc curve analysis  clinical query data set  roc curve analysis use ngs data set correspond   stool sample   child  mexico  diarrhea      nasal swab sample   patient  acute respiratory illness greninger      serum sample   patient  california  hantavirus pulmonary syndrome nunez    harbor rsv influenza ah1n1pdm2009  sin nombre virus respectively supplemental methods seven million unique preprocessed read  select   data set  human  bacterial read  remove prior  roc curve analysis  snap alignment   human   bacterial  respectively use  edit distance    gold standard criterion   correct viral classification  blastn alignment   target viral genome obtain  sanger sequence   evalue cutoff   surpi pipeline  surpi pipeline  comprise   series  shell python  perl script  linux  incorporate several opensource tool include  snap  rapsearch aligners surpi   set  fix external software  database dependencies supplemental fig   userdefined custom parameters supplemental methods  pipeline accept  raw fastq file  input  recognize  presence  multiple barcodes use  index pairedend read  handle  concatenate  file correspond   individual read   mate pair   single file  streamline analysis  preprocessing step consist   trim lowquality  adapter sequence use cutadapt martin  retain read  trim length    remove lowcomplexity sequence use  dust algorithm  prinseq schmieder  edwards    normalize read lengths  snap alignment  crop read  length      fast mode snap alignments  first perform   human  follow  separate alignments   human backgroundsubtracted read  bacterial  viral nucleotide dbs whereas  comprehensive mode  initial snap alignment   human database  follow  sequential alignments   index  subdatabases follow snap alignment match read  taxonomically classify  lookup  match giaccession number   ncbi taxonomy database  taxonomic classification   append   sam sequence alignmentmap file output  snap  comprehensive mode  surpi pipeline continue    novo assembly step  use  empiric approach   optimize  ngs metagenomics data supplemental material duplicate   level  crop read  first remove use genometools  sequniq gremme     correspond fulllength read   demultiplexed  barcode  analyze use  message pass interface mpibased parallel version   aby  novo assembler aby  release simpson    increase robustness    brujin graphbased assembly  obtain  run aby multiple time   kmer size   use   entire data set  individually partition set   read  input output contig sequence  length greater   equal   read length   combine   single file   analyze use  olc  novo assembler minimo minimo  release treangen     default parameters contigs  retain      length   original read finally  fulllength unmatched read along   final assemble contigs  subject   protein homology search   viral protein  use rapsearch   evalue cutoff    userdefined option also allow   protein homology search   ncbi   retrieve taxonomic information  sequence  fasta format  append   rapsearch output  generate coverage map read classify  surpi  viral  bacterial  automatically map    likely reference genome present  follow   discrete viral  bacterial genus assign ngs read  directly map   nucleotide reference sequence correspond   genus   species strain  substrain level use blastn   evalue cutoff     genus  coverage map   reference sequence   highest percent coverage  generate  priority give  reference sequence   follow order  complete genomes  complete sequence   partial sequencesindividual genes detection  clinically relevant pathogens use surpi  output   surpi pipeline include  list   classify read annotate   taxonomic assignment  summary table  read count stratify  family genus species  accession number supplemental methods   series  coverage map  detect microbial genomes supplemental fig  coverage map show  figure   edit use microsoft excel   pie chart derive   summary table supplemental table s6s21 sequence correspond  bacteriophages  group together   single category speed benchmarking  surpi endtoend process time   surpi pipeline fig  supplemental table   measure use  elapse wallclock time  include  follow individually time step  preprocessing  computational subtraction   human   snap alignment   bacterial  fast mode  snap alignment   viral nucleotide  fast mode  snap alignment   complete ncbi   comprehensive mode   novo contig assembly comprehensive mode  rapsearch viral protein homology search use translate nucleotide query comprehensive mode   overhead time include file conversion sequence retrieval determination  read count  generation  summary table  coverage map process time trend line  regression  value  generate use microsoft excel",3
82,MetaViC,"Metaviromics Reveals Unknown Viral Diversity in the Biting Midge Culicoides impunctatus
Sample Collection and Sequencing Three haematophageous midge pools (Culicoides spp.), formed of 10 midges each were collected during July 2015 from two sites located within the Loch Lomond and Trossachs National Park, Scotland [27]. Two pools (M1 and M2) were collected in the oak woodlands immediately surrounding the Scottish Centre for Ecology and Natural Environment (SCENE) (56°07’34”N, 4°37’04”W). The third pool was collected at the edge of Cashel Forest near livestock and dispersed cottages (56°6’38.3”N, 4°34’37.2”W). The midges were morphologically identified as Culicoides impunctatus, the most abundant Culicoides species found in the area. The midge species was identified using an interactive identification key for Culicoides (Ceratopogonidae) females from the Western Palearctic region [28]. These midges were then crushed by pestle and RNA was isolated from the 10 midges using 1 ml Trizol followed by standard RNA extraction, including isopropanol precipitation, using glycogen as carrier. A TruSeq Illumina stranded library preparation was carried out with input of 250 ng RNA per sample, following the manufacturer’s protocol. Briefly, RNA was mildly fragmented followed by a reverse transcription step and double-strand DNA (dsDNA) was synthesised. This dsDNA was A-tailed, followed by adaptor ligation and amplification using 15 PCR cycles. Libraries were quantified, pooled and loaded on the Illumina MiSeq. Paired-end reads were generated with read length 2 × 150 bp. This yielded 2,132,297, 1,881,438 and 1,657,675 paired reads for midge pools M1, M2 and M3 respectively. The raw reads and assembled contigs sequences are submitted to European Nucleotide Archives with BioProject number PRJEB33833 and accession numbers LR701640-LR701660. 2.2. Metagenomic Analysis Using MetaViC Pipeline Reads from all samples were collated and analysed using the MetaViC pipeline, which has been designed to identify viral sequences in metagenomic datasets in the absence of a known host genome (the pipeline and documentation is available from: https://github.com/sejmodha/MetaViC) [29]. However, MetaViC can be applied directly to any virus metagenomics study. The MetaViC pipeline is divided into two major components (Figure 1). Firstly, reads were cleaned, and non-viral content was removed; secondly, the curated reads were assembled using de novo approaches. The reads were pre-processed by removing the Illumina sequence adapters using Trim_Galore [30]. Small and large subunits ribosomal RNA (rRNA) sequences were identified and removed using the in silico ribosomal sequence identification tool Ribopicker version 0.4.3 [31] with the SILVA rRNA database version 123 [32,33,34]. Each read was translated and searched against the RefSeq protein databases using a DIAMOND [35] blastx approach. The DIAMOND output was in turn parsed to identify and remove sequences that matched any known bacteria, invertebrates, mammal, rodent, phages, plants, vertebrates, primates and synthetic construct sequences. Read pairs that matched any known viral or environmental sequences and reads that did not match any known sequences in the RefSeq database [36] using an e-value of 0.0001 were extracted and kept for further analysis. At the end of this cleaning pipeline, sequences were confirmed to be properly paired using Prinseq-lite.pl [37]. These cleaned sequences were then submitted to the second component of the MetaViC pipeline that performed the de novo assembly. The de novo assembly step was carried out using two de bruijin graph-based assemblers: SPAdes version 3.7.0 [38] and IDBA-UD version 1.1.2 [39] with multiple k-mer values ranging from 21 to 121. These two assemblers have been developed to reconstruct genomes with uneven coverage and depth which is typically the case for viral metagenomic samples [40]. The assembled contigs from the two tools were then consolidated using GARM version 0.7.5 [41], an assembly merging pipeline that uses MUMmer3 [42] to find overlaps between two assemblies and join them. The contig consolidation step is performed to generate supercontigs from both assemblies. This step is useful for constructing longer stretches of sequence and can also help to identify assembler-specific mis-assemblies and unique regions of the genomes from the shorter contigs generated independently by separate assembly tools [43,44]. Contig reconciliation also helps to improve assembly metric values such as N50. Reads were aligned back to these contigs and supercontigs using bowtie2 [45] and assembly statistics were generated using weeSAMv1.1 (https://github.com/centre-for-virus-research/weeSAM). Unmapped read pairs were extracted to retain any viral reads that might not have assembled into contigs. In order to check the assembly quality, QUAST [46] analysis was performed on each contig assembly and on supercontigs. Contigs longer than 200 nucleotides (nt) and supercontigs generated by GARM were then combined and classified using DIAMOND [35] against the RefSeq protein database. KronaTools [47] were used to create an interactive HTML output to visualise the formatted results generated by DIAMOND. Results obtained from the MetaViC pipeline were further investigated to identify viral signatures in the contigs. Any contig matching viruses that were run on the previous or same MiSeq run were excluded from the analysis (Table S1). 2.3. Protein Domain Identification The contig sequences that match viruses were translated using getorf [48] in all six frames keeping open reading frames (ORFs) with a minimum length of 300 nt. Local Interproscan [49] version 5.25–64.0 analysis was performed to search for protein domains present in the ORFs generated from the contigs. InterProScan domain search was applied to identify the known domain signatures within the ORFs. To put the newly identified sequences into the context of currently known viral genomes, each ORF sequence that contained viral signature domains was analysed using blastp [50] with e-value 0.001, and the top 20 hits were extracted. The top hits were used to produce a set of sequences for phylogenetic analysis. The metadata was extracted from the GenBank file for all sequences using a customised python script that collated details about protein accession, protein description, genome accession, RefSeq genome accessions, source, host and country of origin of the sequences. Multiple sequence alignments were performed using MAFFT [51] and converted to nucleotide alignments using pal2nal [52]. These alignments were used to compute the best substitution model based on the Bayesian Information Criteria (BIC) using jModelTest [53]. This substitution model was subsequently used for reconstructing a Maximum Likelihood phylogeny using RAxML [54] with node support evaluated using 1000 bootstraps replicates.",VirusIdentification,"metaviromics reveal unknown viral diversity   bite midge culicoides impunctatus
sample collection  sequence three haematophageous midge pool culicoides spp form   midges   collect  july   two sit locate within  loch lomond  trossachs national park scotland  two pool     collect   oak woodlands immediately surround  scottish centre  ecology  natural environment scene °” °”  third pool  collect   edge  cashel forest near livestock  disperse cottages °” °”  midges  morphologically identify  culicoides impunctatus   abundant culicoides species find   area  midge species  identify use  interactive identification key  culicoides ceratopogonidae females   western palearctic region   midges   crush  pestle  rna  isolate    midges use   trizol follow  standard rna extraction include isopropanol precipitation use glycogen  carrier  truseq illumina strand library preparation  carry   input    rna per sample follow  manufacturers protocol briefly rna  mildly fragment follow   reverse transcription step  doublestrand dna dsdna  synthesise  dsdna  atailed follow  adaptor ligation  amplification use  pcr cycle libraries  quantify pool  load   illumina miseq pairedend read  generate  read length      yield     pair read  midge pool     respectively  raw read  assemble contigs sequence  submit  european nucleotide archive  bioproject number prjeb33833  accession number lr701640lr701660  metagenomic analysis use metavic pipeline read   sample  collate  analyse use  metavic pipeline    design  identify viral sequence  metagenomic datasets   absence   know host genome  pipeline  documentation  available    however metavic   apply directly   virus metagenomics study  metavic pipeline  divide  two major components figure  firstly read  clean  nonviral content  remove secondly  curated read  assemble use  novo approach  read  preprocessed  remove  illumina sequence adapters use trim_galore  small  large subunits ribosomal rna rrna sequence  identify  remove use   silico ribosomal sequence identification tool ribopicker version     silva rrna database version    read  translate  search   refseq protein databases use  diamond  blastx approach  diamond output   turn parse  identify  remove sequence  match  know bacteria invertebrates mammal rodent phages plant vertebrates primates  synthetic construct sequence read pair  match  know viral  environmental sequence  read    match  know sequence   refseq database  use  evalue    extract  keep   analysis   end   clean pipeline sequence  confirm   properly pair use prinseqlitepl   clean sequence   submit   second component   metavic pipeline  perform   novo assembly   novo assembly step  carry  use two  bruijin graphbased assemblers spade version    idbaud version    multiple kmer value range      two assemblers   develop  reconstruct genomes  uneven coverage  depth   typically  case  viral metagenomic sample   assemble contigs   two tool   consolidate use garm version    assembly merge pipeline  use mummer3   find overlap  two assemblies  join   contig consolidation step  perform  generate supercontigs   assemblies  step  useful  construct longer stretch  sequence   also help  identify assemblerspecific misassemblies  unique regions   genomes   shorter contigs generate independently  separate assembly tool  contig reconciliation also help  improve assembly metric value   n50 read  align back   contigs  supercontigs use bowtie2   assembly statistics  generate use weesamv1  unmapped read pair  extract  retain  viral read  might   assemble  contigs  order  check  assembly quality quast  analysis  perform   contig assembly   supercontigs contigs longer   nucleotides   supercontigs generate  garm   combine  classify use diamond    refseq protein database kronatools   use  create  interactive html output  visualise  format result generate  diamond result obtain   metavic pipeline   investigate  identify viral signatures   contigs  contig match viruses   run   previous   miseq run  exclude   analysis table   protein domain identification  contig sequence  match viruses  translate use getorf    six frame keep open read frame orfs   minimum length    local interproscan  version  analysis  perform  search  protein domains present   orfs generate   contigs interproscan domain search  apply  identify  know domain signatures within  orfs  put  newly identify sequence   context  currently know viral genomes  orf sequence  contain viral signature domains  analyse use blastp   evalue    top  hit  extract  top hit  use  produce  set  sequence  phylogenetic analysis  metadata  extract   genbank file   sequence use  customise python script  collate detail  protein accession protein description genome accession refseq genome accession source host  country  origin   sequence multiple sequence alignments  perform use mafft   convert  nucleotide alignments use pal2nal   alignments  use  compute  best substitution model base   bayesian information criteria bic use jmodeltest   substitution model  subsequently use  reconstruct  maximum likelihood phylogeny use raxml   node support evaluate use  bootstrap replicate",3
83,MePIC,"MePIC, metagenomic pathogen identification for clinical specimens
Next-generation DNA sequencing technologies have led to a new method of identifying the causative agent of infectious diseases in hospitalized patients and during outbreaks (1,2). By directly sequencing millions of DNA/RNA molecules in a specimen and matching the sequences to those in a database, pathogens can be inferred. The analysis comprises three steps. First, the nucleotide sequences of the specimen, which includes the pathogen, human tissue and commensal microorganisms, are read using a next-generation sequencer. Second, from bioinformatic processing of the reads, the organisms from which the individual reads were derived are inferred. Last, the percentages of the organisms' genomic sequences in the specimen are estimated, and the pathogen is identified. Although the first and last steps have become easy due to the development of benchtop sequencers and metagenomic software, the middle step still requires computational resources and bioinformatic skill. To facilitate the middle step, we developed a cloud-computing pipeline that is easy and fast. The prototype of the pipeline has been used in our metagenomic search for pathogens in various clinical cases. We have reported metagenomic analyses of clinical specimens that successfully identified Francisella tularensis in an abscess as a pathogen (3), Streptococcus spp. in a lymph node as a possible causative candidate of Kawasaki disease (4) and heterogeneity of the 2009 pandemic influenza A virus (A/H1N1/2009) in the lung (5). In an outbreak of 22 adults with myalgia, the majority were infected with human parechovirus type 3, which typically causes disease in young children (6). In recent food poisoning outbreaks that were due to raw fish consumption, a flounder parasite Kudoa septempunctata was discovered as the causative agent (7). In all cases, the pathogen identification was primarily due to metagenomic analyses using next-generation sequencers. In the workflow of metagenomic pathogen identification using MePIC, the first step is performed by the user. DNA and/or RNA is extracted from a specimen, such as sputum, feces, an abscess or blood, and a library of DNA/cDNA is prepared for sequencing. The library is sequenced using a benchtop next-generation sequencer, and the sequenced reads are uploaded to the MePIC pipeline via a secure internet connection. The pipeline accepts input files in FASTQ format, which is the standard for next-generation sequencing analysis. When using Illumina MiSeq sequencers (San Diego, Calif., USA). In the second step, the uploaded reads are processed by the MePIC pipeline (Fig. 1). Unnecessary adapter sequences and low quality bases are trimmed off the reads using the fastq-mcf program in the ea-utils package (http://code.google.com/p/ea-utils/). Humanderived reads are detected through comparisons with the human genome using the BWA (8) program; the 63 Fig. 1. Screenshot of the MePIC pipeline. In box 1, the user specifies the next-generation sequencer reads for upload. In box 2, details are set for trimming adaptor sequences and low quality bases from the reads. In box 3, criteria are set for the exclusion of human reads. In box 4, the user chooses the program for searching the database of known sequences. MePIC number of human reads are counted, but the reads themselves are removed from the downstream analysis. For each of the remaining reads, similar sequences are searched in the database of all known nucleotide sequences (NCBI nt) using the MEGABLAST (9) or BWA program (10). Based on the information of the database sequences that match with the read, we can infer the gene (e.g., virulence gene) and organism (e.g., Escherichia coli) that the read is derived from. The run time of the pipeline is primarily allotted to searching the database of known sequences. The required time can be drastically shortened by splitting the job and running in parallel using a cloud-computing system or local server. Respectively, it takes 10 h for one core of 2.67 GHz and 6 min for 100 cores to perform MEGABLAST search against the nt nucleotide database (as of year 2013) for one million reads of length 200 bp. The run time varies according to the sample source and condition. In blood samples, À90z of reads are derived from human and removed in the preprocessing step, and accordingly the time for database search of the remaining reads is reduced. Human derived reads are less in sputum samples (60z) or normal feces (¿0z), which accordingly demands more database search time. In the final step of the workflow, the user downloads the database search result to a local PC, including the reads annotated with the organism and gene function. To summarize the taxonomic and functional informa- 64 Fig. 2. (Color online) Analytic results of a case where Saffold virus was detected in a pharyngeal specimen. DNA and RNA were extracted from the specimen and sequenced. (A) Taxonomic view of MEGAN software. Saffold virus was detected in the RNA-seq reads. In this case, 0.4z of the reads were derived from the virus, which was sufficient to detect the virus in this patient. (B) Reads mapped to the reference genome of Saffold virus. The horizontal axis represents the position on the viral genome, and the vertical axis represents the abundance of mapped reads. The reads cover the whole 8 kb genome. Colored bars indicate the nucleotides where the patient's strain and the reference strain differ. The plot was made using Genome Jack software (http://www.mss.co.jp/ businessfield/bioinformatics/solution/products/genomejack/). 64 tion over all reads, a metagenome browser can be used, such as MEGAN (11), which is one useful free software program (Fig. 2). The existence and quantity of pathogenic organisms and virulence genes are inferable from the number of detected reads, which is proportional to the number of the corresponding nucleotide sequences in the original specimen. We developed a simple metagenomic analysis pipeline for removing ambiguous and host-derived short reads and rapidly identifying disease-causing pathogens in hospitalized patients and during outbreaks. The MePIC pipeline has a webpage interface that can be used easily by clinicians and epidemiologists, who do not have bioinformatic skill. The locally required equipment includes a benchtop next-generation DNA sequencer and a desktop PC for viewing the results. The adoption of cloud computing for metagenomic pathogen identification was proposed in the PathSeq software (12), which required bioinformatic skill for cloud computing. The MePIC pipeline, in contrast, manages the computational aspects in the background. The sequence similarity search of the database is the most computationally demanding step of metagenomic studies, and one solution is to thin the database. Using such an approach, the MetaPhlAn system (13) can speedily identify bacterial and archaeal organisms. However, we opted to maintain the entire database and rely on augmenting the computational power to hasten the analysis because clinical applications require finer taxonomic distinction: for example, the distinction of eneterohemorrhagic and commensal E. coli is critical. Within the broad possible applications of metagenomics, our pipeline is tailored for clinical use. Metagenomic pathogen identification using nextgeneration sequencers surpasses conventional detection systems in sensitivity. The approach of directly sequencing nucleotides of a specimen is particularly powerful for unculturable or slow-growth pathogens (e.g., Mycobacterium). Whereas conventional PCR-based detection can miss new variants of a known pathogen due to mismatches of pre-designed primer sets, the de novo DNA/RNA sequencing approach overcomes this limitation. Metagenomic analysis can also identify a causal agent that was not known to be pathogenic (7). As for quantitative sensitivity, the metagenomic approach has been shown to be comparable to RT-PCR in 65 MePIC virus detection (2). The major drawback of metagenomic pathogen identification is the cost of next-generation sequencers and reagents. Although the sequencers remain expensive, their versatile clinical and research utility (not restricted to infectious diseases) is pushing their widespread implementation in research institutes and hospitals. The rapidly decreasing reagent cost has reached approximately $100 for one million reads, which would be appropriate for pathogen identification. The current methodology of metagenomic pathogen identification is based on sequence matches with known pathogenic species/strains. To enable detection of unknown pathogens, an abundant dataset of ``disease cases'' and ``normal flora controls'' is necessary. If the number of reads of an organism (which is proportional to the amount of its DNA in the specimen) is much larger in cases than controls, infection by the organism can be suspected. The development of such pathogen discovery will require the accumulation of metagenomic data for disease-causing and normal flora and the invention of analytic tools. We believe that the use of the MePIC pipeline will promote metagenomic pathogen identification and improve the understanding of infectious diseases. The source code for installing on a local server is available from the authors upon request. The website of the pipeline is https://mepic.nih.go.jp/. The sequence reads of the pharyngeal specimen that included the Saffold virus are available from the DDBJ Sequence Read Archive under accession number DRA000973.",VirusIdentification,"mepic metagenomic pathogen identification  clinical specimens
nextgeneration dna sequence technologies  lead   new method  identify  causative agent  infectious diseases  hospitalize patients   outbreaks   directly sequence millions  dnarna molecules   specimen  match  sequence     database pathogens   infer  analysis comprise three step first  nucleotide sequence   specimen  include  pathogen human tissue  commensal microorganisms  read use  nextgeneration sequencer second  bioinformatic process   read  organisms    individual read  derive  infer last  percentages   organisms' genomic sequence   specimen  estimate   pathogen  identify although  first  last step  become easy due   development  benchtop sequencers  metagenomic software  middle step still require computational resources  bioinformatic skill  facilitate  middle step  develop  cloudcomputing pipeline   easy  fast  prototype   pipeline   use   metagenomic search  pathogens  various clinical case   report metagenomic analyse  clinical specimens  successfully identify francisella tularensis   abscess   pathogen  streptococcus spp   lymph node   possible causative candidate  kawasaki disease   heterogeneity    pandemic influenza  virus ah1n1   lung    outbreak   adults  myalgia  majority  infect  human parechovirus type   typically cause disease  young children   recent food poison outbreaks   due  raw fish consumption  flounder parasite kudoa septempunctata  discover   causative agent    case  pathogen identification  primarily due  metagenomic analyse use nextgeneration sequencers   workflow  metagenomic pathogen identification use mepic  first step  perform   user dna andor rna  extract   specimen   sputum feces  abscess  blood   library  dnacdna  prepare  sequence  library  sequence use  benchtop nextgeneration sequencer   sequence read  upload   mepic pipeline via  secure internet connection  pipeline accept input file  fastq format    standard  nextgeneration sequence analysis  use illumina miseq sequencers san diego calif usa   second step  upload read  process   mepic pipeline fig  unnecessary adapter sequence  low quality base  trim   read use  fastqmcf program   eautils package  humanderived read  detect  comparisons   human genome use  bwa  program   fig  screenshot   mepic pipeline  box   user specify  nextgeneration sequencer read  upload  box  detail  set  trim adaptor sequence  low quality base   read  box  criteria  set   exclusion  human read  box   user choose  program  search  database  know sequence mepic number  human read  count   read   remove   downstream analysis     remain read similar sequence  search   database   know nucleotide sequence ncbi  use  megablast   bwa program  base   information   database sequence  match   read   infer  gene  virulence gene  organism  escherichia coli   read  derive   run time   pipeline  primarily allot  search  database  know sequence  require time   drastically shorten  split  job  run  parallel use  cloudcomputing system  local server respectively  take    one core   ghz   min   core  perform megablast search    nucleotide database   year   one million read  length    run time vary accord   sample source  condition  blood sample à90z  read  derive  human  remove   preprocessing step  accordingly  time  database search   remain read  reduce human derive read  less  sputum sample   normal feces ¿  accordingly demand  database search time   final step   workflow  user download  database search result   local  include  read annotate   organism  gene function  summarize  taxonomic  functional informa  fig  color online analytic result   case  saffold virus  detect   pharyngeal specimen dna  rna  extract   specimen  sequence  taxonomic view  megan software saffold virus  detect   rnaseq read   case    read  derive   virus   sufficient  detect  virus   patient  read map   reference genome  saffold virus  horizontal axis represent  position   viral genome   vertical axis represent  abundance  map read  read cover  whole   genome color bar indicate  nucleotides   patient' strain   reference strain differ  plot  make use genome jack software  businessfieldbioinformaticssolutionproductsgenomejack  tion   read  metagenome browser   use   megan    one useful free software program fig   existence  quantity  pathogenic organisms  virulence genes  inferable   number  detect read   proportional   number   correspond nucleotide sequence   original specimen  develop  simple metagenomic analysis pipeline  remove ambiguous  hostderived short read  rapidly identify diseasecausing pathogens  hospitalize patients   outbreaks  mepic pipeline   webpage interface    use easily  clinicians  epidemiologists     bioinformatic skill  locally require equipment include  benchtop nextgeneration dna sequencer   desktop   view  result  adoption  cloud compute  metagenomic pathogen identification  propose   pathseq software   require bioinformatic skill  cloud compute  mepic pipeline  contrast manage  computational aspects   background  sequence similarity search   database    computationally demand step  metagenomic study  one solution   thin  database use   approach  metaphlan system   speedily identify bacterial  archaeal organisms however  opt  maintain  entire database  rely  augment  computational power  hasten  analysis  clinical applications require finer taxonomic distinction  example  distinction  eneterohemorrhagic  commensal  coli  critical within  broad possible applications  metagenomics  pipeline  tailor  clinical use metagenomic pathogen identification use nextgeneration sequencers surpass conventional detection systems  sensitivity  approach  directly sequence nucleotides   specimen  particularly powerful  unculturable  slowgrowth pathogens  mycobacterium whereas conventional pcrbased detection  miss new variants   know pathogen due  mismatch  predesigned primer set   novo dnarna sequence approach overcome  limitation metagenomic analysis  also identify  causal agent    know   pathogenic    quantitative sensitivity  metagenomic approach   show   comparable  rtpcr   mepic virus detection   major drawback  metagenomic pathogen identification   cost  nextgeneration sequencers  reagents although  sequencers remain expensive  versatile clinical  research utility  restrict  infectious diseases  push  widespread implementation  research institute  hospitals  rapidly decrease reagent cost  reach approximately   one million read  would  appropriate  pathogen identification  current methodology  metagenomic pathogen identification  base  sequence match  know pathogenic speciesstrains  enable detection  unknown pathogens  abundant dataset  ``disease cases''  ``normal flora controls''  necessary   number  read   organism   proportional   amount   dna   specimen  much larger  case  control infection   organism   suspect  development   pathogen discovery  require  accumulation  metagenomic data  diseasecausing  normal flora   invention  analytic tool  believe   use   mepic pipeline  promote metagenomic pathogen identification  improve  understand  infectious diseases  source code  instal   local server  available   author upon request  website   pipeline    sequence read   pharyngeal specimen  include  saffold virus  available   ddbj sequence read archive  accession number dra000973",3
84,VIRALpro,"VIRALpro: a tool to identify viral capsid and tail sequences 
Data We built a dataset of, respectively, 2648 and 483 non-redundant capsid and nucleocapsid sequences. The non-capsid sequences were randomly chosen from the phage non-structural sequences (Seguritan et al., 2012) and from the NCBI protein database by querying for non-phage, non-structural, proteins (see Supplementary materials). When merging this set of sequences with the training set of iVireons MCP1:1 denoted by iVcapsid, we obtained 3888 positive and 4071 negative sequences. We refer to the resulting dataset as Cprotrain. Using the same process, we built a dataset of 1719 positives tail sequences and when merged with the training set of iVireons tail 1:1 ANN, we obtained 2574 positive and 4095 negative tail sequences. We refer to the resulting dataset as Tprotrain. We denote by iVtest the test set of phage protein sequences described in Seguritan et al. (2012). We used 10-fold cross validation on the training sets to assess performance. For each training set, one of the folds is used as the validation set to produce the plots in Figure 1. Features To identify capsid or tail protein sequences, we use the average amino acid composition (20 features) as well the average secondary structure composition (three features), as predicted by SSpro (Magnan and Baldi, 2014). In addition, we built 3380 Profile Hidden Markov Models (HMMs) to locally probe the sequences. These HMMs are built using HMMER (Eddy, 2009) from multiple sequence alignment of contact fragments of capsid proteins—which are essentially pairs of fragments that are close in the tertiary structure (see Supplementary materials) and whose structure has been shown to be well conserved even for distantly related homologs (Galiez and Coste, 2015). The e-values of the different HMMs are linearly combined using coefficients that are obtained using the RankBoost algorithm (Freund et al., 2003). Finally, the HMMs yield three features: the boosted linear combination of HMM e-values, the e-value of the best HMM hit and the number of HMM hits (e-value of 10 or better). Finally, we train an SVM (Chang and Lin, 2011) with these 26 features.",VirusIdentification,"viralpro  tool  identify viral capsid  tail sequence 
data  build  dataset  respectively    nonredundant capsid  nucleocapsid sequence  noncapsid sequence  randomly choose   phage nonstructural sequence seguritan       ncbi protein database  query  nonphage nonstructural proteins see supplementary materials  merge  set  sequence   train set  ivireons mcp1 denote  ivcapsid  obtain  positive   negative sequence  refer   result dataset  cprotrain use   process  build  dataset   positives tail sequence   merge   train set  ivireons tail  ann  obtain  positive   negative tail sequence  refer   result dataset  tprotrain  denote  ivtest  test set  phage protein sequence describe  seguritan     use fold cross validation   train set  assess performance   train set one   fold  use   validation set  produce  plot  figure  feature  identify capsid  tail protein sequence  use  average amino acid composition  feature  well  average secondary structure composition three feature  predict  sspro magnan  baldi   addition  build  profile hide markov model hmms  locally probe  sequence  hmms  build use hmmer eddy   multiple sequence alignment  contact fragment  capsid proteins—  essentially pair  fragment   close   tertiary structure see supplementary materials  whose structure   show   well conserve even  distantly relate homologs galiez  coste   evalues   different hmms  linearly combine use coefficients   obtain use  rankboost algorithm freund    finally  hmms yield three feature  boost linear combination  hmm evalues  evalue   best hmm hit   number  hmm hit evalue    better finally  train  svm chang  lin     feature",3
85,READSCAN,"READSCAN: a fast and scalable pathogen discovery program with accurate genome relative abundance estimation
The software first indexes the host and pathogen database sequences on a chosen k-mer value r based on the principle discussed in Baeza-Yates and Perleberg (BYP) This k-mer value r allows us to detect the mutated sequences with maximum error or mutation rate of k in a string of length m. The search phase as described in Figure 1 divides the input sequences into manageable chunks, and each chunk is processed in parallel. Each chunk is mapped against the host and pathogen references simultaneously using SMALT aligner. The result of the mapping procedure is filtered for per cent identity cut-off. The reads are then classified into several bins, namely, host, pathogen, ambiguous and unmapped. The classification is based on the alignment score reported by SMALT. The directed acyclic graph representing the set of tasks and its dependencies is abstracted out, and the result is passed on to GNU make on a desktop computer and Makeflow (Yu et al., 2010) on a multicore cluster to efficiently execute the tasks in parallel to speed up the overall throughput. The Makeflow abstractions are the key that make the program highly portable and execute directly without any modification on Load Sharing Facility (LSF), Sun Grid Engine and various other load levelers. The memory and resource requirements for the alignment tasks are computed using the formula provided by the SMALT aligner, and these values are passed to the appropriate job scheduler. SMALT-like other short read aligners have an inherent maximum limitation (Martin et al., 2012) on the size of the database that can be indexed. This limitation is overcome by splitting up the database into manageable parts, such that each part does not exceed the random access memory limitation on a particular compute node. This helps the workflow to accommodate multiple human references to improve the accuracy of human reads removal and also multiple pathogen references grouped by taxon like bacteria, virus, protozoa and fungi. Choosing an appropriate chunk size can control the speed of the entire search phase. Because of sequence similarity between reference sequences in the pathogen database the same read may map to multiple references in a non-unique mapping. Hence, the resulting statistics file is clustered by NCBI taxonomy tree and the GRA for particular species is reported as a sum of the GRA of all reference sequences of that species.",VirusIdentification,"readscan  fast  scalable pathogen discovery program  accurate genome relative abundance estimation
 software first index  host  pathogen database sequence   choose kmer value  base   principle discuss  baezayates  perleberg byp  kmer value  allow   detect  mutate sequence  maximum error  mutation rate     string  length   search phase  describe  figure  divide  input sequence  manageable chunk   chunk  process  parallel  chunk  map   host  pathogen reference simultaneously use smalt aligner  result   map procedure  filter  per cent identity cutoff  read   classify  several bin namely host pathogen ambiguous  unmapped  classification  base   alignment score report  smalt  direct acyclic graph represent  set  task   dependencies  abstract    result  pass   gnu make   desktop computer  makeflow       multicore cluster  efficiently execute  task  parallel  speed   overall throughput  makeflow abstractions   key  make  program highly portable  execute directly without  modification  load share facility lsf sun grid engine  various  load levelers  memory  resource requirements   alignment task  compute use  formula provide   smalt aligner   value  pass   appropriate job scheduler smaltlike  short read aligners   inherent maximum limitation martin      size   database    index  limitation  overcome  split   database  manageable part    part   exceed  random access memory limitation   particular compute node  help  workflow  accommodate multiple human reference  improve  accuracy  human read removal  also multiple pathogen reference group  taxon like bacteria virus protozoa  fungi choose  appropriate chunk size  control  speed   entire search phase   sequence similarity  reference sequence   pathogen database   read may map  multiple reference   nonunique map hence  result statistics file  cluster  ncbi taxonomy tree   gra  particular species  report   sum   gra   reference sequence   species",3
86,MagicBLAST,"Magic-BLAST, an accurate RNA-seq aligner for long and short reads
Algorithm overview The Magic-BLAST algorithm has a structure similar to that of other BLAST programs [10]. It reads the data in batches and builds a “lookup table”, which is an index of word locations in the reads, 16-bases by default. It then scans the database sequences, usually a reference genome, for matches in the lookup table and attempts to extend selected initial matches to the length specified by the user (18 by default). The resulting matches form a seed for computation of local gapped alignments. Collinear local alignments are combined into spliced alignments. In order to be used as a seed, the original 18 base match must be completely contained within one exon (i.e., cannot span two exons). Consequently, exons shorter than the seed length cannot be captured, but they are rare (less than 0.2% of RefSeq exons), and most will be recognized by aligning in parallel on the known transcriptome. For paired reads, the best alignments are selected based on the alignment quality of the pair. For example, if one read of a pair maps equally well at two genomic sites, and the second read maps best at a single site, the read pair will be reported as mapping uniquely at the position dictated by the second read. In this way, the specificity of the mapping truly reflects the number of bases sequenced in the whole fragment, i.e. 200 bases specificity for 100 + 100 paired-end reads. Below, we present a detailed description of the above steps. Figure 1 presents an overview of these steps. Repeat filtering Most genomes contain interspersed repeats and gene families that complicate correct placement of reads in a genome. To avoid seeding to ambiguous positions, Magic-BLAST scans the reference sequence and counts 16-base words. Those words that appear in the reference database more than a user-specified number of times (by default 60) are not indexed in the lookup table, so that they never form a seed alignment. To make this procedure more efficient, only words present in the reads are counted. The cut-off number 60 was selected experimentally as the best trade-off between sensitivity and runtime for RNA-seq. Additionally, Magic-BLAST specifically masks out 16-base words that contain at least 15 A’s or 15 T’s, effectively avoiding seeding on poly-A tails. This approach is similar to soft masking in other BLAST programs. Local gapped alignment Magic-BLAST computes a local alignment by extending exact word matches (18-bases by default) between a read and a reference sequence. We use a simplified greedy alignment extension procedure, previously used in Magic [7]. Starting with the seed, the alignment is extended until the first mismatch. Next, we attempt to characterize the mismatch as a substitution, insertion or deletion of one to three bases by recursively testing the quality of the alignment of the following few bases. This is done by applying successively a table of candidate alignment operations (Table 1) until the associated requirement is met. A requirement is that a specific number of bases must match within a given number of bases following the applied operation. The first operation whose requirement is met is applied to the alignment and the algorithm proceeds to the next position on both sequences. A single substitution is reported if no requirement is satisfied. The list of alignment operations and their associated conditions used in Magic-BLAST Figure 2 shows an example alignment extension. First, there are two matches and the algorithm moves to the right by two positions on both sequences. When a mismatch (T-G) is encountered the algorithm tries successively each alignment operation and checks its requirements. The first operation, a substitution which requires nine matching bases following the mismatch, fails. The second operation, an insertion which requires ten consecutive matches, succeeds and is applied to the alignment. In the last step there is a match We use the X-drop algorithm [8] to stop the extension. At each position, we record the running alignment score. The algorithm stops at the end of a sequence or when the current score is smaller than the best score found so far by more than the most penalized gapped alignment operation (three-base gap in Table 1). The algorithm then backtracks to the position with the best score. Because most reads align to a reference with few or no mismatches, this method is faster and more memory efficient than the dynamic programming-based extension procedure used in other BLAST programs. Moreover, this approach facilitates collection of traceback information at little additional cost. This method can be tuned to a given sequencing technology for an expected rate of mismatches or gaps simply by adapting Table 1. For example, in Roche 454 or PacBio, where insertions and deletions are more frequent than substitutions, one could switch to a modified table. We compute an alignment score using the following system: 1 for each matching pair of bases, − 4 for a base substitution, zero for gap opening (either a read or reference sequence), and − 4 for each base of gap extension (insertion or deletion). A user can modify the mismatch and gap penalties. The quality coefficients present in the FASTQ file have no impact on the alignment score and are not exported in the SAM output. About half the time, a matching base can be placed on either side of a gap, so the gap can slide at equal score. To avoid difficulties in SNP detection, Magic-BLAST by convention shifts the sliding bases upstream of the gap, in the orientation of the target. Spliced alignments Spliced alignments are found by combining collinear local alignments on a read and a reference sequence. Magic-BLAST constructs a chain of local alignments that maximizes the combined alignment score. It then updates the alignment extents so that the spliced alignment is continuous on the read and the intron donor and acceptor sites are, whenever possible, consistent with the canonical splice signals. If two local alignments are continuous on a read (possibly with an overlap), then we first search for the canonical splice site (GT-AG or CT-AC) where the alignments meet. If this site is not found and each alignment has a score of at least 50, we search successively for the minor splice sites or their complement: GC-AG or CT-GC, AT-AC or GT-AT, then for any other non-canonical site. The first site found is accepted. The alignment score threshold of 50 was chosen because minor and non-canonical splice sites are rare, but pairs of di-nucleotides are frequently found in the genome. As a result, for reads shorter than 100 bases, Magic-BLAST conservatively only calls GT-AG introns. Magic-BLAST also attempts to produce spliced alignments if a read has several local alignments separated by one to ten unaligned bases. First, we look for a splice signal within four bases of the end of the left alignment and, if found, we fix the beginning of the candidate intron. Second, we search for the corresponding end of intron signal at offsets that ensure a continuous alignment on the read, allowing at most one additional insertion or deletion. If this fails, the procedure is repeated with the end of the intron fixed and a search for the signal indicating the start of the intron. When the candidate splice signals are found, the alignments are trimmed or extended to the splice signals. The benefit of this method is that it correctly identifies introns even in the presence of a substitution, insertion, or deletion close to the intron boundaries. Because this procedure is very sensitive and can produce many spurious alignments, Magic-BLAST only allows the GT-AG signal in this situation. The spliced alignment is scored with the same scoring system as the local alignment. There is no reward or penalty for splice sites and no preference is given to continuous versus spliced alignments. When mapping RNA to the genome, Magic-BLAST does not support the use of an annotation file or a two-pass method. If desired, one can map both on the genome and on an annotated transcriptome, then use the universal scoring system of Magic-BLAST to select the best alignment, be it genomic or transcriptomic, for each fragment. In this paper, we mapped only to the genome. Output Magic-BLAST returns results in the Sequence Alignment/Map SAM/BAM format [19] or in a tab-delimited format similar to the tabular format in other BLAST programs, which is less standard but richer and easier to mine.",Mapping,"magicblast  accurate rnaseq aligner  long  short reads
algorithm overview  magicblast algorithm   structure similar     blast program   read  data  batch  build  “lookup table”    index  word locations   read base  default   scan  database sequence usually  reference genome  match   lookup table  attempt  extend select initial match   length specify   user   default  result match form  seed  computation  local gap alignments collinear local alignments  combine  splice alignments  order   use   seed  original  base match must  completely contain within one exon  cannot span two exons consequently exons shorter   seed length cannot  capture    rare less    refseq exons     recognize  align  parallel   know transcriptome  pair read  best alignments  select base   alignment quality   pair  example  one read   pair map equally well  two genomic sit   second read map best   single site  read pair   report  map uniquely   position dictate   second read   way  specificity   map truly reflect  number  base sequence   whole fragment   base specificity     pairedend read   present  detail description    step figure  present  overview   step repeat filter  genomes contain intersperse repeat  gene families  complicate correct placement  read   genome  avoid seed  ambiguous position magicblast scan  reference sequence  count base word  word  appear   reference database    userspecified number  time  default    index   lookup table    never form  seed alignment  make  procedure  efficient  word present   read  count  cutoff number   select experimentally   best tradeoff  sensitivity  runtime  rnaseq additionally magicblast specifically mask  base word  contain  least      effectively avoid seed  polya tail  approach  similar  soft mask   blast program local gap alignment magicblast compute  local alignment  extend exact word match base  default   read   reference sequence  use  simplify greedy alignment extension procedure previously use  magic  start   seed  alignment  extend   first mismatch next  attempt  characterize  mismatch   substitution insertion  deletion  one  three base  recursively test  quality   alignment   follow  base     apply successively  table  candidate alignment operations table    associate requirement  meet  requirement    specific number  base must match within  give number  base follow  apply operation  first operation whose requirement  meet  apply   alignment   algorithm proceed   next position   sequence  single substitution  report   requirement  satisfy  list  alignment operations   associate condition use  magicblast figure  show  example alignment extension first   two match   algorithm move   right  two position   sequence   mismatch   encounter  algorithm try successively  alignment operation  check  requirements  first operation  substitution  require nine match base follow  mismatch fail  second operation  insertion  require ten consecutive match succeed   apply   alignment   last step    match  use  xdrop algorithm   stop  extension   position  record  run alignment score  algorithm stop   end   sequence    current score  smaller   best score find  far      penalize gap alignment operation threebase gap  table   algorithm  backtrack   position   best score   read align   reference     mismatch  method  faster   memory efficient   dynamic programmingbased extension procedure use   blast program moreover  approach facilitate collection  traceback information  little additional cost  method   tune   give sequence technology   expect rate  mismatch  gap simply  adapt table   example  roche   pacbio  insertions  deletions   frequent  substitutions one could switch   modify table  compute  alignment score use  follow system    match pair  base     base substitution zero  gap open either  read  reference sequence      base  gap extension insertion  deletion  user  modify  mismatch  gap penalties  quality coefficients present   fastq file   impact   alignment score    export   sam output  half  time  match base   place  either side   gap   gap  slide  equal score  avoid difficulties  snp detection magicblast  convention shift  slide base upstream   gap   orientation   target splice alignments splice alignments  find  combine collinear local alignments   read   reference sequence magicblast construct  chain  local alignments  maximize  combine alignment score   update  alignment extents    splice alignment  continuous   read   intron donor  acceptor sit  whenever possible consistent   canonical splice signal  two local alignments  continuous   read possibly   overlap   first search   canonical splice site gtag  ctac   alignments meet   site   find   alignment   score   least   search successively   minor splice sit   complement gcag  ctgc atac  gtat     noncanonical site  first site find  accept  alignment score threshold    choose  minor  noncanonical splice sit  rare  pair  dinucleotides  frequently find   genome   result  read shorter   base magicblast conservatively  call gtag introns magicblast also attempt  produce splice alignments   read  several local alignments separate  one  ten unaligned base first  look   splice signal within four base   end   leave alignment   find  fix  begin   candidate intron second  search   correspond end  intron signal  offset  ensure  continuous alignment   read allow   one additional insertion  deletion   fail  procedure  repeat   end   intron fix   search   signal indicate  start   intron   candidate splice signal  find  alignments  trim  extend   splice signal  benefit   method    correctly identify introns even   presence   substitution insertion  deletion close   intron boundaries   procedure   sensitive   produce many spurious alignments magicblast  allow  gtag signal   situation  splice alignment  score    score system   local alignment    reward  penalty  splice sit   preference  give  continuous versus splice alignments  map rna   genome magicblast   support  use   annotation file   twopass method  desire one  map    genome    annotate transcriptome  use  universal score system  magicblast  select  best alignment   genomic  transcriptomic   fragment   paper  map    genome output magicblast return result   sequence alignmentmap sambam format     tabdelimited format similar   tabular format   blast program   less standard  richer  easier  mine",4
87,MGmapper,"MGmapper: Reference based mapping and taxonomy annotation of metagenomics sequence reads
The MGmapper package consists of a pipeline of scripts to process FASTQ files as either single or paired-end reads to perform sequence mapping and taxonomy annotation against user defined reference sequence databases. MGmapper utilizes a number of publicly available programs: Cutadapt [17] for trimming and adaptor removal, BWA-mem [15] and SAMtools [18] to produce and process the reference based sequence alignments to one of many reference sequence databases. A short summary of the procedure is described below for paired-end sequence data, followed by more details outlined in the section “Fastq mapping procedure”. Initially, a filtering step checks for properly paired reads, followed by trimming and adaptor removal. The biological relevant reads are obtained by always mapping to a PhiX bacteria phage and continuing with the subset of reads that do not align to the PhiX genome (commonly used as a control in Illumina sequencing runs). Next, sequence reads are mapped to user defined reference sequences and only properly paired reads are accepted, provided that both reads pass a lower alignment score threshold and relative alignment length. After mapping reads to all reference sequence databases (eg human, bacteria, fungi etc.), some reads may align to reference sequences in different databases and depending on the mapping mode (bestmode or fullmode explained further down) the best hit is identified and used to assign taxonomy. Taxonomy annotations (ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz) are added via lookup in a pre-made Kyoto Cabinet database (http://fallabs.com/kyotocabinet/) containing key, value pairs in form of the reference sequence name (the key) and full taxonomy path from strain to superfamily clades (the value). Finally, a post-processing step (section “post-processing”) identifies confident assignments at strain, species, genus or any user defined taxonomy clade up to superfamily. MGmapper can map sequence reads against any nucleotide sequence database i.e. both genomic and gene sequence databases and for each database the mapping can be performed in either bestmode or fullmode. In bestmode, reads are assigned to only one reference sequence if it is the best hit that is observed when mapping to all databases specified for bestmode mapping. Best hit is identified based on the highest alignment score. In fullmode reads are assigned to a reference sequence even if a better hit is seen when mapping to another database. Typically the fullmode is used to search for sequences (e.g. a gene database), that may be a subset of another database (e.g. a full genome database). Analyzing a sample for both genomic bacterial composition and anti-microbial resistance genes is a situation where MGmapper should be run with the bacterial database specified for bestmode mapping and at the same time specifying the anti-microbial resistance gene database for fullmode mapping. The reason is that the resistance genes are or may be a subset of the bacterial genomic sequences and we want to assign a sequence read both a bacteria genome and also a resistance gene. If both databases were specified for bestmode mapping (bacteria, anti-microbial genes), then a read can only be assigned to one of the databases and if identical alignment scores are observed, then priority is to the database that was specified first. Fastq read mapping procedure The MGmapper pipeline analysis is done in four main steps: I. Pre-processing of raw reads to remove potential positive control reads, II. Mapping of reads to one or more reference sequence databases and filtration of alignment hits, III. Identification the best hits, and IV. Post-processing of taxonomy annotations and preparation of excel and text files with insert-size distribution, size normalized abundances, read and nucleotide count statistics, depth, and coverage. A schematic flowchart of the paired-end mapping processing is shown Pre-processing of raw reads. An optional trimming and filtering of raw reads is performed by use of the Cutadapt [17] program. Users can skip this step if reads are already trimmed. Default setting is that reads are initially trimmed before searching for adaptor sequences (equivalent to the Cutadapt option—q). In addition, a read is discarded unless a minimum of 30 nucleotides remains after trimming. Trimmed reads are next paired up and singleton reads are removed when using the paired-end version of MGmapper. To this follows another cleaning process where reads from potential PhiX Control v3 adapter-ligated libraries are removed via BWA-mem [15] and SAMtools [18], as they may originate from a control for Illumina sequencing runs (http://www.illumina.com/products/phix_control_v3.html). The outcome is a cleaned set of reads that are believed to originate from the biological sample of interest. The number of reads in this set (noPhiX dataset) is set to 100% and used for calculation of R_abundance, a read count abundance measure. II. Mapping of reads to reference sequence databases and alignment based filtering. FASTQ reads are first extracted from the noPhiX set and mapped to one or several reference databases via ‘bwa mem—t procs—M database’ marking shorter splits as secondary hits, which are then removed when piping to ‘samtools view -F 256 -Sb -f2’ in paired-end mode or ‘samtools view -F 260 –Sb’ in single-end mode i.e. keeping properly paired reads or mapped reads, respectively. Next, reads with insufficient alignment qualities are removed based on user-defined minimum alignment score (MAS) and minimum fraction of nucleotides being assigned with an ‘M’ state in the CIGAR format, where an ‘M’ indicates a match or mismatch. The user-defined threshold for fraction of matches+mismatches (FMM) is in relation to the full length of a read. In paired-end mapping both reads are removed if just one of them does not fulfill the filtering criteria. Default settings in the MGmapper programs are MAS = 30 and FMM = 0.8. At this step properly paired read may align to more than one reference sequences, located in different reference sequence databases. In bestmode a read pair can only be assigned to one reference sequence (section “Identification of the best hit”). III. Identification of the best hit. Having paired-end sequences, both the forward and the reverse fastq reads are aligned to a reference sequence, each with an associated alignment score. The sum of alignment scores (SAS) is used as a measure to identify the best hit for a read-pair. Typically, all input query reads are mapped to multiple reference sequence databases e.g. bacteria, virus, fungi, human and others. Thus a read-pair may map to multiple reference sequences from different databases and in bestmode the taxonomy annotation is only assigned to one best hit, namely the one with the highest SAS score. For single-end reads mapped to several databases, the best hit is the one with the highest alignment score. In cases where a read or read-pair achieves identical alignment scores to reference sequences from different databases, the priority is given to the order by which the databases are specified by the user, and thus a read or read-pair can still be associated to one single reference sequence. IV. Output and post-processing of results. The fastq reads are mapped to multiple user-defined reference sequence databases. A tab-separated file is produced for each database including reference sequence hits with read count statistics provided at strain level. A strain is named according to the header name originating from the fasta file that was used to make the database. The tab-separated file is composed of 14+16 columns of read count statistics and annotations, where the latter are taxid and taxonomy clade name for 8 clades, i.e. strain, species, genus, family, order, class, order and superfamily. The tab-separated files contains the unprocessed results as obtained by the BWA-mem [15] mapping and Samtools mpileup [18]. As false positive annotations are likely to be present, a subset of confident mapping results is obtained at a specified clade level (strain, species …superfamily) via a post-processing procedure described in the section below. Post-processing. A combination of four criteria (I-IV) is used to identify a positive taxonomy annotation. Identifiers highlighted in italics are also described in Table 2. I. Minimum ReadCount of 10 II. Mismatch ratio < 0.01, defined as Mismatches/Nucloetides. III. S_Abundance, the size normalized abundance > 0.01. IV. Unique read count fraction > 0.5%, defined as ReadCount uniq/ReadCount. At strain level all four criteria are imposed. At species level, the criteria IV is used in a pre-cycle, to identify the lowest S_Abundance for the selected species. The new S_Abundance threshold is used in a second round where criteria IV are omitted. At genus level or higher only criteria I, II and III are used. Taxid values are used to identify strains belonging to the same species or species belonging to the same genus etc. All identifiers as shown in Table 2, are summed at clade levels higher than strain i.e. the S_Abundance value for a species is the sum of all strain S_Abundance values. It is likewise for R_Abundance, Size, Seq_Count, Nucleotides, Covered positions, Coverage Depth, ReadCount, ReadCount uniq and Mismatches.",Mapping,"mgmapper reference base map  taxonomy annotation  metagenomics sequence reads
 mgmapper package consist   pipeline  script  process fastq file  either single  pairedend read  perform sequence map  taxonomy annotation  user define reference sequence databases mgmapper utilize  number  publicly available program cutadapt   trim  adaptor removal bwamem   samtools   produce  process  reference base sequence alignments  one  many reference sequence databases  short summary   procedure  describe   pairedend sequence data follow   detail outline   section “fastq map procedure” initially  filter step check  properly pair read follow  trim  adaptor removal  biological relevant read  obtain  always map   phix bacteria phage  continue   subset  read    align   phix genome commonly use   control  illumina sequence run next sequence read  map  user define reference sequence   properly pair read  accept provide   read pass  lower alignment score threshold  relative alignment length  map read   reference sequence databases  human bacteria fungi etc  read may align  reference sequence  different databases  depend   map mode bestmode  fullmode explain    best hit  identify  use  assign taxonomy taxonomy annotations ftpftpncbinihgovpubtaxonomytaxdumptargz  add via lookup   premade kyoto cabinet database  contain key value pair  form   reference sequence name  key  full taxonomy path  strain  superfamily clades  value finally  postprocessing step section “postprocessing” identify confident assignments  strain species genus   user define taxonomy clade   superfamily mgmapper  map sequence read   nucleotide sequence database   genomic  gene sequence databases    database  map   perform  either bestmode  fullmode  bestmode read  assign   one reference sequence     best hit   observe  map   databases specify  bestmode map best hit  identify base   highest alignment score  fullmode read  assign   reference sequence even   better hit  see  map  another database typically  fullmode  use  search  sequence   gene database  may   subset  another database   full genome database analyze  sample   genomic bacterial composition  antimicrobial resistance genes   situation  mgmapper   run   bacterial database specify  bestmode map     time specify  antimicrobial resistance gene database  fullmode map  reason    resistance genes   may   subset   bacterial genomic sequence   want  assign  sequence read   bacteria genome  also  resistance gene   databases  specify  bestmode map bacteria antimicrobial genes   read    assign  one   databases   identical alignment score  observe  priority    database   specify first fastq read map procedure  mgmapper pipeline analysis    four main step  preprocessing  raw read  remove potential positive control read  map  read  one   reference sequence databases  filtration  alignment hit iii identification  best hit   postprocessing  taxonomy annotations  preparation  excel  text file  insertsize distribution size normalize abundances read  nucleotide count statistics depth  coverage  schematic flowchart   pairedend map process  show preprocessing  raw read  optional trim  filter  raw read  perform  use   cutadapt  program users  skip  step  read  already trim default set   read  initially trim  search  adaptor sequence equivalent   cutadapt option—  addition  read  discard unless  minimum   nucleotides remain  trim trim read  next pair   singleton read  remove  use  pairedend version  mgmapper   follow another clean process  read  potential phix control  adapterligated libraries  remove via bwamem   samtools    may originate   control  illumina sequence run   outcome   clean set  read   believe  originate   biological sample  interest  number  read   set nophix dataset  set    use  calculation  r_abundance  read count abundance measure  map  read  reference sequence databases  alignment base filter fastq read  first extract   nophix set  map  one  several reference databases via bwa mem— procs— database mark shorter split  secondary hit    remove  pip  samtools view      pairedend mode  samtools view     singleend mode  keep properly pair read  map read respectively next read  insufficient alignment qualities  remove base  userdefined minimum alignment score mas  minimum fraction  nucleotides  assign    state   cigar format    indicate  match  mismatch  userdefined threshold  fraction  matchesmismatches fmm   relation   full length   read  pairedend map  read  remove   one     fulfill  filter criteria default settings   mgmapper program  mas    fmm     step properly pair read may align    one reference sequence locate  different reference sequence databases  bestmode  read pair    assign  one reference sequence section “identification   best hit” iii identification   best hit  pairedend sequence   forward   reverse fastq read  align   reference sequence    associate alignment score  sum  alignment score sas  use   measure  identify  best hit   readpair typically  input query read  map  multiple reference sequence databases  bacteria virus fungi human  others thus  readpair may map  multiple reference sequence  different databases   bestmode  taxonomy annotation   assign  one best hit namely  one   highest sas score  singleend read map  several databases  best hit   one   highest alignment score  case   read  readpair achieve identical alignment score  reference sequence  different databases  priority  give   order    databases  specify   user  thus  read  readpair  still  associate  one single reference sequence  output  postprocessing  result  fastq read  map  multiple userdefined reference sequence databases  tabseparated file  produce   database include reference sequence hit  read count statistics provide  strain level  strain  name accord   header name originate   fasta file   use  make  database  tabseparated file  compose   columns  read count statistics  annotations   latter  taxid  taxonomy clade name   clades  strain species genus family order class order  superfamily  tabseparated file contain  unprocessed result  obtain   bwamem  map  samtools mpileup   false positive annotations  likely   present  subset  confident map result  obtain   specify clade level strain species …superfamily via  postprocessing procedure describe   section  postprocessing  combination  four criteria iiv  use  identify  positive taxonomy annotation identifiers highlight  italics  also describe  table   minimum readcount    mismatch ratio   define  mismatchesnucloetides iii s_abundance  size normalize abundance    unique read count fraction   define  readcount uniqreadcount  strain level  four criteria  impose  species level  criteria   use   precycle  identify  lowest s_abundance   select species  new s_abundance threshold  use   second round  criteria   omit  genus level  higher  criteria    iii  use taxid value  use  identify strain belong    species  species belong    genus etc  identifiers  show  table   sum  clade level higher  strain   s_abundance value   species   sum   strain s_abundance value   likewise  r_abundance size seq_count nucleotides cover position coverage depth readcount readcount uniq  mismatch",4
88,BWA,"Tandem repeats analysis in DNA sequences based on improved Burrows-Wheeler transform
The BWT is a reversible block-sorting transform that operates on a sequence of n data symbols to produce a permuted data sequence of the same symbols and a single integer in{1,....,n} . If we let (1) denote the n-dimensional BWT function and (2) denote the inverse of BWT . Since the sequence length isevident from the source argument, the functional transcript istypically dropped, giving (3) and (4) The notations BWTx and BWTN denote the character andinteger portions of the BWT, respectively. The forward BWTproceeds by forming all n cyclic shifts of the original datastring and sorting those cyclic shifts lexicographically. TheBWT output has two parts. The BWT of the sequence “googol.” The original data sequence(in bold) appears in row 1 of the ordered figure(Step 2); the final column of that table contains the sequence“lo$oogg.” Hence BWT(googol$) = (lo$oogg; 3). Constructingsuffix array and BWT string for X = googol$. String X iscirculated to generate seven strings, which are then lexicographicallysorted. After sorting, the positions of the firstsymbols form the suffix array (7, 2, 1, 6, 3, 5, 2) and theconcatenation. Analogy of the BWT process. If we consider table constructionfor i = 1; :::n - 1 then : Step (3i - 2) : Place column n in front of columns 1,...i-1. Step (3i - 1) : Odder the resulting length –i string lexicographically. Step 3i : Place the ordered list in the first i column of the table If we consider string W as a substring of X, the position of each occurrence of W in X will occur in an interval in the suffix array. This is because all the suffixes that have Was prefix are sorted together In this experiment we achieved, our first objective i.e. to increases the search speed and accuracy of tandem repeats. We discovered that the BWT requires only linear complexity ,making the algorithm O(n) in space and time complexity.(Throughout this work, space and time complexity appear as a single result since most algorithms allow easy trade-offs between the two). In contrast with the original BWA algorithm, the extension code is universal however; the resulting code appears to be more expensive in space and time complexity. In particular, allowing m to grow with n as tandem repeat size|x| = O(nlognlogn) a value similar in size to the sequence length itself hence increase search accuracy. Applying Prefix trie algorithm on the new larger tandem repeats results in worst case O(|x|mn) = O(n2lognlogn) space and time complexity therefore this results into significantly reduced complexicty. On the other hand, we also manage to realize our second objective using the LZ77 algorithms optimize BWT by replacing the repeated occurrences of genome data string with references to a single copy of that data string existing earlier in the uncompressed data stream. A pair of numbers called a length-distance pair encodes a match. To spot matches, the encoder keeps track of some amount of the most recent data, such as the last 2 kB, 4 kB, or 32kB. The data kept in a stricture called a sliding window. In LZ77 algorithm the encoder, keep the data to look for matches, and the decoder keep data to interpret the matches the encoder referred. From the experiment the larger the sliding window the longer back the encoder may search for creating references. We discovered it is not only acceptable but also useful to BWA by allowing length-distance pairs to specify a length that actually exceeds the distance hence BWA is able to align longer tandem repeats. Encoding for the search pointer to continue finding matched pairs past the end of the search window. Therefore, all nucleotide characters from the first match at offset D and forward to the end of the search window must have matched input, and these are the(previously seen) nucleotide characters that comprise a single run unit of length LR, which must equal D. The prefix trie, test whether a query W( quarry sequence) is an exact substring of X (wild sequence) is equivalent by finding the node that represents W, which can be done in O(|W|) time by matching each symbol in W to an edge, starting from the root. To allow mismatches, we exhaustively traverse the trie and match W to each possible path consequently, significant improvement in time complexity O(nlogn) and the overall search performance of our Improved-BWA hence quick search of tandem repeats sequences in a genome.",Mapping,"tandem repeat analysis  dna sequence base  improve burrowswheeler transform
 bwt   reversible blocksorting transform  operate   sequence   data symbols  produce  permute data sequence    symbols   single integer {}    let  denote  ndimensional bwt function   denote  inverse  bwt  since  sequence length isevident   source argument  functional transcript istypically drop give     notations bwtx  bwtn denote  character andinteger portion   bwt respectively  forward bwtproceeds  form   cyclic shift   original datastring  sort  cyclic shift lexicographically thebwt output  two part  bwt   sequence “googol”  original data sequencein bold appear  row    order figurestep   final column   table contain  sequence“looogg” hence bwtgoogol  looogg  constructingsuffix array  bwt string    googol string  iscirculated  generate seven string    lexicographicallysorted  sort  position   firstsymbols form  suffix array         theconcatenation analogy   bwt process   consider table constructionfor         step     place column   front  columns  step     odder  result length  string lexicographically step   place  order list   first  column   table   consider string    substring    position   occurrence      occur   interval   suffix array      suffix    prefix  sort together   experiment  achieve  first objective   increase  search speed  accuracy  tandem repeat  discover   bwt require  linear complexity make  algorithm   space  time complexitythroughout  work space  time complexity appear   single result since  algorithms allow easy tradeoffs   two  contrast   original bwa algorithm  extension code  universal however  result code appear    expensive  space  time complexity  particular allow   grow    tandem repeat sizex  onlognlogn  value similar  size   sequence length  hence increase search accuracy apply prefix trie algorithm   new larger tandem repeat result  worst case oxmn  on2lognlogn space  time complexity therefore  result  significantly reduce complexicty    hand  also manage  realize  second objective use  lz77 algorithms optimize bwt  replace  repeat occurrences  genome data string  reference   single copy   data string exist earlier   uncompress data stream  pair  number call  lengthdistance pair encode  match  spot match  encoder keep track   amount    recent data    last      32kb  data keep   stricture call  slide window  lz77 algorithm  encoder keep  data  look  match   decoder keep data  interpret  match  encoder refer   experiment  larger  slide window  longer back  encoder may search  create reference  discover     acceptable  also useful  bwa  allow lengthdistance pair  specify  length  actually exceed  distance hence bwa  able  align longer tandem repeat encode   search pointer  continue find match pair past  end   search window therefore  nucleotide character   first match  offset   forward   end   search window must  match input    thepreviously see nucleotide character  comprise  single run unit  length   must equal   prefix trie test whether  query  quarry sequence   exact substring   wild sequence  equivalent  find  node  represent        time  match  symbol     edge start   root  allow mismatch  exhaustively traverse  trie  match    possible path consequently significant improvement  time complexity onlogn   overall search performance   improvedbwa hence quick search  tandem repeat sequence   genome",4
89,SOAP2,"SOAP2: an improved ultrafast tool for short read alignment
Next-generation DNA sequencing technologies, including Illumina/ Solexa and AB/SOLiD, have been dominant tools for genomic data collection. Various applications have been developed using these technologies to promote biological research, such as detecting genetic variation through whole genome or target region resequencing, refining gene annotation by whole transcriptome sequencing, profiling mRNA and miRNA expression and studying DNA methylation. One of the common key data analysis steps of these applications is to align huge amounts of short reads onto a reference genome. New efficient programs have been developed to meet the challenges for such alignment. Among them, SOAP (Short Oligonucleotide Alignment Program; Li et al., 2008) has been used widely for these types of analyses due to its fast speed and richness of features. With further improvement on sequencing throughput and the launch of big research projects, much faster short-read alignment methods are required to handle the data analysis of such large-scale sequence production. For example, the 1000 Genomes project that aims to create the most detailed and medically useful human genetic variation map, will generate about 15 Tb of sequence using nextgeneration sequencing technologies. With even the fastest programs currently available, one would need ∼1000 CPU months to align these short reads onto the human reference genome. Additionally, new methods are now needed to support longer reads as the existing methods were primarily designed for very short reads with typical lengths shorter than 50 bp. With improvements in sequencing chemistry and data processing algorithms, the Illumina Genome Analyzer can now generate up to 75–100 bp high-quality reads, and longer reads are expected in the near future. Here, we have developed an improved version of SOAP, called SOAP2. The new program uses the Burrows Wheeler Transformation (BWT) compressed index instead of the seed algorithm that was used in the previous version for indexing the reference sequence in the main memory. Use of BWT substantially improved alignment speed; additionally, it significantly reduced memory usage. Big eukaryotic genomes always consist of a large number of repetitive sequences (e.g. 45% of the human genome). Suffix trees and suffix arrays are considered the most appropriate methods for indexing DNA sequence, through which only one alignment is needed to for repetitive sequence with multiple identical copies in the genome. The complexity in space and time of such index construction has limited such algorithm usage in only small genomes. But the recent development of compressed indexing has reduced the space complexity from O(n) bytes to O(n) bits. Among these is the BWT (Burrow and Wheeler, 1994), a reversible data compression algorithm, which was found to be the most efficient. The space complexity of BWT is n/4 bytes, and only 1 GB memory in RAM is required for indexing the whole human genome. This algorithm has been used for efficient whole-genome comparison and indexing for Smith–Waterman local alignment to the human genome (Lam et al., 2008). Using this in our alignment method, we determined an exact match, by constructing a hash table to accelerate searching for the location of a read in the BWT reference index. For example, if we use a 13mer on the hash, then the reference index would be partitioned into 226 blocks, and very few search interactions are sufficient to identify the exact location inside the block. For inexact (both mismatch and indel) alignment, we applied a ‘split-read strategy’. To allow one mismatch, a read was split into two fragments. The mismatch can exist in, at most, one of the two fragments at the same time. Likewise, we split a read into three fragments to search for hits that allow two mismatches. This enumeration algorithm was used to identify mutation sites on the reads. In paired-end alignment mode, we first independently aligned the two reads belonging to a pair then searched for the pair of hits with the correct orientation relationship and proper distance. Similar to SOAP, we preferentially select the best hit of each read or read pair, which have the lowest number of mismatches or small gaps. In general practice, a user can also choose the option to report all hits that satisfy their selected preset similarity rate. For most analyses, to guarantee alignment accuracy, we recommend allowing at most two mismatches or one continuous gap in the high-quality part of a read. For the low-quality regions of a read (the 3 -end, which can contain a higher rate of sequencing errors), we provided an option that allows more mismatches within this defined 3 -end region. Since sequencing read length is getting longer and longer, the SOAP2 program is now compatible with read lengths up to 1024 bp. We evaluated the performance of this software on a dataset containing one million read pairs generated from a human Asian individual (Wang et al., 2008). Although SOAP2 was designed for improved Illumina GA sequencing with read length over 50 bp, we chose a 44 bp read length in this evaluation, which is compatible with the tools SOAP (Li,R. et al., 2008), MAQ (Li,H. et al., 2008) and the recently developed BWT-based alignment tool Bowtie (Langmead et al., 2009). SOAP2 takes 7200 s to build a BWT index for the human reference genome, which is 12 times slower than building the seed index that was implemented in SOAP. Thus, we prebuild the index on a hard disk, and then load it into RAM directly when starting a new alignment job for that genome. The memory usage was reduced from 14.7 GB in SOAP to 5.4 GB in SOAP2. SOAP2 was more than 20 times faster than SOAP and MAQ with similar amount of reads aligned (Table 1). SOAP2 and Bowtie have comparable speed on aligning single-end reads, while Bowtie cannot always find the best alignment hits and cannot align paired-end reads (Langmead et al., 2009). It should be made aware that the alignment sensitivity was determined by sequencing quality and the parameter setting of each alignment tool, so the percent of reads aligned would vary in different datasets. SOAP2 supports multiple input and output file formats. The reference sequence can be loaded as either a text or a gzipped FASTA format, and the query reads can be in either FASTA or FASTQ format. The output formats include a SOAP tab-delimited text table, a gzipped text table, a Sequence Alignment/Map (SAM) format and its binary equivalent (BAM) that is recommended by the 1000 Genomes Consortium, and a Consed format that fits with the assembly viewer. As SOAP2 is specifically designed for ultrafast alignment of short reads onto a reference sequence for large-scale resequencing projects, we have developed a companion assembler for consensus assembly of the sequenced individual based on the alignment of the reads on the reference sequence. The assembler has been included in the SOAP software package and is also freely available from the website. With this, we can detect SNPs by comparing the assembled sequence to the reference genome. The assembler uses Bayes’ theorem to infer the genotype of each base pair from the aligned reads and sequencing quality scores. The estimated SNP rate between the sequenced individual and the reference genome is used as prior probability, the raw sequencing qualities were recalibrated according to the alignment, the reads generated from potential duplicate clones were removed, and finally the genotype was called from the posterior probabilities with a Phred-like score transformed from the probability to indicate its accuracy",Mapping,"soap2  improve ultrafast tool  short read alignment
nextgeneration dna sequence technologies include illumina solexa  absolid   dominant tool  genomic data collection various applications   develop use  technologies  promote biological research   detect genetic variation  whole genome  target region resequencing refine gene annotation  whole transcriptome sequence profile mrna  mirna expression  study dna methylation one   common key data analysis step   applications   align huge amount  short read onto  reference genome new efficient program   develop  meet  challenge   alignment among  soap short oligonucleotide alignment program       use widely   type  analyse due   fast speed  richness  feature   improvement  sequence throughput   launch  big research project much faster shortread alignment methods  require  handle  data analysis   largescale sequence production  example   genomes project  aim  create   detail  medically useful human genetic variation map  generate     sequence use nextgeneration sequence technologies  even  fastest program currently available one would need  cpu months  align  short read onto  human reference genome additionally new methods   need  support longer read   exist methods  primarily design   short read  typical lengths shorter     improvements  sequence chemistry  data process algorithms  illumina genome analyzer   generate     highquality read  longer read  expect   near future    develop  improve version  soap call soap2  new program use  burrow wheeler transformation bwt compress index instead   seed algorithm   use   previous version  index  reference sequence   main memory use  bwt substantially improve alignment speed additionally  significantly reduce memory usage big eukaryotic genomes always consist   large number  repetitive sequence     human genome suffix tree  suffix array  consider   appropriate methods  index dna sequence    one alignment  need   repetitive sequence  multiple identical copy   genome  complexity  space  time   index construction  limit  algorithm usage   small genomes   recent development  compress index  reduce  space complexity   bytes   bits among    bwt burrow  wheeler   reversible data compression algorithm   find     efficient  space complexity  bwt   bytes     memory  ram  require  index  whole human genome  algorithm   use  efficient wholegenome comparison  index  smithwaterman local alignment   human genome lam    use    alignment method  determine  exact match  construct  hash table  accelerate search   location   read   bwt reference index  example   use  13mer   hash   reference index would  partition   block    search interactions  sufficient  identify  exact location inside  block  inexact  mismatch  indel alignment  apply  splitread strategy  allow one mismatch  read  split  two fragment  mismatch  exist    one   two fragment    time likewise  split  read  three fragment  search  hit  allow two mismatch  enumeration algorithm  use  identify mutation sit   read  pairedend alignment mode  first independently align  two read belong   pair  search   pair  hit   correct orientation relationship  proper distance similar  soap  preferentially select  best hit   read  read pair    lowest number  mismatch  small gap  general practice  user  also choose  option  report  hit  satisfy  select preset similarity rate   analyse  guarantee alignment accuracy  recommend allow   two mismatch  one continuous gap   highquality part   read   lowquality regions   read   end   contain  higher rate  sequence errors  provide  option  allow  mismatch within  define  end region since sequence read length  get longer  longer  soap2 program   compatible  read lengths      evaluate  performance   software   dataset contain one million read pair generate   human asian individual wang    although soap2  design  improve illumina  sequence  read length     choose    read length   evaluation   compatible   tool soap lir    maq lih      recently develop bwtbased alignment tool bowtie langmead    soap2 take    build  bwt index   human reference genome    time slower  build  seed index   implement  soap thus  prebuild  index   hard disk   load   ram directly  start  new alignment job   genome  memory usage  reduce     soap     soap2 soap2     time faster  soap  maq  similar amount  read align table  soap2  bowtie  comparable speed  align singleend read  bowtie cannot always find  best alignment hit  cannot align pairedend read langmead       make aware   alignment sensitivity  determine  sequence quality   parameter set   alignment tool   percent  read align would vary  different datasets soap2 support multiple input  output file format  reference sequence   load  either  text   gzipped fasta format   query read    either fasta  fastq format  output format include  soap tabdelimited text table  gzipped text table  sequence alignmentmap sam format   binary equivalent bam   recommend    genomes consortium   con format  fit   assembly viewer  soap2  specifically design  ultrafast alignment  short read onto  reference sequence  largescale resequencing project   develop  companion assembler  consensus assembly   sequence individual base   alignment   read   reference sequence  assembler   include   soap software package   also freely available   website     detect snps  compare  assemble sequence   reference genome  assembler use bay theorem  infer  genotype   base pair   align read  sequence quality score  estimate snp rate   sequence individual   reference genome  use  prior probability  raw sequence qualities  recalibrated accord   alignment  read generate  potential duplicate clone  remove  finally  genotype  call   posterior probabilities   phredlike score transform   probability  indicate  accuracy",4
90,MAQ,"Mapping short DNA sequencing reads and calling variants using mapping quality scores
Single end read mapping To map reads efficiently, MAQ first indexes read sequences and scans the reference genome sequence to identify hits that are extended and scored. With the Eland-like (A.J. Cox, unpubl.) hashing technique, MAQ, by default, guarantees to find alignments with up to two mismatches in the first 28 bp of the reads. MAQ maps a read to a position that minimizes the sum of quality values of mismatched bases. If there are multiple equally best positions, then one of them is chosen at random. In this article, we will call a potential read alignment position a hit. The algorithm MAQ uses to find the best hit is quite similar to the one used in Eland. It builds multiple hash tables to index the reads and scans the reference sequence against the hash tables to find the hits. By default, six hash tables are used, ensuring that a sequence with two mismatches or fewer will be hit. The six hash tables correspond to six noncontiguous seed templates (Buhler 2001; Ma et al. 2002). Given 8-bp reads, for example, the six templates are 11110000, 00001111, 11000011, 00111100, 11001100, and 00110011, where nucleotides at 1 will be indexed while those at 0 are not. By default, MAQ indexes the first 28 bp of the reads, which are typically the most accurate part of the read. In alignment, MAQ loads all reads into memory and then applies the first template as follows. For each read, MAQ takes the nucleotides at the 1 positions of the template, hashes them into a 24-bit integer, and puts the integer together with the read identifier into a list. When all the reads are processed, MAQ orders the list based on the 24-bit integers, such that reads with the same hashing integer are grouped together in memory. Each integer and its corresponding region are then recorded in a hash table with the integer as the key. We call this process indexing. At the same time that MAQ indexes the reads with the first template, it also indexes the reads with the second template that is complementary to the first one. Taking two templates at a time helps the mate-pair mapping, which will be explained in the section below. After the read indexing with the two templates, the reference will be scanned base by base on both forward and reverse strands. Each 28-bp subsequence of the reference will be hashed through the two templates used in indexing and will be looked up in the two hash tables, respectively. If a hit is found to a read, MAQ will calculate the sum of qualities of mismatched bases q over the whole length of the read, extending out from the 28-bp seed without gaps (the current implementation has a read length limit of 63 bp). MAQ then hashes the coordinate of the hit and the read identifier into another 24-bit integer h and scores the hit as q⋅224 + h. In this score, h can be considered as a pseudorandom number, which differentiates hits with identical q: If there are multiple hits with the same q, the hit with the smallest h will be identified as the best, effectively selecting randomly from the candidates. For each read, MAQ only holds in memory the position and score of its two best scored hits and the number of 0-, 1-, and 2-mismatch hits in the seed region. When the scan of the reference is complete, the next two templates are applied and the reference will be scanned once again until no more templates are left. Using six templates guarantees to find seed hits with no more than two mismatches, and it also finds 57% of hits with three mismatches. In addition, MAQ can use 20 templates to guarantee finding all seed hits with three mismatches at the cost of speed. In this configuration, 64% of seed hits with four mismatches are also found, though our experience is that these hits are not useful in practice. Single end mapping qualities MAQ assigns each individual alignment a mapping quality. The mapping quality Qs is the phred-scaled probability (Ewing and Green 1998) that a read alignment may be wrong For example, Qs = 30 implies there is a 1 in 1000 probability that the read is incorrectly mapped. In this section, we only consider a simplistic case where all reads are known to come from the reference and an ungapped exhaustive alignment is performed. A practical model for alignment with heuristic algorithms will be presented in the Supplemental material. Suppose we have a reference sequence x and a read sequence z. On the assumption that sequencing errors are independent at different sites of the read, the probability p(z|x,u) of z coming from the position u equals the product of the error probabilities of the mismatched bases at the aligned position. For example, if read z mapped to position u has two mismatches: one with phred base quality 20 and the other with 10, then p(z|x,u) = 10−(20 + 10)/10 = 0.001. To calculate the posterior probability ps(u|x,z), we assume a uniform prior distribution p(u|x), and applying the Bayesian formula gives Where q1 is the sum of quality values of mismatches of the best hit, q2 is the corresponding sum for the second best hit, n2 is the number of hits having the same number of mismatches as the second best hit, k′ is the minimum number of mismatches in the 28-bp seed, q is the average base quality in the 28-bp seed, 4.343 is 10/log10, and p1(k,28) is the probability that a perfect hit and a k-mismatch hit coexists given a 28-bp sequence that can be estimated during alignment. Detailed deduction of this equation is given in the Supplemental material. It is also worth noting that in minimizing the sum of quality values of mismatched bases, MAQ is effectively maximizing the posterior probability ps(u|x,z). This is the statistical interpretation of MAQ alignments. On sequencing real samples, reads may also be different from the reference sequence due to the existence of sequence variants in different samples or strains. These variants behave in a similar manner to sequencing errors for mapping purposes, and therefore at the alignment stage, we should set the minimum base error probability as the rate of differences between the reference and the reads. However, this strategy is an approximation. When there are differences between the reference and reads, the best position might consistently give wrong alignments even if there are no sequencing errors, which can invalidate the calculation of mapping qualities. It would be possible in an iterative scheme to update the reference with an estimate of the new sample sequence from the first mapping and then remap to the updated reference. Paired-end read alignment MAQ jointly aligns the two reads in a read pair and fully utilizes the mate-pair information in the alignment. In the paired-end alignment mode, MAQ will by default build six hash tables for each end (12 tables in total). In one round of indexing, MAQ indexes the first end with two templates and the second end also with two templates. Four hash tables, two for each end, will be put in memory at a time. In the scan of the reference, when a hit of a read is found on the forward strand of the reference sequence, MAQ appends its position to a queue that always keeps the last two hits of this read on the forward strand. When a hit of a read is found on the reverse strand, MAQ checks the queue of its mate and tests whether its mate has a hit on the forward strand within a maximum allowed distance ahead of the current read. If there is one, MAQ will mark the two ends as a pair. In this way, MAQ jointly maps the reads without independently storing all the potential hits of each end. For each end, MAQ will only hold in memory two hash tables corresponding to two complementary templates (e.g., 11110000 and 00001111 for 8-bp reads). This strategy guarantees that any hit with no more than one mismatch can be always found in each round of the scan. Holding more hash tables in memory would help to find pairs containing more mismatches, but doing this would also increase memory footprint. Paired-end mapping qualities are derived from single end mapping qualities. There are two different cases when a pair can be wrongly mapped. In the first case, one of the two ends is wrongly aligned and the other is correct. This scenario may happen if a repetitive sequence appear twice or more in a short region. In the second instance, a pair is wrong because both ends are wrong at the same time. In MAQ, if there is a unique pair mapping in which both ends hit consistently (i.e., in the right orientation within the proper distance), we give the mapping quality Qp = Qs1+Qs2 to both reads, assuming independent errors. If there are multiple consistent hit pairs, we take their single end mapping qualities as the final mapping qualities. Detecting short indels MAQ first aligns reads with the ungapped alignment algorithm described above and then finds short indels by utilizing mate-pair information. Given a pair of reads, if one end can be mapped with confidence but the other end is unmapped, a possible scenario is that a potential indel interrupts the alignment of the unmapped read. For this unmapped read, we can apply a standard Smith-Waterman gapped alignment (Smith and Waterman 1981) in a region determined by the aligned read. The coordinate and the size of the region is estimated from the distribution of all the aligned reads by taking the mean separation of read pairs plus or minus twice the standard deviation. As Smith-Waterman will only be applied to a small fraction of reads in short regions, efficiency is not a serious issue. Consensus genotype calling By default, MAQ assumes the sample is diploid. It calculates the posterior distribution of genotypes and calls the genotype that maximizes the posterior probability. Before consensus calling, MAQ first combines mapping quality and base quality. If a read is incorrectly mapped, any sequence differences inferred from the read cannot be reliable. Therefore, the base quality used in SNP calling cannot exceed the mapping quality of the read. MAQ reassigns the quality of each base as the smaller value between the read mapping quality and the raw sequencing base quality. We first calculate the probability of data given each possible genotype. In consensus calling, if there are no sequencing errors, at most two different nucleotides can be legitimately seen. Therefore, we can consider only the two most frequent nucleotides at any position and ignore others as errors. Assume we are observing data D which consist of k nucleotides b and n−k nucleotides b′ with b,b′∈{A,C,G,T} and b ≠ b′. Then the three possible genotypes are 〈b,b〉, 〈b,b′〉, and 〈b′,b′〉. If the true genotype is 〈b,b〉, we have n−k errors from n bases. Let the probability of observing these errors be αn,n-k, and therefore P(D|〈b,b〉) = αn,n-k. Similarly we have P(D|〈b′,b′〉) = αnk. If the true genotype is 〈b,b′〉, the probability can be approximated with a binomial distribution: P(D|〈b,b′〉) = (nk)/2n. If we further assume the prior of genotypes is P(〈b,b〉) = P(〈b′,b′〉) = (1 − r)/2 and P(〈b,b′〉) = r, we can calculate the posterior probability P(g|D) of genotype g given the observation D. Then the estimated genotype is ĝ = argmaxgP(g|D) with a quality Qg = −10log10[1 − P(ĝ|D)]. Here r is the probability of observing a heterozygote. We usually use r = 0.001 for the discovery of new SNPs and r = 0.2 for inferring genotypes at known SNP sites. In principle, a site-specific r can be used given known allele frequencies. Where εi is the ith smallest base error probability and c′nk is a function of εi but varies little with εi. The only unknown parameter is θ , which controls the dependency of errors. The deduction of this equation and the calculation of c′nk will be presented in the Supplemental material. Taking a form like Equation 2 is inspired by CAP3 (Huang and Madan, 1999), where θ is arbitrarily set to 0.5. In principle, θ can be estimated from real data. In practice, however, the estimate is complicated by the requirement of large data set where SNPs are known, by the inaccuracy of sequencing qualities, by the dependencies of mapping qualities, and also by the approximation made to derive the equation. To estimate θ, we just tried different values and selected the one that was giving the best final genotype calls. We found θ = 0.85 is a reasonable value for Illumina Genetic Analyzer data. Simulating diploid genomes and short reads MAQ also generates in silico mutated diploid sequences by adding random mutations to the known reference sequence. The human reference genome does not contain heterozygotes, but when we resequence a human sample and map reads to the reference genome, we will see both homozygous and heterozygous variants in comparison to the reference. If the sample and the reference come from the same population and at a potential polymorphic site the allele frequency is f, the probability of observing a heterozygote is 2f(1 − f) and of observing a homozygous variant is f(1 − f) (= f2(1 − f) + f(1 − f)2). Consequently, on the condition that a site is different from the reference, the probability of a heterozygote is always 2/3, regardless of the allele frequency f, assuming the sample comes from the same population as the reference. Based on this observation, we can simulate a diploid genome as follows. We first used the reference genome as the two preprocessed haplotypes. We then generated a set of polymorphic sites, randomly selected two thirds of them as heterozygotes, and took the rest as homozygotes. At a heterozygous site, we randomly selected one haplotype and mutated the base into another one; on a homozygous site, we mutated both haplotypes. Both substitutions and indels can be simulated in this way. This simulation ignores linkage disequilibrium between variants. Although coalescent-based simulation (Hudson 2002) gives a more accurate long-range picture, the procedure described here is sufficient for the evaluation of the variant calling method for a single individual. From a known sequence, paired-end reads can be simulated with insert sizes drawn from a normal distribution and with base qualities drawn from the empirical distribution estimated from real sequence data. Sequencing errors are introduced based on the base quality. With sufficiently large data, we are able to estimate the position-specific distributions of base qualities and the correlation between adjacent qualities as well. An order-one Markov chain is constructed, based on these statistics, to capture the fact that low-quality bases tend to appear at the 3′-end of a read and to appear successively along a read. Alignment for Applied Biosystems SOLiD reads SOLiD reads are presented in the color space, which comprises four colors with each color representing four types of combinations of two adjacent nucleotides. The SOLiD sequencing machine gives the last primer nucleotide base and the color read sequence. This information makes it possible to write down the nucleotide read sequence based on the meaning of colors. However, a single color error will completely change the nucleotide sequencing following that error. Mapping reads in the color space is preferable to mapping in the nucleotide space. To map reads in the color space, we need to convert the reference sequences into color sequences and to perform the alignment in the color space. Between the color alignment and nucleotide alignment, the main difference is that the complement of a color is identical to itself, and therefore in the color space, reads coming from the reverse strand of the reference only need to be reversed without complementation. Most alignment programs can be adapted to perform such an alignment with little effort. Another difference is for paired-end reads. In SOLiD sequencing, the two ends of a read pair should always come from the same strand, instead from two different strands like Illumina sequencing. MAQ is able to map SOLiD mate-pair reads to the reference, but it has to trim off the primer nucleotide base and the following color because currently MAQ cannot work with color sequences and nucleotide sequences at the same time. Trimming the first color is equivalent to using reads 1 bp shorter, which should not greatly affect the alignment results.",Mapping,"map short dna sequence read  call variants use map quality scores
single end read map  map read efficiently maq first index read sequence  scan  reference genome sequence  identify hit   extend  score   elandlike  cox unpubl hash technique maq  default guarantee  find alignments    two mismatch   first     read maq map  read   position  minimize  sum  quality value  mismatch base    multiple equally best position  one    choose  random   article   call  potential read alignment position  hit  algorithm maq use  find  best hit  quite similar   one use  eland  build multiple hash table  index  read  scan  reference sequence   hash table  find  hit  default six hash table  use ensure   sequence  two mismatch  fewer   hit  six hash table correspond  six noncontiguous seed templates buhler      give  read  example  six templates          nucleotides     index        default maq index  first     read   typically   accurate part   read  alignment maq load  read  memory   apply  first template  follow   read maq take  nucleotides    position   template hash    bite integer  put  integer together   read identifier   list    read  process maq order  list base   bite integers   read    hash integer  group together  memory  integer   correspond region   record   hash table   integer   key  call  process index    time  maq index  read   first template  also index  read   second template   complementary   first one take two templates   time help  matepair map    explain   section    read index   two templates  reference   scan base  base   forward  reverse strand   subsequence   reference   hash   two templates use  index    look    two hash table respectively   hit  find   read maq  calculate  sum  qualities  mismatch base    whole length   read extend     seed without gap  current implementation   read length limit    maq  hash  coordinate   hit   read identifier  another bite integer   score  hit  ⋅     score    consider   pseudorandom number  differentiate hit  identical     multiple hit      hit   smallest    identify   best effectively select randomly   candidates   read maq  hold  memory  position  score   two best score hit   number     mismatch hit   seed region   scan   reference  complete  next two templates  apply   reference   scan      templates  leave use six templates guarantee  find seed hit     two mismatch   also find   hit  three mismatch  addition maq  use  templates  guarantee find  seed hit  three mismatch   cost  speed   configuration   seed hit  four mismatch  also find though  experience    hit   useful  practice single end map qualities maq assign  individual alignment  map quality  map quality    phredscaled probability ewing  green    read alignment may  wrong  example    imply       probability   read  incorrectly map   section   consider  simplistic case   read  know  come   reference   ungapped exhaustive alignment  perform  practical model  alignment  heuristic algorithms   present   supplemental material suppose    reference sequence    read sequence    assumption  sequence errors  independent  different sit   read  probability pzxu   come   position  equal  product   error probabilities   mismatch base   align position  example  read  map  position   two mismatch one  phred base quality        pzxu        calculate  posterior probability psuxz  assume  uniform prior distribution pux  apply  bayesian formula give     sum  quality value  mismatch   best hit    correspond sum   second best hit    number  hit    number  mismatch   second best hit ′   minimum number  mismatch    seed    average base quality    seed   log10  p1k   probability   perfect hit   kmismatch hit coexist give   sequence    estimate  alignment detail deduction   equation  give   supplemental material   also worth note   minimize  sum  quality value  mismatch base maq  effectively maximize  posterior probability psuxz    statistical interpretation  maq alignments  sequence real sample read may also  different   reference sequence due   existence  sequence variants  different sample  strain  variants behave   similar manner  sequence errors  map purpose  therefore   alignment stage   set  minimum base error probability   rate  differences   reference   read however  strategy   approximation    differences   reference  read  best position might consistently give wrong alignments even     sequence errors   invalidate  calculation  map qualities  would  possible   iterative scheme  update  reference   estimate   new sample sequence   first map   remap   update reference pairedend read alignment maq jointly align  two read   read pair  fully utilize  matepair information   alignment   pairedend alignment mode maq   default build six hash table   end  table  total  one round  index maq index  first end  two templates   second end also  two templates four hash table two   end   put  memory   time   scan   reference   hit   read  find   forward strand   reference sequence maq append  position   queue  always keep  last two hit   read   forward strand   hit   read  find   reverse strand maq check  queue   mate  test whether  mate   hit   forward strand within  maximum allow distance ahead   current read    one maq  mark  two end   pair   way maq jointly map  read without independently store   potential hit   end   end maq   hold  memory two hash table correspond  two complementary templates       read  strategy guarantee   hit     one mismatch   always find   round   scan hold  hash table  memory would help  find pair contain  mismatch    would also increase memory footprint pairedend map qualities  derive  single end map qualities   two different case   pair   wrongly map   first case one   two end  wrongly align     correct  scenario may happen   repetitive sequence appear twice     short region   second instance  pair  wrong   end  wrong    time  maq     unique pair map    end hit consistently    right orientation within  proper distance  give  map quality   qs1qs2   read assume independent errors    multiple consistent hit pair  take  single end map qualities   final map qualities detect short indels maq first align read   ungapped alignment algorithm describe    find short indels  utilize matepair information give  pair  read  one end   map  confidence    end  unmapped  possible scenario    potential indel interrupt  alignment   unmapped read   unmapped read   apply  standard smithwaterman gap alignment smith  waterman    region determine   align read  coordinate   size   region  estimate   distribution    align read  take  mean separation  read pair plus  minus twice  standard deviation  smithwaterman    apply   small fraction  read  short regions efficiency    serious issue consensus genotype call  default maq assume  sample  diploid  calculate  posterior distribution  genotypes  call  genotype  maximize  posterior probability  consensus call maq first combine map quality  base quality   read  incorrectly map  sequence differences infer   read cannot  reliable therefore  base quality use  snp call cannot exceed  map quality   read maq reassign  quality   base   smaller value   read map quality   raw sequence base quality  first calculate  probability  data give  possible genotype  consensus call     sequence errors   two different nucleotides   legitimately see therefore   consider   two  frequent nucleotides   position  ignore others  errors assume   observe data   consist   nucleotides    nucleotides ′  ′∈{acgt}   ≠ ′   three possible genotypes  〈〉 〈′〉  〈′′〉   true genotype  〈〉    errors   base let  probability  observe  errors  αnnk  therefore 〈〉  αnnk similarly   〈′′〉  αnk   true genotype  〈′〉  probability   approximate   binomial distribution 〈′〉  nk2n    assume  prior  genotypes  〈〉  〈′′〉      〈′〉     calculate  posterior probability pgd  genotype  give  observation    estimate genotype    argmaxgpgd   quality   10log10  pĝd     probability  observe  heterozygote  usually use      discovery  new snps      infer genotypes  know snp sit  principle  sitespecific    use give know allele frequencies     ith smallest base error probability  ′   function    vary little     unknown parameter     control  dependency  errors  deduction   equation   calculation  ′   present   supplemental material take  form like equation   inspire  cap3 huang  madan     arbitrarily set    principle    estimate  real data  practice however  estimate  complicate   requirement  large data set  snps  know   inaccuracy  sequence qualities   dependencies  map qualities  also   approximation make  derive  equation  estimate    try different value  select  one   give  best final genotype call  find      reasonable value  illumina genetic analyzer data simulate diploid genomes  short read maq also generate  silico mutate diploid sequence  add random mutations   know reference sequence  human reference genome   contain heterozygotes    resequence  human sample  map read   reference genome   see  homozygous  heterozygous variants  comparison   reference   sample   reference come    population    potential polymorphic site  allele frequency    probability  observe  heterozygote       observe  homozygous variant             consequently   condition   site  different   reference  probability   heterozygote  always  regardless   allele frequency  assume  sample come    population   reference base   observation   simulate  diploid genome  follow  first use  reference genome   two preprocessed haplotypes   generate  set  polymorphic sit randomly select two thirds    heterozygotes  take  rest  homozygotes   heterozygous site  randomly select one haplotype  mutate  base  another one   homozygous site  mutate  haplotypes  substitutions  indels   simulate   way  simulation ignore linkage disequilibrium  variants although coalescentbased simulation hudson  give   accurate longrange picture  procedure describe   sufficient   evaluation   variant call method   single individual   know sequence pairedend read   simulate  insert size draw   normal distribution   base qualities draw   empirical distribution estimate  real sequence data sequence errors  introduce base   base quality  sufficiently large data   able  estimate  positionspecific distributions  base qualities   correlation  adjacent qualities  well  orderone markov chain  construct base   statistics  capture  fact  lowquality base tend  appear   ′end   read   appear successively along  read alignment  apply biosystems solid read solid read  present   color space  comprise four color   color represent four type  combinations  two adjacent nucleotides  solid sequence machine give  last primer nucleotide base   color read sequence  information make  possible  write   nucleotide read sequence base   mean  color however  single color error  completely change  nucleotide sequence follow  error map read   color space  preferable  map   nucleotide space  map read   color space  need  convert  reference sequence  color sequence   perform  alignment   color space   color alignment  nucleotide alignment  main difference    complement   color  identical    therefore   color space read come   reverse strand   reference  need   reverse without complementation  alignment program   adapt  perform   alignment  little effort another difference   pairedend read  solid sequence  two end   read pair  always come    strand instead  two different strand like illumina sequence maq  able  map solid matepair read   reference     trim   primer nucleotide base   follow color  currently maq cannot work  color sequence  nucleotide sequence    time trim  first color  equivalent  use read   shorter    greatly affect  alignment result",4
91,RMAP,"Using quality scores and longer reads improves accuracy of Solexa read mapping
Design of the RMAP algorithm In this section we describe the algorithmic strategy used in RMAP. We treat the mapping problem as approximately matching a set of patterns in a text – the set of patterns being the reads, and the text being the genome. This problem has been well studied, and several general algorithmic strategies have emerged for solving it (see [13] for a detailed treatment). The major motivation for developing the RMAP algorithm was to incorporate base-call quality scores to weight mismatches and improve mapping accuracy. In addition to having high mapping accuracy, RMAP was designed under the restrictions that it must be capable of (1) mapping reads with length exceeding 50 bases (for the applications discussed in the introduction), (2) allowing the number of mismatches to be controlled (not being restricted to a small fixed number), and (3) completing mapping tasks under reasonable time constraints on widely available computing hardware. The algorithm implemented in RMAP uses the filtration method described by [14]. For reads of length n, and mapping with up to k mismatches, each read is partitioned into k + 1 contiguous seeds (each seed is a substring of the read, and has length ⌊n/(k + 1)⌋). Because there can only be k mismatches in a mapping, and there are k + 1 seeds for each read, any mapping must have at least one seed with no mismatches. The algorithm first identifies locations in the genome where the seeds match exactly. Exact matching can be done much more quickly than approximate matching, and evaluating the approximate match between a read and a genomic region only needs to be done for those regions surrounding an exactly-matching seed. To efficiently implement the filtration strategy, RMAP pre-processes the set of reads, building a hash-table (which we refer to as the seed-table) indexed by the seeds. The table entry for a particular seed lists all reads containing that seed, along with the offset of that seed within the read. For a set of r reads, each having length n, if k mismatches are allowed in the search, the seed table has size O(4n/k+ rk). The mapping proceeds by scanning the genome, with a window of size equal to the seed size. Each segment of the genome is tested as a seed by hashing that segment to determine the set of reads that must be compared in their entirety with a larger genomic region surrounding the segment of the genome currently being scanned. This is a common strategy to implement the filtration stage of approximate matching. The influence of the size of the genome in the time complexity of RMAP is therefore linear, and importantly the space complexity of RMAP is independent of the size of the genome. The step of comparing the full read to portions of the genome where a seed has been found is implemented to require time that is logarithmic in the length of the reads. The comparison takes advantage of bit-wise operations, and the reads are encoded in a binary format (see additional file 4 for supplementary method). A series of logical operations produce a vector indicating the locations of mismatches between the read and the genomic segment being compared, and the weight of the bit-vector indicating mismatches computed using a well-known technique described by [15]. RMAP is sufficiently fast that several million reads can be mapped to a mammalian genome in one day on a computer with a single processor. No portion of the reference genome is maintained by RMAP, and the size of the seed table dominates the space requirements. Because these requirements are sufficiently small, RMAP can be run on widely available hardware. This includes the nodes typically used in cluster computers, and allows the processing to be easily and effectively parallelized by simply partitioning the set of reads. On a test data set generated by randomly sampling one million 50 nt segments (simulated reads) from the hg18 genome, and randomly changing up to 4 bases in each read, our current implementation of RMAP was able to map the reads back to the hg18 genome in 140 minutes using roughly 620 MB of memory.",Mapping,"use quality score  longer read improve accuracy  solexa read mapping
design   rmap algorithm   section  describe  algorithmic strategy use  rmap  treat  map problem  approximately match  set  pattern   text   set  pattern   read   text   genome  problem   well study  several general algorithmic strategies  emerge  solve  see    detail treatment  major motivation  develop  rmap algorithm   incorporate basecall quality score  weight mismatch  improve map accuracy  addition   high map accuracy rmap  design   restrictions   must  capable   map read  length exceed  base   applications discuss   introduction  allow  number  mismatch   control   restrict   small fix number   complete map task  reasonable time constraints  widely available compute hardware  algorithm implement  rmap use  filtration method describe    read  length   map     mismatch  read  partition     contiguous seed  seed   substring   read   length ⌊  ⌋       mismatch   map       seed   read  map must   least one seed   mismatch  algorithm first identify locations   genome   seed match exactly exact match    much  quickly  approximate match  evaluate  approximate match   read   genomic region  need      regions surround  exactlymatching seed  efficiently implement  filtration strategy rmap preprocesses  set  read build  hashtable   refer    seedtable index   seed  table entry   particular seed list  read contain  seed along   offset   seed within  read   set   read   length    mismatch  allow   search  seed table  size o4nk   map proceed  scan  genome   window  size equal   seed size  segment   genome  test   seed  hash  segment  determine  set  read  must  compare   entirety   larger genomic region surround  segment   genome currently  scan    common strategy  implement  filtration stage  approximate match  influence   size   genome   time complexity  rmap  therefore linear  importantly  space complexity  rmap  independent   size   genome  step  compare  full read  portion   genome   seed   find  implement  require time   logarithmic   length   read  comparison take advantage  bitwise operations   read  encode   binary format see additional file   supplementary method  series  logical operations produce  vector indicate  locations  mismatch   read   genomic segment  compare   weight   bitvector indicate mismatch compute use  wellknown technique describe   rmap  sufficiently fast  several million read   map   mammalian genome  one day   computer   single processor  portion   reference genome  maintain  rmap   size   seed table dominate  space requirements   requirements  sufficiently small rmap   run  widely available hardware  include  nod typically use  cluster computers  allow  process   easily  effectively parallelize  simply partition  set  read   test data set generate  randomly sample one million   segment simulate read   hg18 genome  randomly change    base   read  current implementation  rmap  able  map  read back   hg18 genome   minutes use roughly    memory",4
92,GSNAP,"Fast and SNP-tolerant detection of complex variants and splicing in short reads 
We view alignment as a search problem over a space of genomic regions in the reference sequence, or combinations of regions if gaps are allowed. (Although a reference sequence may consist of chromosomes, contigs, transcripts or artificial segments, we simplify our discourse by referring to it as a ‘genome’.) Searching involves the steps of generating, filtering and verifying candidate genomic regions, and its efficiency depends on designing the generation and filtering steps to produce as few candidates as possible. Several alignment programs, including MAQ (Li et al., 2008a), RMAP (Smith et al., 2008), SeqMap (Jiang and Wong, 2008) and RazerS (Weese et al., 2009), preprocess the reads and then generate and filter candidate genomic regions by scanning a read index against the genome. For large genomes, it is more efficient to pre-process the genome rather than the reads to create genomic index files, which provide genomic positions for a given oligomer. Genomic indexing also permits parts of reads to be aligned to arbitrary genomic regions, needed for long-distance splice detection. Indexing need be done only once for each reference sequence, with the resulting index files usable by each new dataset. Oligomers of all lengths can be indexed using a suffix array or its compressed BWT equivalent, as used in Bowtie, BWA and SOAP2, which can represent a reference sequence compactly, in 2 GB for a human-sized genome of 3 billion nt. However, when only a single oligomer length q is needed by an algorithm, a simple hash table (Ning et al., 2001) or q-gram index (Rasmussen et al., 2006) applied to the genome will suffice (Fig. 2A). This data structure consists of an offset file (or lookup table) of all possible q-mers, with pointers to a position file (or occurrence table) containing a list of genomic positions for each q-mer. For our search algorithm to work most efficiently, it is important that each position list in the position file be pre-sorted, which allows intersections to be computed quickly among multiple q-mer lookups. The intersection process also requires the positions in each position list to be adjusted at run time for its location in the given read, so they correspond to the diagonals in an alignment matrix between genome and read. Although our alignment algorithm could potentially work with another data structure that provides genomic positions for a given q-mer, a suffix array would require the additional step of sorting each position list at run time. A set of n sorted lists can be merged in time O(l log n), where l is the sum of list lengths, by using a heap-based multiway merging procedure (Knuth, 1973). A merging procedure can produce not only a list of candidate genomic regions, but also information about the count and read location of the position lists that support each region. This support information can provide evidence about the number of mismatches in the read, and can therefore be used to filter out candidate regions. To use multiway merging effectively, our algorithm depends on another idea, that of successive score constraints. For a given read, our program is designed to report the ‘best’ alignment or alignments, those with the lowest score based on mismatches plus an opening gap penalty for an indel or a splice. Therefore, our search process is constrained successively by an increasing score level K, starting from K=0 for an exact match, and ending either with a successful alignment at some K or at a maximum score level specified by the user. In addition to finding the best alignment, a constrained search process can also find suboptimal alignments, by continuing the search at successive score levels beyond the first, or optimal, one that yields an alignment. Our algorithm could also find an exhaustive set of alignments up to a given score level by searching at that score level and reporting all results. Depending on the score constraint K and the read length L, a multiway merging process can be formulated in two different ways to generate and filter genomic regions. For low values of K involving none or a few mismatches relative to L, we apply a merging procedure based on a spanning set of oligomers, which filters genomic regions based on the count of q-mers that support the region. For higher levels of K involving more mismatches, we apply a merging procedure based on a complete set of oligomers, which filters genomic regions based on the pattern of q-mers that support the region. Both the count- and pattern-based criteria provide lower bounds on the number of mismatches present in a read or part of a read. If a lower bound exceeds the given score constraint K of allowed mismatches, the read may be filtered out and need not be verified against the genome to determine the actual number of mismatches. A hash table is relatively large, requiring 12 GB to represent a human-sized genome if every overlapping oligomer is indexed. Accordingly, SOAP (Li et al., 2008b) requires 14 GB of memory to process a human-sized genome. Although modern computers generally have sufficient physical memory to query such large hash tables, smaller data structures can speed up programs by using memory paging and caching resources more effectively. We can reduce the size of a hash table by sampling the genomic oligomers that are indexed in the table. In our program, we index 12mers every 3 nt in the genome, which reduces the size of a human genomic hash table to 4 GB. As a result, our algorithm is designed to use a hash table sampled every 3 nt and still achieve full sensitivity as if every overlapping oligomer were indexed. A hash table indexing scheme can be extended to align major and minor alleles equally well in SNP-tolerant alignment. (For ease of discussion, we refer to the alleles in the reference sequence as ‘major’ and their corresponding alternate versions as ‘minor’, regardless of their actual frequencies in a population.) Because a hash table represents the genome in q-mer pieces, it can represent the enormous space of all combinations of major and minor alleles in a relatively straightforward way. To construct a SNP-tolerant hash table, we scan the genome and process each sampled genomic q-mer that contains one or more SNPs, by generating each possible combination of the major and minor alleles contained within and duplicating this genomic position for each generated q-mer. Finally, we re-sort the position list for each q-mer (Fig. 2B). A lookup in this hash table of any combination of major and minor alleles in a q-mer at a given genomic position will all contain the desired position. Our experience shows that a SNP-tolerant hash table is only slightly larger than the original. When we incorporate the 12 million SNPs from dbSNP version 129 into human genome version 36, the hash table increases in size from 3.8 to 4.0 GB. Our construction algorithm requires that the computer have sufficient memory to store the hash table, thereby requiring 4 GB for a human-sized genome. Verification in a SNP-tolerant manner is discussed in Section 2.4. 2.2 Spanning set generation and filtering A spanning set is a minimal set of 12mers that covers the read (Fig. 3). This structure exploits the pigeonhole principle that the number of non-supporting 12mers—those that fail to contain a given position in their corresponding position list—provides a lower bound on the number of mismatches in the read. However, implementation of this pigeonhole principle is complicated by our use of sampling in the hash table, which creates uncertainty about the phase of the aligned read relative to the sampled genomic 12mers. Therefore, the program must construct six spanning sets, one for each shift of 0, 1 or 2 nt in both the forward and reverse complement directions (Fig. 3A). In addition, sampled hash tables cause genomic position information to be available only at intervals of 3 nt, thereby causing information to incomplete for 12mers that overhang past read boundaries. To handle such cases, the program computes the position list for a 12mer that overhangs the end of the read by 1 or 2 nt by substituting all possible nucleotides in the overhanging positions and taking the union of the resulting position lists Another complication is that a spanning set will often contain 12mers that overlap. To address this issue, we consider an overlapping pair of 12mers to be a single ‘element’ in the spanning set, with a position list equal to the intersection of the two constituent position lists (Fig. 3C). The resulting set of elements is non-overlapping, so the pigeonhole principle now holds where k non-supporting elements implies a lower bound of k mismatches, and the region may be filtered out if k>K. There may be several choices for the pair of 12mers that overlap to create a single element; our program heuristically selects the 12mer with the longest position list or union of position lists as the site of the overlap, because the intersection operation on that 12mer is likely to eliminate the largest number of positions from subsequent consideration. Although we could use all spanning set elements to generate candidates and then proceed to the verification step, we can make our algorithm faster if we designate some elements for generating candidates (Fig. 3D) and reserve others for a separate filtering step (Fig. 3E). This division of labor is intended to reduce the O(l log n) complexity for a heap-based priority queue, which is linear in l. If we check a sorted list of length li for the presence of a given position in a filtering step, this can be done in logarithmic time O(log li) through a binary search process. Consequently, our method performs a heap-based merge of some position lists (the generating elements), and counts the number of elements that support each of the resulting candidate regions. If this count is high enough to allow the possibility that K or fewer total elements will be non-supporting, then the candidate region undergoes a filtering step that checks each of the filtering elements for support. The algorithm eliminates the candidate if more than K total elements show non-support; otherwise, the region undergoes a verification step to determine the actual number of mismatches. We have made implementation of our spanning set method efficient in various ways. First, the program selects elements with the shortest position lists as generating elements and the longest ones for filtering elements, because while every position in generating elements must be processed, only some of those in the filtering elements need be. Second, we maintain a pointer on each filtering element and advance that pointer only when we check for support, by using a galloping binary search (Hwang and Lin, 1980). Third, filtering elements that involve unions or intersections of position lists need not have these set operations computed explicitly, but can be represented instead by their constituent position lists, and support checked by performing the appropriate disjunctive or conjunctive searches when needed. Allocation of N total elements between generating and filtering purposes depends on the constraint score level K of allowed mismatches. At least (K + 1) elements must be generating to guarantee that at least one generating element has support for a candidate region when the K other generating elements do not. We have found empirically that for K > 1, it is more efficient to allocate (K + 2) elements for generating purposes, because the requirement for two supporting elements greatly reduces the number of candidate regions generated. Because the spanning set method requires at least (K + 2) generating elements [or (K + 1) for the exact and one-mismatch constraints], it can be used to detect only a limited number of mismatches relative to read length L, which limits the total number of elements N. Spanning set elements are non-overlapping in all three shifts when L = 10 (mod 12), so N≤⌊(L + 2)/12⌋. Therefore, (K + 1)<N or (K + 2)<N indicates that the spanning set method can be applied to constraint level K when K = 0 for 14≤L≤21; K≤1 for 22≤L≤33; and K≤⌊(L+2)/12⌋−2 for L≥34. 2.3 Complete set generation and filtering To handle greater numbers of mismatches than those detectable by the spanning set method, we employ a strategy based on the complete set of overlapping 12mers. This complete set method works for any constraint level K of allowed mismatches, as long as the read and candidate region have 14 consecutive matches (a 12mer out of phase by as many as 2 nt). One sufficient condition for 14 consecutive matches is that the number of mismatches be ≤⌊L/14⌋−1. Up to this level of mismatches, GSNAP is an exhaustive algorithm, meaning that it can guarantee to identify and report all available alignments in the genome with that many mismatches. Candidates are generated by performing a multiway merge of position lists for all read locations in a single forward and single reverse complement pass, keeping track of the read location of 12mers that support each candidate region. The pattern of supporting 12mers provides a lower bound on mismatches in the read. If the supporting 12mers have read locations separated by Δp, then the minimum number of mismatches between them is ⌊(Δp+6)/12⌋ (Fig. 4A). Over the entire read, we can sum these lower bounds in a pattern-based lower bound calculation (Fig. 4B). Specifically, if a read of length L has a pattern of supporting 12mers at read locations pi, i=1,…, n, a lower bound on mismatches is ∑i=0n⌊(pi+1−pi+6)/12⌋, where p0=−3 and pn+1=L−9. To make the complete set method more efficient, we note that the merging process must process every position from each position list, and can therefore be slowed down by non-specific 12mers with extremely long position lists that do not help localize the read. We can gain efficiency by ignoring these non-specific 12mers, defined currently as those with position lists that are > 10 times the mean position list length. The lower bound formula must be modified accordingly to compensate for the missing 12mers, essentially by assuming that they are supporting. This strategy can potentially fail to align reads or portions of reads if the non-specific or repetitive nucleotide patterns are necessary for mapping the read. To successfully align these reads, the program provides an option for a greedy strategy in which non-specific or repetitive 12mers are initially ignored, and then subsequently included if an alignment is not found.",Mapping,"fast  snptolerant detection  complex variants  splice  short read 
 view alignment   search problem   space  genomic regions   reference sequence  combinations  regions  gap  allow although  reference sequence may consist  chromosomes contigs transcripts  artificial segment  simplify  discourse  refer     genome search involve  step  generate filter  verify candidate genomic regions   efficiency depend  design  generation  filter step  produce   candidates  possible several alignment program include maq     rmap smith    seqmap jiang  wong   razers weese    preprocess  read   generate  filter candidate genomic regions  scan  read index   genome  large genomes    efficient  preprocess  genome rather   read  create genomic index file  provide genomic position   give oligomer genomic index also permit part  read   align  arbitrary genomic regions need  longdistance splice detection index need       reference sequence   result index file usable   new dataset oligomers   lengths   index use  suffix array   compress bwt equivalent  use  bowtie bwa  soap2   represent  reference sequence compactly      humansized genome   billion  however    single oligomer length   need   algorithm  simple hash table ning     qgram index rasmussen    apply   genome  suffice fig   data structure consist   offset file  lookup table   possible qmers  pointers   position file  occurrence table contain  list  genomic position   qmer   search algorithm  work  efficiently   important   position list   position file  presorted  allow intersections   compute quickly among multiple qmer lookups  intersection process also require  position   position list   adjust  run time   location   give read   correspond   diagonals   alignment matrix  genome  read although  alignment algorithm could potentially work  another data structure  provide genomic position   give qmer  suffix array would require  additional step  sort  position list  run time  set   sort list   merge  time  log      sum  list lengths  use  heapbased multiway merge procedure knuth   merge procedure  produce    list  candidate genomic regions  also information   count  read location   position list  support  region  support information  provide evidence   number  mismatch   read   therefore  use  filter  candidate regions  use multiway merge effectively  algorithm depend  another idea   successive score constraints   give read  program  design  report  best alignment  alignments    lowest score base  mismatch plus  open gap penalty   indel   splice therefore  search process  constrain successively   increase score level  start     exact match  end either   successful alignment       maximum score level specify   user  addition  find  best alignment  constrain search process  also find suboptimal alignments  continue  search  successive score level beyond  first  optimal one  yield  alignment  algorithm could also find  exhaustive set  alignments    give score level  search   score level  report  result depend   score constraint    read length   multiway merge process   formulate  two different ways  generate  filter genomic regions  low value   involve none    mismatch relative    apply  merge procedure base   span set  oligomers  filter genomic regions base   count  qmers  support  region  higher level   involve  mismatch  apply  merge procedure base   complete set  oligomers  filter genomic regions base   pattern  qmers  support  region   count  patternbased criteria provide lower bound   number  mismatch present   read  part   read   lower bind exceed  give score constraint   allow mismatch  read may  filter   need   verify   genome  determine  actual number  mismatch  hash table  relatively large require    represent  humansized genome  every overlap oligomer  index accordingly soap     require    memory  process  humansized genome although modern computers generally  sufficient physical memory  query  large hash table smaller data structure  speed  program  use memory page  cache resources  effectively   reduce  size   hash table  sample  genomic oligomers   index   table   program  index 12mers every     genome  reduce  size   human genomic hash table      result  algorithm  design  use  hash table sample every    still achieve full sensitivity   every overlap oligomer  index  hash table index scheme   extend  align major  minor alleles equally well  snptolerant alignment  ease  discussion  refer   alleles   reference sequence  major   correspond alternate versions  minor regardless   actual frequencies   population   hash table represent  genome  qmer piece   represent  enormous space   combinations  major  minor alleles   relatively straightforward way  construct  snptolerant hash table  scan  genome  process  sample genomic qmer  contain one   snps  generate  possible combination   major  minor alleles contain within  duplicate  genomic position   generate qmer finally  resort  position list   qmer fig   lookup   hash table   combination  major  minor alleles   qmer   give genomic position   contain  desire position  experience show   snptolerant hash table   slightly larger   original   incorporate   million snps  dbsnp version   human genome version   hash table increase  size       construction algorithm require   computer  sufficient memory  store  hash table thereby require     humansized genome verification   snptolerant manner  discuss  section   span set generation  filter  span set   minimal set  12mers  cover  read fig   structure exploit  pigeonhole principle   number  nonsupporting 12mers—  fail  contain  give position   correspond position list—provides  lower bind   number  mismatch   read however implementation   pigeonhole principle  complicate   use  sample   hash table  create uncertainty   phase   align read relative   sample genomic 12mers therefore  program must construct six span set one   shift          forward  reverse complement directions fig   addition sample hash table cause genomic position information   available   intervals    thereby cause information  incomplete  12mers  overhang past read boundaries  handle  case  program compute  position list   12mer  overhang  end   read       substitute  possible nucleotides   overhang position  take  union   result position list another complication    span set  often contain 12mers  overlap  address  issue  consider  overlap pair  12mers    single element   span set   position list equal   intersection   two constituent position list fig   result set  elements  nonoverlapping   pigeonhole principle  hold   nonsupporting elements imply  lower bind   mismatch   region may  filter     may  several choices   pair  12mers  overlap  create  single element  program heuristically select  12mer   longest position list  union  position list   site   overlap   intersection operation   12mer  likely  eliminate  largest number  position  subsequent consideration although  could use  span set elements  generate candidates   proceed   verification step   make  algorithm faster   designate  elements  generate candidates fig   reserve others   separate filter step fig   division  labor  intend  reduce   log  complexity   heapbased priority queue   linear     check  sort list  length    presence   give position   filter step      logarithmic time olog    binary search process consequently  method perform  heapbased merge   position list  generate elements  count  number  elements  support    result candidate regions   count  high enough  allow  possibility    fewer total elements   nonsupporting   candidate region undergo  filter step  check    filter elements  support  algorithm eliminate  candidate     total elements show nonsupport otherwise  region undergo  verification step  determine  actual number  mismatch   make implementation   span set method efficient  various ways first  program select elements   shortest position list  generate elements   longest ones  filter elements   every position  generate elements must  process       filter elements need  second  maintain  pointer   filter element  advance  pointer    check  support  use  gallop binary search hwang  lin  third filter elements  involve unions  intersections  position list need    set operations compute explicitly    represent instead   constituent position list  support check  perform  appropriate disjunctive  conjunctive search  need allocation   total elements  generate  filter purpose depend   constraint score level   allow mismatch  least    elements must  generate  guarantee   least one generate element  support   candidate region     generate elements     find empirically         efficient  allocate    elements  generate purpose   requirement  two support elements greatly reduce  number  candidate regions generate   span set method require  least    generate elements       exact  onemismatch constraints    use  detect   limit number  mismatch relative  read length   limit  total number  elements  span set elements  nonoverlapping   three shift     mod   ≤⌊  ⌋ therefore        indicate   span set method   apply  constraint level       ≤≤ ≤  ≤≤  ≤⌊⌋  ≥  complete set generation  filter  handle greater number  mismatch   detectable   span set method  employ  strategy base   complete set  overlap 12mers  complete set method work   constraint level   allow mismatch  long   read  candidate region   consecutive match  12mer   phase   many    one sufficient condition   consecutive match    number  mismatch  ≤⌊⌋    level  mismatch gsnap   exhaustive algorithm mean    guarantee  identify  report  available alignments   genome   many mismatch candidates  generate  perform  multiway merge  position list   read locations   single forward  single reverse complement pass keep track   read location  12mers  support  candidate region  pattern  support 12mers provide  lower bind  mismatch   read   support 12mers  read locations separate     minimum number  mismatch    ⌊⌋ fig    entire read   sum  lower bound   patternbased lower bind calculation fig  specifically   read  length    pattern  support 12mers  read locations  …   lower bind  mismatch  ∑i0n⌊pipi⌋    pnl  make  complete set method  efficient  note   merge process must process every position   position list   therefore  slow   nonspecific 12mers  extremely long position list    help localize  read   gain efficiency  ignore  nonspecific 12mers define currently    position list     time  mean position list length  lower bind formula must  modify accordingly  compensate   miss 12mers essentially  assume    support  strategy  potentially fail  align read  portion  read   nonspecific  repetitive nucleotide pattern  necessary  map  read  successfully align  read  program provide  option   greedy strategy   nonspecific  repetitive 12mers  initially ignore   subsequently include   alignment   find",4
93,Stampy,"Stampy: a statistical algorithm for sensitive and fast mapping of Illumina sequence reads
Building the hash table Stampy uses a new open‐addressing hashing algorithm to encode the genome (section 1.2). The hash table contains at most 229 long‐word (4 byte) entries, each entry consisting of a genomic coordinate and an additional 2 bits of auxiliary data to support fast searches at high load factors.  The hash table occupies 2 Gb, and to fit mammalian‐size genomes of about 3x109 nt into the table, only every fifth position is entered.   This scheme ensures that to store the position, 30 bits are sufficient, leaving room for 2 auxiliary bits required for the fast search algorithm (see section 1.2 for details).  At each eligible position, a “hash” is constructed from the 15bp DNA word observed at that position in the reference genome.  This is done by first encoding the 15 nucleotides into a 30‐bit word, and then dividing out the reverse‐complement symmetry by subjecting it to a transformation that maps words related by reverse‐complementing to the same 29‐bit word, and words that are not so related to distinct 29‐bit words.  To reduce clustering within the hash table, this word is further pseudo‐randomized by multiplying modulo a large prime.  As a side effect of this, the words 0x1fffffff and 0x1ffffffe become unused as hashes; these words therefore are available for use as flags (see below).  For smaller genomes, a smaller hash table is used, and the resulting hash is further reduced modulo a power of 2 to limit the hash values to the size of the hash table.  Note that for any size, different 15‐mers (unrelated by reverse complementation) may hash to the same value, although this is extremely unlikely for the full‐length hash table. To avoid entering extremely long hash chains related to repetitive sequence, all positions are first scanned and hashes occurring more than 200 times noted.  As candidate mapping positions from such chains are costly to consider, and are unlikely to result in a unique mapping position, their presence is flagged in the hash table by one of the unused words (0x1fffffff), and the actual locations are not entered into the hash table. To improve search times, hash chains are entered roughly in order of decreasing length – this ensures that longer chains are less frequently interrupted by other elements than shorter ones, reducing search times and optimizing cache use for the longer chains that are relatively frequently accessed.  We use a quadratic probing sequence that achieves a good balance of reducing clustering and making reasonably good use of caching1. In addition, for the longest chains we use linear probing to further optimize cache usage; because long chains are entered first, the issue of clustering is virtually absent, and cache usage for long chains is particularly poor under a quadratic probing scheme.  The presence of the second unused word, 0x1ffffffe,in the primary position of the probe sequence signals the use of linear probing; when the initial position is already occupied the algorithm falls back to quadratic probing. 1.2 An improved hash table supporting fast searches Because the scanning algorithm spends most of its time looking for 1‐difference matches (Section 1.3), many hash lookups performed by the mapper will be unsuccessful.  An unsuccessful search in a standard open addressing hash table is slow because the probe sequence is traversed until an empty slot is found, and the density of empty slots is low at high load factors.  Stampy uses a hash table that addresses this by adding flag bits indicating whether more entries exist in the chain.  A chain here is defined as the smallest prefix of a probe sequence that contains all elements inserted in a hash bucket.  As this flag bit will only be present for chains that include at least one element, an additional sentinel bit is used to ensure that the search algorithm does not enter empty chains. The order in which hash slots are probed is given by the probe sequence h(k,i). For standard hash tables this can be an arbitrary function of the object k and probe index i, as long as h(k, . ): {0,…,m}  {0,…,m} is a permutation.  However, the use of the sentinel bit now requires that the probe sequence be determined by the hash h'(k):=h(k,0), rather than by the object k.  Of the standard probe sequences, linear and quadratic probing satisfy this requirement, but double hashing does not.  Another choice, which minimizes clustering but has bad cache performance, is random hashing, which in its simplest form is h(k,i) = h'(k) + h''(i) mod n, where h''(i) is a permutation. To balance the opposing needs of low clustering and good cache behaviour, Stampy uses mainly quadratic probing, supplemented by linear probing for long chains, Scanning the read To find candidate locations for a single read, all overlapping 15mers in the read are considered.  In addition, every 1‐base mismatch (“1‐neighbour”) is considered.  For reads longer than 34 bp, 1‐neighbours are considered for a reduced fraction of initial 15‐mers; half of them for reads up to 49 bp, to a third for reads of 50bp and above.  Simulation experiments showed that this resulted in negligible loss of sensitivity, and a considerable reduction of computational time (data not shown). In addition, for 15mers that contain a single N character, all 4 possibilities for that position are considered, but no other 15mers are. Those 15mers that contain more than a single N character are not considered. 1.4 Searching the hash table If the initial hash table entry corresponding to the 15mer is flagged by the high‐ count flag, this 15mer is not further considered.  A repeat mask table with read locations that are not scanned for this reason is kept for later use.  (More specifically, when the read 15mer is marked as repetitive, the corresponding location is marked with a Phred score 0; for 1‐neighbours, it is marked by the Phred quality of the mutated base.  When more than one 15mer, mutated or otherwise, is repetitive, the minimum Phred score is used.  These values are used to calculate the mapping posterior, for details see section 1.8.  A Phred score is a representation of a probability p as an integer, using the formula ‐10log10 p.  For non‐repetitive 15mers, all positions in the genome that were entered in the hash table and match the 15mer or its reverse complement, together with their orientation, are retrieved. 1.5 Similarity filtering To avoid excessive numbers of potential candidates, a neighbourhood similarity filtering step is included at this stage.  A “fingerprint” is computed from three 4‐ nucleotide words close to but not overlapping the 15mer, and falling within the read.  The fingerprint comprises the counts of A, C and G nucleotides within those 12 positions.  The counts of the corresponding positions at the putative genomic location (in the implied orientation) are obtained, and the sum of absolute differences of A, C, G counts, and the implied absolute difference of T counts, is computed.  This similarity statistic has the property that it increases by at most 2 with every single‐nucleotide change, and every incremental 1bp insertion or deletion.  Longer insertions or deletions have the opportunity to cause more drastic changes to this statistic, but this potential problem is mitigated by the fact that most indels are copy number changes of short tandem repeats or homopolymer runs, to which this statistic is relatively insensitive. Only locations for which this fingerprint match value does not exceed a set threshold are considered.  This threshold is reduced by 2 for one‐mismatch 15mers, so that similar approximate mismatch thresholds are used for the 15+12 nucleotide positions that are considered, independent of whether the original read 15mer or one of its 1‐neighbours generated the candidate location. 1.6 SIMD alignment Locations that pass the similarity filter are then considered for full‐length alignment.  At this stage the number of candidates often exceeds 1000, so that the use of a highly efficient implementation of the alignment algorithm is imperative.  Stampy uses a Single instruction, multiple data (SIMD) implementation of banded affine‐gap alignment.  The implementation traverses the dynamic programming table diagonally, allowing an optimal exploitation of the parallelism provided by the x86 SIMD instructions.  The dynamic programming table is held in registers rather than in memory, so that expensive cache misses are avoided.  Since, in the first instance, the complete read is considered to derive from the reference, a Needleman‐Wunsch global alignment rather than a Smith‐Waterman local alignment is computed.  This requires aligning the possibly low‐quality end of the read containing relatively many mismatches; to handle such cases appropriately the algorithm computes a mapping quality score in Phred units that accounts for nucleotide quality scores, as well as gap opening and extension.  To the probability represented by the base quality Phred scores, a term corresponding to the expected divergence to the reference (by default equivalent to a probability of 10‐3 per nucleotide) is added, as for alignments no distinction ought to be made between read errors, polymorphisms and substitutions. The algorithm uses a 30bp diameter band, allowing insertions and deletions of up to 15bp to be fully considered at this stage.  The limited number of bits available in SIMD registers to accumulate the score puts a limitation on the maximum read length that can be considered in a single alignment; in our implementation, reads of up to 4500 bp can be considered. 1.7 Single­end reads: realignment Stampy reports the best‐matching candidate mapping location, or in case of ties, a deterministic pseudo‐random choice.  When the process of generating candidate mapping locations described above does not result in any candidate, or when the candidate is judged to be no more similar than a best match for random sequence, no mapping location reported. The best‐matching candidate is re‐aligned under the same model as used in the SIMD alignment, but now with a larger band of 60 bp diameter by default, enabling the correct identification of insertions or deletions (‘indels’) of up to 30 bp in length.  When the most likely alignment comes within 10 bp of the edge of the dynamic programming band, the read is again re‐aligned with double the bandwidth.  In this way alignment in the presence of large indels is improved without a large increase in computational time in the general case. The alignment algorithm at this stage allows flushing of indels to the leftmost or rightmost possible location; the desired behaviour can be selected by command‐ line options.  When desired alignment posteriors per alignment column can also be computed, through the use of the Forward and Backward algorithms (REF Durbin et al).  In certain cases this additional information allows potential indels at the edge of reads to be identified.  The computation of this information is relatively costly and is switched off by default. 1.8 Single­end reads: mapping posterior Stampy computes the mapping posterior Phred score, or “mapping quality”, in the standard Bayesian fashion by computing the posterior probability that the reported location is incorrect, in the first instance through the formula 1 ‐ P(read | Lopt ) / Σ P(read | Li ), (1) where Lopt is the maximum likelihood mapping location, and the sum runs over all candidates considered.  Because the alignment model considers read errors, single‐nucleotide polymorphisms and substitutions, and short indels, this accurately estimates the probability that a read is mapped incorrectly when such errors are caused by (near‐)repetitiveness in the genome, in combination with read errors and mutations.  The result is approximate because not all possible mapping locations are considered in the sum, and consequently the resulting posterior does not include the possibility that the correct mapping locations was not considered among the candidates Li.  Broadly, this may happen for three reasons: (i) the read contains highly repetitive sequence and its 15mers were not entered into the genome hash table; (ii) the read is of low quality and/or is divergent from the reference by single‐nucleotide and short indel mutations, so that every 15mer entered into the hash table has more than 1 nucleotide difference with the read, or (iii) the sequence is not represented in the reference. Stampy’s model accounts for all three possibilities.  Sections 1.9 and 1.10 each describe the model for (i) and (ii), for short and longer reads respectively. Section 1.11 describes the model for (iii). Adding the probabilities for these three events to the naïve posterior (1) results in the final mapping quality, which is reported as a Phred score capped at 99.  In addition, when (iii) is deemed likely, no mapping is reported for that read. As an example of the situations handled by this model, consider a read from a highly repetitive genomic region.  A read error within this read may cause a spurious hit elsewhere in the genome.  However, because of the repetitiveness, this location is not deemed to be reliable, since the cumulative likelihood of the repetitive loci (which each require one read error) can overcome the single higher likelihood of the spurious hit.  A similar situation occurs when the genome carries a mutation in an otherwise repetitive region; in this case, the read will be mapped correctly, but for the same reason as before, the map will not be deemed to be reliable. 1.9 Single­end reads: failing to find candidates – non­overlapping 15mers As described above, two reasons for failing to identify the correct candidate locations are that the read is highly repetitive and its 15mers have not been entered into the hash; or that read errors or divergence cause all read 15mers to be more than 1 mutation removed from the reference 15mer.  This section describes an algorithm to estimate the probability of either event occurring. Because the way the read is scanned changes with the length of the read as described in section 1.3, the model is dependent on the read length.  For simplicity only case (1) that 1‐neighbours are considered for all 15mers within the read, and case (2) that these are considered for only one‐third of the 15mers, are distinguished; the intermediate case in which 1‐neighbours of half of the 15mers are considered is approximated by the model for case (1). First, consider case (2), that 1‐neighbours for a third of read 15mers are considered.  More precisely, the algorithm considers 1‐neighbours for a consecutive block of 5 overlapping 15mers from the read; for the subsequent 10 only the read 15mers are considered, then again for a consecutive block of five 15mers the 1‐neighbours are considered, and so on.  The contribution to sensitivity of the 10 read 15mers are ignored in the model, as are the effects of indel mutations on sensitivity. Suppose that the correct mapping location for the whole read is L.  Because only 15mers from genomic loci that are multiples of 5 are entered into the genome hash table, only 15mers at read position k such that mod (L+k) = 0 have a possibility of matching.  From the algorithm described above, and under the stated assumptions, it follows that this happens precisely once in a block of five overlapping 15mers from the read; the next possibility occurs exactly 15 bp further on.  This means that the possible matches involve non‐overlapping adjacent 15mers.  If the offset k were known, the required probability is just the probability that all non‐overlapping adjacent read 15mers at offset k have 2 or more mutations, or that the corresponding 15mer in the reference is repetitive and not included in the hash table.  The final probability is computed by computing this compound probability for each offset k and taking the Bayesian average using a uniform prior of the possible offsets. The compound probability is just the product of the probabilities defined above for all 15mers at the offset k.  To calculate the relevant probability for one 15mer, write qi for the error probability of base i in the 15mer (i = 1,…,15).  The probability that no mutation (read error or substitution) occurs in the 15mer is z = Πi=115 (1‐qi) If we write p1 = Σi=115  qi / (1­qi) then the probability of two or more mutations w.r.t. the reference is 1 – z(1+p1). It remains to include the probability of missing a candidate because of repetitiveness.  If the read 15mer is marked as repetitive in the hash table, the probability is taken to be 1 – this is an approximation, as the possibility of the true reference 15mer not being repetitive, but having been mutated into a highly repetitive one is ignored.  If the read 15mer is not repetitive, but one of its 1‐ neighbours is, then the probability of the true reference 15mer being repetitive is estimated as the probability of the mutation having occurred.  While this is not correct in the strict Bayesian sense, as the prior of the read deriving from a repetitive sequence is not weighed explicitly against the alternative, simulation experiments show that this procedure is effective at reducing false positive rates and estimating well‐calibrated priors (see main text, and results below).  The data required for this procedure is collected at the scanning stage Single­end reads: failing to find candidates – overlapping 15mers This model deals with the case of relatively short reads, in which case 1‐ neighbours of each overlapping 15mer are considered.  The analysis of this situation is complicated because of the dependency structure introduced by the overlapping 15mers, and the non‐uniform distribution of read errors described by the quality scores. We approximate the situation by considering that in a contiguous subsection of the read, the probability of 0, 1, 2, 3 or 4 mutations is known; that 5 mutations or more do not occur; and that conditional on a certain number of mutations occurring, these are distributed uniformly along the subsection.  A simulation was used to estimate the probability that, conditional on the length of the subsection and the number of mutations occurring, every overlapping 15mer fully within the read subsection and at the (unspecified) offset k overlaps with 2 mutations or more, as only in this case would the correct candidate not be considered. In particular, independently of k, for 3 mutations, at least one 15mer is guaranteed to overlap at most 1 mutation in a read of length 34.  This can be seen by considering that in a length‐34 read, two non‐overlapping 15‐mers fall wholly within the read for any k; and for both these 15‐mers to not yield the correct candidate, at least 2 mutations are required in each.  Considering the algorithm in section 1.3 this means that Stampy is guaranteed to consider the correct mapping location for any read with 3 or fewer mutations in the first 34 bp.  For 4 mutations, this is true for reads of 52 bp and longer. More generally, the probability of not identifying the correct candidate is estimated as follows.  First, the probability of precisely m mutations among L positions, each independently having probability qi of mutating, is € qi 1 qim × (1− qi ) i∉{i 1 ∏ ,…,im }       1≤i ∑ 1 <<im <L = (1− qi) qik 1− qi k=1 k m ∏ 1≤i 1 <<im <L ∑ i=1 L ∏ (2) The last sum over m‐fold products is inefficient to calculate as written, particularly as m and L increase.  However, this sum may be recognized as the elementary m‐th degree symmetric polynomial em in the variables qi/(1‐qi), i=1,…,L.  These can be computed from the power sums, using Newton’s identities: Since the pi can be calculated efficiently, so can z em in (2), which represents the probability of precisely m mutations occurring among the L positions. For longer reads, it often occurs that sections are of poor quality.  This happens predominantly at the far end of the sequence, but low‐quality bases can occur anywhere in the read because of localized issues such as air bubbles in the flow cells, focusing or optical alignment problems.  In order not to obtain an overly pessimistic estimate of the probability of missing candidates, particularly for long reads, Stampy considers every 15‐ to 52‐bp subsection of the read, calculates the probabilities of 0 to 5 mutations following the algorithm above, and estimates the probability of missing the true candidate, conditional on finding it in the subsection under consideration.  Since the simulation assumes that fewer than 5 mutations occur, no subsections are considered where the probability of 5 or more mutations exceeds 0.1.  This cutoff was found to provide accurate estimates in practice (data not shown).  The final reported probability is the minimum of these probabilities over all subsections that are considered. The effect of repetitive 15mers is modelled by adding to qi the repeat mask probability obtained by the algorithm in section 1.3, similar to the procedure described in section 1.8. 1.11 Single­end reads: random matches The models for (i) and (ii) are Bayesian models, in the sense that a full description of the process that generated the reads is attempted.  However, when a read is not represented in the reference, for instance because it derived from a contaminant, an generative modeling approach fails.  Algorithmically, a unique best match may exist, but it will be biologically meaningless. To identify such cases, a hypothesis‐testing stage forms the last part of the mapping quality model.  This stage assesses the likelihood that the read is a random sequence, and that any sequence similarity of the candidate mapping results from finding the best match for this random sequence.  This question is complicated by the alignment procedure, which considerably increases the chances of finding fairly good matches at random. Suppose that the best alignment of a read of l nucleotides, n of which align to the reference with m mismatches, includes i insertions and d deletions.  The quintuplet (l,n,m,i,d) forms a summary of the complexity of the alignment.  To estimate the size of the search space, we need to compute the number of alignments of similar complexity. Consider the quintuplet to be fixed, and denote the set of all alignments characterized by the quintuplet, at a particular locus and strand x, by Ax.  Any alignment a ∈ Ax implies a particular sequence sa of n nucleotides at the aligning positions of the read.  Denote the set of all (locus, strand) pairs for the reference by G.  Suppose that the set S = { sa | a ∈ Ax , x ∈ G } may be considered to be drawn uniformly from the set of all sequences of length n.  Then, the likelihood of randomly obtaining a match that is as good as the best alignment that was in fact found, is , since this alignment implies n matches, and |S| loci and alignments have potentially been considered. The second approximation holds only if |S|4‐n ≪ 1. If |S|4‐n is of order 1 or greater, the likelihood of finding a random match of comparable quality is nearly 1, under the stated assumptions.  As mentioned above, in this case no mapping is reported. The formula above assumes that every alignment is equally probable; more specifically, that for a random read, mismatches; insertion starts and ends; and deletions all occur uniformly along the read.  We in addition assume that deletions can have any size uniformly from 0 to a set maximum, here 30.  To calculate |S|, note that there are m mismatches to be distributed among n aligning locations; 2i+d positions are to be chosen within the read of length l at which to start or end an alignment, or place a deletion; and d deletion lengths to be chosen; in addition the mismatching nucleotide at m position is to be chosen. In approximation, the total number of possibilities thus becomes, € 2g × 3m n m       l 2i + d      30d << 4n where g is the size of the reference, and the factor 2 accounts for the choice of strand.  This formula is approximate as it does not consider the fact that deletions cannot occur between insertion starts and ends; in practice such situations arise most often for incorrectly mapped reads, in which case an overestimation is conservative. Note that when insertions run off either end of the sequence, one less insertion start or end position needs to be chosen, and 2i+d should be replaced by 2i+d­1 in the formula above.  A similar consideration does not apply to deletions as they cannot occur at the read boundaries. Nucleotides are not uniformly chosen from the 4 possibilities, but rather often are biased somewhat towards A/T or G/C.  To account for this, in the formulas above we use b (and b‐1) instead of 4 (and 3), where b is somewhat less than 4 to account for any G/C bias, using the formula b = exp(‐f log ½f ‐ (1‐f) log ½(1‐f)), where f is the G+C content.  For instance, for f=0.35, a value b=3.8 is used. 1.12 Paired­end reads: paired­end candidates The paired‐end pathway follows the single‐end one up and including the point of calculating the single‐end mapping quality, for each of the reads independently. If no candidates were obtained for both reads, the paired‐end read is reported as unmapped. When the best locations for the single reads are close together on the genome (defined as an implied insert size within 4 standard deviations of the mean), the resulting paired‐end mapping positions are considered.  If in addition both single reads map sufficiently uniquely (each with a posterior probability of less than 1% of having been mapped incorrectly due to near‐repetitiveness), and in addition the estimated probability of not having found the correct candidate is sufficiently low for both (again less than 1%), the paired‐end mapping location is reported. When any of these conditions are not met, Stampy creates a shortlist of pairs of mapping locations.  From the candidate locations for each member of the read pair, the locations that together constitute 99.9% of the single‐read posterior mapping probability are extracted, up to a maximum of 20 locations, and subject to a minimum of 3.  For each of these locations, the mate is aligned against the reference around the location implied by the library insert size distribution, plus or minus 4 standard deviations.  The alignment model used is as for single‐end reads, but contains an additional term modeling the likelihood of the implied insert size, which helps to disambiguate the mapping position of locally repetitive reads.  In addition to this list of novel pairs, the pairing of the top‐ scoring single‐end mapping locations is added.  When these are not close together, a score corresponding to the prior probability of the physical insert overlapping the breakpoint of structural variation, which could give rise to such configurations, is added.  This prior probability is user‐specified, and is 3x10‐6 (Phred 55) by default. (Here, ‘close’ is defined as the distance at which the likelihood of the insert size under the insert size model, approximated by a Gaussian distribution, becomes less than the prior probability for a structural variant.) The posterior mapping quality is calculated as the product of the single‐end mapping qualities in case of the top‐scoring single‐end hits being selected as the pair, or the single‐end posterior of the anchoring read in other cases. ",Mapping,"stampy  statistical algorithm  sensitive  fast map  illumina sequence reads
building  hash table stampy uses  new open‐addressing hashing algorithm  encode  genome section   hash table contains    long‐word  byte entries  entry consisting   genomic coordinate   additional  bits  auxiliary data  support fast searches  high load factors   hash table occupies     fit mammalian‐size genomes   3x109    table  every fifth position  entered    scheme ensures   store  position  bits  sufficient leaving room   auxiliary bits required   fast search algorithm see section   details    eligible position  “hash”  constructed   15bp dna word observed   position   reference genome    done  first encoding   nucleotides   ‐bit word   dividing   reverse‐complement symmetry  subjecting    transformation  maps words related  reverse‐complementing    ‐bit word  words     related  distinct ‐bit words   reduce clustering within  hash table  word   pseudo‐randomized  multiplying modulo  large prime    side effect    words 0x1fffffff  0x1ffffffe become unused  hashes  words therefore  available  use  flags see    smaller genomes  smaller hash table  used   resulting hash   reduced modulo  power    limit  hash values   size   hash table  note    size different ‐mers unrelated  reverse complementation may hash    value although   extremely unlikely   full‐length hash table  avoid entering extremely long hash chains related  repetitive sequence  positions  first scanned  hashes occurring    times noted   candidate mapping positions   chains  costly  consider   unlikely  result   unique mapping position  presence  flagged   hash table  one   unused words 0x1fffffff   actual locations   entered   hash table  improve search times hash chains  entered roughly  order  decreasing length   ensures  longer chains  less frequently interrupted   elements  shorter ones reducing search times  optimizing cache use   longer chains   relatively frequently accessed   use  quadratic probing sequence  achieves  good balance  reducing clustering  making reasonably good use  caching1  addition   longest chains  use linear probing   optimize cache usage  long chains  entered first  issue  clustering  virtually absent  cache usage  long chains  particularly poor   quadratic probing scheme   presence   second unused word 0x1ffffffein  primary position   probe sequence signals  use  linear probing   initial position  already occupied  algorithm falls back  quadratic probing   improved hash table supporting fast searches   scanning algorithm spends    time looking  ‐difference matches section  many hash lookups performed   mapper   unsuccessful   unsuccessful search   standard open addressing hash table  slow   probe sequence  traversed   empty slot  found   density  empty slots  low  high load factors  stampy uses  hash table  addresses   adding flag bits indicating whether  entries exist   chain   chain   defined   smallest prefix   probe sequence  contains  elements inserted   hash bucket    flag bit    present  chains  include  least one element  additional sentinel bit  used  ensure   search algorithm   enter empty chains  order   hash slots  probed  given   probe sequence hki  standard hash tables     arbitrary function   object   probe index   long     {…}  {…}   permutation  however  use   sentinel bit  requires   probe sequence  determined   hash 'khk rather    object     standard probe sequences linear  quadratic probing satisfy  requirement  double hashing    another choice  minimizes clustering   bad cache performance  random hashing    simplest form  hki  '  '' mod   ''   permutation  balance  opposing needs  low clustering  good cache behaviour stampy uses mainly quadratic probing supplemented  linear probing  long chains scanning  read  find candidate locations   single read  overlapping 15mers   read  considered   addition every ‐base mismatch “‐neighbour”  considered   reads longer    ‐neighbours  considered   reduced fraction  initial ‐mers half    reads       third  reads  50bp    simulation experiments showed   resulted  negligible loss  sensitivity   considerable reduction  computational time data  shown  addition  15mers  contain  single  character   possibilities   position  considered    15mers   15mers  contain    single  character   considered  searching  hash table   initial hash table entry corresponding   15mer  flagged   high‐ count flag  15mer    considered   repeat mask table  read locations    scanned   reason  kept  later use   specifically   read 15mer  marked  repetitive  corresponding location  marked   phred score   ‐neighbours   marked   phred quality   mutated base     one 15mer mutated  otherwise  repetitive  minimum phred score  used   values  used  calculate  mapping posterior  details see section    phred score   representation   probability    integer using  formula ‐10log10    non‐repetitive 15mers  positions   genome   entered   hash table  match  15mer   reverse complement together   orientation  retrieved  similarity filtering  avoid excessive numbers  potential candidates  neighbourhood similarity filtering step  included   stage   “fingerprint”  computed  three ‐ nucleotide words close    overlapping  15mer  falling within  read   fingerprint comprises  counts      nucleotides within   positions   counts   corresponding positions   putative genomic location   implied orientation  obtained   sum  absolute differences     counts   implied absolute difference   counts  computed   similarity statistic   property   increases      every single‐nucleotide change  every incremental 1bp insertion  deletion  longer insertions  deletions   opportunity  cause  drastic changes   statistic   potential problem  mitigated   fact   indels  copy number changes  short tandem repeats  homopolymer runs    statistic  relatively insensitive  locations    fingerprint match value   exceed  set threshold  considered   threshold  reduced    one‐mismatch 15mers   similar approximate mismatch thresholds  used    nucleotide positions   considered independent  whether  original read 15mer  one   ‐neighbours generated  candidate location  simd alignment locations  pass  similarity filter   considered  full‐length alignment    stage  number  candidates often exceeds     use   highly efficient implementation   alignment algorithm  imperative  stampy uses  single instruction multiple data simd implementation  banded affine‐gap alignment   implementation traverses  dynamic programming table diagonally allowing  optimal exploitation   parallelism provided   x86 simd instructions   dynamic programming table  held  registers rather   memory   expensive cache misses  avoided  since   first instance  complete read  considered  derive   reference  needleman‐wunsch global alignment rather   smith‐waterman local alignment  computed   requires aligning  possibly low‐quality end   read containing relatively many mismatches  handle  cases appropriately  algorithm computes  mapping quality score  phred units  accounts  nucleotide quality scores  well  gap opening  extension    probability represented   base quality phred scores  term corresponding   expected divergence   reference  default equivalent   probability  ‐ per nucleotide  added   alignments  distinction ought   made  read errors polymorphisms  substitutions  algorithm uses  30bp diameter band allowing insertions  deletions    15bp   fully considered   stage   limited number  bits available  simd registers  accumulate  score puts  limitation   maximum read length    considered   single alignment   implementation reads        consider  single­end reads realignment stampy reports  best‐matching candidate mapping location   case  ties  deterministic pseudo‐random choice    process  generating candidate mapping locations described    result   candidate    candidate  judged     similar   best match  random sequence  mapping location reported  best‐matching candidate  ‐aligned    model  used   simd alignment     larger band    diameter  default enabling  correct identification  insertions  deletions indels       length     likely alignment comes within     edge   dynamic programming band  read   ‐aligned  double  bandwidth    way alignment   presence  large indels  improved without  large increase  computational time   general case  alignment algorithm   stage allows flushing  indels   leftmost  rightmost possible location  desired behaviour   selected  command‐ line options   desired alignment posteriors per alignment column  also  computed   use   forward  backward algorithms ref durbin     certain cases  additional information allows potential indels   edge  reads   identified   computation   information  relatively costly   switched   default  single­end reads mapping posterior stampy computes  mapping posterior phred score  “mapping quality”   standard bayesian fashion  computing  posterior probability   reported location  incorrect   first instance   formula  ‐ pread  lopt    pread      lopt   maximum likelihood mapping location   sum runs   candidates considered    alignment model considers read errors single‐nucleotide polymorphisms  substitutions  short indels  accurately estimates  probability   read  mapped incorrectly   errors  caused  near‐repetitiveness   genome  combination  read errors  mutations   result  approximate    possible mapping locations  considered   sum  consequently  resulting posterior   include  possibility   correct mapping locations   considered among  candidates   broadly  may happen  three reasons   read contains highly repetitive sequence   15mers   entered   genome hash table   read   low quality andor  divergent   reference  single‐nucleotide  short indel mutations   every 15mer entered   hash table     nucleotide difference   read  iii  sequence   represented   reference stampys model accounts   three possibilities  sections     describe  model      short  longer reads respectively section  describes  model  iii adding  probabilities   three events   naïve posterior  results   final mapping quality   reported   phred score capped     addition  iii  deemed likely  mapping  reported   read   example   situations handled   model consider  read   highly repetitive genomic region   read error within  read may cause  spurious hit elsewhere   genome  however    repetitiveness  location   deemed   reliable since  cumulative likelihood   repetitive loci   require one read error  overcome  single higher likelihood   spurious hit   similar situation occurs   genome carries  mutation   otherwise repetitive region   case  read   mapped correctly     reason    map    deemed   reliable  single­end reads failing  find candidates  non­overlapping 15mers  described  two reasons  failing  identify  correct candidate locations    read  highly repetitive   15mers    entered   hash   read errors  divergence cause  read 15mers      mutation removed   reference 15mer   section describes  algorithm  estimate  probability  either event occurring   way  read  scanned changes   length   read  described  section   model  dependent   read length   simplicity  case   ‐neighbours  considered   15mers within  read  case     considered   one‐third   15mers  distinguished  intermediate case   ‐neighbours  half   15mers  considered  approximated   model  case  first consider case   ‐neighbours   third  read 15mers  considered   precisely  algorithm considers ‐neighbours   consecutive block   overlapping 15mers   read   subsequent    read 15mers  considered     consecutive block  five 15mers  ‐neighbours  considered      contribution  sensitivity    read 15mers  ignored   model    effects  indel mutations  sensitivity suppose   correct mapping location   whole read      15mers  genomic loci   multiples    entered   genome hash table  15mers  read position    mod      possibility  matching    algorithm described     stated assumptions  follows   happens precisely    block  five overlapping 15mers   read  next possibility occurs exactly       means   possible matches involve non‐overlapping adjacent 15mers    offset   known  required probability    probability   non‐overlapping adjacent read 15mers  offset      mutations    corresponding 15mer   reference  repetitive   included   hash table   final probability  computed  computing  compound probability   offset   taking  bayesian average using  uniform prior   possible offsets  compound probability    product   probabilities defined    15mers   offset    calculate  relevant probability  one 15mer write    error probability  base    15mer   …   probability   mutation read error  substitution occurs   15mer     ‐   write       ­   probability  two   mutations wrt  reference    zp1  remains  include  probability  missing  candidate   repetitiveness    read 15mer  marked  repetitive   hash table  probability  taken        approximation   possibility   true reference 15mer   repetitive    mutated   highly repetitive one  ignored    read 15mer   repetitive  one   ‐ neighbours    probability   true reference 15mer  repetitive  estimated   probability   mutation  occurred      correct   strict bayesian sense   prior   read deriving   repetitive sequence   weighed explicitly   alternative simulation experiments show   procedure  effective  reducing false positive rates  estimating well‐calibrated priors see main text  results    data required   procedure  collected   scanning stage single­end reads failing  find candidates  overlapping 15mers  model deals   case  relatively short reads   case ‐ neighbours   overlapping 15mer  considered   analysis   situation  complicated    dependency structure introduced   overlapping 15mers   non‐uniform distribution  read errors described   quality scores  approximate  situation  considering    contiguous subsection   read  probability        mutations  known   mutations     occur   conditional   certain number  mutations occurring   distributed uniformly along  subsection   simulation  used  estimate  probability  conditional   length   subsection   number  mutations occurring every overlapping 15mer fully within  read subsection    unspecified offset  overlaps   mutations       case would  correct candidate   consider  particular independently     mutations  least one 15mer  guaranteed  overlap    mutation   read  length      seen  considering    length‐ read two non‐overlapping ‐mers fall wholly within  read        ‐mers   yield  correct candidate  least  mutations  required    considering  algorithm  section   means  stampy  guaranteed  consider  correct mapping location   read    fewer mutations   first      mutations   true  reads     longer  generally  probability   identifying  correct candidate  estimated  follows  first  probability  precisely  mutations among  positions  independently  probability   mutating  €   qim     ∉{  ∏ … }       ≤ ∑       qik      ∏ ≤    ∑   ∏   last sum  ‐fold products  inefficient  calculate  written particularly     increase  however  sum may  recognized   elementary ‐ degree symmetric polynomial    variables ‐ …     computed   power sums using newtons identities since     calculated efficiently        represents  probability  precisely  mutations occurring among   positions  longer reads  often occurs  sections   poor quality   happens predominantly   far end   sequence  low‐quality bases  occur anywhere   read   localized issues   air bubbles   flow cells focusing  optical alignment problems   order   obtain  overly pessimistic estimate   probability  missing candidates particularly  long reads stampy considers every ‐  ‐ subsection   read calculates  probabilities     mutations following  algorithm   estimates  probability  missing  true candidate conditional  finding    subsection  consideration  since  simulation assumes  fewer   mutations occur  subsections  considered   probability     mutations exceeds    cutoff  found  provide accurate estimates  practice data  shown   final reported probability   minimum   probabilities   subsections   considered  effect  repetitive 15mers  modelled  adding    repeat mask probability obtained   algorithm  section  similar   procedure described  section   single­end reads random matches  models      bayesian models   sense   full description   process  generated  reads  attempted  however   read   represented   reference  instance   derived   contaminant  generative modeling approach fails  algorithmically  unique best match may exist     biologically meaningless  identify  cases  hypothesis‐testing stage forms  last part   mapping quality model   stage assesses  likelihood   read   random sequence    sequence similarity   candidate mapping results  finding  best match   random sequence   question  complicated   alignment procedure  considerably increases  chances  finding fairly good matches  random suppose   best alignment   read   nucleotides    align   reference   mismatches include  insertions   deletions   quintuplet lnmid forms  summary   complexity   alignment   estimate  size   search space  need  compute  number  alignments  similar complexity consider  quintuplet   fixed  denote  set   alignments characterized   quintuplet   particular locus  strand      alignment  ∈  implies  particular sequence    nucleotides   aligning positions   read  denote  set   locus strand pairs   reference    suppose   set   {    ∈    ∈  } may  considered   drawn uniformly   set   sequences  length     likelihood  randomly obtaining  match    good   best alignment    fact found   since  alignment implies  matches   loci  alignments  potentially  considered  second approximation holds   ‐ ≪   ‐   order   greater  likelihood  finding  random match  comparable quality  nearly    stated assumptions   mentioned    case  mapping  reported  formula  assumes  every alignment  equally probable  specifically    random read mismatches insertion starts  ends  deletions  occur uniformly along  read    addition assume  deletions    size uniformly     set maximum     calculate  note     mismatches   distributed among  aligning locations 2id positions    chosen within  read  length     start  end  alignment  place  deletion   deletion lengths   chosen  addition  mismatching nucleotide   position    chosen  approximation  total number  possibilities thus becomes €                            size   reference   factor  accounts   choice  strand   formula  approximate     consider  fact  deletions cannot occur  insertion starts  ends  practice  situations arise  often  incorrectly mapped reads   case  overestimation  conservative note   insertions run  either end   sequence one less insertion start  end position needs   chosen  2id   replaced  2id­   formula    similar consideration   apply  deletions   cannot occur   read boundaries nucleotides   uniformly chosen    possibilities  rather often  biased somewhat towards      account     formulas   use   ‐ instead        somewhat less    account    bias using  formula   exp‐ log  ‐ ‐ log ‐      content   instance    value   used  paired­end reads paired­end candidates  paired‐end pathway follows  single‐end one   including  point  calculating  single‐end mapping quality     reads independently   candidates  obtained   reads  paired‐end read  reported  unmapped   best locations   single reads  close together   genome defined   implied insert size within  standard deviations   mean  resulting paired‐end mapping positions  considered    addition  single reads map sufficiently uniquely    posterior probability  less      mapped incorrectly due  near‐repetitiveness   addition  estimated probability    found  correct candidate  sufficiently low    less    paired‐end mapping location  report     conditions   met stampy creates  shortlist  pairs  mapping locations    candidate locations   member   read pair  locations  together constitute    single‐read posterior mapping probability  extracted    maximum   locations  subject   minimum        locations  mate  aligned   reference around  location implied   library insert size distribution plus  minus  standard deviations   alignment model used    single‐end reads  contains  additional term modeling  likelihood   implied insert size  helps  disambiguate  mapping position  locally repetitive reads   addition   list  novel pairs  pairing   top‐ scoring single‐end mapping locations  added      close together  score corresponding   prior probability   physical insert overlapping  breakpoint  structural variation  could give rise   configurations  added   prior probability  user‐specified   3x10‐ phred   default  close  defined   distance    likelihood   insert size   insert size model approximated   gaussian distribution becomes less   prior probability   structural variant  posterior mapping quality  calculated   product   single‐end mapping qualities  case   top‐scoring single‐end hits  selected   pair   single‐end posterior   anchoring read   cases ",4
94,Shrimp2,"SHRiMP2: sensitive yet practical SHort Read Mapping
SHRiMP2 indexes the genome using multiple spaced seeds, projecting each read to identify candidate mapping locations (CMLs), and ultimately investigating these CMLs with the Smith–Waterman algorithm. A major difference between the original SHRiMP and SHRiMP2 is that the former indexed the reads; switching to a genome index [similar to other read mappers, e.g. Langmead et al. (2009); Li and Durbin (2009); Wu and Nacu (2010)] resulted in a dramatic speed increase and further allowed us to add a paired mapping mode and utilize multi-threaded computation. For more details on the methods described below, see the original SHRiMP paper (Rumble et al., 2009), as well as the supplement. Genome Index: SHRiMP2 starts by projecting the reference genome using several spaced seeds (Ilie and Ilie, 2007). Each seed is applied at each genome location, obtaining a (spaced) k-mer. For every seed and every k-mer, the genome index contains a list of locations where that k-mer can be found using that seed. Ubiquitous k-mers (with very long lists) are discarded, as they do not help identify CMLs. RAM Usage: The genome index is loaded in RAM, and lookups are performed while running through the read set. The index of a genome of length n with k seeds of weight w takes k× 4w×12+n×4 bytes. With the default parameters (k =4, w=12), the index of the human genome (hg19) takes 48 GB. SHRiMP2 provides tools to break a genome into pieces that fit in a target RAM size. The overhead introduced by splitting is insignificant: as demonstrated in the supplementary Material, using one node with 16 GB and a 4-way split of hg19 versus one node with 32 GB and a 2-way split results in ∼2% slowdown. Projecting the Reads: Several threads are used to map the reads in parallel. Each read is projected using the spaced seeds, and the genome locations where those k-mers appear are looked up in the index. These k-mers are the matching diagonals in the matrix where the genome is laid out on the x axis and the read on the y axis. Generating CMLs: Given a length and a score, the list of matching diagonals is scanned for genomic windows of the given length where an alignment with the given score (between the read and the genome) can be constructed from two diagonals, and a CML is generated for every such window. This process is analogous to q-gram filters (Rasmussen et al., 2006). The CML generation step is one of the major differences between BFAST and SHRiMP: while BFAST uses a larger number of long seeds, and generates CMLs based on a single seed match, SHRiMP2 (and the original SHRiMP) requires multiple seed matches between the read and the reference. This allows for the effective use of seeds with smaller weight and length, and improves sensitivity. Paired Mapping Mode: In this mode, the reads in every pair are analyzed and mapped together: a CML for one is analyzed only if a CML for the other exists within a specified range of the first. A ‘rescue’ mode is available to re-map pairs with anomalous spacing. Smith–Waterman Alignment: The CMLs are eventually investigated by the Smith–Waterman (SW) string matching algorithm (Smith and Waterman, 1981), Similar to the original version of SHRiMP, SHRiMP2 supports full alignment (with indels) of both letter space and color-space data. For SOLiD reads we align the genome to the four possible ‘translations’ of the read, thus allowing for sequencing errors (see Homer et al., 2009; Rumble et al., 2009). SHRiMP2 uses a caching heuristic to speed up the alignment of reads from repetitive regions: after alignment, we compute a hash of the target region, and store it together with the score. Before starting a SW, we first check if an identical region has already been aligned, and if so just reuse the score. 3 RESULTS AND DISCUSSION We compared SHRiMP2 to three other leading read mapping programs: BFAST (Homer et al., 2009), BOWTIE (Langmead et al., 2009) and BWA (Li and Durbin, 2009). We generated 2 datasets, each containing 6 000 000 paired color-space reads, of 50 and 75 bp, respectively, simulated from the human chromosome 1. The reads contain variants (SNPs and indels), as well as sequencing errors distributed according to typical (non-uniform) error profiles of the SOLiD machine (4% average per-color error rate). We mapped both datasets as both paired and single-end reads. A read (pair) is mapped ‘uniquely’ if the mapping with the highest score is unique. This mapping is ‘correct’ if it is within 10 bp of the location where the read (or both reads in the pair) was simulated from. We define recall as the fraction of all reads (pairs) that are mapped correctly, and precision as the fraction of all uniquely mapped reads (pairs) that are mapped correctly. In Figure 1A we present precision and recall of each algorithm, and in Figure 1B we demonstrate the runtimes for each tool on the datasets. Of all the other short read mapping programs, we found that BFAST is the only one directly comparable to SHRiMP2 in providing high sensitivity even for highly polymorphic reads, practical speed and wealth of features. In our tests, SHRiMP2 achieves similar or better sensitivity for all polymorphism classes, with a running time that is 2–5 times faster than BFAST. While we include BOWTIE and BWA in the comparison, these programs primarily target speed, and do not match the sensitivity of SHRiMP2 or BFAST for highly polymorphic reads. We also evaluated the speed of SHRiMP2 on real AB SOLiD data. We estimate that a 30× coverage of hg19 by unpaired 50 bp color space reads can be mapped by 20 nodes, each with 8 cores and 16 GB of RAM, in 3 days",Mapping,"shrimp2 sensitive yet practical short read mapping
shrimp2 index  genome use multiple space seed project  read  identify candidate map locations cmls  ultimately investigate  cmls   smithwaterman algorithm  major difference   original shrimp  shrimp2    former index  read switch   genome index similar   read mappers  langmead      durbin    nacu  result   dramatic speed increase   allow   add  pair map mode  utilize multithreaded computation   detail   methods describe  see  original shrimp paper rumble     well   supplement genome index shrimp2 start  project  reference genome use several space seed ilie  ilie   seed  apply   genome location obtain  space kmer  every seed  every kmer  genome index contain  list  locations   kmer   find use  seed ubiquitous kmers   long list  discard     help identify cmls ram usage  genome index  load  ram  lookups  perform  run   read set  index   genome  length    seed  weight  take  4wn bytes   default parameters     index   human genome hg19 take   shrimp2 provide tool  break  genome  piece  fit   target ram size  overhead introduce  split  insignificant  demonstrate   supplementary material use one node      way split  hg19 versus one node      way split result   slowdown project  read several thread  use  map  read  parallel  read  project use  space seed   genome locations   kmers appear  look    index  kmers   match diagonals   matrix   genome  lay     axis   read    axis generate cmls give  length   score  list  match diagonals  scan  genomic windows   give length   alignment   give score   read   genome   construct  two diagonals   cml  generate  every  window  process  analogous  qgram filter rasmussen     cml generation step  one   major differences  bfast  shrimp  bfast use  larger number  long seed  generate cmls base   single seed match shrimp2   original shrimp require multiple seed match   read   reference  allow   effective use  seed  smaller weight  length  improve sensitivity pair map mode   mode  read  every pair  analyze  map together  cml  one  analyze    cml    exist within  specify range   first  rescue mode  available  remap pair  anomalous space smithwaterman alignment  cmls  eventually investigate   smithwaterman  string match algorithm smith  waterman  similar   original version  shrimp shrimp2 support full alignment  indels   letter space  colorspace data  solid read  align  genome   four possible translations   read thus allow  sequence errors see homer    rumble    shrimp2 use  cache heuristic  speed   alignment  read  repetitive regions  alignment  compute  hash   target region  store  together   score  start    first check   identical region  already  align     reuse  score  result  discussion  compare shrimp2  three  lead read map program bfast homer    bowtie langmead     bwa   durbin   generate  datasets  contain    pair colorspace read      respectively simulate   human chromosome   read contain variants snps  indels  well  sequence errors distribute accord  typical nonuniform error profile   solid machine  average percolor error rate  map  datasets   pair  singleend read  read pair  map uniquely   map   highest score  unique  map  correct    within     location   read   read   pair  simulate   define recall   fraction   read pair   map correctly  precision   fraction   uniquely map read pair   map correctly  figure   present precision  recall   algorithm   figure   demonstrate  runtimes   tool   datasets     short read map program  find  bfast    one directly comparable  shrimp2  provide high sensitivity even  highly polymorphic read practical speed  wealth  feature   test shrimp2 achieve similar  better sensitivity   polymorphism class   run time    time faster  bfast   include bowtie  bwa   comparison  program primarily target speed    match  sensitivity  shrimp2  bfast  highly polymorphic read  also evaluate  speed  shrimp2  real  solid data  estimate    coverage  hg19  unpaired   color space read   map   nod    core     ram   days",4
95,mrsFAST,"mrsFAST-Ultra: a compact, SNP-aware mapper for high performance sequencing applications
mrsFAST-Ultra is a seed and extend aligner in the sense that it works in two main stages: (i) it builds an index from the reference genome for exact ‘anchor’ matching and (ii) it computes all anchor matchings for each of the reads in the reference genome through the index and extends each match to both left and right; it reports the overall alignment if it is within the user defined error threshold. Indexing In the indexing step, mrsFAST-Ultra slides a window of size k = r/(e + 1) (where r is the read length and e is the user defined error threshold) through the reference genome and identifies all occurrences of each k-mer present in the genome. For small values of k, mrsFAST-Ultra's genome index is an array of all possible k-mers in lexicographic order. For each k-mer, the index keeps an array of all locations the k-mer is observed in the reference genome. In case the value of k is prohibitively large, only a prefix of user defined size ℓ (for each k-mer) is used for indexing. For each such ℓ-mer, its locations on the reference genome are then sorted with respect to the k − ℓ-mers following it. (In fact, for most applications, even keeping track of all k − ℓ-mers following a particular ℓ-mer is not necessary: we just hash these k − ℓ-mers via a simple checksum scheme.) For further compacting the index, the reference genome itself is first converted to a 3 bit per base encoding. The genome sequence is stored in 8 byte long machine words implying that each machine word contains 21 bases. In addition, the index of the reference genome actually does not keep every occurrence of each k-mer, but rather keeps how many occurrences of each k-mer is present in the genome. The actual locations of the k-mers (seeds) are recalculated each time the reference is loaded. This reduces the I/O requirements of mrsFAST-Ultra significantly. One may think that such a set up would increase the overall running time of the search step but the savings from I/O reduction significantly offsets the cost of recalculating the k-mer locations on the fly. Overall, the storage requirement of the index we construct for the human reference genome is 2GB, including the reference genome sequence itself. This represents a 10-fold improvement in the index storage requirement of the original mrsFAST. Search In this step, mrsFAST-Ultra processes the reads from an input HTS data set and computes ‘all’ locations on the reference genome that can be aligned to each read within the user-defined error threshold e. mrsFAST-Ultra is a fully sensitive aligner meaning that it guarantees to find and report all mapping locations of a given read within e mismatches. mrsFAST-Ultra achieves this by partitioning the read into e + 1 non-overlapping fragments of length k for a given error threshold e. Due to the pigeon hole principle, at least one of these fragments should have an exactly matching k-mer of the reference genome in each location the read can be mapped to. The search step then validates whether each location of the reference genome with an exact k-mer match of the read is indeed a mapping location. In order to perform the search step as fast as possible, mrsFAST-Ultra loads the genome index (see above) to the main memory and computes the locations of each k-mer on-the fly—for significant savings in I/O. For each k-mer, the number of locations in the reference genome is already stored in the index, thus we can preallocate the required memory for each array that keeps the locations of a given k-mer. Once this extended reference genome index is set up in the main memory, the remaining memory is allocated for the reads. At each subsequent stage, mrsFAST-Ultra retrieves sufficiently many (unprocessed) reads that can fit in the main memory and searches them in the reference genome simultaneously. (Alternatively, the user can specify an upper bound on the memory usage.) These reads are also indexed with respect to the e + 1 non-overlapping fragments of size k it extracts from each read. Basically, for each possible fragment of length k, the read index keeps the read ID, the fragment number and the direction the fragment is observed in the read. Once the read index is set, it is compared to the reference genome index, in a divide and conquer fashion as per mrsFAST, in order to achieve cache obliviousness. In other words, for each possible k-mer, the list of its occurrences in the reference genome is compared against the list of its occurrences among the reads in a divide-and-conquer fashion (rather than linear fashion) to ensure an optimal cache performance at any level of the cache structure, within a factor 2 (14). Because mrsFAST-Ultra aims to be fully sensitive, it needs to verify whether each reference genome location and each corresponding read that have the same k-mer have indeed an alignment within the user defined error tolerance. Note that, the value of k, set to r/(e + 1) can be too big for creating an index that has an entry for every possible k-mer from the four letter deoxyribonucleic acid (DNA) alphabet. Thus, the primary indexing is performed on a prefix of length ℓ = 12 for each k-mer and all locations/reads that share this prefix are further sorted according to the k − ℓ-mer succeeding this prefix. This is achieved by hashing the k − ℓ-mer through a simple checksum scheme. As a result, the divide-and-conquer comparison of reference genome locations and reads is performed on those entries that have the same ℓ-mer and the same checksum value for the succeeding k − ℓ-mer. The comparison for each genomic location and a read involves the calculation of the Hamming distance between the read and the k-mer location in the genome, extended by the appropriate length towards left and right. Before calculating the Hamming distance, mrsFAST-Ultra applies another filter that compares the number of As, Cs, Gs and Ts in the read and the genomic locus; if the total number of symbol differences is more than 2e, then we do not need to compute the Hamming distance explicitly as it will be at least e + 1—above the error threshold. In comparison to the original mrsFAST, our new search strategy significantly reduces the number of Hamming distance calculations that is the main bottleneck for the search step. When combined with reduced I/O (due to compact index representation) and the introduction of new filters, this implies a five factor reduction in the overall running time of search. SNP awareness The user has the option of setting mrsFAST-Ultra to tolerate known SNP locations in the mappings: i.e. in this mode, SNPs in an alignment location simply do not contribute to the error count in the Hamming distance computation provided that a SNP location's base quality is above user-defined threshold and it is matching the alternate allele. For this feature, mrsFAST-Ultra parses dbSNP file in VCF4 format (35) and generates a compact structure that it uses for mapping. Although conceptually simple, this feature is highly desired by users as it significantly reduces the number of reads that can not be mapped to anywhere in the reference genome. In this mode, mrsFAST-Ultra reports the number of SNPs in addition to the number of mismatches per mapping location. Best and limited mapping mrsFAST-Ultra provides the user the option of returning a single best mapping locus per read—which it performs much faster than computing all mapping loci. As per BWA, Bowtie2, SRmapper and others, a best mapping location (on the reference genome) is considered to be one which has the smallest number of differences with the read and in the case of a tie one is chosen at random and assigned a low mapping quality. In addition, mrsFAST-Ultra has the option to return only mapping loci of reads which map to at most n locations within the user-defined error threshold. These features help the users to control the mapping multiplicity—which can grow prohibitively for further downstream analysis. Parallelization mrsFAST-Ultra is designed to utilize the parallelism offered by contemporary multicore architectures. The mapping task is simply partitioned into independent threads each of which is executed by a single core. For efficiency purposes, the only locks used by the threads are for allocating memory and I/O.",Mapping,"mrsfastultra  compact snpaware mapper  high performance sequence applications
mrsfastultra   seed  extend aligner   sense   work  two main stag   build  index   reference genome  exact anchor match    compute  anchor match     read   reference genome   index  extend  match   leave  right  report  overall alignment    within  user define error threshold index   index step mrsfastultra slide  window  size          read length     user define error threshold   reference genome  identify  occurrences   kmer present   genome  small value   mrsfastultra' genome index   array   possible kmers  lexicographic order   kmer  index keep  array   locations  kmer  observe   reference genome  case  value    prohibitively large   prefix  user define size    kmer  use  index    ℓmer  locations   reference genome   sort  respect     ℓmers follow   fact   applications even keep track     ℓmers follow  particular ℓmer   necessary   hash    ℓmers via  simple checksum scheme   compact  index  reference genome   first convert    bite per base encode  genome sequence  store   byte long machine word imply   machine word contain  base  addition  index   reference genome actually   keep every occurrence   kmer  rather keep  many occurrences   kmer  present   genome  actual locations   kmers seed  recalculate  time  reference  load  reduce   requirements  mrsfastultra significantly one may think    set  would increase  overall run time   search step   save   reduction significantly offset  cost  recalculate  kmer locations   fly overall  storage requirement   index  construct   human reference genome  2gb include  reference genome sequence   represent  fold improvement   index storage requirement   original mrsfast search   step mrsfastultra process  read   input hts data set  compute  locations   reference genome    align   read within  userdefined error threshold  mrsfastultra   fully sensitive aligner mean   guarantee  find  report  map locations   give read within  mismatch mrsfastultra achieve   partition  read     nonoverlapping fragment  length    give error threshold  due   pigeon hole principle  least one   fragment    exactly match kmer   reference genome   location  read   map   search step  validate whether  location   reference genome   exact kmer match   read  indeed  map location  order  perform  search step  fast  possible mrsfastultra load  genome index see    main memory  compute  locations   kmer onthe fly— significant save     kmer  number  locations   reference genome  already store   index thus   preallocate  require memory   array  keep  locations   give kmer   extend reference genome index  set    main memory  remain memory  allocate   read   subsequent stage mrsfastultra retrieve sufficiently many unprocessed read   fit   main memory  search    reference genome simultaneously alternatively  user  specify  upper bind   memory usage  read  also index  respect      nonoverlapping fragment  size   extract   read basically   possible fragment  length   read index keep  read   fragment number   direction  fragment  observe   read   read index  set   compare   reference genome index   divide  conquer fashion  per mrsfast  order  achieve cache obliviousness   word   possible kmer  list   occurrences   reference genome  compare   list   occurrences among  read   divideandconquer fashion rather  linear fashion  ensure  optimal cache performance   level   cache structure within  factor    mrsfastultra aim   fully sensitive  need  verify whether  reference genome location   correspond read     kmer  indeed  alignment within  user define error tolerance note   value   set        big  create  index    entry  every possible kmer   four letter deoxyribonucleic acid dna alphabet thus  primary index  perform   prefix  length      kmer   locationsreads  share  prefix   sort accord     ℓmer succeed  prefix   achieve  hash    ℓmer   simple checksum scheme   result  divideandconquer comparison  reference genome locations  read  perform   entries     ℓmer    checksum value   succeed   ℓmer  comparison   genomic location   read involve  calculation   ham distance   read   kmer location   genome extend   appropriate length towards leave  right  calculate  ham distance mrsfastultra apply another filter  compare  number         read   genomic locus   total number  symbol differences         need  compute  ham distance explicitly      least   —  error threshold  comparison   original mrsfast  new search strategy significantly reduce  number  ham distance calculations    main bottleneck   search step  combine  reduce  due  compact index representation   introduction  new filter  imply  five factor reduction   overall run time  search snp awareness  user   option  set mrsfastultra  tolerate know snp locations   mappings    mode snps   alignment location simply   contribute   error count   ham distance computation provide   snp location' base quality   userdefined threshold    match  alternate allele   feature mrsfastultra parse dbsnp file  vcf4 format   generate  compact structure   use  map although conceptually simple  feature  highly desire  users   significantly reduce  number  read     map  anywhere   reference genome   mode mrsfastultra report  number  snps  addition   number  mismatch per map location best  limit map mrsfastultra provide  user  option  return  single best map locus per read—  perform much faster  compute  map loci  per bwa bowtie2 srmapper  others  best map location   reference genome  consider   one    smallest number  differences   read    case   tie one  choose  random  assign  low map quality  addition mrsfastultra   option  return  map loci  read  map     locations within  userdefined error threshold  feature help  users  control  map multiplicity—  grow prohibitively   downstream analysis parallelization mrsfastultra  design  utilize  parallelism offer  contemporary multicore architectures  map task  simply partition  independent thread     execute   single core  efficiency purpose   lock use   thread   allocate memory  ",4
96,BAM-matcher,"BAM-matcher: a tool for rapid NGS sample matching 
To facilitate sample matching using existing NGS data, we have developed BAM-matcher, a tool that provides rapid pair-wise comparison of binary Sequence Alignment/Map (BAM) files ( http://samtools.github.io/hts-specs ) by comparing the sample genotypes at pre-determined genomic locations. BAM-matcher has the following features: first, it is easy to use and can be deployed at early stages of processing pipelines. Second, BAM-matcher is very fast: by limiting genotype-calling to predetermined positions, a comparison between two samples can be made in ∼2 min using the provided default set of variants (Intel Xeon E5, 3.6 GHz). As BAM-matcher caches sample genotype data, subsequent comparisons involving previously calculated samples can be much faster (∼1 s), thus significantly reducing overall processing time for large cohorts. Third, BAM-matcher is flexible; it can compare different types of NGS data, including whole-genome sequencing (WGS), whole-exome sequencing (WES) and RNA-sequencing (RNA-seq) data. If an appropriate genome reference and suitable list of SNPs are provided, BAM-matcher can also be used for non-human genomes. BAM-matcher is a Python command line tool (Python v2.7) for Linux operating systems. It relies on external third party tools for genotype calling, and currently supports the Genome Analysis Toolkit ( McKenna et al. , 2010 ) and Freebayes ( Garrison et al., 2012 ). The only sample-specific input data required by BAM-matcher are mapped read data, thus it can be used at early stages of processing pipelines to facilitate early detection of sample mislabelling. By default, BAM-matcher compares sample genotypes at 1500 exonic SNP sites extracted from the 1000 Genomes database ( The 1000 Genomes Project Consortium, 2012 ) with global minor allele frequencies between 0.45 and 0.55. If required, users can also substitute a customised list of loci. BAM-matcher can write the output report ( Supplementary Figure S1 ) to a file (TXT or HTML) or to the command line. A short-form tab-separated output is also available, useful for batch processing. Although 1500 SNPs were used for comparison, genotype comparison at any site is only carried out if the coverage is above a depth threshold (default 15) in both samples, thus the reported number of sites compared is typically fewer than 1500. The sub-classification of genotype discordance is particularly useful when comparing WGS or WES data against RNA-seq data because the latter can involve allele-specific expression ",Mapping,"bammatcher  tool  rapid ngs sample match 
 facilitate sample match use exist ngs data   develop bammatcher  tool  provide rapid pairwise comparison  binary sequence alignmentmap bam file     compare  sample genotypes  predetermine genomic locations bammatcher   follow feature first   easy  use    deploy  early stag  process pipelines second bammatcher   fast  limit genotypecalling  predetermine position  comparison  two sample   make   min use  provide default set  variants intel xeon   ghz  bammatcher cache sample genotype data subsequent comparisons involve previously calculate sample   much faster   thus significantly reduce overall process time  large cohorts third bammatcher  flexible   compare different type  ngs data include wholegenome sequence wgs wholeexome sequence wes  rnasequencing rnaseq data   appropriate genome reference  suitable list  snps  provide bammatcher  also  use  nonhuman genomes bammatcher   python command line tool python   linux operate systems  rely  external third party tool  genotype call  currently support  genome analysis toolkit  mckenna       freebayes  garrison       samplespecific input data require  bammatcher  map read data thus    use  early stag  process pipelines  facilitate early detection  sample mislabelling  default bammatcher compare sample genotypes   exonic snp sit extract    genomes database    genomes project consortium    global minor allele frequencies      require users  also substitute  customise list  loci bammatcher  write  output report  supplementary figure     file txt  html    command line  shortform tabseparated output  also available useful  batch process although  snps  use  comparison genotype comparison   site   carry    coverage    depth threshold default    sample thus  report number  sit compare  typically fewer    subclassification  genotype discordance  particularly useful  compare wgs  wes data  rnaseq data   latter  involve allelespecific expression ",4
97,TAGdb,"Targeted identification of genomic regions using TAGdb
We have developed an online system for the identification and visualisation of second generation paired sequence tags matching to query sequences. While relatively simple in its concept, the system provides a powerful means to interrogate the vast quantity of data produced by the latest sequencing technologies in a user-friendly and intuitive manner, enabling the identification and cloning of novel genes and the surrounding genomic regions. We have demonstrated the application of TAGdb for gene and promoter discovery in genomes where complete genome sequences are unavailable. We highlight the ability to amplify and sequence less conserved genomic regions, such as promoter sequences, using paired sequence tags where only one tag may align significantly to a query sequence. This tool can be applied for any species where paired read sequence data is available. While the current datasets are limited to a few species, the generation of short paired read sequence data is becoming increasingly common and this approach is likely to become a standard method for the discovery of genes, promoters and genetic variation in a wide range of species. While the current tool is specifically designed for Illumina paired reads, similar data produced by other sequencing platforms may also be hosted. Identification of Brassica WD40 orthologs A 4.5 Kbp fragment of the Arabidopsis genomic region beginning 1000 bp upstream of the transducin/WD-40 repeat family gene AT3G51930 (referred to as AtWD40) was used to query all currently available TAGdb datasets (Table ​(Table1).1). PCR primer pairs were designed from Brassica rapa tags aligning to the query sequence for the amplification of the WD40 genomic region in Brassica rapa cv. Chiifu (Table ​(Table2).2). PCR products (a) and (b) were amplified from 60 ng of Brassica rapa cv. Chiifu DNA using 0.5 μM forward and reverse oligonucleotide primer, 1 U of Phusion Hot Start High-Fidelity DNA Polymerase (Finnzymes), 1× Phusion GC Buffer (Finnzymes) with 1.5 mM MgCl2, and 200 μM each dNTP in a PTC-200 Thermocycler (MJ Research). Cycling conditions were 98°C for 30 sec followed by 35 cycles of 98°C for 10 s, 63°C for 30 s and 72°C for 1 min, with a final extension of 72°C for 10 min. Amplified products were visualised under UV light on 1% TAE-agarose gels containing Ethidium Bromide and using the GeneRuler™ 1 Kb marker as a size standard. PCR products (a) and (b) were cloned using the pGEM®-T-easy (Promega) and pCR®-XL-Topo® (Invitrogen) vector systems respectively and sequenced using T7 and SP6 or M13R primers, and the internal primers 1F, 2R, 3F, 3R, 4F (Table ​(Table2)2) and WD40_3pseqR (5'-TGGAAGAGATTAGGTGAAATGTGA-3'). A consensus sequence for the BrWD40 region (3985 bp) was generated in Geneious Pro [9] from a contig assembly of sequenced products. Alignments and dotplots (window size 10, threshold 35) were generated in Geneious Pro using MUSCLE [10] and ClustalW [11] with default settings. ",Mapping,"target identification  genomic regions use tagdb
  develop  online system   identification  visualisation  second generation pair sequence tag match  query sequence  relatively simple   concept  system provide  powerful mean  interrogate  vast quantity  data produce   latest sequence technologies   userfriendly  intuitive manner enable  identification  clone  novel genes   surround genomic regions   demonstrate  application  tagdb  gene  promoter discovery  genomes  complete genome sequence  unavailable  highlight  ability  amplify  sequence less conserve genomic regions   promoter sequence use pair sequence tag   one tag may align significantly   query sequence  tool   apply   species  pair read sequence data  available   current datasets  limit    species  generation  short pair read sequence data  become increasingly common   approach  likely  become  standard method   discovery  genes promoters  genetic variation   wide range  species   current tool  specifically design  illumina pair read similar data produce   sequence platforms may also  host identification  brassica wd40 orthologs   kbp fragment   arabidopsis genomic region begin   upstream   transducinwd repeat family gene at3g51930 refer   atwd40  use  query  currently available tagdb datasets table ​table1 pcr primer pair  design  brassica rapa tag align   query sequence   amplification   wd40 genomic region  brassica rapa  chiifu table ​table2 pcr products     amplify     brassica rapa  chiifu dna use   forward  reverse oligonucleotide primer    phusion hot start highfidelity dna polymerase finnzymes  phusion  buffer finnzymes    mgcl2     dntp   ptc thermocycler  research cycle condition  °   sec follow   cycle  °    °     °   min   final extension  °   min amplify products  visualise   light   taeagarose gel contain ethidium bromide  use  generuler™   marker   size standard pcr products     clone use  pgem®teasy promega  pcr®xltopo® invitrogen vector systems respectively  sequence use   sp6  m13r primers   internal primers      table ​table2  wd40_3pseqr 'tggaagagattaggtgaaatgtga'  consensus sequence   brwd40 region    generate  geneious pro    contig assembly  sequence products alignments  dotplots window size  threshold   generate  geneious pro use muscle   clustalw   default settings ",4
98,HGA,"HGA: de novo genome assembly method for bacterial genomes using high coverage short sequencing reads.
As referred by GAGE-B, the data can be downloaded from the Sequence Read Archive at NIH’s National Center for Biotechnology Information using the following SRR accession numbers. R. sphaeroides MiSeq: SRR522246, HiSeq: SRR522244. M. abscessus MiSeq: SRR768269, HiSeq: SRR315382. V. cholerae MiSeq: SRR769320, HiSeq: SRR227312. B. cereus MiSeq data were downloaded from the Illumina website. GAGE-B then down-sampled the data to collect 250 × coverage with HiSeq data and 100 × coverage with MiSeq data. After that, they cleaned the raw data by removing adapter sequences and trimming the reads based on q10 quality. Both the raw (down-sampled) and the cleaned dataset are available at GAGE-B website http://ccb.jhu.edu/gage_b, and they are the datasets that were considered in this paper. All tested genomes in this paper have multiple chromosomes and/or plasmids. V. cholerae has two chromosomes, B. cereus and M. abscessus have one chromosome and one plasmid, and R. sphaeroides has two chromosomes and five plasmids. In order to compute the correctness of assemblies, we used the following strains as reference genomes: B. cereus ATCC 10987 (GenBank accession numbers NC_003909, NC_005707), M. abscessus ATCC 19977 (NC_010394, NC_010397), R. sphaeroides 2.4.1 (NC_007488, NC_007489, NC_007490, NC_007493, NC_007494, NC_009007, NC_009008), and V. cholerae 01 biovar eltor str. N16961 (NC_002505, NC_002506). ■■■ Assemblers: We tested our method using eight open source genome assemblers, that were also tested in GAGE-B: Abyss v1.5.1, Cabog v7.0, Mira v4.0.2 [21], MaSuRCA v2.2.1, SGA v0.10.13, SoapDenovo2 v2.04, SPAdes v3.0.0, and Velvet v1.2.10. In order to describe the methods, we will use metrics that were used in QUAST tool [22] as it has been a common and accurate tool in evaluating and analyzing assembles’ results. Namely, we will use the following metrics Number of contigs, N50, NA50, NG50, NGA50, Genome fraction (%), Duplication ratio, Global misassemblies, Local misassemblies, # mismatches per 100 kbp (MP100K), # indels per 100 kbp (IP100K), and # Unaligned length. For the descriptions of these metrics we refer the reader to QUAST [22], as well, we added their descriptions into Additional file 1. Hierarchical genome assembly Hierarchical genome assembly method includes the following steps. Firstly, all reads are partitioned into p disjoint partitions where p > 1. Then each partition is assembled independently. After assembling all partitions sequentially or in parallel, all the partitions’ assemblies will be assembled together to form combined contigs, or merged together to form merged contigs. Lastly, the merged contigs or the combined contigs will be re-assembled with the whole reads again. Figure ​Figure11 depicts a diagram of these steps. Flow diagrams represent the basic assembly flow and the hierarchical assembly flows. The basic flow represents the assembly of all reads in the dataset together. HGA flow using merged contigs represent the flow of partitioning the reads in the dataset into p disjoint partitions, then assembling each partition independently. After that the contigs of each partition’s assembly will be merged together. Lastly re-assemble the merged contigs with the whole reads. The only difference between HGA flow using merged contigs and HGA flow using combined contigs is that instead of merging the contigs of all partitions’ assemblies, they will be combined (assembled) together. In this paper, we used Velvet to assemble the contigs This method will be mainly compared to the basic assembly process, as it’s shown in Fig. ​Fig.1;1; that involves assembling the whole reads together and then output the assembly results. We denote the basic assembly as B(kmer, c) where kmer is the kmer length used in the assembly and c is the coverage of the reads. Partitioning step The main motivation of partitioning the reads set into smaller partitions is to gain lower coverage data. So that we expect to obtain a graph with less complexity, as a result, resolving the assembly’s ambiguities will be more efficient. It’s true that at higher coverage we may get longer contigs than lower coverage, but mostly these contigs will have more errors, in terms of global and local misassemblies, MP100K, IP100K, and unaligned contigs. In order to show this experimentally, we added into Tables S3–S9 (Additional file 1) a row that present the average values over all the partitions for each metric, so it can be compared with the basic flow results. The results show that, for most assemblers and for most genomes, the average values of local misassemblies, global misassemblies, MP100K, and IP100K over all the partitions are less than the values for the basic flow (the flow that is without partitioning the reads dataset), specially for HiSeq datasets where we have more and shorter reads, hence more graph complexities. It’s critical for the steps of combining the contigs (contigs assembly) and the re-assembly step, to have contigs that are longer and with less errors in terms of local misassemblies, global misassemblies, MP100K, and IP100K. The results of these steps will be more efficient and accurate when they are given contigs that are more corrected and longer. The first step of the method is to partition the reads set, by splitting the whole reads set (N reads) into p disjoint partitions. So, each partition has N p reads. After performing several experiments on finding how many partitions to produce, we observed that there is no constraints on how many partitions to produce, as long as we have ≥10x coverage for each of the partitions. So, in general, the partition’s coverage is the only constraint to be considered for this step, not how many partitions to be produced. Contigs assembly (combining) step After assembling all the partitions, we merge all partitions’ assemblies together to form the merged contigs, or assemble (combine) them together forming the combined contigs. Initially, we assembled the contigs of the partitions using minimus2 [23], but after the analysis of several experiments we found that minimus2 is actually misleading in combining contigs. Despite of the improvement in term of NA50 of minimus2’s results, there was significant increase in both duplication ratio and misassemblies events. This occurs because the input data are not short reads, but instead they are long reads (contigs). So, as minimus2 compute the pairwise alignment between all the reads (which are here contigs), this would mean that if we have two (or more) contigs that are true (aligned) but not contiguous in the reference and they share an x-mer (which may be repeat), then minimus2 will output these contigs as one contig. Moreover, one of these contigs may most likely again share x-mer truly or falsely with other contigs, as a result, this will lead to output the same contigs for multiple times which will eventually increase the duplication ratio. In addition, assembling true and not contiguous contigs will increase the number of global or local misassemblies. Hence, the improvement in the results of NA50 is mostly false positive. It’s clear that string graph assemblers such as SGA would not work effectively on assembling contigs. Some contigs may start overlapping in the middle of other contigs and this is not covered by string graph by definition, as it computes the overlap at the ends of the reads (contigs in this case). So, we switched to assemblers that use the de Bruijn graph. Among all assemblers that are based on de Bruijn grapg and which take contigs as input data, we tested which assembler has the best contigs-only assembly results; we found Velvet is the best choice. Moreover, after several experiments of varying the kmer length used in running Velvet to assemble the contigs, kmer value of 31 as well as providing the expected coverage of the input data to Velvet to be as same as the number of partitions, led to the best contigs assembly results. The results of running Velvet as contigs assembler and the comparisons with the results of minimus2 were provided in Additional file 1: Table S11. We denote the results of combining contigs as C(kmer, p, c); where p is the number of partitions, c is the coverage of each partition, and kmer as the used kmer in the assembly of each partition. While we denote the resultant contigs from merging the partitions’ contigs as M(kmer, p, c). Re-assembly step In this step, the merged or combined contigs will be reassembled with all the reads again. To accomplish this, we had to select an assembler that takes long sequences (contigs) as an input in assembling contigs, as well the assembler preferably should be based on de Bruign graphs. For these reasons SPAdes and Velvet were the convenient candidates. After testing both of them on two genomes, SPAdes produced better re-assembly results than Velvet; details of the tests and the comparison results are provided in Additional file 1: Tables S12 & S13. Hence, all re-assembly results in this paper were performed using SPAdes assembler. We denote this step as HGA(kmer, contigs), where kmer is the kmer length that was used in the reassembly process, contigs is whether the merged contigs or the combined contigs. So, in general, we have the following flows: B(kmer). HGA Preprocessing flows: M(kmer, p, c) and C(kmer, p, c)). HGA re-assembly flows HGA(kmer, M(kmer, p, c)) and HGA(kmer, C(kmer, p, c)). Reassembling the reads with long sequences (contigs) has several advantages. Firstly, these contigs were produced not from assembling all the reads together but from combined or merged contigs of the assembly of different partitions. So, they are more corrected and refined in terms of errors, much longer, also they are not redundant contigs meaning that they were not already produced from the assembly of the same whole reads. Instead they were assembled from several disjoint subsets, hence they are structurally different as well as they were produced from a subset of the same reads that they will re-assemble with. Moreover, we experienced that re-assembling contigs, which were produced from some reads dataset, again with the same dataset will not improve the assembly and even it deteriorate the new assembly. Secondly, reassembling long sequences (contigs) may lead to have different connected components to get connected. Finally, during the path finding process this may increase the chances of selecting the true paths by traversing the longest path which induced from having long sequences in the input data. To further explore and justify the advantages of the re-assembly step we performed a simple test on a real dataset of M. abscessus bacteria (the dataset is used in this study and is described in the next section). We assembled the real HiSeq dataset of M. abscessus along with the genome of M. abscessus itself, using SPAdes assembler and a range of kmer lengths of 21, 31,.., 91. The NA50 result of the assembly at kmer =91 was 99 % of the length of the genome of M. abscessus. This indicates that the assembly of contigs with the reads will be computationally indeed effective and could lead to an optimal assembly, but the more correctness of these contigs the better the re-assembly results.",Assembly,"hga  novo genome assembly method  bacterial genomes use high coverage short sequence reads
 refer  gageb  data   download   sequence read archive  nihs national center  biotechnology information use  follow srr accession number  sphaeroides miseq srr522246 hiseq srr522244  abscessus miseq srr768269 hiseq srr315382  cholerae miseq srr769320 hiseq srr227312  cereus miseq data  download   illumina website gageb  downsampled  data  collect   coverage  hiseq data    coverage  miseq data    clean  raw data  remove adapter sequence  trim  read base  q10 quality   raw downsampled   clean dataset  available  gageb website      datasets   consider   paper  test genomes   paper  multiple chromosomes andor plasmids  cholerae  two chromosomes  cereus   abscessus  one chromosome  one plasmid   sphaeroides  two chromosomes  five plasmids  order  compute  correctness  assemblies  use  follow strain  reference genomes  cereus atcc  genbank accession number nc_003909 nc_005707  abscessus atcc  nc_010394 nc_010397  sphaeroides  nc_007488 nc_007489 nc_007490 nc_007493 nc_007494 nc_009007 nc_009008   cholerae  biovar eltor str n16961 nc_002505 nc_002506 ■■■ assemblers  test  method use eight open source genome assemblers   also test  gageb aby  cabog  mira   masurca  sga  soapdenovo2  spade   velvet   order  describe  methods   use metrics   use  quast tool       common  accurate tool  evaluate  analyze assemble result namely   use  follow metrics number  contigs n50 na50 ng50 nga50 genome fraction  duplication ratio global misassemblies local misassemblies # mismatch per  kbp mp100k # indels per  kbp ip100k  # unaligned length   descriptions   metrics  refer  reader  quast   well  add  descriptions  additional file  hierarchical genome assembly hierarchical genome assembly method include  follow step firstly  read  partition   disjoint partition       partition  assemble independently  assemble  partition sequentially   parallel   partition assemblies   assemble together  form combine contigs  merge together  form merge contigs lastly  merge contigs   combine contigs   reassemble   whole read  figure ​figure11 depict  diagram   step flow diagram represent  basic assembly flow   hierarchical assembly flow  basic flow represent  assembly   read   dataset together hga flow use merge contigs represent  flow  partition  read   dataset   disjoint partition  assemble  partition independently    contigs   partition assembly   merge together lastly reassemble  merge contigs   whole read   difference  hga flow use merge contigs  hga flow use combine contigs   instead  merge  contigs   partition assemblies    combine assemble together   paper  use velvet  assemble  contigs  method   mainly compare   basic assembly process   show  fig ​fig  involve assemble  whole read together   output  assembly result  denote  basic assembly  bkmer   kmer   kmer length use   assembly     coverage   read partition step  main motivation  partition  read set  smaller partition   gain lower coverage data    expect  obtain  graph  less complexity   result resolve  assemblys ambiguities    efficient  true   higher coverage  may get longer contigs  lower coverage  mostly  contigs    errors  term  global  local misassemblies mp100k ip100k  unaligned contigs  order  show  experimentally  add  table s3s9 additional file   row  present  average value    partition   metric     compare   basic flow result  result show    assemblers    genomes  average value  local misassemblies global misassemblies mp100k  ip100k    partition  less   value   basic flow  flow   without partition  read dataset specially  hiseq datasets      shorter read hence  graph complexities  critical   step  combine  contigs contigs assembly   reassembly step   contigs   longer   less errors  term  local misassemblies global misassemblies mp100k  ip100k  result   step    efficient  accurate    give contigs    correct  longer  first step   method   partition  read set  split  whole read set  read   disjoint partition   partition    read  perform several experiment  find  many partition  produce  observe     constraints   many partition  produce  long    ≥ coverage     partition   general  partition coverage    constraint   consider   step   many partition   produce contigs assembly combine step  assemble   partition  merge  partition assemblies together  form  merge contigs  assemble combine  together form  combine contigs initially  assemble  contigs   partition use minimus2     analysis  several experiment  find  minimus2  actually mislead  combine contigs despite   improvement  term  na50  minimus2s result   significant increase   duplication ratio  misassemblies events  occur   input data   short read  instead   long read contigs   minimus2 compute  pairwise alignment    read    contigs  would mean     two   contigs   true align   contiguous   reference   share  xmer  may  repeat  minimus2  output  contigs  one contig moreover one   contigs may  likely  share xmer truly  falsely   contigs   result   lead  output   contigs  multiple time   eventually increase  duplication ratio  addition assemble true   contiguous contigs  increase  number  global  local misassemblies hence  improvement   result  na50  mostly false positive  clear  string graph assemblers   sga would  work effectively  assemble contigs  contigs may start overlap   middle   contigs     cover  string graph  definition   compute  overlap   end   read contigs   case   switch  assemblers  use   bruijn graph among  assemblers   base   bruijn grapg   take contigs  input data  test  assembler   best contigsonly assembly result  find velvet   best choice moreover  several experiment  vary  kmer length use  run velvet  assemble  contigs kmer value    well  provide  expect coverage   input data  velvet       number  partition lead   best contigs assembly result  result  run velvet  contigs assembler   comparisons   result  minimus2  provide  additional file  table s11  denote  result  combine contigs  ckmer       number  partition    coverage   partition  kmer   use kmer   assembly   partition   denote  resultant contigs  merge  partition contigs  mkmer   reassembly step   step  merge  combine contigs   reassemble    read   accomplish     select  assembler  take long sequence contigs   input  assemble contigs  well  assembler preferably   base   bruign graph   reason spade  velvet   convenient candidates  test     two genomes spade produce better reassembly result  velvet detail   test   comparison result  provide  additional file  table s12  s13 hence  reassembly result   paper  perform use spade assembler  denote  step  hgakmer contigs  kmer   kmer length   use   reassembly process contigs  whether  merge contigs   combine contigs   general    follow flow bkmer hga preprocessing flow mkmer    ckmer   hga reassembly flow hgakmer mkmer    hgakmer ckmer   reassemble  read  long sequence contigs  several advantage firstly  contigs  produce   assemble   read together   combine  merge contigs   assembly  different partition     correct  refine  term  errors much longer also    redundant contigs mean     already produce   assembly    whole read instead   assemble  several disjoint subsets hence   structurally different  well    produce   subset    read    reassemble  moreover  experience  reassemble contigs   produce   read dataset     dataset   improve  assembly  even  deteriorate  new assembly secondly reassemble long sequence contigs may lead   different connect components  get connect finally   path find process  may increase  chance  select  true paths  traverse  longest path  induce   long sequence   input data   explore  justify  advantage   reassembly step  perform  simple test   real dataset   abscessus bacteria  dataset  use   study   describe   next section  assemble  real hiseq dataset   abscessus along   genome   abscessus  use spade assembler   range  kmer lengths      na50 result   assembly  kmer       length   genome   abscessus  indicate   assembly  contigs   read   computationally indeed effective  could lead   optimal assembly    correctness   contigs  better  reassembly result",5
99,Velvet,"Velvet: algorithms for de novo short read assembly using de Bruijn graphs
Velvet parameters Velvet was implemented in C and tested on a 64-bit Linux machine. The results of Velvet are very sensitive to the parameter k as mentioned previously. The optimum depends on the genome, the coverage, the quality, and the length of the reads. One approach consists in testing several alternatives in parallel and picking the best. Another method consists in estimating the expected number X of times a unique k-mer in a genome of length G is observed in a set of n reads of length l. We can link this number to the traditional value of coverage, noted C, with the relations: Formula Experience shows that all the parameters should be set so that E(X) is between 10 and 15. In practice, given the limited number of possible values for k, it is common to try out various values in parallel then choose the one that produces the highest N50 contig length. The Tour Bus algorithm decides whether to merge two paths based on three thresholds. Firstly, both paths must contain less than 200 nodes; secondly, their respective sequences must be shorter than 100 bp; and thirdly, the sequences must be at least 80% similar. Experimental data The experimental trials were run on human BAC bCX98J21 by Illumina. The reads are available from Illumina at info@solexa.com. The 4,805,808 35-bp reads came from the 200 tiles of lane 5 in Flowcell 2012M. They were selected by Illumina’s in-house “purity filter.” Velvet was run with a hash length of 31 bp and a coverage cutoff of 15×. The experiments on S. suis were run by the Sanger Center on strain P1/7. The data are available at http://www.sanger.ac.uk/Projects/S_suis/. The 2,726,374 36-bp reads came from the 200 tiles of a single lane. The first lane of the flow cell was used, and the reads were those passing the purity filter, but no other filter, as supplied by the Solexa software. Velvet used a hash length of 21 and a final coverage cutoff of 7×. The test on coverage was done with 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, and 100%, respectively, of the reads. Both data sets will be publicly available through the Short Read Archive when it will be in service. Comparative tests were run on SSAKE version 2.0, VCAKE version 1.0, SHARCGS, EULER version 2.0, and Velvet version 0.3. To settle different definitions, we only considered contigs longer than 100 bp. SSAKE and VCAKE were run with default options. Simulations Simulations were run on: E. coli K12 genome (GenBank: U00096); S. cerevisiae chromosomes I to VIII (SGD1.01 assembly); C. elegans chromosome V, between positions 5,000,000 and 10,000,000 (WS170 assembly); human chromosome 20, between positions 40,000,000 and 45,000,000 (NCBI 36 assembly). The first (i.e., unpaired) simulations were run with coverage depths of 5×, 10×, 15×, 20×, 25×, 30×, 35×, 40×, 45×, and 50×. The 35-bp reads were randomly placed and randomly orientated on the genome. Errors were simulated by random mutations uniformly distributed over the reads, at a rate of 1%. The paired read simulations were all run with random 35-bp reads, for a coverage of 50×. All the reads were paired and insert lengths varied with a standard deviation of 5%. Alignments of SCSCs onto the reference were done with exonerate (Slater and Birney 2005). With the non-equivalenced regions (ner) model, exonerate is able to jump over the buffer regions that connect the SCSCs, thus forming long alignments.",Assembly,"velvet algorithms   novo short read assembly use  bruijn graphs
velvet parameters velvet  implement    test   bite linux machine  result  velvet   sensitive   parameter   mention previously  optimum depend   genome  coverage  quality   length   read one approach consist  test several alternatives  parallel  pick  best another method consist  estimate  expect number   time  unique kmer   genome  length   observe   set   read  length    link  number   traditional value  coverage note    relations formula experience show    parameters   set          practice give  limit number  possible value     common  try  various value  parallel  choose  one  produce  highest n50 contig length  tour bus algorithm decide whether  merge two paths base  three thresholds firstly  paths must contain less   nod secondly  respective sequence must  shorter     thirdly  sequence must   least  similar experimental data  experimental trials  run  human bac bcx98j21  illumina  read  available  illumina  info@solexacom    read come    tile  lane   flowcell    select  illuminas inhouse “purity filter” velvet  run   hash length      coverage cutoff    experiment   suis  run   sanger center  strain   data  available      read come    tile   single lane  first lane   flow cell  use   read   pass  purity filter    filter  supply   solexa software velvet use  hash length     final coverage cutoff    test  coverage               respectively   read  data set   publicly available   short read archive      service comparative test  run  ssake version  vcake version  sharcgs euler version   velvet version   settle different definitions   consider contigs longer    ssake  vcake  run  default options simulations simulations  run   coli k12 genome genbank u00096  cerevisiae chromosomes   viii sgd1 assembly  elegans chromosome   position    ws170 assembly human chromosome   position    ncbi  assembly  first  unpaired simulations  run  coverage depths               read  randomly place  randomly orientate   genome errors  simulate  random mutations uniformly distribute   read   rate    pair read simulations   run  random  read   coverage     read  pair  insert lengths vary   standard deviation   alignments  scscs onto  reference    exonerate slater  birney    nonequivalenced regions ner model exonerate  able  jump   buffer regions  connect  scscs thus form long alignments",5
100,MetaVelvet,"MetaVelvet: an extension of Velvet assembler to de novo metagenome assembly from short sequence reads
Information obtained from the DNA sequencer is a set of sequence fragments, called reads , rather than the entire genomic DNA sequence. Therefore, genome assembly is required to reconstruct the original genome sequence from sequence reads. Although each read is short, it is possible to reconstruct longer sequences, called contigs , by identifying an overlap between reads and merging the reads. Genome assembly is generally performed in the following steps: If a large amount of reads sufficient to ‘cover’ the genome are given to the assembly program, overlaps exist between the reads and the contigs are obtained by merging the reads. The term ‘coverage’ for a position in a contig is defined as the number of reads that overlap at that position. The ‘coverage’ of a ‘contig’ is defined to be the average of coverages for all positions in the contig. The input is a set of the nucleotide sequences of DNA fragments. The overlap between every pair of sequences is calculated by pairwise alignment. A pair of sequences with significant overlap is merged to obtain a longer sequence. The above Steps 2 and 3 are repeated. First, we briefly review the de Bruijn graph-based assembly method for single genomes and the Velvet assembler upon which our method is based. Second, we describe our extension of Velvet to metagenome assembly. De Bruijn graph-based assembly The previous conventional assembly method is based on the so-called ‘overlap graph’, where each read is assigned to a node and an edge connects two nodes if the corresponding reads overlap. The assembly problem is reduced to finding a path visiting every node exactly once in the overlap graph, that is, a Hamiltonian path problem. However, the Hamiltonian path problem is nondeterministic polynomial time-complete (NP-complete). Furthermore, the overlap-graph-based assembly method cannot work effectively when applied to very short reads generated from a next-generation sequencer, because there are so many short overlaps between short reads and most of these overlaps are false. Therefore, several de novo assembly methods based on de Bruijn graphs have been proposed for short reads generated from next-generation sequencers ( 15 , 18 , 19 , 20 ). A de Bruijn graph is a data structure that compactly represents an overlap between short reads. A notable difference between a de Bruijn graph and an overlap graph is that each k -mer (word of length k ) instead of a read is assigned to a node, and thus, the size of a de Bruijn graph becomes independent of the size of the input of reads. The detailed definition of de Bruijn graph is shown below. Given a set of sequence reads, the de Bruijn graph-based assemblers first break each read according to a predefined k -mer length. It is clear that two adjacent k -mers in the read overlap at k  − 1 nucleotides. Second, a directed graph (de Bruijn graph) is constructed from the given sequence reads as follows: each overlapping ( k  − 1)-mer is encoded into a node in the directed graph so that each k -mer is represented by a directed edge in the graph. Each k -mer is encoded into a directed edge that connects a node labeled the first ( k  − 1)-mer of the k -mer and a node labeled the second ( k  − 1)-mer. On the constructed de Bruijn graph, each read is mapped to a path traversing the graph. Therefore, the assembly (reconstruction) of the target genome from the de Bruijn graph can be reduced to finding a Eulerian path Brief outline of Velvet and its de Bruijn graph representation In Velvet, the de Bruijn graph is implemented slightly differently, such that each node represents a series of overlapping k -mers where adjacent k -mers overlap by k  − 1 nucleotides. Each node is labeled by the sequence of the last nucleotides of the k -mers ( Figure 1 ). Furthermore, each node is attached to a twin node that represents the reverse series of reverse complement k -mers for reads from opposite strands. For each input read, the ordered set of overlapping k -mers is defined. Next, the ordered set is cut whenever an overlap with another read begins or ends. For each uninterrupted ordered subset of original k -mers, a node is created. Two nodes can be connected by a directed edge. If two nodes are connected, the last k -mer of an origin node overlaps by k  − 1 nucleotides with the first of its destination node. New directed edges are created by tracing the read through the constructed graph. Second, Velvet executes three functions, ‘simplification’ for node merging, and ‘removing tips’ and ‘removing bubbles’ for error removal. Simplification merges two nodes where one node has only one outgoing edge and the other has only one incoming edge. A ‘tip’, which is defined as a chain of nodes disconnected on one end, is removed. A ‘bubble’, which is defined as two redundant paths that start and end at the same nodes and contain similar sequences, is merged. Those tips and bubbles are created by sequencing errors or biological variants, such as single nuleotide polymorphisms (SNPs). Then, the ‘coverage of a node’ is defined as the coverage of the contig assigned to the node. Finally, two functions, ‘Pebble’ and ‘Rock Band’, are called for constructing the scaffold and for repeat resolution using paired-end information and long read information. In these functions, Velvet distinguishes the unique nodes from the repeat nodes based on node coverage. A repeat node represents a sequence that occurs several times in the genome. Simply put, a repeat node is at a crossing point between two paths with multiple incoming and outgoing edges. Note that in multiple genome assembly, the nodes at a crossing point between two paths are not necessarily repeats. Such nodes are sometimes shared between the genomes of two closely related species and represent orthologous sequences, conserved sequences (such as rRNA sequences) and horizontal transfer sequences. Extension to metagenome assembly The MetaVelvet assembler consists of four major steps: [1] Construction of a de Bruijn graph from the input reads. [2] Detection of multiple peaks on k -mer frequency distribution. [3] Decomposition of the constructed de Bruijn graph into individual subgraphs. [4] Assembly of contigs and scaffolds based on the decomposed subgraphs. In Step [1], for a given set of mixed sequence reads generated from multiple species, MetaVelvet constructs the main de Bruijn graph using Velvet functions. In Step [2], MetaVelvet calculates the histogram of k -mer frequencies and detects multiple peaks on the histogram, each of which would correspond to the genome of one species in a microbial community. The expected frequencies of k -mer occurrences were shown to follow a Poisson distribution in a single-genome assembly ( 21 ) and the expected k -mer frequencies in metagenome assembly were shown to follow a mixture of Poisson distributions ( 12 ). Hence, MetaVelvet approximates the empirical histogram of k -mer frequencies by a mixture of Poisson distributions and detects multiple peaks in the Poisson mixture. Furthermore, MetaVelvet classifies every node into one peak of the Poisson mixture. In Step [3], MetaVelvet distinguishes a subgraph composed of nodes belonging to a same peak from the other subgraphs in the main de Bruijn graph. MetaVelvet identifies shared nodes (chimeric nodes) between two subgraphs and disconnects two subgraphs by separating the shared nodes. In step [4], MetaVelvet builds contigs and scaffolds based on the decomposed subgraphs using Velvet functions. The essential part of Step [3] is to design and develop an algorithm to identify and separate ‘chimeric nodes’ in the main de Bruijn graph. If two species contain a common or similar subsequence in their genomes, the main de Bruijn graph contains a node assigned to the subsequence with two incoming edges and two outgoing edges, one of which comes from one species and the other comes from the other species. On the other hand, if the genome of one species contains a repeat subsequence (that is, a subsequence with multiple occurrences in the genome), the de Bruijn graph also contains a node assigned to the repeat subsequence with two incoming edges and two outgoing edges. All other nodes in the main de Bruijn graph must have only one incoming edge and one outgoing edge. To distinguish the chimeric node from the repeat node, the method uses coverage difference. Although the origin nodes of two incoming edges for the repeat node have the same k -mer frequencies, the origin nodes of two incoming edges for the chimeric node belong to two different species and hence have different k -mer frequencies. The formal definition of ‘chimeric node’ is given as a crossing node satisfying the following three conditions: (i) (necessary condition) the number of incoming edges is 2 and the number of outgoing edges is 2; (ii) (sufficient condition) the origin nodes of two incoming edges ( a and b ) belong to two different peaks and the destination nodes of outgoing edges ( c and d ) also belong to the same two peaks as the origin nodes and (iii) (sufficient condition) the chimeric node has a confluent node coverage of the two origin nodes. More precisely, the node coverage of the candidate chimeric node should be between ( a .cov +  b .cov +  c .cov +  d .cov)/2 × (1 −  y ) and ( a .cov +  b .cov +  c .cov +  d .cov)/2 × (1 +  y ), where a .cov represents the node coverage of a node a and y is a parameter in MetaVelvet called ‘allowable coverage difference’. Once a candidate for chimeric node is identified, the candidate node is checked for ‘consistency’ with paired-end information. If a significant amount of paired-end reads connect an origin node of an incoming edge of the chimeric node with a destination node labeled differently from the origin node (that is, the paired-end reads connect an origin node labeled ON and a destination node labeled OFF or vice versa, as shown in Figure 4 ), the candidate node is discarded. The detailed procedure of MetaVelvet is as follows: It might be thought that in Substep 3 above, a chimeric node could have the highest expected coverage. However, the contigs of chimeric nodes are very short compared with the unique nodes; therefore, the length-weighted frequencies of coverage values for the chimeric nodes do not form any significant peaks. [1] Construction of the de Bruijn graph: 1. For a given set of sequence reads generated from mixed species, construct a de Bruijn graph by calling Velvet first stage functions. [2] Detection of multiple peaks on k-mer frequencies: 2. Calculate the empirical distribution of ‘length-weighted frequencies’ of node coverages, where a node coverage is assigned to each node by Velvet on the construction of the de Bruijn graph ( Figure 5 ). 3. Approximate the empirical distribution by a mixture of Poisson distributions and detect multiple peaks in the Poisson mixture. Then, the highest peak of expected coverage is chosen as the ‘primary expected coverage’, and the next highest is chosen as the ‘secondary expected coverage’. 4. Classify every node into one distribution of the Poisson mixture by calculating its posterior probability for the node coverage value. [3] Decomposition of the de Bruijn graph: 5. (Decomposition by connectivity) Decompose the initial de Bruijn graph into connected subgraphs. 6. (Decomposition by coverage value) If the coverage of a node belongs to the primary expected coverage, the node is classified as a ‘primary node’. Subsequently, the primary nodes are labeled as ‘ON’ and the other nodes are labeled as ‘OFF’. Then, a chimeric node is detected as having two incoming edges whose origin nodes are labeled ON and OFF, and two outgoing edges whose destination nodes are labeled ON and OFF, and having a coverage value mostly equal (within 5% difference by default) to the average between the sum of the coverage values of the two origin nodes and the sum of the two destination nodes. Second, check the consistency of the ON and OFF labeling for the two origin nodes and two destination nodes using paired-end information. If the consistency is satisfied, resolve every chimeric node by separating the node into two nodes with only one incoming edge and one outgoing edge, whose origin and destination nodes have the same label, as shown in Figure 4 . After separating the chimeric nodes, further decompose the resulting graph into connected subgraphs. 7. If a connected subgraph consists of more than x % (a predefined parameter, the default is set to 100%) of nodes labeled ‘ON’, the subgraph is unmasked. All other subgraphs are masked. [4] Assembly of contigs and scaffolds: 8. Apply the Velvet functions to the unmasked subgraphs to build contigs and then apply Pebble and Rock Band functions to build scaffolds. 9. Remove the unmasked subgraphs and recursively apply Step 2–8 to the remaining de Bruijn graph until no node remains. ",Assembly,"metavelvet  extension  velvet assembler   novo metagenome assembly  short sequence reads
information obtain   dna sequencer   set  sequence fragment call read  rather   entire genomic dna sequence therefore genome assembly  require  reconstruct  original genome sequence  sequence read although  read  short   possible  reconstruct longer sequence call contigs   identify  overlap  read  merge  read genome assembly  generally perform   follow step   large amount  read sufficient  cover  genome  give   assembly program overlap exist   read   contigs  obtain  merge  read  term coverage   position   contig  define   number  read  overlap   position  coverage   contig  define    average  coverages   position   contig  input   set   nucleotide sequence  dna fragment  overlap  every pair  sequence  calculate  pairwise alignment  pair  sequence  significant overlap  merge  obtain  longer sequence   step     repeat first  briefly review   bruijn graphbased assembly method  single genomes   velvet assembler upon   method  base second  describe  extension  velvet  metagenome assembly  bruijn graphbased assembly  previous conventional assembly method  base   socalled overlap graph   read  assign   node   edge connect two nod   correspond read overlap  assembly problem  reduce  find  path visit every node exactly    overlap graph    hamiltonian path problem however  hamiltonian path problem  nondeterministic polynomial timecomplete npcomplete furthermore  overlapgraphbased assembly method cannot work effectively  apply   short read generate   nextgeneration sequencer     many short overlap  short read     overlap  false therefore several  novo assembly methods base   bruijn graph   propose  short read generate  nextgeneration sequencers            bruijn graph   data structure  compactly represent  overlap  short read  notable difference    bruijn graph   overlap graph     mer word  length   instead   read  assign   node  thus  size    bruijn graph become independent   size   input  read  detail definition   bruijn graph  show  give  set  sequence read   bruijn graphbased assemblers first break  read accord   predefined  mer length   clear  two adjacent  mers   read overlap      nucleotides second  direct graph  bruijn graph  construct   give sequence read  follow  overlap     mer  encode   node   direct graph     mer  represent   direct edge   graph   mer  encode   direct edge  connect  node label  first     mer    mer   node label  second     mer   construct  bruijn graph  read  map   path traverse  graph therefore  assembly reconstruction   target genome    bruijn graph   reduce  find  eulerian path brief outline  velvet    bruijn graph representation  velvet   bruijn graph  implement slightly differently    node represent  series  overlap  mers  adjacent  mers overlap      nucleotides  node  label   sequence   last nucleotides    mers  figure   furthermore  node  attach   twin node  represent  reverse series  reverse complement  mers  read  opposite strand   input read  order set  overlap  mers  define next  order set  cut whenever  overlap  another read begin  end   uninterrupted order subset  original  mers  node  create two nod   connect   direct edge  two nod  connect  last  mer   origin node overlap      nucleotides   first   destination node new direct edge  create  trace  read   construct graph second velvet execute three function simplification  node merge  remove tip  remove bubble  error removal simplification merge two nod  one node   one outgo edge      one incoming edge  tip   define   chain  nod disconnect  one end  remove  bubble   define  two redundant paths  start  end    nod  contain similar sequence  merge  tip  bubble  create  sequence errors  biological variants   single nuleotide polymorphisms snps   coverage   node  define   coverage   contig assign   node finally two function pebble  rock band  call  construct  scaffold   repeat resolution use pairedend information  long read information   function velvet distinguish  unique nod   repeat nod base  node coverage  repeat node represent  sequence  occur several time   genome simply put  repeat node    cross point  two paths  multiple incoming  outgo edge note   multiple genome assembly  nod   cross point  two paths   necessarily repeat  nod  sometimes share   genomes  two closely relate species  represent orthologous sequence conserve sequence   rrna sequence  horizontal transfer sequence extension  metagenome assembly  metavelvet assembler consist  four major step  construction    bruijn graph   input read  detection  multiple peak   mer frequency distribution  decomposition   construct  bruijn graph  individual subgraphs  assembly  contigs  scaffold base   decompose subgraphs  step    give set  mix sequence read generate  multiple species metavelvet construct  main  bruijn graph use velvet function  step  metavelvet calculate  histogram   mer frequencies  detect multiple peak   histogram    would correspond   genome  one species   microbial community  expect frequencies   mer occurrences  show  follow  poisson distribution   singlegenome assembly      expect  mer frequencies  metagenome assembly  show  follow  mixture  poisson distributions    hence metavelvet approximate  empirical histogram   mer frequencies   mixture  poisson distributions  detect multiple peak   poisson mixture furthermore metavelvet classify every node  one peak   poisson mixture  step  metavelvet distinguish  subgraph compose  nod belong    peak    subgraphs   main  bruijn graph metavelvet identify share nod chimeric nod  two subgraphs  disconnect two subgraphs  separate  share nod  step  metavelvet build contigs  scaffold base   decompose subgraphs use velvet function  essential part  step    design  develop  algorithm  identify  separate chimeric nod   main  bruijn graph  two species contain  common  similar subsequence   genomes  main  bruijn graph contain  node assign   subsequence  two incoming edge  two outgo edge one   come  one species    come    species    hand   genome  one species contain  repeat subsequence    subsequence  multiple occurrences   genome   bruijn graph also contain  node assign   repeat subsequence  two incoming edge  two outgo edge   nod   main  bruijn graph must   one incoming edge  one outgo edge  distinguish  chimeric node   repeat node  method use coverage difference although  origin nod  two incoming edge   repeat node     mer frequencies  origin nod  two incoming edge   chimeric node belong  two different species  hence  different  mer frequencies  formal definition  chimeric node  give   cross node satisfy  follow three condition  necessary condition  number  incoming edge     number  outgo edge    sufficient condition  origin nod  two incoming edge      belong  two different peak   destination nod  outgo edge      also belong    two peak   origin nod  iii sufficient condition  chimeric node   confluent node coverage   two origin nod  precisely  node coverage   candidate chimeric node      cov    cov    cov    cov          cov    cov    cov    cov         cov represent  node coverage   node      parameter  metavelvet call allowable coverage difference   candidate  chimeric node  identify  candidate node  check  consistency  pairedend information   significant amount  pairedend read connect  origin node   incoming edge   chimeric node   destination node label differently   origin node    pairedend read connect  origin node label    destination node label   vice versa  show  figure    candidate node  discard  detail procedure  metavelvet   follow  might  think   substep    chimeric node could   highest expect coverage however  contigs  chimeric nod   short compare   unique nod therefore  lengthweighted frequencies  coverage value   chimeric nod   form  significant peak  construction    bruijn graph    give set  sequence read generate  mix species construct   bruijn graph  call velvet first stage function  detection  multiple peak  kmer frequencies  calculate  empirical distribution  lengthweighted frequencies  node coverages   node coverage  assign   node  velvet   construction    bruijn graph  figure    approximate  empirical distribution   mixture  poisson distributions  detect multiple peak   poisson mixture   highest peak  expect coverage  choose   primary expect coverage   next highest  choose   secondary expect coverage  classify every node  one distribution   poisson mixture  calculate  posterior probability   node coverage value  decomposition    bruijn graph  decomposition  connectivity decompose  initial  bruijn graph  connect subgraphs  decomposition  coverage value   coverage   node belong   primary expect coverage  node  classify   primary node subsequently  primary nod  label      nod  label     chimeric node  detect   two incoming edge whose origin nod  label     two outgo edge whose destination nod  label       coverage value mostly equal within  difference  default   average   sum   coverage value   two origin nod   sum   two destination nod second check  consistency      label   two origin nod  two destination nod use pairedend information   consistency  satisfy resolve every chimeric node  separate  node  two nod   one incoming edge  one outgo edge whose origin  destination nod    label  show  figure    separate  chimeric nod  decompose  result graph  connect subgraphs    connect subgraph consist       predefined parameter  default  set    nod label   subgraph  unmask   subgraphs  mask  assembly  contigs  scaffold  apply  velvet function   unmask subgraphs  build contigs   apply pebble  rock band function  build scaffold  remove  unmask subgraphs  recursively apply step    remain  bruijn graph   node remain ",5
101,Rnnotator,"Rnnotator: an automated de novo transcriptome assembly pipeline from stranded RNA-Seq reads
Library construction and sequencing The Candida RNA-Seq library construction and sequencing are described elsewhere [15]. Read quality filtering and duplicate read removal Condition-specific reads were pooled together and identical reads were removed. After removing duplicate reads, read error filtering was performed using a rare k-mer filtering approach. The frequency of each k-mer was calculated using a hash table and reads containing rare k-mers were not used in the assembly. Rare k-mers were defined as those that occurred less than three times in the set of unique reads. Several rare k-mer read filtering strategies were tested in order to determine the effect of the read filtering. The three filtering strategies were: i) no filter applied, ii) filter applied after removing duplicate reads, and iii) filter applied before removing duplicate reads (Additional file 1). The order of filtering and duplicate read removal is significant since a k-mer is more likely to be a low abundant k-mer after duplicate read removal than before. We discovered that filtering reads prior to assembly reduces the runtime and memory required by the assembly at the cost of slightly decreasing the assembly quality. Multiple Velvet assembly For assembly of short read Illumina sequences, the Velvet assembler was used in conjunction with the AMOS assembly package [10, 11]. Eight runs of velveth were executed in parallel (once for each hash length, 19 through 33). Next eight runs of velvetg were run in parallel with parameters: cov_cutoff = 1, exp_cov = auto. Prior to merging contigs, all duplicates were removed and contigs were combined into a single FASTA file. The minimus2 pipeline [11], a lightweight assembler which is part of the AMOS package, was run using REFCOUNT = 0 (other parameters default). Splitting contigs using stranded RNA-Seq The protocol used to split misassembled contigs using stranded RNA-Seq reads includes: i) splitting contigs with long stretches of less than three mapped reads which are longer than one read length, ii) orienting contigs in the correct mRNA sense strand orientation, iii) generating a consensus contig by counting the number of A,C,G,T residues at each base position. BWA [16] was used to align the reads to the assembled contigs. Aligning contigs to the reference The UCSC Blat software [17] was used to align contigs to both genome and transcriptome references. For yeast datasets the maximum intron size was set to 5,000. In all cases, only the best hits were taken, unless there were multiple best-scoring hits. The score of each alignment was calculated by the formula: s = matches - mismatches, as recommended. A similar strategy was used when aligning gene models to contigs (SC5314), again only taking the best scoring hits. Detecting gene fusion events Gene fusion events were detected by first aligning contigs to the reference genome (outlined above). Genomic coordinates for each aligned contig were compared with the genomic coordinates of every annotated gene. A contig and gene were considered overlapping if they shared an overlap which was longer than 50% of the gene length. Contigs containing two or more such genes were identified as containing a gene fusion event. Comparing with other assemblers When performing the single-run Velvet assemblies and the Oases assemblies hash length 21 was used (28 to 34 base pair read lengths). All other parameters were set to the default parameter set. Contigs > = 100 bp in length were used for comparison against other assemblers. For the Multiple-k assemblies, eight Velvet assemblies were first performed. In order to have a fair comparison against the Rnnotator assemblies, the same hash lengths were used when running Velvet (i.e., 19, 21, 23, 25, 27, 29, 31, 33). The Multiple-k script was then run using the eight Velvet assemblies as input.",Assembly,"rnnotator  automate  novo transcriptome assembly pipeline  strand rnaseq reads
library construction  sequence  candida rnaseq library construction  sequence  describe elsewhere  read quality filter  duplicate read removal conditionspecific read  pool together  identical read  remove  remove duplicate read read error filter  perform use  rare kmer filter approach  frequency   kmer  calculate use  hash table  read contain rare kmers   use   assembly rare kmers  define    occur less  three time   set  unique read several rare kmer read filter strategies  test  order  determine  effect   read filter  three filter strategies    filter apply  filter apply  remove duplicate read  iii filter apply  remove duplicate read additional file   order  filter  duplicate read removal  significant since  kmer   likely    low abundant kmer  duplicate read removal    discover  filter read prior  assembly reduce  runtime  memory require   assembly   cost  slightly decrease  assembly quality multiple velvet assembly  assembly  short read illumina sequence  velvet assembler  use  conjunction   amos assembly package   eight run  velveth  execute  parallel    hash length    next eight run  velvetg  run  parallel  parameters cov_cutoff   exp_cov  auto prior  merge contigs  duplicate  remove  contigs  combine   single fasta file  minimus2 pipeline   lightweight assembler   part   amos package  run use refcount    parameters default split contigs use strand rnaseq  protocol use  split misassembled contigs use strand rnaseq read include  split contigs  long stretch  less  three map read   longer  one read length  orient contigs   correct mrna sense strand orientation iii generate  consensus contig  count  number  acgt residues   base position bwa   use  align  read   assemble contigs align contigs   reference  ucsc blat software   use  align contigs   genome  transcriptome reference  yeast datasets  maximum intron size  set     case   best hit  take unless   multiple bestscoring hit  score   alignment  calculate   formula   match  mismatch  recommend  similar strategy  use  align gene model  contigs sc5314   take  best score hit detect gene fusion events gene fusion events  detect  first align contigs   reference genome outline  genomic coordinate   align contig  compare   genomic coordinate  every annotate gene  contig  gene  consider overlap   share  overlap   longer     gene length contigs contain two    genes  identify  contain  gene fusion event compare   assemblers  perform  singlerun velvet assemblies   oases assemblies hash length   use    base pair read lengths   parameters  set   default parameter set contigs      length  use  comparison   assemblers   multiplek assemblies eight velvet assemblies  first perform  order    fair comparison   rnnotator assemblies   hash lengths  use  run velvet           multiplek script   run use  eight velvet assemblies  input",5
102,Taipan,"A fast hybrid short read fragment assembly algorithm 
Input to Taipan is a multi-set of reads R of length l each (consisting of the original reads plus their reverse complements), the minimal overlap parameter k(k <l), and a threshold T. R is stored in a hash table. This hash table allows efficient processing of queries of the form get_overlapping_reads(S,t) for a DNA string S and t∈{k,…, l}, which return all reads in R whose prefix t matches the suffix t of S. Taipan assembles a new contig by choosing a read from R as a seed. This seed is iteratively extended in 3′ direction by one base at a time until there are either insufficient overlapping reads or a repeat is found. Subsequently, the same algorithm is used in 5′ direction. The algorithm to extend contig S in 3′ direction by a single base works as follows. A set O of overlapping reads is retrieved from the hash table by calling get_overlapping_reads(S,t) for each t∈{k,…, l}. Afterwards the directed overlap graph G=(O, E) is constructed, where (ri, rj)∈E if suffix t of ri matches prefix t of rj for some t∈{k,…, l}. G′ = (O, E′) is then built by removing all associative edges from G ((ri, rj)∈E is associative, if there exist two other edges (ri, rk), (rk, rj)∈E)). The set P consisting of all vertex-disjoint paths of G′ is determined. P is analyzed to determine the single-base extension of S using threshold T as follows: (a) if P contains at least two paths p1, p2 with length (pi) ≥ T, i∈{1, 2}, then a repeat is found and extension of S is terminated; (b) if P contains exactly one path p with length (p) ≥ T, then S is extended by the first nucleotide of the first read in p; (c) if P contains no path p with length (p) ≥ 1, then there are insufficient overlapping reads and extension of S is terminated; (d) if none of the above cases apply, G′ is enlarged along each path p and the rules (a)–(d) are recursively applied (up to a maximum of T steps) to the enlarged graph. After the assembly of a new contig all reads that exactly match this contig are removed from R and the hash table. The implementation of Taipan gains efficiency by the fact that O only slightly changes for subsequent extensions and by using a sorted trie data structure to represent G⊥ ← and P. Choice of seeds can affect assembly results. Taipan selects an unassembled read with highest occurrence as seed in order to minimize extensions of reads containing sequencing errors. ",Assembly," fast hybrid short read fragment assembly algorithm 
input  taipan   multiset  read   length   consist   original read plus  reverse complement  minimal overlap parameter     threshold    store   hash table  hash table allow efficient process  query   form get_overlapping_readsst   dna string   ∈{… }  return  read   whose prefix  match  suffix    taipan assemble  new contig  choose  read     seed  seed  iteratively extend  ′ direction  one base   time    either insufficient overlap read   repeat  find subsequently   algorithm  use  ′ direction  algorithm  extend contig   ′ direction   single base work  follow  set   overlap read  retrieve   hash table  call get_overlapping_readsst   ∈{… } afterwards  direct overlap graph    construct   ∈  suffix    match prefix      ∈{… } ′   ′   build  remove  associative edge    ∈  associative   exist two  edge    ∈  set  consist   vertexdisjoint paths  ′  determine   analyze  determine  singlebase extension   use threshold   follow    contain  least two paths    length  ≥  ∈{ }   repeat  find  extension    terminate    contain exactly one path   length  ≥     extend   first nucleotide   first read      contain  path   length  ≥     insufficient overlap read  extension    terminate   none    case apply ′  enlarge along  path    rule   recursively apply    maximum   step   enlarge graph   assembly   new contig  read  exactly match  contig  remove     hash table  implementation  taipan gain efficiency   fact    slightly change  subsequent extensions   use  sort trie data structure  represent ⊥    choice  seed  affect assembly result taipan select  unassembled read  highest occurrence  seed  order  minimize extensions  read contain sequence errors ",5
103,AByss,"De novo transcriptome assembly with ABySS
Patient sample The transcriptome data belongs to a patient who presented at 44 years of age with bulky stage II A intra-abdominal follicular, grade 1 non-Hodgkin lymphoma based on an inguinal lymph node biopsy. The staging bone marrow biopsy revealed no lymphoma. Initial treatment consisted of eight cycles of CVP-R (cyclophosphamide, vincristine, prednisone and rituximab) chemotherapy and produced a partial response. However, within 3 months symptomatic progression of lymphoma was evident within the abdomen and a repeat inguinal lymph node biopsy revealed residual grade 1 follicular lymphoma. We obtained informed consent from the patient, approved by the Research Ethics Board, and material from this biopsy was subjected to genomic analyses including whole transcriptome shotgun sequencing (WTSS). 2.2 Library construction and sequencing RNA was extracted from the tumour biopsy sample using AllPrep DNA/RNA Mini Kit (Qiagen, USA) and DNaseI (Invitrogen, USA) treated following the manufacturer's protocol. We generated three WTSS libraries, one from amplified complementary DNA (cDNA), another from the same amplified cDNA with normalization, and the last from unamplified cDNA, as follows. 2.2.1 WTSS-lite and normalized WTSS-lite libraries Double-stranded amplified cDNA was generated from 200 ng RNA by template-switching cDNA synthesis kit (Clontech, USA) using Superscript Reverse Transcriptase (Invitrogen, USA), followed by amplification using Advantage 2 PCR kit (Clontech, USA) in 20 cycle reactions. Custom biotinylated PCR primers containing MmeI recognition sequences were used to facilitate the removal of primer sequences from cDNA template for WTSS-Lite library construction. Normalized cDNA was generated from 1.2μg of the above amplified cDNA using Trimmer cDNA Normalization Kit (Evrogen, Russia) followed by amplification using the same biotinylated PCR primers, in a single 15 cycle reaction with Advantage 2 Polymerase (Clontech, USA). The normalized and amplified cDNA pool generated with the 1/2× duplex-specific nuclease (DSN) enzyme dilution was chosen to be the template for WTSS-Lite normalized library construction. For both WTSS-Lite and normalized WTSS-Lite libraries, the removal of amplification oligonucleotide templates from the cDNA ends was accomplished by the binding to M-280 Streptavidin beads (Invitrogen, USA), followed by MmeI digestion. The supernatant of digest was purified and prepared for library construction as follows: roughly 500 ng of cDNA template was sonicated for 5 min using a Sonic Dismembrator 550 (cup horn, Fisher Scientific, Canada), and size fractionated using 8% PAGE gel. The 100–300 bp size fraction was excised for library construction according to the Genomic DNA Sample Prep Kit protocol (Illumina, USA), using 10 cycles of PCR and purified using a Spin-X Filter Tube (Fisher Scientific) and ethanol precipitation. The library DNA quality was assessed and quantified using an Agilent DNA 1000 series II assay and Nanodrop 1000 spectrophotometer (Nanodrop, USA) and diluted to 10 nM. 2.2.2 Unamplified WTSS library We used 5 μg RNA to purify polyA+RNA fraction using the MACS mRNA Isolation Kit (Miltenyi Biotec, Germany). Double-stranded cDNA was synthesized from the purified polyA+RNA using Superscript Double-Stranded cDNA Synthesis kit (Invitrogen, USA) and random hexamer primers (Invitrogen, USA) at a concentration of 5 μM. The cDNA was sheared and library was constructed by following the same Genomic DNA Sample Prep Kit protocol (Illumina, USA). 2.2.3 Sequencing Derived cDNA libraries were used to generate clusters on the Illumina cluster station and sequenced on the Illumina Genome Analyzer II platform following the manufacturer's instructions. We ran seven lanes each of the two amplified libraries to generate 36 bp single end tag (SET) reads, and seven lanes of the unamplified library to generate 36 bp paired end tag (PET) reads. 2.3 Assembly method We assembled the reads using ABySS (Simpson et al., 2009). The ABySS algorithm is based on a de Bruijn di-graph representation of sequence neighborhoods (de Bruijn, 1946), where a sequence read is decomposed into tiled sub-reads of length k(k-mers) and sequences sharing k−1 bases are connected by directed edges. This approach was introduced to DNA sequence assembly by Pevzner et al. (2001) and was followed by others (Butler et al., 2008; Chaisson and Pevzner, 2008; Jackson et al., 2009; Zerbino and Birney, 2008). Although memory requirements for implementing de Bruijn graphs scale linearly with the underlying sequence, ABySS uses a distributed representation that relaxes these memory and computation time restrictions (Simpson et al., 2009). A de Bruijn graph captures the adjacency information between sequences of a uniform length k, defined by an overlap between the last and the first k−1 characters of two adjacent k-mers. ABySS starts by cataloging k-mers in a given set of reads and establishes their adjacency, represented in a distributed data format. The resulting graph is then inspected to identify potential sequencing errors and small-scale sequence variation. When a sequence has a read error, it alters all the k-mers that span it, which form branches in the graph. However, since such errors are stochastic in nature, their rate of observation is substantially lower than that of correct sequences. Hence, they can be discerned using coverage information, and branches with low coverage can be culled to increase the quality and contiguity of an assembly. This is especially true for genomic sequences. For transcriptomes, however, sequence coverage depth is a function of the transcript expression level it represents. Therefore, such culling needs to be performed with extra care. Accordingly, in the assembly stage, we applied trimming for those (false) branches when the absolute coverage levels were below a threshold of 2-fold (2×). In the analysis stage, we evaluated assembly branches using the local coverage information, as well as contig lengths. For instance, in a neighborhood where a contig C1 branches into contig C2 and C3, with coverage levels of x1, x2 and x3, and contig lengths l1, l2 and l3, respectively, if x1 and x2 are significantly higher than x3, and l3 is shorter than a threshold, then we assume that C3 is a false branch. Some repeat read errors and small-scale sequence variation between approximate repeats or alleles result in some of the branches merging back to the trunk of the de Bruijn graph. We call such structures ‘bubbles’, and remove them during the assembly. Since they may represent real albeit alternative sequence at that location, we preserve the information they carry by recording them in a special log file, along with the variant we leave in the assembly contig and their coverage levels. These log entries are later used to postulate effects of allelic variations on expression levels. After the false branches are culled and bubbles removed, unambiguously linear paths along the de Bruijn graph are connected to form the single end tag assembly (SET) contigs. The branching information is also recorded for the subsequent paired end tag assembly (PET) stage and further analysis. At the SET stage, every k-mer represented in the assembly contigs has a unique occurrence. Using that information, we apply a streamlined read-to-assembly alignment routine. We use the aligned read pairs to (i) infer read distance distributions between pairs in libraries that form our read set, and (ii) identify contigs that are in a certain neighborhood defined by these distributions. The adjacency and the neighborhood information are used by the PET routine to merge SET contigs connected by pairs unambiguously, while keeping the list of the merged SET contigs (or the pedigree information) in the FASTA header for backtracking. The adjacency, neighborhood and pedigree information, along with the contig coverage information are also used by our assembly visualization tool, ABySS-Explorer. Figure 1 shows the ABySS-Explorer representation of some SET contigs in a neighborhood. Note that both the edges and the nodes are polarized in accordance with the directionality of the contigs and the k − 1-mer overlaps between them, respectively. In the interactive view, when a user double-clicks on a contig, its direction and node connection polarizations flip to reflect the reverse complement. Paired end tags often resolve paths along SET contigs and they are subsequently merged in the PET stage. We indicate such merged contigs by dark gray paths in the viewer, when one of the contigs contributing to a merge is selected. The ABySS-Explorer representation encodes additional information including contig coverage (indicated by the edge thickness) and contig length (indicated by the edge lengths). A wave representation is used to indicate contig length such that a single oscillation corresponds to a user-defined number of nucleotides. A long contig results in packed oscillations, which obscure the arrowhead indicating its direction. To resolve this ambiguity, the envelope of the oscillations outlines a leaf-like shape with the thicker stem of the shape marking the start of the contig and the thinner tip pointing to its end. For example, contig 383 936 is a 627 bp long contig, which is much longer than the shortest contig 297 333 (29 bp), but its direction is still evident from its shape, with the thinner tip pointing to the right. We performed a parameter search for assembly optimization by varying k values in the range between 25 bp and 32 bp for the SET stage. Figure 2 shows some key statistics of our assemblies as a function of k. We picked the best assembly to be that for k=28, as the number of contigs drops significantly between k=27 and k=28, while the number of contigs 100 bp or longer do not increase, which indicate a substantial improvement in contiguity. Beyond k=28, the number of contigs in both categories keeps decreasing, but so does the assembly N50. ",Assembly," novo transcriptome assembly  abyss
patient sample  transcriptome data belong   patient  present   years  age  bulky stage   intraabdominal follicular grade  nonhodgkin lymphoma base   inguinal lymph node biopsy  stag bone marrow biopsy reveal  lymphoma initial treatment consist  eight cycle  cvpr cyclophosphamide vincristine prednisone  rituximab chemotherapy  produce  partial response however within  months symptomatic progression  lymphoma  evident within  abdomen   repeat inguinal lymph node biopsy reveal residual grade  follicular lymphoma  obtain inform consent   patient approve   research ethics board  material   biopsy  subject  genomic analyse include whole transcriptome shotgun sequence wtss  library construction  sequence rna  extract   tumour biopsy sample use allprep dnarna mini kit qiagen usa  dnasei invitrogen usa treat follow  manufacturer' protocol  generate three wtss libraries one  amplify complementary dna cdna another    amplify cdna  normalization   last  unamplified cdna  follow  wtsslite  normalize wtsslite libraries doublestranded amplify cdna  generate    rna  templateswitching cdna synthesis kit clontech usa use superscript reverse transcriptase invitrogen usa follow  amplification use advantage  pcr kit clontech usa   cycle reactions custom biotinylated pcr primers contain mmei recognition sequence  use  facilitate  removal  primer sequence  cdna template  wtsslite library construction normalize cdna  generate  2μg    amplify cdna use trimmer cdna normalization kit evrogen russia follow  amplification use   biotinylated pcr primers   single  cycle reaction  advantage  polymerase clontech usa  normalize  amplify cdna pool generate    duplexspecific nuclease dsn enzyme dilution  choose    template  wtsslite normalize library construction   wtsslite  normalize wtsslite libraries  removal  amplification oligonucleotide templates   cdna end  accomplish   bind   streptavidin bead invitrogen usa follow  mmei digestion  supernatant  digest  purify  prepare  library construction  follow roughly    cdna template  sonicated   min use  sonic dismembrator  cup horn fisher scientific canada  size fractionate use  page gel    size fraction  excise  library construction accord   genomic dna sample prep kit protocol illumina usa use  cycle  pcr  purify use  spinx filter tube fisher scientific  ethanol precipitation  library dna quality  assess  quantify use  agilent dna  series  assay  nanodrop  spectrophotometer nanodrop usa  dilute     unamplified wtss library  use   rna  purify polyarna fraction use  macs mrna isolation kit miltenyi biotec germany doublestranded cdna  synthesize   purify polyarna use superscript doublestranded cdna synthesis kit invitrogen usa  random hexamer primers invitrogen usa   concentration     cdna  shear  library  construct  follow   genomic dna sample prep kit protocol illumina usa  sequence derive cdna libraries  use  generate cluster   illumina cluster station  sequence   illumina genome analyzer  platform follow  manufacturer' instructions  run seven lanes    two amplify libraries  generate   single end tag set read  seven lanes   unamplified library  generate   pair end tag pet read  assembly method  assemble  read use aby simpson     aby algorithm  base    bruijn digraph representation  sequence neighborhoods  bruijn    sequence read  decompose  tile subreads  length kkmers  sequence share  base  connect  direct edge  approach  introduce  dna sequence assembly  pevzner      follow  others butler    chaisson  pevzner  jackson    zerbino  birney  although memory requirements  implement  bruijn graph scale linearly   underlie sequence aby use  distribute representation  relax  memory  computation time restrictions simpson      bruijn graph capture  adjacency information  sequence   uniform length  define   overlap   last   first  character  two adjacent kmers aby start  catalog kmers   give set  read  establish  adjacency represent   distribute data format  result graph   inspect  identify potential sequence errors  smallscale sequence variation   sequence   read error  alter   kmers  span   form branch   graph however since  errors  stochastic  nature  rate  observation  substantially lower    correct sequence hence    discern use coverage information  branch  low coverage   cull  increase  quality  contiguity   assembly   especially true  genomic sequence  transcriptomes however sequence coverage depth   function   transcript expression level  represent therefore  cull need   perform  extra care accordingly   assembly stage  apply trim   false branch   absolute coverage level    threshold  fold    analysis stage  evaluate assembly branch use  local coverage information  well  contig lengths  instance   neighborhood   contig  branch  contig     coverage level       contig lengths     respectively      significantly higher      shorter   threshold   assume     false branch  repeat read errors  smallscale sequence variation  approximate repeat  alleles result     branch merge back   trunk    bruijn graph  call  structure bubble  remove    assembly since  may represent real albeit alternative sequence   location  preserve  information  carry  record    special log file along   variant  leave   assembly contig   coverage level  log entries  later use  postulate effect  allelic variations  expression level   false branch  cull  bubble remove unambiguously linear paths along   bruijn graph  connect  form  single end tag assembly set contigs  branch information  also record   subsequent pair end tag assembly pet stage   analysis   set stage every kmer represent   assembly contigs   unique occurrence use  information  apply  streamline readtoassembly alignment routine  use  align read pair   infer read distance distributions  pair  libraries  form  read set   identify contigs     certain neighborhood define   distributions  adjacency   neighborhood information  use   pet routine  merge set contigs connect  pair unambiguously  keep  list   merge set contigs   pedigree information   fasta header  backtrack  adjacency neighborhood  pedigree information along   contig coverage information  also use   assembly visualization tool abyssexplorer figure  show  abyssexplorer representation   set contigs   neighborhood note    edge   nod  polarize  accordance   directionality   contigs     mer overlap   respectively   interactive view   user doubleclicks   contig  direction  node connection polarizations flip  reflect  reverse complement pair end tag often resolve paths along set contigs    subsequently merge   pet stage  indicate  merge contigs  dark gray paths   viewer  one   contigs contribute   merge  select  abyssexplorer representation encode additional information include contig coverage indicate   edge thickness  contig length indicate   edge lengths  wave representation  use  indicate contig length    single oscillation correspond   userdefined number  nucleotides  long contig result  pack oscillations  obscure  arrowhead indicate  direction  resolve  ambiguity  envelope   oscillations outline  leaflike shape   thicker stem   shape mark  start   contig   thinner tip point   end  example contig       long contig   much longer   shortest contig       direction  still evident   shape   thinner tip point   right  perform  parameter search  assembly optimization  vary  value   range         set stage figure  show  key statistics   assemblies   function    pick  best assembly        number  contigs drop significantly       number  contigs    longer   increase  indicate  substantial improvement  contiguity beyond   number  contigs   categories keep decrease     assembly n50 ",5
104,ALLPATHS,"ALLPATHS: de novo assembly of whole-genome shotgun microreads
K-mer terminology (Pevzner et al. 2001) A K-mer in a genome is a sequence of K consecutive bases in it. K-mer x is adjacent to K-mer y (written x → y) if there is a (K + 1)-mer in the genome whose first K bases are x and whose last K bases are y. (It follows that x and y overlap by K − 1 bases.) By taking the K-mers as vertices and the adjacencies as edges, one obtains a graph, called the de Bruijn graph. This is related to, but distinct from, the unipath graph described next. Unipath and unipath graph definitions Informally, a “unipath” is a maximal unbranched sequence in a genome G, relative to a given minimum overlap K. Formally, consider the de Bruijn graph of G. A sequence U of length ≥K in the genome is expressible as a sequence of successively adjacent K-mers x1, . . . , xN, in which case N = length(U) − K + 1. If x1, . . . , xN − 1 have outdegree one and x2, . . . , xN have indegree one, and U cannot be lengthened without violating these constraints, U is a “unipath” (Fig. 7). Branchpoint-free circles are also unipaths. A given K-mer from G can occur in only one unipath, and every K-mer in a unipath is represented by one or more instances in G. In this sense, every K-mer in G lies in exactly one unipath. Two unipaths x, y are “adjacent” if the last K-mer of x is adjacent to the first K-mer of y. This allows us to define a graph, that we call the “unipath graph,” whose edges are the unipaths. Sequence graphs We represent knowledge about a given genome using a directed graph whose edges are DNA sequences. We call this structure a “sequence graph.” A unipath graph, which we compute exactly from a genome or approximately from reads, is one example, as are the assemblies that ALLPATHS produces. A connected component of a sequence graph is a connected component of it as a graph, that is, a physically connected subgraph, that is disconnected from all vertices and edges not included within it. Any sequence graph may be divided into its connected components. For a haploid genome whose sequence is completely known, there is one connected component for each chromosome, and each component has no branches, that is, each component is a single edge. In all other cases, branches represent our uncertainty about the exact genomic sequence. For a diploid genome, homologous chromosomes are merged in the graph, generally leading to bubbles (Fig. 6C) in places where there is polymorphism. Formally, for fixed K, a “sequence graph” is a directed graph whose edges are sequences, having the property that whenever x → y → z are edges, the end of the first edge perfectly overlaps the beginning of the second edge by exactly K − 1 bases. (The case K = 1 is allowed, in which case these edges abut but do not overlap.) Note that if we represent edges as sequences of adjacent K-mers, then the last K-mer of the first edge is adjacent to the first K-mer of the second edge. This is computationally convenient. The value of K can be changed by adjusting the edge sequences. A sequence graph has a natural output format: a FASTA file of the edge sequences, with header lines of the form Formula where v1 and v2 are vertex indices for the edge (v1 → v2). Note that the overall numbering of vertices is arbitrary. The vertex numbering does not contain any genomic information, other than indicating which edges are juxtaposed in the graph. Error correction We correct errors in reads using an approach related to Pevzner et al. (2001). For each read, we either keep it as is, edit it, or discard it. The process for this is as follows: Create a list of all K-mers in the reads. If there are N reads of length L, then the list has N(L − K + 1) entries. Any given K-mer occurs a certain number of times in the list. Let f(m) denote the total number of entries in the list that occur m times in the list. For example, if the list were AC, AC, AC, AT, AT, CG, CG, then f(2) would be 4. We expect the graph of f to have a sharp peak for low m, especially m = 1, arising primarily from incorrect K-mers (those containing sequencing errors), and a smooth hump for higher m, arising primarily from correct K-mers (those not containing sequencing errors). Let m1 be the first local minimum of f, expected to lie between the “sharp peak” and “smooth hump,” as above. Most K-mers having multiplicity less than m1 are incorrect, whereas most K-mers having multiplicity at least m1 are correct. We call these high-frequency K-mers “strong.” We consider various values of K (16, 20, 24) and compute m1 (and thus define strong) for each. If for all values of K, all the K-mers in a read are strong, we leave the read as is. Otherwise, for each K, we attempt to correct the read by making a “change”: one or two substitutions. (More could be allowed, but the process would take longer.) Each change is assigned a probability based on the quality scores at the changed bases. We consider only changes that make all the K-mers in the read strong, for all K values. If the most probable change is 10 times more likely than the next most probable change, we make the most probable change. Otherwise we discard the read. K-mer numbering and database In ALLPATHS, genome sequence is represented in one of three ways: as a sequence of bases, via K-mer numbering (as described here), and as a sequence of unipaths. For fixed K and a fixed collection S of DNA sequences closed under reverse-complementation, a “numbering of K-mer space” is an assignment of a unique integer to each K-mer that appears in S. If the same K-mer appears more than once, each instance must be assigned the same integer. We regard a numbering as “good” if it tends to assign consecutive numbers to K-mers appearing consecutively in the genome. It is reasonable to assume that there exists such a good numbering because if we knew the sequence of the genome, we could walk through it from beginning to end, numbering K-mers as 1, 2, 3, and so on, changing the numbering only when we hit a K-mer that had already been assigned a number. If we do not know the genome in advance, we need an algorithm that takes reads as input (see Methods, “K-mer Numbering Algorithm”). Given a good numbering of the K-mers of S, any DNA sequence that is in S may be translated first into a sequence of K-mers, then into the corresponding sequence of K-mer numbers (e.g., 100, 101, 102, 500, 501, 502, 503, 504, 505), and thence into a sequence of closed intervals of K-mer numbers (e.g., [100, 102], [500, 505]), which we call a “K-mer path.” A good K-mer numbering thus enables a compact representation of each sequence in S. More importantly, these K-mer paths, once computed, may be represented as a searchable database in the form of a sorted vector of pairs (x, y), where x is a K-mer path interval (e.g., [100, 102]) and y identifies the K-mer path from whence it came. Given any sequence s from S, represented as a K-mer path, this database allows rapid identification of all sequences in S that share a K-mer with s. (Software note: see KmerPath.{cc,h} and KmerPathInterval.{cc,h} for implementation.) K-mer numbering algorithm First we fix some terminology. A collection of reads is given. We distinguish between a K-mer, and an “occurrence” of that K-mer in the reads (or their reverse complements). A given K-mer may occur several times, but all will be assigned the same K-mer number. Now we describe the process for defining K-mer numbers, which has three steps. Given two sequences, a “maximal perfect alignment” between them is a choice of a window of equal length on each sequence, such that the bases in the respective windows match exactly, and such that the windows cannot be extended in either direction without violating this matching. We build certain maximal perfect alignments between the reads (and also their reverse complements). We do not build all such alignments, which would be computationally prohibitive. Rather, for a given K-mer x occurring in the reads (or their reverse complements), we consider the set S of all its occurrences (id, or, pos) where “id” is the numerical identifier of a read, “or” is the orientation of x on read id (forward or reverse), and “pos” is the start position of x on read id. We define the “canonical” occurrence of x to be the element of S that is lexicographically first. Read id is called the canonical read associated to x. For each other occurrence of x, we create a maximal perfect alignment between the read for the given occurrence and the read for the canonical occurrence, seeding on x. These alignments are shared in the sense that two different K-mers may ultimately contribute to the same alignment, and by so doing, we reduce the number of alignments to a manageable level. (The methodology of this step is adapted from the algorithm in Batzoglou et al. 2002 for finding all proper alignments between reads seeded on a K-mer not occurring more than a given number of times in the reads.) Assign temporary numbers to the occurrences of K-mers in the reads and their reverse complements. This temporary numbering system assigns different numbers to occurrences of the same K-mer. It numbers the occurrences in read 0 consecutively, starting with the number 0, then continues this consecutive numbering with the occurrences in read 1, and so forth. Now if n is the number of an occurrence of a K-mer on a read, then the number of the reverse complemented K-mer on the reverse complemented read is set to M − n, where M is a large fixed constant, unless the K-mer is a palindrome, in which case the number is set to n. Use the alignments of Step 1 to map K-mers on an arbitrary read to K-mers on the canonical reads associated to those K-mers, then assign them numbers via Step 2, thereby causing all occurrences of a given K-mer to have the same number. (Software note: see ReadsToPathsCore.cc for implementation.) Unipath generation We build the unipaths as K-mer paths (see Methods, “K-mer Numbering and Database”). The unipaths can then be converted into sequences as needed. Each K-mer path is a sequence of K-mer path intervals. The first step in generating unipaths is to find all the K-mer path intervals that will appear in any of them. Then we string together the intervals to form the unipaths. To find the intervals that make up the unipaths, we note for each interval in the K-mer path database the K-mer number before and after it, if any, in the path from which it came. Starting with the first K-mer number of the first interval in the table, we set the goal of finding the longest branchless interval of K-mer numbers containing that K-mer number, which will form a K-mer path interval in some unipath. To do this, starting with the first interval that contains that K-mer number, a branchless interval is posited beginning at the first K-mer number of that interval and ending at the last K-mer number of that interval. We proceed forward through the database looking for intervals that extend the posited interval or that indicate branches within it, lengthening or shortening the posited interval accordingly. As soon as an interval in the database is encountered that begins after the posited interval ends, work on the posited interval is complete, and it is a unipath interval, since all subsequent intervals in the database will not intersect the posited interval. The process is repeated for the next highest K-mer number not yet in a unipath interval, until no K-mers remain. With the unipath intervals in hand, it is a simple matter to build the unipaths. All we have to do is take the first (last) K-mer number in a given unipath interval, look it up in the database, thereby determine its possible predecessors (successors), and if there is exactly one, join the given unipath interval to the one on its left (right). This iterative joining process produces the unipaths. All paths definition Given a read pair (L, R) from a library with fragment size distribution D ± d, given K, and given a fixed constant e (typically ∼3), a path across the read pair (or closure) is a sequence that starts with L, ends with R, has length between D − ed and D + ed, and that can be covered by reads that perfectly match it, in such a way that one can walk from L to R using overlaps between reads that are all ≥K. That is one path across the read pair. By all paths across a read pair, we mean all the paths that can be obtained in this way. How to find all paths across a given read pair First, we assign numerical identifiers to each read in the set to be used in the search, including the reads in the pair. Then we find the minimal extensions of each read in that set. Conceptually, these are the reads that extend the given read in distinct ways by the smallest amount. Most reads have exactly one minimal extension; reads that have multiple minimal extensions border on branches in the genome. The search for closures over the minimal extensions therefore branches only when such a branch is determined by the content of the genome. To define an extension, we must choose a direction; without loss of generality, we consider extensions to the right. An extension is a read that aligns perfectly with the given read such that it overhangs the given read to the right, or it ends at the same base and has an identifier greater than that of the given read. A minimal extension is an extension that cannot be found transitively, i.e., if B is a minimal extension of A, then there is no extension X of A where B is also an extension of X. We also find the subsumptions of each read, where read A subsumes B if they align perfectly and A overhangs B to the left and right. The computation of minimal extensions and subsumptions can be done collectively for a large set of pairs that will be crossed using the same set of reads, as is the case with localized assembly. We then perform a depth-first search with these minimal extensions, beginning with one read of the pair and terminating a branch of the search either when the other read is encountered at a suitable distance (either directly or indirectly through a read that subsumes it), or the maximum distance (D + ed) has been exceeded. To make the results of this search usable, the solutions are stored as a graph structure in which the nodes are reads annotated with their offset from the start of the search. If the read under consideration can be extended by a read that has already led to solutions, the reads in the current search path are added to the solution graph, and the last read is linked to its previously encountered extending read, sharing the search results from that read on. This allows a further optimization: if a read has already been seen at a given offset and it is not part of the solution graph, that branch of the search can be pruned. Finding seed unipaths To find the seed unipaths, the idea is to start with all unipaths, then iteratively remove unipaths from that set. On a given iteration, we test unipaths for removal, in an order that favors unipaths of higher copy number, and secondarily to that, shorter unipaths. To see if a given unipath can be removed, we use read pairing to find the closest unipaths in the set that are to the left and to the right of the given unipath. We infer the distance between these left and right neighbors. If this distance is less than a threshold (set to 4 kb), then the given (middle) unipath can be removed. The iterations continue until no further unipaths can be removed from the set. (Software note: see UnipathSeeds.cc for implementation.) Short-fragment pair merger We start with the set of short-insert pairs for a neighborhood, that is, the secondary read cloud. The goal is to condense this set, reducing to a smaller set of pairs, and in the process making the residual pairs more informative, both by lengthening their “reads” and by reducing the SD of the separation between them. To that end, we first translate to a natural and highly compact local representation for all the short-fragment read pairs in the neighborhood. More specifically, we use the reads in the neighborhood to define local unipaths, in exactly the same way that approximate unipaths for the entire data set are defined. Each read may then be expressed as a sequence of local unipaths. (If a read does not fall on a unipath end, we extend it to the end of the unipath.) Furthermore, every read pair has a representation in terms of local unipaths, which might look like the following: Formula where the local unipaths are symbolized A, B, C, . . . for convenience, and the notation means that the predicted gap between the reads is 50 ± 5 K-mers. The right read has been reverse complemented so that a closure for the read pair has the form B.C. . . . .W.X.X.Y, where the ellipsis is filled with local unipath symbols. Each local unipath is itself expressible as a sequence of global unipaths. We assign to each local unipath the minimum of the predicted copy numbers for each of its constituent global unipaths. This “local copy number” is an upper bound on the number of times that the local unipath can appear in the genome. With these tools in hand, the short-fragment read pairs now admit a certain calculus that enables their condensation into a smaller set of pairs, whose reads are longer and whose separation SDs are smaller (and hence which tend to have fewer closures). For example, suppose we have pairs Formula where B has local copy number one and C is 10 K-mers long. Then we may merge the two pairs together, yielding a single pair Formula one of whose reads is longer and whose separation SD is smaller. Via several similar rules, the short-fragment read pairs may typically be condensed to a much smaller and more specific set. In relatively easy parts of haploid genomes, it is common for all the short-fragment pairs to reduce to a single “degenerate” pair, for example: Formula where n is the length of D.E.F in K-mers—so that the read pair is its own closure and this closure is itself the assembly of the neighborhood. Filtering of Solexa reads Reads were filtered based on their intrinsic quality by removing non-passing reads. We describe here the definition of a passing read. The Solexa system reports an intensity for each possible base (A,C,G,T) at each position on the read. We define a read to be “passing” if for each of the first 10 bases of the read, the intensity at the called base is at least 1000 and the ratio of the intensity at the called base to the next highest intensity is at least 2. Artificially paired Solexa reads A DNA isolate for E. coli K12 MG1655 from the Weinstock Lab at Baylor University was sequenced by Solexa. We combined the reads from 14 lanes on six flowcells: 7986.{1,2}, 7987.{1,2}, 8009.{1,2}, 7706.{1,2}, 9547.{1,2,3}, 8898.{2,3,4}. Passing reads were selected as in the “Filtering of Solexa Reads” section, above, except that we required intensity ≥1500. (Non-passing reads were discarded.) Then we aligned each read to the reference, picking at random one of its best placements. If a read was not aligned, we placed it randomly on the reference, thereby ensuring that every read was placed. The order of the reads was randomized. We then selected simulated read pairs, as described in the text. Prior to assembly, we trimmed the reads, using the following procedure. (1) For each read X, find all other reads that have a gap-free end-to-end alignment with it of length ≥20, seeded on a 12-mer, with four or less errors. (2) Call a base on X “supported” if there exist two such reads, with different orientations or offsets, that agree with the given base. Treat all other bases as “errors.” (3) Trim bases from the end of X until at most two errors remain.",Assembly,"allpaths  novo assembly  wholegenome shotgun microreads
kmer terminology pevzner     kmer   genome   sequence   consecutive base   kmer   adjacent  kmer  write  →        mer   genome whose first  base    whose last  base    follow     overlap     base  take  kmers  vertices   adjacencies  edge one obtain  graph call   bruijn graph   relate   distinct   unipath graph describe next unipath  unipath graph definitions informally  “unipath”   maximal unbranched sequence   genome  relative   give minimum overlap  formally consider   bruijn graph    sequence   length ≥   genome  expressible   sequence  successively adjacent kmers         case   lengthu               outdegree one         indegree one   cannot  lengthen without violate  constraints    “unipath” fig  branchpointfree circle  also unipaths  give kmer    occur   one unipath  every kmer   unipath  represent  one   instance     sense every kmer   lie  exactly one unipath two unipaths    “adjacent”   last kmer    adjacent   first kmer    allow   define  graph   call  “unipath graph” whose edge   unipaths sequence graph  represent knowledge   give genome use  direct graph whose edge  dna sequence  call  structure  “sequence graph”  unipath graph   compute exactly   genome  approximately  read  one example    assemblies  allpaths produce  connect component   sequence graph   connect component     graph    physically connect subgraph   disconnect   vertices  edge  include within   sequence graph may  divide   connect components   haploid genome whose sequence  completely know   one connect component   chromosome   component   branch    component   single edge    case branch represent  uncertainty   exact genomic sequence   diploid genome homologous chromosomes  merge   graph generally lead  bubble fig   place    polymorphism formally  fix   “sequence graph”   direct graph whose edge  sequence   property  whenever  →  →   edge  end   first edge perfectly overlap  begin   second edge  exactly    base  case     allow   case  edge abut    overlap note    represent edge  sequence  adjacent kmers   last kmer   first edge  adjacent   first kmer   second edge   computationally convenient  value     change  adjust  edge sequence  sequence graph   natural output format  fasta file   edge sequence  header line   form formula      vertex indices   edge  →  note   overall number  vertices  arbitrary  vertex number   contain  genomic information   indicate  edge  juxtapose   graph error correction  correct errors  read use  approach relate  pevzner      read  either keep    edit   discard   process     follow create  list   kmers   read     read  length    list       entries  give kmer occur  certain number  time   list let  denote  total number  entries   list  occur  time   list  example   list           would    expect  graph      sharp peak  low  especially    arise primarily  incorrect kmers  contain sequence errors   smooth hump  higher  arise primarily  correct kmers   contain sequence errors let    first local minimum   expect  lie   “sharp peak”  “smooth hump”    kmers  multiplicity less    incorrect whereas  kmers  multiplicity  least   correct  call  highfrequency kmers “strong”  consider various value       compute   thus define strong      value     kmers   read  strong  leave  read   otherwise     attempt  correct  read  make  “change” one  two substitutions  could  allow   process would take longer  change  assign  probability base   quality score   change base  consider  change  make   kmers   read strong    value    probable change   time  likely   next  probable change  make   probable change otherwise  discard  read kmer number  database  allpaths genome sequence  represent  one  three ways   sequence  base via kmer number  describe     sequence  unipaths  fix    fix collection   dna sequence close  reversecomplementation  “numbering  kmer space”   assignment   unique integer   kmer  appear      kmer appear     instance must  assign   integer  regard  number  “good”   tend  assign consecutive number  kmers appear consecutively   genome   reasonable  assume   exist   good number    know  sequence   genome  could walk    begin  end number kmers        change  number    hit  kmer   already  assign  number     know  genome  advance  need  algorithm  take read  input see methods “kmer number algorithm” give  good number   kmers    dna sequence     may  translate first   sequence  kmers    correspond sequence  kmer number            thence   sequence  close intervals  kmer number        call  “kmer path”  good kmer number thus enable  compact representation   sequence    importantly  kmer paths  compute may  represent   searchable database   form   sort vector  pair       kmer path interval      identify  kmer path  whence  come give  sequence    represent   kmer path  database allow rapid identification   sequence    share  kmer   software note see kmerpath{cch}  kmerpathinterval{cch}  implementation kmer number algorithm first  fix  terminology  collection  read  give  distinguish   kmer   “occurrence”   kmer   read   reverse complement  give kmer may occur several time     assign   kmer number   describe  process  define kmer number   three step give two sequence  “maximal perfect alignment”     choice   window  equal length   sequence    base   respective windows match exactly     windows cannot  extend  either direction without violate  match  build certain maximal perfect alignments   read  also  reverse complement    build   alignments  would  computationally prohibitive rather   give kmer  occur   read   reverse complement  consider  set     occurrences   pos  “”   numerical identifier   read “”   orientation    read  forward  reverse  “pos”   start position    read   define  “canonical” occurrence      element     lexicographically first read   call  canonical read associate      occurrence    create  maximal perfect alignment   read   give occurrence   read   canonical occurrence seed    alignments  share   sense  two different kmers may ultimately contribute    alignment      reduce  number  alignments   manageable level  methodology   step  adapt   algorithm  batzoglou     find  proper alignments  read seed   kmer  occur    give number  time   read assign temporary number   occurrences  kmers   read   reverse complement  temporary number system assign different number  occurrences    kmer  number  occurrences  read  consecutively start   number   continue  consecutive number   occurrences  read    forth      number   occurrence   kmer   read   number   reverse complement kmer   reverse complement read  set         large fix constant unless  kmer   palindrome   case  number  set   use  alignments  step   map kmers   arbitrary read  kmers   canonical read associate   kmers  assign  number via step  thereby cause  occurrences   give kmer     number software note see readstopathscorecc  implementation unipath generation  build  unipaths  kmer paths see methods “kmer number  database”  unipaths    convert  sequence  need  kmer path   sequence  kmer path intervals  first step  generate unipaths   find   kmer path intervals   appear       string together  intervals  form  unipaths  find  intervals  make   unipaths  note   interval   kmer path database  kmer number         path    come start   first kmer number   first interval   table  set  goal  find  longest branchless interval  kmer number contain  kmer number   form  kmer path interval   unipath    start   first interval  contain  kmer number  branchless interval  posit begin   first kmer number   interval  end   last kmer number   interval  proceed forward   database look  intervals  extend  posit interval   indicate branch within  lengthen  shorten  posit interval accordingly  soon   interval   database  encounter  begin   posit interval end work   posit interval  complete     unipath interval since  subsequent intervals   database   intersect  posit interval  process  repeat   next highest kmer number  yet   unipath interval   kmers remain   unipath intervals  hand    simple matter  build  unipaths       take  first last kmer number   give unipath interval look     database thereby determine  possible predecessors successors     exactly one join  give unipath interval   one   leave right  iterative join process produce  unipaths  paths definition give  read pair     library  fragment size distribution  ±  give   give  fix constant  typically   path across  read pair  closure   sequence  start   end    length             cover  read  perfectly match     way  one  walk     use overlap  read    ≥   one path across  read pair   paths across  read pair  mean   paths    obtain   way   find  paths across  give read pair first  assign numerical identifiers   read   set   use   search include  read   pair   find  minimal extensions   read   set conceptually    read  extend  give read  distinct ways   smallest amount  read  exactly one minimal extension read   multiple minimal extensions border  branch   genome  search  closure   minimal extensions therefore branch     branch  determine   content   genome  define  extension  must choose  direction without loss  generality  consider extensions   right  extension   read  align perfectly   give read    overhang  give read   right   end    base    identifier greater     give read  minimal extension   extension  cannot  find transitively      minimal extension       extension       also  extension    also find  subsumptions   read  read  subsume    align perfectly   overhang    leave  right  computation  minimal extensions  subsumptions    collectively   large set  pair    cross use   set  read    case  localize assembly   perform  depthfirst search   minimal extensions begin  one read   pair  terminate  branch   search either    read  encounter   suitable distance either directly  indirectly   read  subsume    maximum distance      exceed  make  result   search usable  solutions  store   graph structure    nod  read annotate   offset   start   search   read  consideration   extend   read   already lead  solutions  read   current search path  add   solution graph   last read  link   previously encounter extend read share  search result   read   allow   optimization   read  already  see   give offset     part   solution graph  branch   search   prune find seed unipaths  find  seed unipaths  idea   start   unipaths  iteratively remove unipaths   set   give iteration  test unipaths  removal   order  favor unipaths  higher copy number  secondarily   shorter unipaths  see   give unipath   remove  use read pair  find  closest unipaths   set     leave    right   give unipath  infer  distance   leave  right neighbor   distance  less   threshold set      give middle unipath   remove  iterations continue    unipaths   remove   set software note see unipathseedscc  implementation shortfragment pair merger  start   set  shortinsert pair   neighborhood    secondary read cloud  goal   condense  set reduce   smaller set  pair    process make  residual pair  informative   lengthen  “reads”   reduce     separation     end  first translate   natural  highly compact local representation    shortfragment read pair   neighborhood  specifically  use  read   neighborhood  define local unipaths  exactly   way  approximate unipaths   entire data set  define  read may   express   sequence  local unipaths   read   fall   unipath end  extend    end   unipath furthermore every read pair   representation  term  local unipaths  might look like  follow formula   local unipaths  symbolize        convenience   notation mean   predict gap   read   ±  kmers  right read   reverse complement    closure   read pair   form     wxxy   ellipsis  fill  local unipath symbols  local unipath   expressible   sequence  global unipaths  assign   local unipath  minimum   predict copy number     constituent global unipaths  “local copy number”   upper bind   number  time   local unipath  appear   genome   tool  hand  shortfragment read pair  admit  certain calculus  enable  condensation   smaller set  pair whose read  longer  whose separation sds  smaller  hence  tend   fewer closure  example suppose   pair formula    local copy number one     kmers long   may merge  two pair together yield  single pair formula one  whose read  longer  whose separation   smaller via several similar rule  shortfragment read pair may typically  condense   much smaller   specific set  relatively easy part  haploid genomes   common    shortfragment pair  reduce   single “degenerate” pair  example formula     length  def  kmers—   read pair    closure   closure    assembly   neighborhood filter  solexa read read  filter base   intrinsic quality  remove nonpassing read  describe   definition   pass read  solexa system report  intensity   possible base acgt   position   read  define  read   “passing”      first  base   read  intensity   call base   least    ratio   intensity   call base   next highest intensity   least  artificially pair solexa read  dna isolate   coli k12 mg1655   weinstock lab  baylor university  sequence  solexa  combine  read   lanes  six flowcells {} {} {} {} {} {} pass read  select    “filtering  solexa reads” section  except   require intensity ≥ nonpassing read  discard   align  read   reference pick  random one   best placements   read   align  place  randomly   reference thereby ensure  every read  place  order   read  randomize   select simulate read pair  describe   text prior  assembly  trim  read use  follow procedure    read  find   read    gapfree endtoend alignment    length ≥ seed   mer  four  less errors  call  base   “supported”   exist two  read  different orientations  offset  agree   give base treat   base  “errors”  trim base   end      two errors remain",5
105,Oases,"Oases: robust de novo RNA-seq assembly across the dynamic range of expression levels.
The Oases assembly process, explained in detail below and illustrated in Figure 1, consists of independent assemblies, which vary by one important parameter, the hash (or k-mer) length. In each of the assemblies, the reads are used to build a de Bruijn graph, which is then simplified for errors, organized into a scaffold, divided into loci and finally analyzed to extract transcript assemblies or transfrags. Once all of the individual k-mer assemblies are finished, they are merged into a final assembly. Contig assembly The Oases pipeline receives as input a preliminary assembly produced by the Velvet assembler (Zerbino and Birney, 2008) which was designed to produce scaffolds from genomic readsets. Its initial stages, namely hashing and graph construction can be used indifferently on transcriptome data. We only run these stages of Velvet to produce a preliminary fragmented assembly, containing the mapping of the reads onto a set of contigs. However, the later stage algorithms, Pebble and Rock Band, which resolve repeats in Velvet, are not used because they rely on assumptions related to genomic sequencing (Zerbino et al., 2009). Namely, the coverage distribution should be roughly uniform across the genome and the genome should not contain any branching point. These conditions prevent those algorithms from being reliable and efficient on RNA-seq data. 2.3 Contig correction After reading the contigs produced by Velvet, Oases proceeds to correct them again with a set of dynamic and static filters. The first dynamic correction is a slightly modified version of Velvet's error correction algorithm, TourBus. TourBus searches through the graph for parallel paths that have the same starting and end node. If their sequences are similar enough, the path with lower coverage is merged into the path with higher coverage, irrespective of their absolute coverage. In this sense, the TourBus algorithm is adapted to RNA-seq data and fluctuating coverage depths. However, for performance issues, the Velvet version of TourBus only visits each node once, meaning that it does not exhaustively compare all possible pairs of paths. Given the high coverage of certain genes, and the complexity of the corresponding graphs, with numerous false positive paths, it is necessary for Oases to exhaustively examine the graph, visiting nodes several times if necessary. In addition to this correction, Oases includes a local edge removal. For each node, an outgoing edge is removed if its coverage represents <10% of the sum of coverages of outgoing edges from that same node. This approach, similar to the one presented by Yassour et al. (2011), is based on the assumption that on high coverage regions, spurious errors are likely to reoccur more often. Finally, all contigs with less than a static coverage cutoff (by default 3×) are removed from the assembly. The rationale for this filter is that any transcript with such a low coverage cannot be properly assembled in the first place, so it is expedient to remove them from the assembly, along with many low coverage contigs created by spurious errors. 2.4 Scaffold construction The distance information between the contigs is then summarized into a set of distance estimates called a scaffold, as described in (Zerbino et al., 2009). Because a read in a de Bruijn graph can be split between several contigs, the distance estimate for a connection between two contigs can be supported by both spanning single reads or paired-end reads. The total number of spanning reads and pair-end reads confirming a connection is called its support. A connection which is supported by at least one spanning read is called direct, otherwise, it is indirect. Connections are assigned a total weight. It is calculated by adding 1 for each supporting spanning read and a probabilistic weight for each spanning pair, proportional to the likelihood of observing the paired reads at their observed positions on the contigs given the estimated distance between the contigs and assuming a normal insert length distribution model. 2.5 Scaffold filtering Much like the contig correction phase, several filters are applied to the scaffold: static coverage thresholds for the very low coverage sequences and a dynamic coverage threshold that adapts to the local coverage depth. Because coverage is no longer indicative of the uniqueness of a sequence, contig length is used as an indicator. Based on the decreasing likelihood of high identity conservation as a function of sequence length (Whiteford et al., 2005), contigs longer than a given threshold [by default (50+k−1) bp] are labeled as long and treated as if unique and the other nodes are labeled as short. Connections with a low support (by default 3× or lower) or with a weight <0.1 are first removed. Two short contigs can only be joined by a direct connection with no intermediate gap. A short and a long contig can only be connected by a direct connection. Finally, connections between long contigs are tested against a modified version of the statistic presented in (Zerbino et al., 2009), which estimates how many read pairs should connect two contigs given their respective coverages and the estimated distance separating them (see Supplementary Material). Indirect connections with a support lower than a given threshold (by default 10% of this expected count) are thus eliminated. 2.6 Locus construction Oases then organizes the contigs into clusters called loci, as illustrated in Figure 1. This terminology stems from the fact that in the ideal case, where no gap in coverage or overlap with exterior sequences complicate matters, all the transcripts from one gene should be assembled into a connected component of contigs. Unfortunately, in experimental conditions, this equivalence between components and genes cannot be guaranteed. It is to be expected that loci sometimes represent fragments of genes or clusters of homologous sequences. Scaffold construction takes place in two stages similarly to the approach described by Butler et al. (2008). Long contigs are first clustered into connected components. These long nodes have a higher likelihood of being unique, therefore it is assumed that two contigs which belong to the same component also belong to the same gene. To each locus are added the short nodes which are connected to one of the long nodes in the cluster. 2.7 Transitive reduction of the loci For the following analyses to function properly, it is necessary to remove redundant long distance connections, and retain only connections between immediate neighbors, as seen in Figure 1. For example, it is common that two contigs which are not consecutive in a locus are connected by a paired-end read. A connection is considered redundant if it connects two nodes that are connected by a distinct path of connections such that the connection and the two paths have comparable lengths. The transitive reduction implemented in Oases is inspired from the one described in (Myers, 2005) but had to be adapted to the conditions of short read data. In particular, short contigs can be repeated or even inverted within a single transcript and form loops in the connection graph. Because of this, occasional situations arise where every connection coming out of a node can be transitively reduced by another one, thus removing all of them, and breaking the connectivity of the locus. To avoid this, a limit is imposed on the number of removed connections. If two connections have the capacity to reduce each other, the shortest one is preserved. 2.8 Extracting transcript assemblies The sequence information of the transcripts is now contained in the loci. These loci can be fragmented because of alternative splicing events which cause the de Bruijn graph to have a branch. Oases, therefore, analyses the topology of the loci to extract full length isoform assemblies. In many cases, the loci present a simple topology which can be trivially and uniquely decomposed as one or two transcripts. We define three categories of trivial locus topologies (Fig. 1): chains, forks and bubbles, which if isolated from any other branching point, are straightforward to resolve. These three topologies are easily identifiable using the degrees of the nodes. Oases, therefore, detects all the trivial loci and enumerates the possible transcripts for each of them. Because the above exact method only applies to specific cases, an additional robust heuristic method is applied to the remaining loci, referred to as complex loci. Oases uses a reimplementation of the algorithm described in (Lee, 2003), which efficiently produces a parsimonious set of putative highly expressed transcripts, assuming independence of the alternative splicing events. This extension of the algorithm is quite intuitive, since there is a direct analogy between the de Bruijn graph built from the transcripts of a gene and its splicing graph, as noted by Heber et al. (2002). Using dynamic programming, it enumerates heavily weighted paths through the locus graph in decreasing order of coverage, until either all the contigs of the locus are covered, or a specified number of transcripts is produced (by default 10). As in the transitive reduction phase, this algorithm had to be slightly modified to allow for loops in the putative splicing graph of the locus. Loops are problematic because their presence can prevent the propagation of the dynamic programming algorithm to all the contigs of a locus. When a loop is detected, it is broken at a contig which connects the loop to the rest of the locus, so as to leave a minimum number of branch points, as described in the Supplementary Material. 2.9 Merging assemblies with Oases-M De Bruijn graph assemblers are very sensitive to the setting of the hash length k. For transcriptome data, this optimization is more complex as transcript expression levels and coverage depths are distributed over a wide range. A way to avoid the dependence on the parameter k is to produce a merged transcriptome assembly of previously generated transfrags from Oases. Oases is run for a set of [kMIN,…,kMAX] values and the output transfrags are stored. All predicted transfrags from runs in the interval are then fed into the second stage of the pipeline, Oases-M, with a user selected kMERGE. A de Bruijn graph for kMERGE is built from these transfrags. After removing small variants with the Tourbus algorithm, any transfrag in the graph that is identical or included in another transfrag is removed. The final assembly is constructed by following the remaining transfrags through the merged graph.",Assembly,"oases robust  novo rnaseq assembly across  dynamic range  expression levels
 oases assembly process explain  detail   illustrate  figure  consist  independent assemblies  vary  one important parameter  hash  kmer length     assemblies  read  use  build   bruijn graph    simplify  errors organize   scaffold divide  loci  finally analyze  extract transcript assemblies  transfrags     individual kmer assemblies  finish   merge   final assembly contig assembly  oases pipeline receive  input  preliminary assembly produce   velvet assembler zerbino  birney    design  produce scaffold  genomic readsets  initial stag namely hash  graph construction   use indifferently  transcriptome data   run  stag  velvet  produce  preliminary fragment assembly contain  map   read onto  set  contigs however  later stage algorithms pebble  rock band  resolve repeat  velvet   use   rely  assumptions relate  genomic sequence zerbino    namely  coverage distribution   roughly uniform across  genome   genome   contain  branch point  condition prevent  algorithms   reliable  efficient  rnaseq data  contig correction  read  contigs produce  velvet oases proceed  correct     set  dynamic  static filter  first dynamic correction   slightly modify version  velvet' error correction algorithm tourbus tourbus search   graph  parallel paths     start  end node   sequence  similar enough  path  lower coverage  merge   path  higher coverage irrespective   absolute coverage   sense  tourbus algorithm  adapt  rnaseq data  fluctuate coverage depths however  performance issue  velvet version  tourbus  visit  node  mean     exhaustively compare  possible pair  paths give  high coverage  certain genes   complexity   correspond graph  numerous false positive paths   necessary  oases  exhaustively examine  graph visit nod several time  necessary  addition   correction oases include  local edge removal   node  outgo edge  remove   coverage represent    sum  coverages  outgo edge    node  approach similar   one present  yassour     base   assumption   high coverage regions spurious errors  likely  reoccur  often finally  contigs  less   static coverage cutoff  default   remove   assembly  rationale   filter    transcript    low coverage cannot  properly assemble   first place    expedient  remove    assembly along  many low coverage contigs create  spurious errors  scaffold construction  distance information   contigs   summarize   set  distance estimate call  scaffold  describe  zerbino      read    bruijn graph   split  several contigs  distance estimate   connection  two contigs   support   span single read  pairedend read  total number  span read  pairend read confirm  connection  call  support  connection   support   least one span read  call direct otherwise   indirect connections  assign  total weight   calculate  add    support span read   probabilistic weight   span pair proportional   likelihood  observe  pair read   observe position   contigs give  estimate distance   contigs  assume  normal insert length distribution model  scaffold filter much like  contig correction phase several filter  apply   scaffold static coverage thresholds    low coverage sequence   dynamic coverage threshold  adapt   local coverage depth  coverage   longer indicative   uniqueness   sequence contig length  use   indicator base   decrease likelihood  high identity conservation   function  sequence length whiteford    contigs longer   give threshold  default    label  long  treat   unique    nod  label  short connections   low support  default   lower    weight   first remove two short contigs    join   direct connection   intermediate gap  short   long contig    connect   direct connection finally connections  long contigs  test   modify version   statistic present  zerbino     estimate  many read pair  connect two contigs give  respective coverages   estimate distance separate  see supplementary material indirect connections   support lower   give threshold  default    expect count  thus eliminate  locus construction oases  organize  contigs  cluster call loci  illustrate  figure   terminology stem   fact    ideal case   gap  coverage  overlap  exterior sequence complicate matter   transcripts  one gene   assemble   connect component  contigs unfortunately  experimental condition  equivalence  components  genes cannot  guarantee     expect  loci sometimes represent fragment  genes  cluster  homologous sequence scaffold construction take place  two stag similarly   approach describe  butler    long contigs  first cluster  connect components  long nod   higher likelihood   unique therefore   assume  two contigs  belong    component also belong    gene   locus  add  short nod   connect  one   long nod   cluster  transitive reduction   loci   follow analyse  function properly   necessary  remove redundant long distance connections  retain  connections  immediate neighbor  see  figure   example   common  two contigs    consecutive   locus  connect   pairedend read  connection  consider redundant   connect two nod   connect   distinct path  connections    connection   two paths  comparable lengths  transitive reduction implement  oases  inspire   one describe  myers      adapt   condition  short read data  particular short contigs   repeat  even invert within  single transcript  form loop   connection graph    occasional situations arise  every connection come    node   transitively reduce  another one thus remove     break  connectivity   locus  avoid   limit  impose   number  remove connections  two connections   capacity  reduce    shortest one  preserve  extract transcript assemblies  sequence information   transcripts   contain   loci  loci   fragment   alternative splice events  cause   bruijn graph    branch oases therefore analyse  topology   loci  extract full length isoform assemblies  many case  loci present  simple topology    trivially  uniquely decompose  one  two transcripts  define three categories  trivial locus topologies fig  chain fork  bubble   isolate    branch point  straightforward  resolve  three topologies  easily identifiable use  degrees   nod oases therefore detect   trivial loci  enumerate  possible transcripts        exact method  apply  specific case  additional robust heuristic method  apply   remain loci refer   complex loci oases use  reimplementation   algorithm describe  lee   efficiently produce  parsimonious set  putative highly express transcripts assume independence   alternative splice events  extension   algorithm  quite intuitive since    direct analogy    bruijn graph build   transcripts   gene   splice graph  note  heber    use dynamic program  enumerate heavily weight paths   locus graph  decrease order  coverage  either   contigs   locus  cover   specify number  transcripts  produce  default     transitive reduction phase  algorithm    slightly modify  allow  loop   putative splice graph   locus loop  problematic   presence  prevent  propagation   dynamic program algorithm    contigs   locus   loop  detect   break   contig  connect  loop   rest   locus    leave  minimum number  branch point  describe   supplementary material  merge assemblies  oasesm  bruijn graph assemblers   sensitive   set   hash length   transcriptome data  optimization   complex  transcript expression level  coverage depths  distribute   wide range  way  avoid  dependence   parameter    produce  merge transcriptome assembly  previously generate transfrags  oases oases  run   set  kmin…kmax value   output transfrags  store  predict transfrags  run   interval   feed   second stage   pipeline oasesm   user select kmerge   bruijn graph  kmerge  build   transfrags  remove small variants   tourbus algorithm  transfrag   graph   identical  include  another transfrag  remove  final assembly  construct  follow  remain transfrags   merge graph",5
106,SPADES,"SPAdes: A New Genome Assembly Algorithm and Its Applications to Single-Cell Sequencing
Below we outline the four stages of SPAdes, which deal with issues that are particularly troublesome in SCS: sequencing errors; non-uniform coverage; insert size variation; and chimeric reads and bireads: (1) Stage 1 (assembly graph construction) is addressed by every NGS assembler and is often referred to as de Bruijn graph simplification (e.g., bulge/bubble removal in EULER/Velvet). We propose a new approach to assembly graph construction that uses the multisized de Bruijn graph, implements new bulge/tip removal algorithms, detects and removes chimeric reads, aggregates biread information into distance histograms, and allows one to backtrack the performed graph operations. (2) Stage 2 (k-bimer adjustment) derives accurate distance estimates between k-mers in the genome (edges in the assembly graph) using joint analysis of distance histograms and paths in the assembly graph. (3) Stage 3 constructs the paired assembly graph, inspired by the PDBG approach. (4) Stage 4 (contig construction) was well studied in the context of Sanger sequencing (Ewing et al., 1998). Since NGS projects typically feature high coverage, NGS assemblers generate rather accurate contigs (although the accuracy deteriorates for SCS). SPAdes constructs DNA sequences of contigs and the mapping of reads to contigs by backtracking graph simplifications (see Section 8.6). Previous studies demonstrated that coupling various assemblers with error correction tools improves their performance (Pevzner et al., 2001; Kelley et al., 2010; Ilie et al., 2010; Gnerre et al., 2011).4 However, most error correction tools (e.g., Quake [Kelley et al., 2010]) perform poorly on single-cell data since they implicitly assume nearly uniform read coverage. Chitsaz et al. (2011) coupled Velvet-SC with the error-correction in EULER (Chaisson and Pevzner, 2008), resulting in the tool E+V-SC. In this article, SPAdes uses a modification of Hammer (Medvedev et al., 2011b) (aimed at SCS) for error correction and quality trimming prior to assembly.5 Our paired assembly graph approach differs from existing approaches to assembly and dictates new algorithmic solutions for various stages of SPAdes. Thus, we will describe several variations of de Bruijn graphs, leading to construction of the paired assembly graph (covering stages 2 and 3), ",Assembly,"spade  new genome assembly algorithm   applications  singlecell sequencing
  outline  four stag  spade  deal  issue   particularly troublesome  scs sequence errors nonuniform coverage insert size variation  chimeric read  bireads  stage  assembly graph construction  address  every ngs assembler   often refer    bruijn graph simplification  bulgebubble removal  eulervelvet  propose  new approach  assembly graph construction  use  multisized  bruijn graph implement new bulgetip removal algorithms detect  remove chimeric read aggregate biread information  distance histograms  allow one  backtrack  perform graph operations  stage  kbimer adjustment derive accurate distance estimate  kmers   genome edge   assembly graph use joint analysis  distance histograms  paths   assembly graph  stage  construct  pair assembly graph inspire   pdbg approach  stage  contig construction  well study   context  sanger sequence ewing    since ngs project typically feature high coverage ngs assemblers generate rather accurate contigs although  accuracy deteriorate  scs spade construct dna sequence  contigs   map  read  contigs  backtrack graph simplifications see section  previous study demonstrate  couple various assemblers  error correction tool improve  performance pevzner    kelley    ilie    gnerre    however  error correction tool  quake kelley    perform poorly  singlecell data since  implicitly assume nearly uniform read coverage chitsaz    couple velvetsc   errorcorrection  euler chaisson  pevzner  result   tool evsc   article spade use  modification  hammer medvedev    aim  scs  error correction  quality trim prior  assembly  pair assembly graph approach differ  exist approach  assembly  dictate new algorithmic solutions  various stag  spade thus   describe several variations   bruijn graph lead  construction   pair assembly graph cover stag    ",5
107,SOAPdenovo2,"SOAPdenovo2: an empirically improved memory-efficient short-read de novo assembler
The increased use of next generation sequencing (NGS) has resulted in an increased growth of the number of de novo genome assemblies being carried out using short reads. Although there are several de novo assemblers available, there remains room for improvement as shown in recent assembly evaluation projects such as Assemblathon 1 [1] and GAGE [2]. Since the publication of the first version of SOAPdenovo [3], it has been used to assemble many large eukaryotic genomes, but reports have indicated areas that would benefit from updates, including assembly coverage and length [4,5]. SOAPdenovo2, as with SOAPdenovo, is made up of six modules that handle read error correction, de Bruijn graph (DBG) construction, contig assembly, paired-end (PE) reads mapping, scaffold construction, and gap closure. The major improvements we have made for in SOAPdenovo2 are: 1) enhancing the error correction algorithm, 2) providing a reduction in memory consumption in DBG constructions, 3) resolving longer repeat regions in contig assembly, 4) increasing assembly length and coverage in scaffolding and 5) improving gap closure. Our data show that SOAPdenovo2 outperforms its predecessor on the majority of the metrics benchmarked in the Assemblathon 1 as well as GAGE; and in addition, was able to substantially improve the original assembly of the Asian (YH) genome [6] that was done using SOAPdenovo. Improvements in SOAPdenovo2 Dealing with sequencing error in NGS data is inevitable, especially for genome assembly applications, the outcome of which could be largely affected by even a small amount of sequencing error. Hence it is mandatory to detect and revise these sequencing errors in reads before assembly [2,7]. However, the error correction module in SOAPdenovo was designed for short Illumina reads (35–50 bp), which consumes an excessive amount of computational time and memory on longer reads, for example, over 150 GB memory running for two days using 40-fold 100 bp paired-end Illumina HiSeq 2000 reads. Thus, by a skillful exploitation of data indexing strategies, we redeveloped the module, which supports memory efficient long-k-mer error correction and uses a new space k-mer scheme to improve the accuracy and sensitivity (see Additional file 1: Supplementary Method 1 and Figures S1–S3). Simulation test shows that the new version runs efficiently and corrects more reads authentically (see Additional file 1: Tables S1 and S2). In DBG-based large-genome assembly, the graph construction step consumes the largest amount of memory. To reduce this in SOAPdenovo2, we implemented a sparse de Bruijn graph method [8] (see Additional file 1: Supplementary Method 2), where reads are cut into k-mers and a large number of the linear unique k-mers are combined as a group instead of being stored independently. Another important factor in the success of DBG-based assembly is k-mer size selection. Using a large k-mer has the advantage of resolving more repeat regions; whereas, use of small k-mers is advantageous for assembling low coverage depth and removing sequencing errors. To fully utilize both these advantages, we introduced a multiple k-mer strategy [9] in SOAPdenovo2 (see Additional file 1: Supplementary Method 3 and Figure S4). First, we removed sequencing errors using small k-mers for graph building, and then we rebuilt the graph using larger k-mers iteratively by mapping the reads back to the previous DBG to resolve longer repeats. Scaffold construction is another area that needs improvement in NGS de novo assembly programs [10]. In the original SOAPdenovo, scaffolds were built by utilizing PE reads starting with short insert sizes (∼200 bp) followed iteratively to large insert sizes (∼10 kbp) [3]. Although this iterative method greatly decreased the complexity of scaffolding and enabled the assembly of larger genomes, there remained many issues that resulted in lower scaffold quality and shorter length. For example, 1) the heterozygous contigs were improperly handled; 2) chimeric scaffolds erroneously built with the smaller insert size PE reads which then hindered the later steps to increase of scaffold length when adding PE reads with larger insert size; and 3) false relationships between contigs without sufficient PE information support were created occasionally. To improve this in SOAPdenovo2, the main changes during the scaffolding stage were as follows: 1) we detected heterozygous contig pairs using contig depth and local contig relationships. Under these conditions, only the contig with higher depth in the heterozygous pairs was kept in scaffold, which reduced the influence of heterozygosity on the scaffolds length; 2) chimeric scaffolds that were built using a smaller insert size library were rectified using information from a larger insert size library, and 3) we developed a topology-based method to reestablish relationships between contigs that had insufficient PE information support (see Additional file 1: Supplementary Method 4 and Figures S5–S7). Short reads enabled us to reconstruct large vertebrate and plant genomes, but the assembly of repetitive sequences longer than the read length still remain to be tackled. In scaffold construction, contigs with certain distance relationship, but without genotypes amid were connected with wildcards. The GapCloser module was designed to replace these wildcards using the context and PE reads information. In SOAPdenovo2, we have improved the original SOAPdenovo GapCloser module, which assembled sequences iteratively in the gaps to fill large gaps. At each iterative cycle, the previous release of GapCloser considered only the reads that could be aligned in current cycle. This method could potentially make for an incorrect selection at inconsistent locations with insufficient information for distinguishment due to the high similarity between repetitive sequences. For SOAPdenovo2, we developed a new method that considered all reads aligned during previous cycles, which allowed for better resolution of these conflicting bases, and thus improved the accuracy of gap closure. ",Assembly,"soapdenovo2  empirically improve memoryefficient shortread  novo assembler
 increase use  next generation sequence ngs  result   increase growth   number   novo genome assemblies  carry  use short read although   several  novo assemblers available  remain room  improvement  show  recent assembly evaluation project   assemblathon    gage  since  publication   first version  soapdenovo     use  assemble many large eukaryotic genomes  report  indicate areas  would benefit  update include assembly coverage  length  soapdenovo2   soapdenovo  make   six modules  handle read error correction  bruijn graph dbg construction contig assembly pairedend  read map scaffold construction  gap closure  major improvements   make   soapdenovo2   enhance  error correction algorithm  provide  reduction  memory consumption  dbg constructions  resolve longer repeat regions  contig assembly  increase assembly length  coverage  scaffold   improve gap closure  data show  soapdenovo2 outperform  predecessor   majority   metrics benchmarked   assemblathon   well  gage   addition  able  substantially improve  original assembly   asian  genome     use soapdenovo improvements  soapdenovo2 deal  sequence error  ngs data  inevitable especially  genome assembly applications  outcome   could  largely affect  even  small amount  sequence error hence   mandatory  detect  revise  sequence errors  read  assembly  however  error correction module  soapdenovo  design  short illumina read    consume  excessive amount  computational time  memory  longer read  example    memory run  two days use fold   pairedend illumina hiseq  read thus   skillful exploitation  data index strategies  redevelop  module  support memory efficient longkmer error correction  use  new space kmer scheme  improve  accuracy  sensitivity see additional file  supplementary method   figure s1s3 simulation test show   new version run efficiently  correct  read authentically see additional file  table     dbgbased largegenome assembly  graph construction step consume  largest amount  memory  reduce   soapdenovo2  implement  sparse  bruijn graph method  see additional file  supplementary method   read  cut  kmers   large number   linear unique kmers  combine   group instead   store independently another important factor   success  dbgbased assembly  kmer size selection use  large kmer   advantage  resolve  repeat regions whereas use  small kmers  advantageous  assemble low coverage depth  remove sequence errors  fully utilize   advantage  introduce  multiple kmer strategy   soapdenovo2 see additional file  supplementary method   figure  first  remove sequence errors use small kmers  graph build    rebuild  graph use larger kmers iteratively  map  read back   previous dbg  resolve longer repeat scaffold construction  another area  need improvement  ngs  novo assembly program    original soapdenovo scaffold  build  utilize  read start  short insert size   follow iteratively  large insert size  kbp  although  iterative method greatly decrease  complexity  scaffold  enable  assembly  larger genomes  remain many issue  result  lower scaffold quality  shorter length  example   heterozygous contigs  improperly handle  chimeric scaffold erroneously build   smaller insert size  read   hinder  later step  increase  scaffold length  add  read  larger insert size   false relationships  contigs without sufficient  information support  create occasionally  improve   soapdenovo2  main change   scaffold stage   follow   detect heterozygous contig pair use contig depth  local contig relationships   condition   contig  higher depth   heterozygous pair  keep  scaffold  reduce  influence  heterozygosity   scaffold length  chimeric scaffold   build use  smaller insert size library  rectify use information   larger insert size library    develop  topologybased method  reestablish relationships  contigs   insufficient  information support see additional file  supplementary method   figure s5s7 short read enable   reconstruct large vertebrate  plant genomes   assembly  repetitive sequence longer   read length still remain   tackle  scaffold construction contigs  certain distance relationship  without genotypes amid  connect  wildcards  gapcloser module  design  replace  wildcards use  context   read information  soapdenovo2   improve  original soapdenovo gapcloser module  assemble sequence iteratively   gap  fill large gap   iterative cycle  previous release  gapcloser consider   read  could  align  current cycle  method could potentially make   incorrect selection  inconsistent locations  insufficient information  distinguishment due   high similarity  repetitive sequence  soapdenovo2  develop  new method  consider  read align  previous cycle  allow  better resolution   conflict base  thus improve  accuracy  gap closure ",5
108,SSAKE,"Assembling millions of short DNA sequences using SSAKE.
DNA sequences in a single multi fasta file are read in memory, populating a hash table keyed by unique sequence reads with values representing the number of occurrences of that sequence in the set. A prefix tree is used to organize the sequences and their reverse-complemented counterparts by their first eleven 5′ end bases. The sequence reads are sorted by decreasing number of occurrences to reflect coverage and minimize extension of reads containing sequencing errors. Each unassembled read, u, is used in turn to nucleate an assembly. Each possible 3′ most k-mer is generated from u and is used for the search until the word length is smaller than a user-defined minimum, m, or until the k-mer has a perfect match with the 5′ end bases of read r. In that latter case, u is extended by the unmatched 3′ end bases contained in r, and r is removed from the hash table and prefix tree. The process of cycling through progressively shorter 3′-most k-mers is repeated after every extension of u. Since only left-most searches are possible with a prefix tree, when all possibilities have been exhausted for the 3′ extension, the complementary strand of the contiguous sequence generated (contig) is used to extend the contig on the 5′ end. The DNA prefix tree is used to limit the search space by efficiently binning the sequence reads. There are two ways to control the stringency in SSAKE. The first is to stop the extension when a k-mer matches the 5′ end of more than one sequence read (−s 1). This leads to shorter contigs, but minimizes sequence misassemblies. The second is to stop the extension when a k-mer is smaller than a user-set minimum word length (m). SSAKE outputs a log file with run information along with two multi fasta files, one containing all sequence contigs constructed and the other containing the unassembled sequence reads. ",Assembly,"assemble millions  short dna sequence use ssake
dna sequence   single multi fasta file  read  memory populate  hash table key  unique sequence read  value represent  number  occurrences   sequence   set  prefix tree  use  organize  sequence   reversecomplemented counterparts   first eleven ′ end base  sequence read  sort  decrease number  occurrences  reflect coverage  minimize extension  read contain sequence errors  unassembled read   use  turn  nucleate  assembly  possible ′  kmer  generate     use   search   word length  smaller   userdefined minimum     kmer   perfect match   ′ end base  read    latter case   extend   unmatched ′ end base contain      remove   hash table  prefix tree  process  cycle  progressively shorter ′ kmers  repeat  every extension   since  leftmost search  possible   prefix tree   possibilities   exhaust   ′ extension  complementary strand   contiguous sequence generate contig  use  extend  contig   ′ end  dna prefix tree  use  limit  search space  efficiently bin  sequence read   two ways  control  stringency  ssake  first   stop  extension   kmer match  ′ end    one sequence read    lead  shorter contigs  minimize sequence misassemblies  second   stop  extension   kmer  smaller   userset minimum word length  ssake output  log file  run information along  two multi fasta file one contain  sequence contigs construct    contain  unassembled sequence read ",5
109,GAML,"GAML: Genome Assembly by Maximum Likelihood
Recently, several probabilistic models were introduced as a measure of the assembly quality [11–13]. All of these authors have shown that the likelihood consistently favours higher quality assemblies. In general, the probabilistic model defines the probability Pr(R|A) that a set of sequencing reads R is observed assuming that assembly A is the correct assembly of the genome. Since the sequencing itself is a stochastic process, it is very natural to characterize concordance of reads and an assembly by giving a probability of observing a particular read. In our work, instead of evaluating the quality of a single assembly, we use the likelihood as an optimization criterion with the goal of finding high likelihood genome assemblies. We adapt the model of Ghodsi et al. [13], which we describe in this section. Basics of the likelihood model The model assumes that individual reads are independently sampled, and thus the overall likelihood is the product of likelihoods of the reads: Pr(R|A) = ∏r∈RPr(r|A). To make the resulting value independent of the number of reads in set R, we use as the main assembly score the log average probability of a read computed as follows: LAP(A|R) = (1/|R|)∑r∈RlogPr(r|A). Note that maximizing Pr(R|A) is equivalent to maximizing LAP(A|R). If the reads were error-free and each position in the genome was sequenced equally likely, the probability of observing read r would simply be Pr(r|A) = nr/(2L), where nr is the number of occurrences of the read as a substring of the assembly A, L is the length of A, and thus 2L is the length of the two strands combined [14]. Ghodsi et al. [13] have shown a dynamic programming computation of read probability for more complex models, accounting for sequencing errors. The algorithm marginalizes over all possible alignments of r and A, weighting each by the probability that a certain number of substitution and indel errors would happen during sequencing. In particular, the probability of a single alignment with m matching positions and s errors (substitutions and indels) is defined as R(s, m)/(2L), where R(s, m) = ϵs(1-ϵ)m and ϵ is the sequencing error rate. However, the full dynamic programming is too time consuming, and in practice only several best alignments contribute significantly to the overall probability. We approximate the probability of observing read r with an estimate based on a set Sr of a few best alignments of r to genome A, as obtained by one of the standard fast read alignment tools: where mj is the number of matches in the jth alignment, and sj is the number of mismatches and indels implied by this alignment. The formula assumes the simplest possible error model, where insertions, deletions, and substitutions have the same probability, and ignores GC content bias. Of course, much more comprehensive read models are possible (see e.g. [12]). Paired reads Many technologies provide paired reads produced from the opposite ends of a sequence insert of a certain size. We assume that the insert size distribution in a set of reads R can be modeled by the normal distribution with known mean μ and standard deviation σ. s before, mji and sji are the numbers of matches and sequencing errors in alignment ji respectively, and d(j1, j2) is the distance between the two alignments as observed in the assembly. If alignments j1 and j2 are in two different contigs, or on inconsistent strands, Pr(d(j1, j2)|μ, σ) is zero. Reads that have no good alignment to A Some reads or read pairs do not align well to A, and as a result, their probability Pr(r|A) is very low; our approximation by a set of high-scoring alignments can even yield zero probability if set Sr is empty. Such extremely low probabilities then dominate the log likelihood score. Ghodsi et al. [13] propose a method that assigns such a read a score approximating the situation when the read would be added as a new contig to the assembly. We modify their formulas for variable read length, and use score ec+kℓ for a single read of length ℓ or ec+k(ℓ1+ℓ2) for a pair of reads of lengths ℓ1 and ℓ2. Values k and c are scaling constants set similarly as by Ghodsi et al. [13]. These alternative scores are used instead of the read probability Pr(r|A) whenever the probability is lower than the score. Multiple read sets Our work is specifically targeted at a scenario, where we have multiple read sets obtained from different libraries with different insert lengths or even with different sequencing technologies. We use different model parameters for each set and compute the final score as a weighted combination of log average probabilities for individual read sets R1,  ⋯ , Rk: LAP(A|R1,  ⋯ , Rk) = w1LAP(A|R1) +  ⋯  + wkLAP(A|Rk).        3 In our experiments, we use weight wi = 1 for most datasets, but we lower the weight for Pacific Biosciences reads, because otherwise they dominate the likelihood value due to their longer length. The user can also increase or decrease weights wi of individual sets based on their reliability. Penalizing spuriously joined contigs The model described above does not penalize obvious misassemblies when two contigs are joined together without any evidence in the reads. We have observed that to make the likelihood function applicable as an optimization criterion for the best assembly, we need to introduce a penalty for such spurious connections. We say that a particular base j in the assembly is connected with respect to read set R if there is a read which covers base j and starts at least k bases before j, where k is a constant specific to the read set. In this setting, we treat a pair of reads as one long read. If the assembly contains d disconnected bases with respect to d, penalty αd is added to the LAP(A|R) score (α is a scaling constant). Properties of different sequencing technologies Our model can be applied to different sequencing technologies by appropriate settings of model parameters. For example, Illumina technology typically produces reads of length 75–150 bp with error rate below 1% [16]. For smaller genomes, we often have a high coverage of Illumina reads. Using paired reads or mate pair technologies, it is possible to prepare libraries with different insert sizes ranging up to tens of kilobases, which are instrumental in resolving longer repeats [4]. To align these reads to proposed assemblies, we use Bowtie2 [17]. Similarly, we can process reads by the Roche 454 technology, which are characteristic by higher read lengths (hundreds of bases). Pacific Biosciences technology produces single reads of variable length, with median length reaching several kilobases, but the error rate exceeds 10% [6, 16]. Their length makes them ideal for resolving ambiguities in assemblies, but the high error rate makes their use challenging. To align these reads, we use BLASR [18]. When we calculate the probability Pr(r|A), we consider not only the best alignments found by BLASR, but for each BLASR alignment, we also add probabilities of similar alignments in its neighborhood. More specifically, we run a banded version of the forward algorithm by [13], considering all alignments in a band of size three around a guide alignment produced by BLASR. Complex probabilistic models, like the one described in “Probabilistic model for sequence assembly”, were previously used to compare the quality of several assemblies [11–13]. In our work, we instead attempt to find the highest likelihood assembly directly. Of course, the search space is huge, and the objective function too complex to admit exact methods. Here, we describe an effective optimization routine based on the simulated annealing framework [19]. Our algorithm for finding the maximum likelihood assembly consists of three main steps: preprocessing, optimization, and postprocessing. In preprocessing, we decrease the scale of the problem by creating an assembly graph, where vertices correspond to contigs and edges correspond to possible adjacencies between contigs supported by reads. In order to make the search viable, we will restrict our search to assemblies that can be represented as a set of walks in this graph. Therefore, the assembly graph should be built in a conservative way, where the goal is not to produce long contigs, but rather to avoid errors inside them. In the optimization step, we start with an initial assembly (a set of walks in the assembly graph), and iteratively propose changes in order to optimize the assembly likelihood. Finally, postprocessing examines the resulting walks and splits some of them into shorter contigs if there are multiple equally likely possibilities of resolving ambiguities. This happens, for example, when the genome contains long repeats that cannot be resolved by any of the datasets. In the rest of this section, we discuss individual steps in more detail. Optimization by simulated annealing To find a high likelihood assembly, we use an iterative simulated annealing scheme. We start from an initial assembly A0 in the assembly graph. In each iteration, we randomly choose a move that proposes a new assembly A′ similar to the current assembly A. The next step depends on the likelihoods of the two assemblies A and A′ as follows: If LAP(A′|R) ≥ LAP(A|R), the new assembly A′ is accepted and the algorithm continues with the new assembly. If LAP(A′|R) < LAP(A|R), the new assembly A′ is accepted with probability e(LAP(A′|R)-LAP(A|R))/T; otherwise A′ is rejected and the algorithm retains the old assembly A for the next step. Here, parameter T is called the temperature, and it changes over time. In general, the higher the temperature, the more aggressive moves are permitted. We use a simple cooling schedule, where T = T0/ln(i) in the ith iteration. The computation ends when there is no improvement in the likelihood for a certain number of iterations. We select the assembly with the highest LAP score as the result. To further reduce the complexity of the assembly problem, we classify all contigs as either long (more than 500 bp) or short and concentrate on ordering the long contigs correctly. The short contigs are used to fill the gaps between the long contigs. Recall that each assembly is a set of walks in the assembly graph. A contig can appear in more than one walk or can be present in a single walk multiple times. Preprocessing and the initial assembly To obtain the assembly graph, we use Velvet with basic error correction and unambiguous concatenation of k-mers. These settings will produce very short contigs, but will also give a much lower error rate than a regular Velvet run. GAML with the default settings then uses each long contig as a separate walk in the starting assembly for the simulated annealing procedure. Postprocessing The assembly obtained by the simulated annealing procedure may contain walks with no evidence for a particular configuration of incoming and outgoing edges in the assembly graph. This happens for example if a repeat is longer than the span of the longest paired read. In this case, there would be several versions of the assembly with the same or very similar likelihood score. In the postprocessing step, we therefore apply the repeat interchange move at every possible location of the assembly. If the likelihood change resulting from such a move is negligible, we break the corresponding walks into shorter contigs to avoid assembly errors. Fast likelihood evaluation The most time consuming step in our algorithm is evaluation of the assembly likelihood, which we perform in each iteration of simulated annealing. This step involves alignment of a large number of reads to the current assembly. However, only a small part of the assembly is changed in each annealing step, which we can use to significantly reduce the running time. Next, we describe three optimizations implemented in our software. Limiting read alignment to affected regions of the assembly Since only a small portion of the assembly is affected in each step, we can keep most alignments from the previous iterations and only align reads to the regions that changed. To determine these regions, we split walks into overlapping windows, each window containing several adjacent contigs of a walk. Windows should be as short as possible, but adjacent windows should overlap by at least 2ℓr bases, where ℓr is the length of the longest read. As a result, each alignment is completely contained in at least one window even in the presence of extensive indels. We determine the window boundaries by a simple greedy strategy, which starts at the first contig of a walk, and then extends the window by at least 2ℓr bases beyond the boundary of the first contig. The next window always starts at the latest possible location that ensures a sufficient overlap and extends at least 2ℓr bases beyond the end of the previous window. For each window, we keep the position and edit distance of all alignments. In each annealing step, we identify which windows of the assembly were changed since the last iteration. We then glue together overlapping windows and align reads against these sequences. We further improve this heuristics by avoiding repeated alignments of reads to interiors of long contigs, because these parts of the assembly never change. In particular, if some window starts with a long contig, we only realign reads to the last 2ℓr bases from that contig, and similarly we use only the first 2ℓr bases from a long contig at the end of a window. Reducing the number of reads which need to be aligned The first improvement eliminates most of the assembly from read mapping. In contrast, the second improvement reduces the set of reads which need to be realigned, because most of the reads will not align to the changed part of the assembly. We use a prefiltering step to find the reads which are likely to align to the target sequence. In the current implementation, we use the following three options for such filtering. In the simplest approach, we look for reads which contain some k-mer (usually k = 13) from the target sequence. We store an index of all k-mers from all reads in a hash map. In each annealing step, we iterate over all k-mers in the target portion of the assembly and retrieve reads that contain them. This approach is very memory consuming, because the identifier of each read is stored for each k-mer from this read. In the second approach, we save memory using min-hashing [20]. Given hash function h, the min-hash of set A is defined as m(A) = minx∈Ah(x). For each read R, we calculate min-hash for the set of all its k-mers. Thus, the identifier of each read is stored in the hash table only once. In each annealing step, we calculate the min-hash for each substring of the target sequence of length ℓr and retrieve the reads that have the same min-hash. An important property of min-hashing is that Pr(m(A) = m(B)) = J(A, B), where J(A,B)=|A∩B||A∪B| is the Jaccard similarity of two sets A and B [21]. The statement holds if the hash function h is randomly chosen from a family with the min-wise independence property, which means that for every subset of elements X, each element in X has the same chance to have the minimum hash. Note that strings with a very small edit distance have a high Jaccard similarity between their k-mer sets, and therefore a high chance that they will hash to the same value using min-hashing. We can use several min-hashes with different hash functions to improve the sensitivity of our filtering at the cost of additional memory. In our implementation, we use a simple hash function which maps k-mers into 32-bit integers. We first represent the k-mer as an integer (where each base corresponds to two bits). We then xor this integer with a random number. Finally, we perform mixing similar to the finalization of the Murmur hash function [22]: An external file that holds a picture, illustration, etc. Object name is 13015_2015_52_Figa_HTML.gif We choose this finalizer because the Murmur hash function is fast and results in few collisions. It is not min-hash independent, but we found it to perform well in practice. To illustrate the specificity and sensitivity of min-hashing, we have compared our min-hashing approach with indexing all k-mers (with k = 15) on evaluating LAP of the Allpaths-LG assembly of Staphylococus aureus (using read set SA1 described in “Experimental evaluation” and aligning it to the whole S. aureus genome). Indexing all k-mers resulted in 3,659,273 alignments found by examining 21,241,474 candidate positions. Using min-hashing with three hash functions, we were able to find 3,639,625 alignments by examining 3,905,595 candidates positions. Since these reads have a low error rate, k-mer indexing retrieves practically all relevant alignments, while the sensitivity of min-hashing is approximately 99.5%. In min-hashing, 93% of examined positions yield an alignment, whereas specificity of k-mer indexing is only 17%. Also min-hashing used 30 times smaller index. Note that min-hashing was previously used in a similar context by Berlin et al. [23] to find similarities among PacBio reads. However, since PacBio reads have a high error rate, the authors had to use a high number of hash functions, whereas we use only a few hash functions to filter Illumina reads, which have a low error rate. In GAML, we filter PacBio reads by a completely different approach, which is based on alignments, rather than k-mers. In particular, we take all reasonably long contigs (at least 100 bases) and align them to PacBio reads. Since BLASR can find alignments where a contig and a read overlap by only around 100 bases, we use these alignments as a filter. Final computation of the likelihood score When all reads are properly aligned to the new version of the assembly, we can combine the alignments to the final score. In the implementation, we need to handle several issues, such as correctly computing likelihood for reads that align to multiple walks, assigning a special likelihood to reads without any good alignment, and avoiding double counting for reads that align to regions covered by two overlapping windows of the same walk. Again we improve the running time by considering only reads that were influenced by the most recent change. Between consecutive iterations, we keep all alignments for each sequence window of the assembly and recompute only alignments to affected windows, as outlined above. We also keep the likelihood value of each read or a read pair. Recall that the likelihood of a read or a read pair is the sum of likelihoods of individual alignments. In each iteration, we then identify which walks were removed and added. Then we calculate likelihoods of all read alignments in these walks (using stored or newly computed alignments) and we use these values to adjust the likelihood values of individual reads, subtracting for removed walks and adding for new walks. At this step, we also handle paired reads, identifying pairs of alignments in correct distance and orientation. Finally, we sum likelihoods of all reads in each dataset and compute the total likelihood score.",Assembly,"gaml genome assembly  maximum likelihood
recently several probabilistic model  introduce   measure   assembly quality     author  show   likelihood consistently favour higher quality assemblies  general  probabilistic model define  probability prra   set  sequence read   observe assume  assembly    correct assembly   genome since  sequence    stochastic process    natural  characterize concordance  read   assembly  give  probability  observe  particular read   work instead  evaluate  quality   single assembly  use  likelihood   optimization criterion   goal  find high likelihood genome assemblies  adapt  model  ghodsi      describe   section basics   likelihood model  model assume  individual read  independently sample  thus  overall likelihood   product  likelihoods   read prra  ∏∈rprra  make  result value independent   number  read  set   use   main assembly score  log average probability   read compute  follow lapar  ∑∈rlogprra note  maximize prra  equivalent  maximize lapar   read  errorfree   position   genome  sequence equally likely  probability  observe read  would simply  prra  nr2l     number  occurrences   read   substring   assembly     length    thus    length   two strand combine  ghodsi     show  dynamic program computation  read probability   complex model account  sequence errors  algorithm marginalize   possible alignments     weight    probability   certain number  substitution  indel errors would happen  sequence  particular  probability   single alignment   match position   errors substitutions  indels  define   m2l     ϵsϵm     sequence error rate however  full dynamic program   time consume   practice  several best alignments contribute significantly   overall probability  approximate  probability  observe read    estimate base   set     best alignments    genome   obtain  one   standard fast read alignment tool     number  match   jth alignment     number  mismatch  indels imply   alignment  formula assume  simplest possible error model  insertions deletions  substitutions    probability  ignore  content bias  course much  comprehensive read model  possible see   pair read many technologies provide pair read produce   opposite end   sequence insert   certain size  assume   insert size distribution   set  read    model   normal distribution  know mean   standard deviation    mji  sji   number  match  sequence errors  alignment  respectively  dj1    distance   two alignments  observe   assembly  alignments      two different contigs   inconsistent strand prdj1 j2μ   zero read    good alignment    read  read pair   align well      result  probability prra   low  approximation   set  highscoring alignments  even yield zero probability  set   empty  extremely low probabilities  dominate  log likelihood score ghodsi    propose  method  assign   read  score approximate  situation   read would  add   new contig   assembly  modify  formulas  variable read length  use score eckℓ   single read  length   eckℓ1ℓ2   pair  read  lengths    value     scale constants set similarly   ghodsi     alternative score  use instead   read probability prra whenever  probability  lower   score multiple read set  work  specifically target   scenario    multiple read set obtain  different libraries  different insert lengths  even  different sequence technologies  use different model parameters   set  compute  final score   weight combination  log average probabilities  individual read set   ⋯   lapar1  ⋯    w1lapar1   ⋯   wklapark           experiment  use weight      datasets   lower  weight  pacific biosciences read  otherwise  dominate  likelihood value due   longer length  user  also increase  decrease weight   individual set base   reliability penalize spuriously join contigs  model describe    penalize obvious misassemblies  two contigs  join together without  evidence   read   observe   make  likelihood function applicable   optimization criterion   best assembly  need  introduce  penalty   spurious connections  say   particular base    assembly  connect  respect  read set      read  cover base   start  least  base       constant specific   read set   set  treat  pair  read  one long read   assembly contain  disconnect base  respect   penalty   add   lapar score    scale constant properties  different sequence technologies  model   apply  different sequence technologies  appropriate settings  model parameters  example illumina technology typically produce read  length    error rate     smaller genomes  often   high coverage  illumina read use pair read  mate pair technologies   possible  prepare libraries  different insert size range   tens  kilobases   instrumental  resolve longer repeat   align  read  propose assemblies  use bowtie2  similarly   process read   roche  technology   characteristic  higher read lengths hundreds  base pacific biosciences technology produce single read  variable length  median length reach several kilobases   error rate exceed     length make  ideal  resolve ambiguities  assemblies   high error rate make  use challenge  align  read  use blasr    calculate  probability prra  consider    best alignments find  blasr    blasr alignment  also add probabilities  similar alignments   neighborhood  specifically  run  band version   forward algorithm   consider  alignments   band  size three around  guide alignment produce  blasr complex probabilistic model like  one describe  “probabilistic model  sequence assembly”  previously use  compare  quality  several assemblies    work  instead attempt  find  highest likelihood assembly directly  course  search space  huge   objective function  complex  admit exact methods   describe  effective optimization routine base   simulate anneal framework   algorithm  find  maximum likelihood assembly consist  three main step preprocessing optimization  postprocessing  preprocessing  decrease  scale   problem  create  assembly graph  vertices correspond  contigs  edge correspond  possible adjacencies  contigs support  read  order  make  search viable   restrict  search  assemblies    represent   set  walk   graph therefore  assembly graph   build   conservative way   goal    produce long contigs  rather  avoid errors inside    optimization step  start   initial assembly  set  walk   assembly graph  iteratively propose change  order  optimize  assembly likelihood finally postprocessing examine  result walk  split     shorter contigs    multiple equally likely possibilities  resolve ambiguities  happen  example   genome contain long repeat  cannot  resolve     datasets   rest   section  discuss individual step   detail optimization  simulate anneal  find  high likelihood assembly  use  iterative simulate anneal scheme  start   initial assembly    assembly graph   iteration  randomly choose  move  propose  new assembly ′ similar   current assembly   next step depend   likelihoods   two assemblies   ′  follow  lapa′ ≥ lapar  new assembly ′  accept   algorithm continue   new assembly  lapa′  lapar  new assembly ′  accept  probability elapa′rlapart otherwise ′  reject   algorithm retain  old assembly    next step  parameter   call  temperature   change  time  general  higher  temperature   aggressive move  permit  use  simple cool schedule    t0lni   ith iteration  computation end     improvement   likelihood   certain number  iterations  select  assembly   highest lap score   result   reduce  complexity   assembly problem  classify  contigs  either long      short  concentrate  order  long contigs correctly  short contigs  use  fill  gap   long contigs recall   assembly   set  walk   assembly graph  contig  appear    one walk    present   single walk multiple time preprocessing   initial assembly  obtain  assembly graph  use velvet  basic error correction  unambiguous concatenation  kmers  settings  produce  short contigs   also give  much lower error rate   regular velvet run gaml   default settings  use  long contig   separate walk   start assembly   simulate anneal procedure postprocessing  assembly obtain   simulate anneal procedure may contain walk   evidence   particular configuration  incoming  outgo edge   assembly graph  happen  example   repeat  longer   span   longest pair read   case  would  several versions   assembly      similar likelihood score   postprocessing step  therefore apply  repeat interchange move  every possible location   assembly   likelihood change result    move  negligible  break  correspond walk  shorter contigs  avoid assembly errors fast likelihood evaluation   time consume step   algorithm  evaluation   assembly likelihood   perform   iteration  simulate anneal  step involve alignment   large number  read   current assembly however   small part   assembly  change   anneal step    use  significantly reduce  run time next  describe three optimizations implement   software limit read alignment  affect regions   assembly since   small portion   assembly  affect   step   keep  alignments   previous iterations   align read   regions  change  determine  regions  split walk  overlap windows  window contain several adjacent contigs   walk windows    short  possible  adjacent windows  overlap   least 2ℓr base     length   longest read   result  alignment  completely contain   least one window even   presence  extensive indels  determine  window boundaries   simple greedy strategy  start   first contig   walk   extend  window   least 2ℓr base beyond  boundary   first contig  next window always start   latest possible location  ensure  sufficient overlap  extend  least 2ℓr base beyond  end   previous window   window  keep  position  edit distance   alignments   anneal step  identify  windows   assembly  change since  last iteration   glue together overlap windows  align read   sequence   improve  heuristics  avoid repeat alignments  read  interiors  long contigs   part   assembly never change  particular   window start   long contig   realign read   last 2ℓr base   contig  similarly  use   first 2ℓr base   long contig   end   window reduce  number  read  need   align  first improvement eliminate    assembly  read map  contrast  second improvement reduce  set  read  need   realign     read   align   change part   assembly  use  prefiltering step  find  read   likely  align   target sequence   current implementation  use  follow three options   filter   simplest approach  look  read  contain  kmer usually      target sequence  store  index   kmers   read   hash map   anneal step  iterate   kmers   target portion   assembly  retrieve read  contain   approach   memory consume   identifier   read  store   kmer   read   second approach  save memory use minhashing  give hash function   minhash  set   define    minx∈ahx   read   calculate minhash   set    kmers thus  identifier   read  store   hash table     anneal step  calculate  minhash   substring   target sequence  length   retrieve  read     minhash  important property  minhashing   prma       jaba∩∪   jaccard similarity  two set      statement hold   hash function   randomly choose   family   minwise independence property  mean   every subset  elements   element      chance    minimum hash note  string    small edit distance   high jaccard similarity   kmer set  therefore  high chance    hash    value use minhashing   use several minhashes  different hash function  improve  sensitivity   filter   cost  additional memory   implementation  use  simple hash function  map kmers  bite integers  first represent  kmer   integer   base correspond  two bits   xor  integer   random number finally  perform mix similar   finalization   murmur hash function   external file  hold  picture illustration etc object name  13015_2015_52_figa_htmlgif  choose  finalizer   murmur hash function  fast  result   collisions    minhash independent   find   perform well  practice  illustrate  specificity  sensitivity  minhashing   compare  minhashing approach  index  kmers      evaluate lap   allpathslg assembly  staphylococus aureus use read set sa1 describe  “experimental evaluation”  align    whole  aureus genome index  kmers result   alignments find  examine  candidate position use minhashing  three hash function   able  find  alignments  examine  candidates position since  read   low error rate kmer index retrieve practically  relevant alignments   sensitivity  minhashing  approximately   minhashing   examine position yield  alignment whereas specificity  kmer index    also minhashing use  time smaller index note  minhashing  previously use   similar context  berlin     find similarities among pacbio read however since pacbio read   high error rate  author   use  high number  hash function whereas  use    hash function  filter illumina read    low error rate  gaml  filter pacbio read   completely different approach   base  alignments rather  kmers  particular  take  reasonably long contigs  least  base  align   pacbio read since blasr  find alignments   contig   read overlap   around  base  use  alignments   filter final computation   likelihood score   read  properly align   new version   assembly   combine  alignments   final score   implementation  need  handle several issue   correctly compute likelihood  read  align  multiple walk assign  special likelihood  read without  good alignment  avoid double count  read  align  regions cover  two overlap windows    walk   improve  run time  consider  read   influence    recent change  consecutive iterations  keep  alignments   sequence window   assembly  recompute  alignments  affect windows  outline   also keep  likelihood value   read   read pair recall   likelihood   read   read pair   sum  likelihoods  individual alignments   iteration   identify  walk  remove  add   calculate likelihoods   read alignments   walk use store  newly compute alignments   use  value  adjust  likelihood value  individual read subtract  remove walk  add  new walk   step  also handle pair read identify pair  alignments  correct distance  orientation finally  sum likelihoods   read   dataset  compute  total likelihood score",5
110,SKESA,"SKESA: strategic k-mer extension for scrupulous assemblies
We present the algorithm design for SKESA, some important implementation details, design of test sets used for running time and assembly quality comparisons, and command lines used for doing the runs. We compare SKESA to SPAdes v3.11.1 and MegaHit v1.1.2. Assessment of assembly quality was done using QUAST. We attempted to use misFinder [45] and ReMILO [46] but neither worked reliably. When misFinder or ReMILO worked, results were similar to that of QUAST. Algorithm design for SKESA A flowchart describing the main modules of SKESA is shown in Fig. 4. Other than reading input and writing output, the four main parts of the SKESA algorithm are as follows: Trimming of reads. Detection of parameters: A user should specify the option for whether the reads are paired or single and the compute resources available. All other parameters are determined internally by SKESA unless explicitly specified. Assembly using a specific k-mer size: In each iteration, the assembly process uses the DeBruijn graph for that k-mer size and an empty or current set of contigs. Multiple k-mer sizes are used. Short k-mers can assemble low-coverage areas of the genome while long k-mers can resolve repeats. Marking reads: This module decides reads that are used up and no longer needed for future iterations. After trimming of reads, the rest of the SKESA process uses trimmed reads only and we overload “read” to mean trimmed reads after this step. If input has paired reads, after iterating using k-mers up to mate length, any read still available for assembly has a mini-assembly performed treating its mates as ends of contigs. Assembled reads are used for generating three sets of k-mers that are longer than the mate size and up to the expected insert size. No explicit error correction of reads is done by SKESA as the heuristics of SKESA can handle the errors in a typical illumina read set. Next, we describe each of the five modules. Read trimming K-mer size of 19 is used for counting frequency of k-mers in the read set. If a k-mer is seen in at least Vf fraction of reads (default 0.05), it is considered suspect and used for trimming reads. Starting from the first k-mer in a mate and checking all consecutive k-mers, the first occurrence of a k-mer flagged as suspect trims the rest of the mate. Parameter detection SKESA builds a histogram for frequency of k-mers at the minimal k-mer length Kmin (default 21) seen in trimmed reads. Using the histogram, it decides the peak where the distribution around the peak likely corresponds to the k-mers from the genome being assembled. This distribution is used to estimate the genome size G. If no peak is detected, then 80% of the entire distribution is used as an estimate of G. Additional peaks present and distributions around those peaks are usually due to noise, repeats, or plasmids. For example, Figs. 5 and 6 are two parts of the histogram for SRR2821438 generated with 21-mers. To account for more noise in high-coverage read sets, the minimum frequency count, Cmin is computed as max(2,T/(G∗50)) where T is the total length of reads. All k-mers with count below the minimum count are ignored in the assembly. The program also computes Cmax as max(10,T/(G∗10)). Choice of k-mer lengths is made by SKESA using Kmin, number of steps S (default 11), and maximal k-mer length Kmax where Kmax is determined using the average of all mate lengths Aread and counts of k-mers. Kmax is initially set to Aread. If the average count of k-mers at current Kmax is below the desired count Cmax, then Kmax is iteratively reduced by Aread/25 bases until a Kmax with average count of at least Cmax is found. If Kmax is more than 1.5 times Kmin, then S−2 additional k-mers between Kmin and Kmax are chosen. These are odd integers that are spread evenly. Otherwise, only Kmin is used for an assembly and a warning that iterations are disabled is printed. For paired runs, if the insert size I is not provided, then it is estimated using a random sample of 10,000 reads. An unambiguous assembly for each of these reads with the two mates as ends of contigs is attempted using Kmin. Using the length of reads assembled, insert size I is estimated. Three additional k-mer sizes added for additional iterations are 1.25Kmax, (1.25Kmax+I)/2 and I. The program also uses 3I as the maximal insert size expected for any read. Assembling using a specific k-mer size K All k-mers of length K with frequency at least Cmin are generated. If a main peak in the histogram of the frequency of generated k-mers is detected, the left low end of the distribution around the main peak is called the Valley for the iteration and only k-mers with count above the valley are used to start new contigs. A valley is set to zero if no main peak is found in the histogram. At any stage, an attempt to extend an end of a contig by the next base results in three possibilities: (i) no k-mer extension is possible, (ii) only one k-mer extension is possible, or (iii) there are alternate choices. In the first case, the end of the contig has been reached and no further extension is possible. In the second case, the contig is extended by one base only if the extension from the new k-mer produced by addition of the base to the previous k-mer (last k-mer of the end of the contig) is also possible using the same criteria used for extending from previous k-mer to the new k-mer. In the third case, all choices with counts below the threshold for extension (default 0.1) with respect to the maximum counts for any choice are considered as noise and dropped. If more than one choice for extension survives this count based filtering, potential Illumina strand-specific systematic error signatures are evaluated. The program does this by comparing counts observed on both strands. If there is a choice with counts balanced on both strands, all choices with counts seen in predominately one strand are dropped. If more than one choice for extension survives this strand-based filtering, each choice is used for finding paths that are extended by a maximum of max(100,K) steps. If only one path survives, then it is kept while others are removed as dead ends. If more than one path survives, a contig break is created. When a contig reaches the stage where an extension is no longer possible, the last k-mer bases are removed to ensure that the sequence built was verified by assembling from both directions. In the process of contig extension, suppose contig C is being extended by a base b resulting in last k-mer L in C that includes b. The program checks if L is already present in any contig D. If no such D exists, C is extended using L. If such a D exists and L is at the end of D, program merges C and D. Otherwise, C is not extended. This ensures that no k-mer is included more than once in the assembled contigs in the iteration for that k-mer. Marking reads as used After each iteration, the reads that have a k-mer located deeper than buffer zoneM inside a contig are marked as used as they cannot contribute any new information. Value of M is I+50+F where flank F is set to Kmax if reads being removed are the input reads and not the ones assembled as a pair for generating k-mers larger than the mate size. Otherwise, F is set to zero. Connecting paired reads If input is for paired reads, after the iterations using k-mers up to mate length, the program attempts to unambiguously connect reads that are not marked as used. Starting from the last k-mer of the first mate to first k-mer of the second mate, all paths up to maximal insert size are assembled. Similarly, an assembly from reverse complement of the first k-mer of the second mate to reverse complement of the last k-mer of the first mate is attempted. If both produce only one path and sequence is same for both paths (except for reverse complement), the assembled sequence is used for generating longer k-mers. For pairs that are inside the buffer zone M, sequence from the contig is used for generating long k-mers.",Assembly,"skesa strategic kmer extension  scrupulous assemblies
 present  algorithm design  skesa  important implementation detail design  test set use  run time  assembly quality comparisons  command line use    run  compare skesa  spade   megahit  assessment  assembly quality   use quast  attempt  use misfinder   remilo   neither work reliably  misfinder  remilo work result  similar    quast algorithm design  skesa  flowchart describe  main modules  skesa  show  fig    read input  write output  four main part   skesa algorithm   follow trim  read detection  parameters  user  specify  option  whether  read  pair  single   compute resources available   parameters  determine internally  skesa unless explicitly specify assembly use  specific kmer size   iteration  assembly process use  debruijn graph   kmer size   empty  current set  contigs multiple kmer size  use short kmers  assemble lowcoverage areas   genome  long kmers  resolve repeat mark read  module decide read   use    longer need  future iterations  trim  read  rest   skesa process use trim read    overload “read”  mean trim read   step  input  pair read  iterate use kmers   mate length  read still available  assembly   miniassembly perform treat  mat  end  contigs assemble read  use  generate three set  kmers   longer   mate size     expect insert size  explicit error correction  read    skesa   heuristics  skesa  handle  errors   typical illumina read set next  describe    five modules read trim kmer size    use  count frequency  kmers   read set   kmer  see   least  fraction  read default    consider suspect  use  trim read start   first kmer   mate  check  consecutive kmers  first occurrence   kmer flag  suspect trim  rest   mate parameter detection skesa build  histogram  frequency  kmers   minimal kmer length kmin default  see  trim read use  histogram  decide  peak   distribution around  peak likely correspond   kmers   genome  assemble  distribution  use  estimate  genome size    peak  detect     entire distribution  use   estimate   additional peak present  distributions around  peak  usually due  noise repeat  plasmids  example figs     two part   histogram  srr2821438 generate  mers  account   noise  highcoverage read set  minimum frequency count cmin  compute  maxtg∗     total length  read  kmers  count   minimum count  ignore   assembly  program also compute cmax  maxtg∗ choice  kmer lengths  make  skesa use kmin number  step  default   maximal kmer length kmax  kmax  determine use  average   mate lengths aread  count  kmers kmax  initially set  aread   average count  kmers  current kmax    desire count cmax  kmax  iteratively reduce  aread base   kmax  average count   least cmax  find  kmax     time kmin   additional kmers  kmin  kmax  choose   odd integers   spread evenly otherwise  kmin  use   assembly   warn  iterations  disable  print  pair run   insert size    provide    estimate use  random sample   read  unambiguous assembly     read   two mat  end  contigs  attempt use kmin use  length  read assemble insert size   estimate three additional kmer size add  additional iterations  25kmax 25kmaxi    program also use    maximal insert size expect   read assemble use  specific kmer size   kmers  length   frequency  least cmin  generate   main peak   histogram   frequency  generate kmers  detect  leave low end   distribution around  main peak  call  valley   iteration   kmers  count   valley  use  start new contigs  valley  set  zero   main peak  find   histogram   stage  attempt  extend  end   contig   next base result  three possibilities   kmer extension  possible   one kmer extension  possible  iii   alternate choices   first case  end   contig   reach    extension  possible   second case  contig  extend  one base    extension   new kmer produce  addition   base   previous kmer last kmer   end   contig  also possible use   criteria use  extend  previous kmer   new kmer   third case  choices  count   threshold  extension default   respect   maximum count   choice  consider  noise  drop    one choice  extension survive  count base filter potential illumina strandspecific systematic error signatures  evaluate  program    compare count observe   strand     choice  count balance   strand  choices  count see  predominately one strand  drop    one choice  extension survive  strandbased filter  choice  use  find paths   extend   maximum  maxk step   one path survive    keep  others  remove  dead end    one path survive  contig break  create   contig reach  stage   extension   longer possible  last kmer base  remove  ensure   sequence build  verify  assemble   directions   process  contig extension suppose contig    extend   base  result  last kmer     include   program check    already present   contig      exist   extend use      exist      end   program merge    otherwise    extend  ensure   kmer  include      assemble contigs   iteration   kmer mark read  use   iteration  read    kmer locate deeper  buffer zonem inside  contig  mark  use   cannot contribute  new information value      flank   set  kmax  read  remove   input read    ones assemble   pair  generate kmers larger   mate size otherwise   set  zero connect pair read  input   pair read   iterations use kmers   mate length  program attempt  unambiguously connect read    mark  use start   last kmer   first mate  first kmer   second mate  paths   maximal insert size  assemble similarly  assembly  reverse complement   first kmer   second mate  reverse complement   last kmer   first mate  attempt   produce  one path  sequence     paths except  reverse complement  assemble sequence  use  generate longer kmers  pair   inside  buffer zone  sequence   contig  use  generate long kmers",5
111,Euler-sr,"Short read fragment assembly of bacterial genomes
Error correction in reads Our assembly proceeds in several steps: error correction, graph construction, graph correction, and assembly by transforming paths in the corrected graph into contigs. Base miscalls and indels are inevitable in sequencing projects, and it is necessary to detect errors in reads to accurately determine the finished sequence. In the past this was done after assembly by mapping reads to the consensus sequence. Pevzner et al. (2001) introduced error correction before the assembly and demonstrated that it greatly simplifies the assembly. Now this is done as a standard preprocessing step before assembly using a multiple alignment of reads (Tammi et al. 2003), or by clustering and aligning reads prior to assembly (Batzoglou et al. 2002). In HTSR sequencing, constructing multiple alignments of short reads is very time consuming, and so we correct errors in reads prior to assembly using a method called spectral alignment (SA) that does not use a multiple sequence alignment. This method takes a read r and a set of l-tuples Graphic, called a spectrum, and finds the minimum number of substitutions, insertions, and deletions in r required to make every l-tuple in r belong to Graphic. The set Graphic is chosen by counting the frequency of all l-tuples present in all reads in a sequencing project, and selecting tuples that occur with multiplicity above some threshold m (called solid l-tuples). An iterative solution to SA was described in Pevzner et al. (2001), followed by a dynamic programming solution in Chaisson et al. (2004). The dynamic programming solution to SA allows for efficient searching through insertions and deletions, and so it is particularly well suited for fixing errors in reads generated by pyrosequencing, in which the errors are biased toward indels. The SA approach implemented in EULER-SR is faster and has a higher rate of error correction than the approach described in Chaisson et al. (2004). The choice of appropriate parameters for fixing errors using SA should minimize the number of erroneous l-tuples remaining in reads while not affecting correct but low-coverage sequences. Consider a sequencing project that produces N reads of average length L over a genome of length G. The average number of reads covering an l-tuple is a = N × (L − l)/G and is approximately distributed by a Poisson with parameter a. To minimize the number of correct sequences considered to be erroneous, we pick a multiplicity m for which there are few expected l-tuples covered by less than m reads. The probability an l-tuple in a genome is covered m times is Formula where Y ∼ Poisson with parameter a. For a typical high-throughput sequencing project of a bacterial genome of 4 million bases with 1 million 100-bp reads, there are under one hundred 20-mers expected to be covered less than five times. Furthermore, under the gross simplification that the sequences are random and there is a 1% random error rate, no erroneous l-tuples are expected to be made solid by coincident errors. de Bruijn graph construction Because of the massive scale of the data in short read assembly, it is necessary to perform assembly in linear or close to linear time. Although it is possible to construct a de Bruijn graph in linear time, memory efficiency is more of a premium for short reads assembly than run time. Our de Bruijn graph construction is implemented in several stages, in some of which we trade linear time for methods that run on sorted lists [thus O(n · logn) time] but reduce memory requirements. For a genome of length L, the de Bruijn graph has O(L) vertices and O(L) edges, regardless of the number of reads in the data set. We find the set of vertices in the de Bruijn graph using an efficient hashing structure in linear time proportional to the number of reads R in the data set. We then represent the vertices as a sorted list V of tuples, allowing us to discard the memory overhead of the hashing structure required for O(1) time access. We next generate adjacencies by querying the vertex list V with a binary search for every adjacent pair of l-tuples in the set of reads R. By representing the graph as an adjacency list, our de Bruijn graph uses a maximum of O(L) * (k + l) bytes, where k is the memory allocated for each vertex (40 bytes in the current implementation). For subsequent phases we form a condensed de Bruijn graph, where every simple path consisting of vertices of in-degree and out-degree 1 is substituted by a single (labeled) edge. Because the reads were used to generate the de Bruijn (and thus the condensed) graph, every (l + 1)-tuple in a read maps to a unique edge and position in the condensed de Bruijn graph. We use this to define a path of edges for each read, and to assign a weight to every edge equal to the number of reads mapped to the edge. These paths are used after graph correction to find the Eulerian path through the de Bruijn graph that corresponds to the assembled genome. de Bruijn graph correction If reads covered every (l + 1)-tuple in the genome and were error-free, the generated de Bruijn graph G would represent the repeat graph of the genome with minimal repeat length l + 1. However, after error correction, a small number of errors remain resulting in some added and removed edges as compared with the repeat graph. For example, a single mutation in a read will create (l + 1) extra edges in the de Bruijn graph. Our goal is to construct the graph G* given G. If we assume every (l + 1)-tuple in the original genome is sequenced correctly by at least one read, then G* is a subset of the vertices and edges of the graph on real reads G. Thus, we may perform additional error correction by detecting and removing erroneous edges on the graph G. We adapt the methods for graph simplification on A-Bruijn graphs from Pevzner et al. (2004) to perform graph correction on de Bruijn graphs. We distinguish the two methods because graph simplification aims to capture the repeat-consensus and mosaic structure of a genome, while the goal of graph correction is to simplify the de Bruijn graph until all erroneous edges are removed, and no further. For example, in graph simplification, repeats with high similarity are typically merged into a single edge corresponding to the repeat consensus sequence. A corrected de Bruijn graph contains a separate path for each distinct repeat sequence. When an A-Bruijn graph is constructed from a set of pairwise alignments, gaps and substitutions in an alignment create undirected cycles called “bulges”, and inconsistencies in an alignment create directed cycles called “whirls” (Pevzner et al. 2004). A cycle is called “short” if its girth is less than a fixed parameter g. The algorithm to generate an A-Bruijn graph employs a heuristic to find a graph that represents the largest possible set of consistent alignments through solving the maximum subgraph with large girth (MSLG) problem (Pevzner et al. 2004). Because this problem is hard for arbitrary girth values, a heuristic is used that replaces the A-Bruijn graph by its maximum spanning tree, and successively adds edges from the original graph to the spanning tree if they do not create a cycle of girth less than g. A similar approach for removing short cycles is used for fragment assembly on the de Bruijn graphs. One property of a de Bruijn graph is that every vertex should be reachable by a directed path from the source vertex, if only one source vertex exists. A shortfall of the bulge removal method in A-Bruijn graphs is that when a de Bruijn graph is replaced by its undirected maximum spanning tree, certain vertices may not be reachable from the source by directed paths. Although a path straightening heuristic (Pevzner et al. 2004) is used in the A-Bruijn graph construction to repair edges to unreachable vertices, we circumvent this problem entirely by replacing the de Bruijn graph with its Maximum branching (Chu and Liu 1965; Edmonds 1967), using the method described in Georgiadis (2003) (the branchings have directed paths from the source vertex to all vertices). Because “alignments” are found at each vertex by perfectly matching sequences of length l, directed cycles are rarely erroneous other than those covering long low-complexity regions, for example, homopolymeric and dinucleotides. When checking to see whether adding an edge to the maximum branching forms a cycle, we distinguish between directed and undirected cycles and add an edge to the branching if it does not create an undirected cycle of girth u nor a directed cycle of length d. While tandem repeats can be detected as whirls in the de Bruijn graph (typically directed cycles with two edges as shown in Figure 2, top), finding the number of copies (multiplicity) in a tandem repeat is a difficult problem for any assembler. For example, the multiplicity of perfect tandem repeats of length longer than half of the read length cannot be inferred from de Bruijn graphs. As a result, inferring the multiplicities of tandem repeats usually amounts to error-prone coverage analysis (e.g., a tandem repeat with multiplicity 3 is expected to have 50% more coverage than a repeat with multiplicity 2). To avoid potential errors caused by the coverage analysis (short read technologies often produce uneven coverage) and to reduce the fragmentation due to tandem repeats, we assume there are only two copies of the perfect tandem repeat and construct the assembly as a path that traverses the repeat twice, as shown in Figure 2 (bottom). While this procedure may underestimate the copy number of tandem repeats we found that it leads to very few errors.10 We therefore believe that it is a practical approach, at least until more information about coverage and specifics of errors of short read technologies becomes available. Reads that are mapped to edges that have been deleted during graph correction are threaded through the remaining edges. Let the path of such a read be (e0,. . ., ei,. . ., em), and ei be the removed edge. We search for an alternative path P = (a0. .. an) from the start to the end of edge ei such that the sequence corresponding to ei and P are sufficiently similar, and replace ei with P if such a path is found. In addition to bulge and whirl removal, we also apply two additional techniques for removing erroneous edges: erosion as described in Pevzner et al. (2004) and low-weight edge removal. Erosion removes short sources and sinks from the graph. Low-weight edge removal is typically a way to detect low-quality chimeric reads. Although most HTSR sequencers do not use clone libraries, sequencing errors at the ends of reads create erroneous edges similar to those created by chimeric reads. After the de Bruijn graph has been corrected we apply the equivalent transformation described in Pevzner et al. (2001) to resolve repeats that are shorter than the length of a read. If no mate-pair information exists, we cut paths that do not cover repeats with an operation similar to the x-cut (Pevzner et al. 2001). When mate-pair information is available, we postpone the cutting operation, and apply equivalent transformation with mate-pairs",Assembly,"short read fragment assembly  bacterial genomes
error correction  read  assembly proceed  several step error correction graph construction graph correction  assembly  transform paths   correct graph  contigs base miscall  indels  inevitable  sequence project    necessary  detect errors  read  accurately determine  finish sequence   past     assembly  map read   consensus sequence pevzner    introduce error correction   assembly  demonstrate   greatly simplify  assembly       standard preprocessing step  assembly use  multiple alignment  read tammi      cluster  align read prior  assembly batzoglou     htsr sequence construct multiple alignments  short read   time consume    correct errors  read prior  assembly use  method call spectral alignment     use  multiple sequence alignment  method take  read    set  ltuples graphic call  spectrum  find  minimum number  substitutions insertions  deletions   require  make every ltuple   belong  graphic  set graphic  choose  count  frequency   ltuples present   read   sequence project  select tuples  occur  multiplicity   threshold  call solid ltuples  iterative solution    describe  pevzner    follow   dynamic program solution  chaisson     dynamic program solution   allow  efficient search  insertions  deletions     particularly well suit  fix errors  read generate  pyrosequencing    errors  bias toward indels   approach implement  eulersr  faster    higher rate  error correction   approach describe  chaisson     choice  appropriate parameters  fix errors use   minimize  number  erroneous ltuples remain  read   affect correct  lowcoverage sequence consider  sequence project  produce  read  average length    genome  length   average number  read cover  ltuple           approximately distribute   poisson  parameter   minimize  number  correct sequence consider   erroneous  pick  multiplicity       expect ltuples cover  less   read  probability  ltuple   genome  cover  time  formula    poisson  parameter    typical highthroughput sequence project   bacterial genome   million base   million  read    one hundred mers expect   cover less  five time furthermore   gross simplification   sequence  random      random error rate  erroneous ltuples  expect   make solid  coincident errors  bruijn graph construction    massive scale   data  short read assembly   necessary  perform assembly  linear  close  linear time although   possible  construct   bruijn graph  linear time memory efficiency     premium  short read assembly  run time   bruijn graph construction  implement  several stag      trade linear time  methods  run  sort list thus   logn time  reduce memory requirements   genome  length    bruijn graph   vertices   edge regardless   number  read   data set  find  set  vertices    bruijn graph use  efficient hash structure  linear time proportional   number  read    data set   represent  vertices   sort list   tuples allow   discard  memory overhead   hash structure require   time access  next generate adjacencies  query  vertex list    binary search  every adjacent pair  ltuples   set  read   represent  graph   adjacency list   bruijn graph use  maximum   *    bytes     memory allocate   vertex  bytes   current implementation  subsequent phase  form  condense  bruijn graph  every simple path consist  vertices  indegree  outdegree   substitute   single label edge   read  use  generate   bruijn  thus  condense graph every   tuple   read map   unique edge  position   condense  bruijn graph  use   define  path  edge   read   assign  weight  every edge equal   number  read map   edge  paths  use  graph correction  find  eulerian path    bruijn graph  correspond   assemble genome  bruijn graph correction  read cover every   tuple   genome   errorfree  generate  bruijn graph  would represent  repeat graph   genome  minimal repeat length    however  error correction  small number  errors remain result   add  remove edge  compare   repeat graph  example  single mutation   read  create    extra edge    bruijn graph  goal   construct  graph * give    assume every   tuple   original genome  sequence correctly   least one read  *   subset   vertices  edge   graph  real read  thus  may perform additional error correction  detect  remove erroneous edge   graph   adapt  methods  graph simplification  abruijn graph  pevzner     perform graph correction   bruijn graph  distinguish  two methods  graph simplification aim  capture  repeatconsensus  mosaic structure   genome   goal  graph correction   simplify   bruijn graph   erroneous edge  remove     example  graph simplification repeat  high similarity  typically merge   single edge correspond   repeat consensus sequence  correct  bruijn graph contain  separate path   distinct repeat sequence   abruijn graph  construct   set  pairwise alignments gap  substitutions   alignment create undirected cycle call “bulges”  inconsistencies   alignment create direct cycle call “whirls” pevzner     cycle  call “short”   girth  less   fix parameter   algorithm  generate  abruijn graph employ  heuristic  find  graph  represent  largest possible set  consistent alignments  solve  maximum subgraph  large girth mslg problem pevzner      problem  hard  arbitrary girth value  heuristic  use  replace  abruijn graph   maximum span tree  successively add edge   original graph   span tree     create  cycle  girth less    similar approach  remove short cycle  use  fragment assembly    bruijn graph one property    bruijn graph   every vertex   reachable   direct path   source vertex   one source vertex exist  shortfall   bulge removal method  abruijn graph      bruijn graph  replace   undirected maximum span tree certain vertices may   reachable   source  direct paths although  path straighten heuristic pevzner     use   abruijn graph construction  repair edge  unreachable vertices  circumvent  problem entirely  replace   bruijn graph   maximum branch chu  liu  edmonds  use  method describe  georgiadis   branch  direct paths   source vertex   vertices  “alignments”  find   vertex  perfectly match sequence  length  direct cycle  rarely erroneous    cover long lowcomplexity regions  example homopolymeric  dinucleotides  check  see whether add  edge   maximum branch form  cycle  distinguish  direct  undirected cycle  add  edge   branch     create  undirected cycle  girth    direct cycle  length   tandem repeat   detect  whirl    bruijn graph typically direct cycle  two edge  show  figure  top find  number  copy multiplicity   tandem repeat   difficult problem   assembler  example  multiplicity  perfect tandem repeat  length longer  half   read length cannot  infer   bruijn graph   result infer  multiplicities  tandem repeat usually amount  errorprone coverage analysis   tandem repeat  multiplicity   expect     coverage   repeat  multiplicity   avoid potential errors cause   coverage analysis short read technologies often produce uneven coverage   reduce  fragmentation due  tandem repeat  assume    two copy   perfect tandem repeat  construct  assembly   path  traverse  repeat twice  show  figure  bottom   procedure may underestimate  copy number  tandem repeat  find   lead    errors  therefore believe     practical approach  least   information  coverage  specifics  errors  short read technologies become available read   map  edge    delete  graph correction  thread   remain edge let  path    read             remove edge  search   alternative path        start   end  edge     sequence correspond      sufficiently similar  replace       path  find  addition  bulge  whirl removal  also apply two additional techniques  remove erroneous edge erosion  describe  pevzner     lowweight edge removal erosion remove short source  sink   graph lowweight edge removal  typically  way  detect lowquality chimeric read although  htsr sequencers   use clone libraries sequence errors   end  read create erroneous edge similar   create  chimeric read    bruijn graph   correct  apply  equivalent transformation describe  pevzner     resolve repeat   shorter   length   read   matepair information exist  cut paths    cover repeat   operation similar   xcut pevzner     matepair information  available  postpone  cut operation  apply equivalent transformation  matepairs",5
112,ALLPATHS2,"ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads
ALLPATHS was first tested on simulated data [10]. Below we describe modifications that were needed for it to work well on real data. Removal of sequencing artifacts We now discard all read pairs where both reads of the pair consist of 90% or more A bases. On the Illumina platform, such pairs are nearly always artifacts of the sequencing process, and on some runs can be sufficiently abundant as to cause problems for the assembly algorithms. Trusted K-mer identification We identify putatively correct (or 'trusted') K-mers in the reads based on quality scores. This affects how we create unipaths and how we correct errors, as described below. For each K-mer that appears in the reads, we find all its instances in the reads, and examine the collection of read quality scores for a given base in the K-mer. That base is called trusted if there are enough good scores (default: 4 scores of at least 25). The entire K-mer is called trusted if each of its bases is trusted. Rehabilitation of K-mers between short fragment pairs If for such a pair there is no path of trusted K-mers from one end to the other, but there is a path that uses some untrusted K-mers, we rehabilitate those K-mers, changing their status to trusted. Unipath creation We find the (K+1)-mers in the reads whose first and last K-mer are trusted. The unipaths are mathematically defined by the trusted K-mers together with these (K+1)-mers, which define adjacencies between the trusted K-mers. Error correction This has been modified to use the new definition of trusted K-mers. Unipath graph shaving After the initial unipath creation, there are many cases in which read errors result in short terminal branches within the graph. Those shorter than 20 K-mers are now removed, provided that there is a longer alternative branch. Unipath recovery This code identifies unipaths that are not represented in the assembly, extends them unambiguously where possible, and then adds them to the assembly. Typically this finds small regions that have relatively high copy number. ALLPATHS computational requirements The five genomes were assembled on a 16-processor Dell server having 128 GB of memory. Some of the code is parallelized. The wall-clock times for the assemblies were: S. aureus, 1.7 hours; E. coli, 8.2 hours; R. sphaeroides, 10.2 hours; S. pombe, 80.5 hours; N. crassa, 86.6 hours.",Assembly,"allpaths  small genomes assemble accurately   high continuity  short pair reads
allpaths  first test  simulate data    describe modifications   need    work well  real data removal  sequence artifacts   discard  read pair   read   pair consist      base   illumina platform  pair  nearly always artifacts   sequence process    run   sufficiently abundant   cause problems   assembly algorithms trust kmer identification  identify putatively correct  'trusted' kmers   read base  quality score  affect   create unipaths    correct errors  describe    kmer  appear   read  find   instance   read  examine  collection  read quality score   give base   kmer  base  call trust    enough good score default  score   least   entire kmer  call trust     base  trust rehabilitation  kmers  short fragment pair     pair    path  trust kmers  one end        path  use  untrusted kmers  rehabilitate  kmers change  status  trust unipath creation  find  kmers   read whose first  last kmer  trust  unipaths  mathematically define   trust kmers together   kmers  define adjacencies   trust kmers error correction    modify  use  new definition  trust kmers unipath graph shave   initial unipath creation   many case   read errors result  short terminal branch within  graph  shorter   kmers   remove provide     longer alternative branch unipath recovery  code identify unipaths    represent   assembly extend  unambiguously  possible   add    assembly typically  find small regions   relatively high copy number allpaths computational requirements  five genomes  assemble   processor dell server     memory    code  parallelize  wallclock time   assemblies   aureus  hours  coli  hours  sphaeroides  hours  pombe  hours  crassa  hours",5
113,VCAKE,"Extending assembly of short DNA sequences to handle error 
Whole genome sequences for SARS-TOR2 and Pseudomonas syringae pv. tomato str. DC3000 (DC3000) came from GenBank (AY274119 and AE016853, respectively). Random 30mer simulated reads were generated at different coverage levels for each whole genome sequence. For those simulations including errors, each base of the sequences was randomly and independently changed to another base with a fixed probability. Three lanes of Solexa sequencing of DC3000 whole DNA were also provided (Baltrus et al., unpublished data) and used for test assembly. All lanes combined were 202 884 352 bases in 6 340 136 reads, making a coverage depth of 31.7x. 2.2 VCAKE algorithm Initially, the VCAKE assembly process is nearly identical to SSAKE, except that two multi-FASTA files separately populate bin and set hash tables from a pool of reads. Divergence from the SSAKE method occurs during extension of the seed sequences from set. VCAKE finds all exact k-mer matches of the 3′ end of the sequence up to a user-defined minimum n. The first 11 bases of the k-mer are used to efficiently search bin and all returned keys are checked against the remainder of the k-mer. Those matching perfectly are pushed into an array a number of times equal to the values keyed by that sequence in bin. As in SSAKE, the bin hash table consists of a treed hash table keyed by the first 11 bases of the read (or its reverse complement) followed by the sequence itself, with the value containing the number of appearances of that read or its reverse complement, allowing efficient searching. Having reached the minimum k-mer length n, if the total matching sequence occurrences (repetition of reads included) is less than a user-defined value, t, then the algorithm will proceed further. In this case, VCAKE extracts all k-mer matches up to a lower user-defined length m. If t reads are still not found, then k-mer matches up to user-defined length, e, are considered. This last group may have one mismatch in overlaps with the k-mer after the first 11 bases. This last procedure halts when t total sequence occurrences have been found or the minimum overlap, e, is reached. If a given sequence matches two different k-mer frames, then contig extension on that side is terminated. The array of matching sequences is then considered. Each sequence offers a ‘vote’ for the first overhanging base called by that read. The votes are totaled and the base exceeding a threshold of representation, c, is added to the contig. However, the contig will be terminated if the number of reads calling the second most common base exceeds the user-defined threshold, v. This value, v, represents the number of occurrences at which a base call can be considered an indication of duplication of the sequence elsewhere in the genome, rather than due to sequencing error. The user may also define a number of found reads, x, at which the algorithm terminates the contig. This is used when high representation of reads from a given sequence suggests repetitive sequence in the genome. Regardless of contig termination, all sequences retrieved that occur completely within the contig, are deleted both from bin and set. Extension proceeds, one base at a time, until no matches are found or the contig is terminated for one of the other reasons above. The contig is then reverse complemented and extension of the opposite end is performed by the same method. The program finally outputs the contig to a file in multi-FASTA format and begins again with an unused seed from set.",Assembly,"extend assembly  short dna sequence  handle error 
whole genome sequence  sarstor2  pseudomonas syringae  tomato str dc3000 dc3000 come  genbank ay274119  ae016853 respectively random 30mer simulate read  generate  different coverage level   whole genome sequence   simulations include errors  base   sequence  randomly  independently change  another base   fix probability three lanes  solexa sequence  dc3000 whole dna  also provide baltrus   unpublished data  use  test assembly  lanes combine     base     read make  coverage depth    vcake algorithm initially  vcake assembly process  nearly identical  ssake except  two multifasta file separately populate bin  set hash table   pool  read divergence   ssake method occur  extension   seed sequence  set vcake find  exact kmer match   ′ end   sequence    userdefined minimum   first  base   kmer  use  efficiently search bin   return key  check   remainder   kmer  match perfectly  push   array  number  time equal   value key   sequence  bin   ssake  bin hash table consist   tree hash table key   first  base   read   reverse complement follow   sequence    value contain  number  appearances   read   reverse complement allow efficient search  reach  minimum kmer length    total match sequence occurrences repetition  read include  less   userdefined value    algorithm  proceed    case vcake extract  kmer match    lower userdefined length    read  still  find  kmer match   userdefined length   consider  last group may  one mismatch  overlap   kmer   first  base  last procedure halt   total sequence occurrences   find   minimum overlap   reach   give sequence match two different kmer frame  contig extension   side  terminate  array  match sequence   consider  sequence offer  vote   first overhang base call   read  vote  total   base exceed  threshold  representation   add   contig however  contig   terminate   number  read call  second  common base exceed  userdefined threshold   value  represent  number  occurrences    base call   consider  indication  duplication   sequence elsewhere   genome rather  due  sequence error  user may also define  number  find read     algorithm terminate  contig   use  high representation  read   give sequence suggest repetitive sequence   genome regardless  contig termination  sequence retrieve  occur completely within  contig  delete   bin  set extension proceed one base   time   match  find   contig  terminate  one    reason   contig   reverse complement  extension   opposite end  perform    method  program finally output  contig   file  multifasta format  begin    unused seed  set",5
114,SHARCGS,"SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing
The SHARCGS algorithm consists of a filtering step to avoid reads with sequencing errors, an assembly step to generate contigs, and a final contig merging step including computation of quality measures. Filtering for confirmed reads SHARCGS obtains a large set of reads of equal length, optionally augmented by quality scores per base call, as its input. Thereby, it is adapted to cope with a substantial number of reads with sequencing errors. When the error rate per base call is 0.6%, about 16% of 30-mer reads contain errors. Pcorrect, the probability that a single read contains no sequencing errors can be estimated by the product of all its base calls being correct independently. Thus: equation image where Perror represents the error rate per base call and r the read length. In order to assemble reliable contigs, we suggest removing unconfirmed low-quality reads from the assembly. We consider a read as being confirmed when the following two conditions hold: Reads are generated multiple times, and overlapping partners exist. Without quality scores, SHARCGS simply filters for reads generated at least n times, n being a parameter of this filtering step. When quality measures are available, the first filtering step is modified: SHARCGS then filters for reads having high minimal quality values. Quality values of all occurrences of a read on the same or the opposite strand are combined, similar to Phrap (Ewing and Green 1994), i.e., the maximum is used when a confirmation comes from the same strand, while the sum is used for confirmations from the opposite strand. The minimal quality threshold q is a parameter of this filtering variant. When reads are generated with very high coverage, some wrong reads may pass the first filtering step. For instance, when simulating 300,000 30-mer reads with an error rate of 0.6% per base call from a 90-kbp target sequence (i.e., coverage ∼100), we observe up to 500 wrong reads among those generated twice. In simulations, the second criterion allows the removal of virtually all wrong reads left over after the first filtering step, at the expense of discarding very few correct ones. In this second filtering step, our algorithm removes reads that lack partners with perfectly matching overlap. The minimal overlap used to search for matching partners is a parameter of this filtering step, to be chosen larger than half of the read length. We consider reads to be confirmed, if at least one matching partner exists for each of its ends, thus if it is covered entirely at least twice. Reads containing sequencing errors are very unlikely to find partners on both sides. After the filtering steps, SHARCGS generates reverse complements for all confirmed reads and keeps only one copy of each read, before starting the core assembly algorithm. The core assembly algorithm The core assembly algorithm is based on a contig extension scheme using a prefix-tree to look up potential extensions efficiently. Reads are used in turn to nucleate novel contigs. A contig is elongated at its end, as long as we find reads with a prefix of minimal length, which perfectly matches the end of the contig. The algorithm tries to extend the current contig by the second part of the matching read but first checks both strands for ambiguities. Such ambiguities occur when parts of the sequence are repeated. At the end of a repeat, the prefix of two reads will match to the repeated sequence, but their suffixes will match to different parts of the sequence that cannot be arranged unambiguously. In this case, the elongation of the contig is terminated. We search for ambiguities using prefixes of minimal length extracted from each of the last few positions of the putative contig’s end. Each of these prefixes is used to search for other reads which start with the same prefix, and all such reads must match perfectly to the end of the putative contig. By applying this approach, we are able to detect ambiguities although several reads in a row may be missing from the input data. After the elongation of a contig at its 3′ end has been terminated, we compute its reverse complement and try the elongation at the other end the same way. Merging contigs from several assembly runs and generation of quality measures Setting the filtering parameters to very stringent levels discards large amounts of data from the input. The resulting assembly has very short contigs, because reads from too many positions in the target sequence are unavailable. When including weakly confirmed reads in the assembly, it is more difficult to filter reads containing sequencing errors. Such reads, however, would stop the algorithm from assembling long contigs, since reads containing sequencing errors cause spurious ambiguities. The contig breaks due to confirmed reads with sequencing errors tolerated by weak filtering are unlikely to occur at the same positions as the contig breaks caused by missing reads after application of strong filtering criteria. This is why we run the core assembly algorithm automatically for weak, medium, and strong filter parameter settings. For each parameter setting, the core algorithm only generates contigs contained in the target sequence. SHARCGS attempts to merge contigs from different core assembly runs by finding exact overlaps at least as long as the read length. If quality measures are supplied with the raw reads, SHARCGS computes quality measures for any position in all contigs. It adopts the paradigm described in the Phrap documentation (Ewing and Green 1994). For a given position x in a contig, SHARCGS finds all covering reads and selects the best quality measure observed for x. It does the same for the opposite strand and adds the two quality measures to obtain the final value. Fine-tuning parameters We have introduced two parameters for our algorithm: the confirmation level for filtering incorrect reads and the minimal overlap used in the second filtering step as well as in the contig elongation step. The proper setting of both is crucial for the assembler’s reliability, and thus we provide automated conservative settings as described below. The optimal setting of both parameters depends on the length of the target sequence. We use the abundance of reads generated several times as a fingerprint of the target sequence length. We expect the number of reads generated x times to be distributed according to a binomial distribution with parameters k, the number of reads generated, and P, the probability to read correctly at a given position in the target sequence. The probability P is given by Pcorrect/n, where n is the length of the target sequence. In the input data, we can count the number of reads generated x times for any x. In a maximum likelihood approach, we determine the combination of error rate and target sequence length, which most probably causes the distribution of read confirmations observed in the input data. This estimation is accurate in simulated data (within 5% of the real sequence length). According to the target sequence length n, we determine the confirmation levels for which n/2 (strong filtering) to n (weak filtering) reads pass the filter. At most we expect to observe one read per position in the target sequence, so we do not expect to have more than n correct reads. Moreover, we do not expect to be able to assemble input data with decent quality, if more than half of the reads are missing. The minimal overlap with which the core assembly algorithm should check for ambiguities, as well as the minimal overlap used in the second filtering step to detect unreliable reads, depends on the lengths of gaps expected in the input data. In this context, we call gaps runs of consecutive positions for which no confirmed read is available. The probability Pg that a gap at least of length g starts at a given position depends on the number of available reads navail and the length of the target sequence n as follows: equation image where Pmiss is the probability of no confirmed read being available at a given position. The number of such gaps expected in the whole target sequence is n · Pg. We suggest setting the minimal overlap omin such that the expected number of gaps larger than r − omin is <1, r being the read length. Implementation We have implemented all steps of the described algorithm in Perl and tested the script under version 5.8.4 of this scripting language. The program runs from any Linux shell without installation of additional software or modules. Full documentation and a user introduction are included in the script and may be viewed by calling the program without any parameters. The assembly of a BAC of size 100 kbp typically takes <15 min on an Intel Xeon 2.8-GHz 32-bit Linux machine. The corresponding memory footprint of our program is smaller than 1 GB of RAM. The assembly of E. coli takes less than 10 h and uses 24 GB of RAM on an AMD Opteron 2.8-GHz 64-bit Linux machine. The filtering step for the assembly of H. acinonychis takes <1 h and uses 24 GB of RAM. The assemblies of H. acinonychis at four different Q levels and the final merging take 4 h in total and use 6 GB of RAM. Evaluation In order to evaluate the performance of our algorithm, we have simulated short reads from a number of sequences, computed contigs, and matched them back to the reference sequences. Since our method is particularly valuable to the de novo sequencing of BAC inserts, we chose three eukaryotic BAC libraries, i.e., the Clemson T-BAC library (Choi et al. 1995) for Arabidopsis thaliana, RPCI-98 (Hoskins et al. 2000) for Drosophila melanogaster, and RPCI-11 (Osoegawa et al. 2001) for Homo sapiens for the selection of sequenced BAC inserts as target sequences. We collected all sequenced BAC inserts of these libraries from the NCBI nucleotide database (http://www.ncbi.nlm.nih.gov, version as of December 2006) and sorted the sequences by length. We chose 20 successive BAC insert sequences from each of the three data sets so that the average sequence length for each of the species was ∼110 kbp (112 kbp in A. thaliana, 109 kbp in D. melanogaster, and 110 kbp in H. sapiens). In order to mimic the real situation for sequencing even more closely, we added the sequence of the cloning vector pBACe3.6 to each of the test sequences and filtered all contigs matched to this vector before evaluating the assembly results. Short-read generation in silico We have implemented a program that simulates short reads from input target sequences. Parameters for this Perl script are the read length, the number of reads, and the average error rate per base call. The script generates reads from any position in the target sequence with equal probability and decides for each base call independently with a constant error probability whether it is generated correctly. When introducing an error, our simulation program decides with equal probability for one of the three possible substitution errors.",Assembly,"sharcgs  fast  highly accurate shortread assembly algorithm   novo genomic sequencing
 sharcgs algorithm consist   filter step  avoid read  sequence errors  assembly step  generate contigs   final contig merge step include computation  quality measure filter  confirm read sharcgs obtain  large set  read  equal length optionally augment  quality score per base call   input thereby   adapt  cope   substantial number  read  sequence errors   error rate per base call      mer read contain errors pcorrect  probability   single read contain  sequence errors   estimate   product    base call  correct independently thus equation image  perror represent  error rate per base call    read length  order  assemble reliable contigs  suggest remove unconfirmed lowquality read   assembly  consider  read   confirm   follow two condition hold read  generate multiple time  overlap partner exist without quality score sharcgs simply filter  read generate  least  time    parameter   filter step  quality measure  available  first filter step  modify sharcgs  filter  read  high minimal quality value quality value   occurrences   read      opposite strand  combine similar  phrap ewing  green    maximum  use   confirmation come    strand   sum  use  confirmations   opposite strand  minimal quality threshold    parameter   filter variant  read  generate   high coverage  wrong read may pass  first filter step  instance  simulate  mer read   error rate   per base call   kbp target sequence  coverage   observe    wrong read among  generate twice  simulations  second criterion allow  removal  virtually  wrong read leave    first filter step   expense  discard   correct ones   second filter step  algorithm remove read  lack partner  perfectly match overlap  minimal overlap use  search  match partner   parameter   filter step   choose larger  half   read length  consider read   confirm   least one match partner exist     end thus    cover entirely  least twice read contain sequence errors   unlikely  find partner   side   filter step sharcgs generate reverse complement   confirm read  keep  one copy   read  start  core assembly algorithm  core assembly algorithm  core assembly algorithm  base   contig extension scheme use  prefixtree  look  potential extensions efficiently read  use  turn  nucleate novel contigs  contig  elongate   end  long   find read   prefix  minimal length  perfectly match  end   contig  algorithm try  extend  current contig   second part   match read  first check  strand  ambiguities  ambiguities occur  part   sequence  repeat   end   repeat  prefix  two read  match   repeat sequence   suffix  match  different part   sequence  cannot  arrange unambiguously   case  elongation   contig  terminate  search  ambiguities use prefix  minimal length extract     last  position   putative contigs end    prefix  use  search   read  start    prefix    read must match perfectly   end   putative contig  apply  approach   able  detect ambiguities although several read   row may  miss   input data   elongation   contig   ′ end   terminate  compute  reverse complement  try  elongation    end   way merge contigs  several assembly run  generation  quality measure set  filter parameters   stringent level discard large amount  data   input  result assembly   short contigs  read   many position   target sequence  unavailable  include weakly confirm read   assembly    difficult  filter read contain sequence errors  read however would stop  algorithm  assemble long contigs since read contain sequence errors cause spurious ambiguities  contig break due  confirm read  sequence errors tolerate  weak filter  unlikely  occur    position   contig break cause  miss read  application  strong filter criteria     run  core assembly algorithm automatically  weak medium  strong filter parameter settings   parameter set  core algorithm  generate contigs contain   target sequence sharcgs attempt  merge contigs  different core assembly run  find exact overlap  least  long   read length  quality measure  supply   raw read sharcgs compute quality measure   position   contigs  adopt  paradigm describe   phrap documentation ewing  green    give position    contig sharcgs find  cover read  select  best quality measure observe         opposite strand  add  two quality measure  obtain  final value finetuning parameters   introduce two parameters   algorithm  confirmation level  filter incorrect read   minimal overlap use   second filter step  well    contig elongation step  proper set    crucial   assemblers reliability  thus  provide automate conservative settings  describe   optimal set   parameters depend   length   target sequence  use  abundance  read generate several time   fingerprint   target sequence length  expect  number  read generate  time   distribute accord   binomial distribution  parameters   number  read generate    probability  read correctly   give position   target sequence  probability   give  pcorrectn     length   target sequence   input data   count  number  read generate  time      maximum likelihood approach  determine  combination  error rate  target sequence length   probably cause  distribution  read confirmations observe   input data  estimation  accurate  simulate data within    real sequence length accord   target sequence length   determine  confirmation level    strong filter   weak filter read pass  filter    expect  observe one read per position   target sequence     expect      correct read moreover    expect   able  assemble input data  decent quality    half   read  miss  minimal overlap    core assembly algorithm  check  ambiguities  well   minimal overlap use   second filter step  detect unreliable read depend   lengths  gap expect   input data   context  call gap run  consecutive position    confirm read  available  probability    gap  least  length  start   give position depend   number  available read navail   length   target sequence   follow equation image  pmiss   probability   confirm read  available   give position  number   gap expect   whole target sequence      suggest set  minimal overlap omin    expect number  gap larger    omin      read length implementation   implement  step   describe algorithm  perl  test  script  version    script language  program run   linux shell without installation  additional software  modules full documentation   user introduction  include   script  may  view  call  program without  parameters  assembly   bac  size  kbp typically take  min   intel xeon ghz bite linux machine  correspond memory footprint   program  smaller     ram  assembly   coli take less     use    ram   amd opteron ghz bite linux machine  filter step   assembly   acinonychis take    use    ram  assemblies   acinonychis  four different  level   final merge take    total  use    ram evaluation  order  evaluate  performance   algorithm   simulate short read   number  sequence compute contigs  match  back   reference sequence since  method  particularly valuable    novo sequence  bac insert  choose three eukaryotic bac libraries   clemson tbac library choi     arabidopsis thaliana rpci hoskins     drosophila melanogaster  rpci osoegawa     homo sapiens   selection  sequence bac insert  target sequence  collect  sequence bac insert   libraries   ncbi nucleotide database  version   december   sort  sequence  length  choose  successive bac insert sequence     three data set    average sequence length     species   kbp  kbp   thaliana  kbp   melanogaster   kbp   sapiens  order  mimic  real situation  sequence even  closely  add  sequence   clone vector pbace3     test sequence  filter  contigs match   vector  evaluate  assembly result shortread generation  silico   implement  program  simulate short read  input target sequence parameters   perl script   read length  number  read   average error rate per base call  script generate read   position   target sequence  equal probability  decide   base call independently   constant error probability whether   generate correctly  introduce  error  simulation program decide  equal probability  one   three possible substitution errors",5
115,Edena,"De novo bacterial genome sequencing: Millions of very short reads assembled on a desktop computer
Edena is based on the classical overlap layout assembly framework (Pop et al. 2002). In addition, it includes two features to improve the assembly of very short sequences: exact matching and detection of spurious reads. The exact matching choice was included for two reasons. First, the inherent sequencing errors result in a significant number of spurious overlaps, impairing the correct sequence determination. Allowing approximate matching would significantly increase the number of such nonspecific spurious overlaps. Second, exact matching is drastically faster than approximate matching. By using an appropriate index, overlaps between millions of short reads can be computed in a few minutes. The key steps of Edena can be summarized as follows. First, the short reads data set is processed to remove redundant information. Second, all overlaps of a minimum size are computed, and an overlap graph is constructed. Third, the graph is cleaned by removing transitive and spurious edges and by resolving bubbles. Finally, all contigs of a minimum size that are unambiguously represented in the graph are provided as an output. The program assumes that all reads have the same length. Reducing reads redundancy Due to the high level of oversampling achieved by the Illumina Genome Analyzer, a significant number of reads are represented several times in the data set. We first process the data set in order to keep a single copy of each read. This step reduces the size of the data set without losing information. It is achieved by indexing all reads in a prefix tree. A given read and its reverse complement are considered to be the same read and are merged in the same tree key. Reads that contain ambiguous base symbols are discarded since they cannot be handled in the exact matching procedure. Since identical reads are merged in the same tree key, a nonredundant set of reads can be produced from the tree structure. The occurrence frequency of each read as observed in the initial data set is kept in order to compute the coverage depth in the contigs for quality control purposes. Overlapping phase The overlapping phase is performed by indexing the nonredundant read data set by a suffix array (Manber and Myers 1993). This structure reveals exact matches, i.e., exact overlaps, at a low memory cost. The set of revealed overlaps is loaded in a bidirected graph structure (Kececioglu and Myers 1995; Myers 2005) where each read ri corresponds to a vertex vi. Two vertices vi and vj are connected by a bidirected edge if ri and rj overlap. Bidirected edges have an arrowhead at each end and can independently be directed in or out of the vertex at each end of the edge. Subsequently, there are four different ways to connect two nodes, depending on the relative orientation of vi and vj, and the sides of the reads that are involved in the overlap. The arrowhead is directed in vi or vj if the overlap implies the left end of ri or rj, and out of vi or vj if it implies the right end of ri or rj. Edges are labeled with the overlap size. In order to build a valid read assembly, vertices must be traversed using two opposed arrowhead orientations. Entering vi from an in-arrowhead and leaving it to an out-arrowhead indicates that the corresponding read is spelled in its direct strand, while traversing vi from an out-arrowhead to an in-arrowhead indicates that it is spelled in the reverse direction. The minimum overlap size is a determinant parameter for the assembly success. A small value will increase the frequency of overlaps that exist by chance, which creates significant branching in the graph. On the other hand, a large value will increase the number of reads that do not overlap on one of their sides, which leads to DE paths in the graph. Removing transitive edges Due to the high oversampling achieved by the Illumina sequencing technology, the great majority of edges in the overlaps graph correspond to transitive edges. These edges are not essential to represent every possible sequence in the graph. For example, consider two paths v1 → v2 → v3 and v1 → v3. The path v1 → v3 is transitive because it bypasses v2 and represents the same sequence as the first path. This is illustrated from the point of view of a multiple alignment in Figure 2. Transitive edge removal is an essential procedure that reduces the graph complexity by a factor of the oversampling rate c calculated as NL/G, where N is the number of reads, G is the size of the genome being sequenced, and L the length of the reads Cleaning up the graph The transitively reduced overlap graph contains a significant amount of branching paths that compromise the production of long contigs. These branching paths are caused by genomic repetitions, sequencing errors, and clonal polymorphisms (see Results). Without additional information, branching paths caused by genomic repetitions cannot be fixed. However, we propose a simple and efficient method to fix the latter two problems. This step is the key to the success of the Edena approach. Base calling errors in reads cause short DE paths, while clonal polymorphisms create small bubbles in the overlap graph. The cleaning operations identify such features by a local graph exploration starting at each branching node. The first cleaning operation removes the nodes that are involved in short DE paths (Fig. 3). The underlying idea is that edge leading to a read that contains a sequencing error should rapidly reach a DE. Each branching node is thus explored for all possible path elongations up to a depth of md nodes. If no path of depth of md exists, the nodes are marked for removal. Once all DE paths have been detected, marked nodes are removed. The md value is the cutoff above which a branching path is considered to be valid. We determined that a value of md = 10 was a good compromise (see Results). The second cleaning operation identifies short bubbles in the graph (Fig. 4). Such bubbles can be caused by nonexact repetitions in the genomic sequence. However, most of these bubbles could be caused by single base substitutions carried by a subset of DNA molecules contained in the analyzed sample (see Results). We use the term of p-bubble to refer to bubbles that are caused by a single base substitution. In other words, a p-bubble represents a base alternative in the assembly. The length of a p-bubble is at most ms = 4 × L − 2 × T − 1, with L and T being the read length and the minimum required overlap size, respectively. Each branching path is explored up to a length ms. Detected p-bubbles are resolved by removing the nodes in its less covered side. The p-bubble is therefore resolved in a simple non-intersecting path corresponding to the most covered path. Despite the fact that the polymorphisms are not represented in the final assembly, this information can be kept in a separated file. Strict and nonstrict assembly modes An additional cleaning operation significantly increased the size of the contigs but also generated a few misassemblies. This cleaning operation corresponds to the “nonstrict” mode that is implemented in Edena. It is based on the fact that longer overlaps are more reliable than shorter ones. Each branching node is examined, and only the edge (or edges) maximizing the overlap value is (are) kept. This operation allows cleaning ambiguities when the edge corresponding to the maximum length overlap is unique. Once all graph cleaning operations are finished, the set of contigs is produced by spelling the sequences modeled by the non-intersecting simple paths, for which only nodes having in- and out-degree of exactly one are traversed. Strain and culture conditions S. aureus strain MW2 was obtained from NARSA (http://www.narsa.net/) and grown in Mueller Hinton Broth (10 mL) for 5 h. Bacterial cells were rinsed twice in 10 mL TE (TrisEDTA, 10 mM and 1 mM, respectively), suspended in 2 mL of TE containing 100 μg/mL lysostaphin (Ambicin, Applied Microbiology Inc.), and incubated for 10 min at 37°C. DNA was then extracted and purified according to the DNeasy kit (Qiagen). DNA purity and quantity were assessed using NanoDrop-1000. Whole-genome sequencing with the Illumina Genome Analyzer technology The genomic DNA of S. aureus strain MW2 was sequenced using the Solexa technology (P. Mayer, L. Farinelli, and E. Kawashima, 1997. Patent application WO98/44151) according to the manufacturer’s protocol (Illumina). Briefly, 5 mg of genomic DNA was physically fragmented by nebulization into 50- to 100-bp fragments. After end-repair and ligation of the adaptors, the products were purified on agarose gel to recover 150- to 250-bp products. Quality control was performed by cloning the library into a TOPO plasmid and capillary sequencing of a few clones. The samples were then used to generate DNA colonies (or DNA clusters) using two channels of a flow-cell at dilutions of 4 or 6 pM, respectively. The flow-cell was then submitted to 36 cycles of sequencing reaction on the Illumina Genome Analyzer (Illumina). Data were analyzed using the Solexa Data Analysis Pipeline v0.2.2.5 software, and after quality filtration using standard parameters, we obtained a total of 3.86 million reads that were 35 bases in length.",Assembly," novo bacterial genome sequence millions   short read assemble   desktop computer
edena  base   classical overlap layout assembly framework pop     addition  include two feature  improve  assembly   short sequence exact match  detection  spurious read  exact match choice  include  two reason first  inherent sequence errors result   significant number  spurious overlap impair  correct sequence determination allow approximate match would significantly increase  number   nonspecific spurious overlap second exact match  drastically faster  approximate match  use  appropriate index overlap  millions  short read   compute    minutes  key step  edena   summarize  follow first  short read data set  process  remove redundant information second  overlap   minimum size  compute   overlap graph  construct third  graph  clean  remove transitive  spurious edge   resolve bubble finally  contigs   minimum size   unambiguously represent   graph  provide   output  program assume   read    length reduce read redundancy due   high level  oversampling achieve   illumina genome analyzer  significant number  read  represent several time   data set  first process  data set  order  keep  single copy   read  step reduce  size   data set without lose information   achieve  index  read   prefix tree  give read   reverse complement  consider     read   merge    tree key read  contain ambiguous base symbols  discard since  cannot  handle   exact match procedure since identical read  merge    tree key  nonredundant set  read   produce   tree structure  occurrence frequency   read  observe   initial data set  keep  order  compute  coverage depth   contigs  quality control purpose overlap phase  overlap phase  perform  index  nonredundant read data set   suffix array manber  myers   structure reveal exact match  exact overlap   low memory cost  set  reveal overlap  load   bidirected graph structure kececioglu  myers  myers    read  correspond   vertex  two vertices     connect   bidirected edge     overlap bidirected edge   arrowhead   end   independently  direct      vertex   end   edge subsequently   four different ways  connect two nod depend   relative orientation       side   read   involve   overlap  arrowhead  direct       overlap imply  leave end             imply  right end     edge  label   overlap size  order  build  valid read assembly vertices must  traverse use two oppose arrowhead orientations enter    inarrowhead  leave    outarrowhead indicate   correspond read  spell   direct strand  traverse    outarrowhead   inarrowhead indicate    spell   reverse direction  minimum overlap size   determinant parameter   assembly success  small value  increase  frequency  overlap  exist  chance  create significant branch   graph    hand  large value  increase  number  read    overlap  one   side  lead   paths   graph remove transitive edge due   high oversampling achieve   illumina sequence technology  great majority  edge   overlap graph correspond  transitive edge  edge   essential  represent every possible sequence   graph  example consider two paths  →  →    →   path  →   transitive   bypass   represent   sequence   first path   illustrate   point  view   multiple alignment  figure  transitive edge removal   essential procedure  reduce  graph complexity   factor   oversampling rate  calculate  nlg     number  read    size   genome  sequence    length   read clean   graph  transitively reduce overlap graph contain  significant amount  branch paths  compromise  production  long contigs  branch paths  cause  genomic repetitions sequence errors  clonal polymorphisms see result without additional information branch paths cause  genomic repetitions cannot  fix however  propose  simple  efficient method  fix  latter two problems  step   key   success   edena approach base call errors  read cause short  paths  clonal polymorphisms create small bubble   overlap graph  clean operations identify  feature   local graph exploration start   branch node  first clean operation remove  nod   involve  short  paths fig   underlie idea   edge lead   read  contain  sequence error  rapidly reach    branch node  thus explore   possible path elongations    depth   nod   path  depth   exist  nod  mark  removal    paths   detect mark nod  remove   value   cutoff    branch path  consider   valid  determine   value       good compromise see result  second clean operation identify short bubble   graph fig   bubble   cause  nonexact repetitions   genomic sequence however    bubble could  cause  single base substitutions carry   subset  dna molecules contain   analyze sample see result  use  term  pbubble  refer  bubble   cause   single base substitution   word  pbubble represent  base alternative   assembly  length   pbubble                     read length   minimum require overlap size respectively  branch path  explore    length  detect pbubbles  resolve  remove  nod   less cover side  pbubble  therefore resolve   simple nonintersecting path correspond    cover path despite  fact   polymorphisms   represent   final assembly  information   keep   separate file strict  nonstrict assembly modes  additional clean operation significantly increase  size   contigs  also generate   misassemblies  clean operation correspond   “nonstrict” mode   implement  edena   base   fact  longer overlap   reliable  shorter ones  branch node  examine    edge  edge maximize  overlap value   keep  operation allow clean ambiguities   edge correspond   maximum length overlap  unique   graph clean operations  finish  set  contigs  produce  spell  sequence model   nonintersecting simple paths    nod    outdegree  exactly one  traverse strain  culture condition  aureus strain mw2  obtain  narsa   grow  mueller hinton broth      bacterial cells  rinse twice     trisedta      respectively suspend      contain  μgml lysostaphin ambicin apply microbiology inc  incubate   min  ° dna   extract  purify accord   dneasy kit qiagen dna purity  quantity  assess use nanodrop wholegenome sequence   illumina genome analyzer technology  genomic dna   aureus strain mw2  sequence use  solexa technology  mayer  farinelli   kawashima  patent application wo98 accord   manufacturers protocol illumina briefly    genomic dna  physically fragment  nebulization     fragment  endrepair  ligation   adaptors  products  purify  agarose gel  recover    products quality control  perform  clone  library   topo plasmid  capillary sequence    clone  sample   use  generate dna colonies  dna cluster use two channel   flowcell  dilutions      respectively  flowcell   submit   cycle  sequence reaction   illumina genome analyzer illumina data  analyze use  solexa data analysis pipeline  software   quality filtration use standard parameters  obtain  total   million read    base  length",5
116,IDBA-a,"IDBA – A Practical Iterative de Bruijn Graph De Novo Assembler
Given a set of reads, we denote the de Bruijn graph for any fixed k as Gk . Instead of using only one fixed k, IDBA (Iterative de Bruijn Graph short read Assembler) iterates on a range of k values from k = kmin to k = kmax and maintains an accumulated de Bruijn graph Hk at each iteration. In the first step, k = kmin, Hk is equivalent to the graph Gk after deleting all vertices whose corresponding k-mers appear no more than m times (we set m = 1 or 2 in practice depending on the coverage of the input reads) in all reads. Theorem 3(in the Appendix) shows that these k-mers are very likely to be false positives. To construct Hk+1 from Hk , we first construct potential contigs in Hk by identifying maximal paths v1, v2, …, vp in which all vertices have in-degree and out-degree equal to 1 except v1 and vp which may have in-degree 0 and out-degree 0, respectively. Note that a path of p vertices represents a potential contig of length p + k – 1. We remove all reads that can be represented by potential contigs in Hk i.e. those reads that are substrings of a contig (as these reads cannot be used to resolve any branch). In the construction of Hk+1, we only consider the remaining reads and the potential contigs in Hk . We perform two steps to convert Hk to Hk+1. (1) For each edge (vi , vj) in Hk , we convert the edge into a vertex (representing a (k+1)-mer xi1 xi2 … xik xjk = xi1 xj1 …xjk). (2) We connect every two such vertices by an edge if the corresponding two consecutive (k+1)-mers have support from one of the remaining reads or potential contigs of Hk , i.e. the corresponding (k+2)-mer exists. Note that in practice, we do not need to go from k to k+1; we can jump from k to k+s, in which case, for (1), we convert each path of length s in Hk into a vertex. In Theorem 5 in the Appendix, we show that by setting s = 1, we may get high quality contigs. As s increases, we expect the quality of contigs will drop, so it is always better to use a small s. The choice of s will represent a trade-off on the efficiency of the algorithm and the quality of the contigs. For each Hk , we follow other algorithms [7] to remove dead-ends (potential contig shorter than 3k – 1 with one end with 0 in-degree or out-degree, which represents a path in Hk of length at most 2k). Note that removing a dead-end may create more dead-ends, the procedure will repeat until no more dead-ends exist in the graph. These dead-end contigs are likely to be false positives (to be discussed in the Appendix). In fact, most of the remaining false positive vertices after the first filtering step can be removed as dead ends and the accuracy of the contigs produced by IDBA is high. After obtaining Hkmax, we merge bubbles where bubbles are two paths representing two different contigs going from the same vertex v1 to the same vertex vp where these two contigs differ by only one nucleotide. This scenario is likely to be caused by an error or a SNP. Like other assembly algorithms [7-9], we merge the two contigs into one. We base on mate-pair information to connect the contigs as much as possible by using a similar algorithm as Abyss[8] and report the final set of contigs. Note that the probability of removing a true positive vertex in our filtering step is very low (Theorem 3 in Appendix A.3 gives the analysis) as long as kmin and the filtering threshold m are set to a reasonable value (e.g. m = 1). For example, if 1.6×106 length-75 reads are sampled from a genome of length 4.1×106 (45x coverage) with error rate 1%, the probability of filtering out a true positive vertex in H25 is 1.14×10-9 , i.e. the expected number of false negative vertices is 0.0047 << 1 which is very small. Even for some cases where the expected number of false negative vertices is large, say 10, it is still relatively very small when compared with the genome size. Thus, for simplicity in analysis, we assume there is no false negative vertex in Hkmin. The filtering step can remove a large portion of the false positive vertices. Most of the remaining false positive vertices are removed in later steps by dead-ends The probability of removing a correct contig as a dead-end is also small (see Theorem 4 in Appendix A.3 for the exact calculation of the probabilities). The probability of determining a dead-end wrongly is only 2.46×10-4 when the above example is considered. Due to the gap problem a contig that appears in Gk , for a small k, might not be a contig in Gk' for k'>k. However, in IDBA, if a contig c appears in Hk , there must be a contig c’ in Hk’ containing c (Theorem 1). That is, the contig information is carried over from Hk to Hk’. As k increases, more branches can be resolved while the gaps solved when k is small in previous iterations will be preserved. The memory used by IDBA is only about 20-30% of that used by the other existing tools because 80% of false positive vertices are removed in the filtering step (line 2 in algorithm IDBA) and IDBA uses a compact hash table to represent de Bruijn graph implicitly with each edge represented by one bit only. Although IDBA constructs Hkmax from Hkmin step by step, the running time of IDBA is not directly proportional to the number of k values between kmax and kmin. According to Theorem 1, a contig in Hk is also a contig in Hk+1, thus IDBA only needs to check whether a branch in Hk can be resolved in Hk+1. Since reads represented by a contig are removed in each iteration, the number of reads in each iteration decreased. In practice, about half of the reads are removed when constructing Hkmin+1 and IDBA runs much faster than Abyss, and about three times slower than Velvet. 3 Experimental Results The genome of Escherichia coli (O157:H7 str. EC4115) from NCBI [16] is used for simulated experiments (the genome length is 5.6 M). Reads are randomly sampled uniformly with coverage 30x. In our experiments, we generated reads with error rates 1%, read length 75 and insert distance 250. Note that we have repeated the experiments using other coverage (e.g. 50x, 100x), error rates (e.g. 2%) and read length (e.g. 50). The results are similar, so we only show the result for 30x coverage with 1% error on length-75 reads. We also use a real data set, namely Bacillus Subtilis, to evaluate our algorithm. The length of the genome is 4.1M. The reads are sequenced using Solexa machine with coverage 45x, read length 75 and insert distance 400. The estimated error rate is about 1%.",Assembly,"idba   practical iterative  bruijn graph  novo assembler
given  set  read  denote   bruijn graph   fix     instead  use  one fix  idba iterative  bruijn graph short read assembler iterate   range   value    kmin    kmax  maintain  accumulate  bruijn graph    iteration   first step   kmin   equivalent   graph   delete  vertices whose correspond kmers appear     time  set       practice depend   coverage   input read   read theorem   appendix show   kmers   likely   false positives  construct      first construct potential contigs    identify maximal paths   …     vertices  indegree  outdegree equal   except     may  indegree   outdegree  respectively note   path   vertices represent  potential contig  length       remove  read    represent  potential contigs     read   substrings   contig   read cannot  use  resolve  branch   construction     consider  remain read   potential contigs     perform two step  convert       edge        convert  edge   vertex represent  kmer xi1 xi2 … xik xjk  xi1 xj1 …xjk   connect every two  vertices   edge   correspond two consecutive kmers  support  one   remain read  potential contigs      correspond kmer exist note   practice    need         jump       case    convert  path  length      vertex  theorem    appendix  show   set     may get high quality contigs   increase  expect  quality  contigs  drop    always better  use  small   choice    represent  tradeoff   efficiency   algorithm   quality   contigs      follow  algorithms   remove deadends potential contig shorter      one end   indegree  outdegree  represent  path    length    note  remove  deadend may create  deadends  procedure  repeat    deadends exist   graph  deadend contigs  likely   false positives   discuss   appendix  fact    remain false positive vertices   first filter step   remove  dead end   accuracy   contigs produce  idba  high  obtain hkmax  merge bubble  bubble  two paths represent two different contigs go    vertex     vertex    two contigs differ   one nucleotide  scenario  likely   cause   error   snp like  assembly algorithms   merge  two contigs  one  base  matepair information  connect  contigs  much  possible  use  similar algorithm  aby  report  final set  contigs note   probability  remove  true positive vertex   filter step   low theorem   appendix  give  analysis  long  kmin   filter threshold   set   reasonable value      example   length read  sample   genome  length   coverage  error rate   probability  filter   true positive vertex  h25      expect number  false negative vertices        small even   case   expect number  false negative vertices  large say    still relatively  small  compare   genome size thus  simplicity  analysis  assume    false negative vertex  hkmin  filter step  remove  large portion   false positive vertices    remain false positive vertices  remove  later step  deadends  probability  remove  correct contig   deadend  also small see theorem   appendix    exact calculation   probabilities  probability  determine  deadend wrongly       example  consider due   gap problem  contig  appear      small  might    contig  '  ' however  idba   contig  appear     must   contig    contain  theorem     contig information  carry        increase  branch   resolve   gap solve    small  previous iterations   preserve  memory use  idba       use    exist tool    false positive vertices  remove   filter step line   algorithm idba  idba use  compact hash table  represent  bruijn graph implicitly   edge represent  one bite  although idba construct hkmax  hkmin step  step  run time  idba   directly proportional   number   value  kmax  kmin accord  theorem   contig    also  contig   thus idba  need  check whether  branch     resolve   since read represent   contig  remove   iteration  number  read   iteration decrease  practice  half   read  remove  construct hkmin  idba run much faster  aby   three time slower  velvet  experimental result  genome  escherichia coli o157h7 str ec4115  ncbi   use  simulate experiment  genome length    read  randomly sample uniformly  coverage    experiment  generate read  error rat  read length   insert distance  note    repeat  experiment use  coverage    error rat    read length    result  similar    show  result   coverage   error  length read  also use  real data set namely bacillus subtilis  evaluate  algorithm  length   genome    read  sequence use solexa machine  coverage  read length   insert distance   estimate error rate   ",5
117,IDBA-tran,"IDBA-tran: a more robust de novo de Bruijn graph assembler for transcriptomes with uneven expression levels.
Similar to Oases-M, IDBA-Tran also adopts the idea of multiple k to handle transcripts with different expression levels. However, instead of generating a de Bruijn graph and finding transcripts for each k value, an accumulated de Bruijn graph is built to capture all information from both high-expressed and low-expressed transcripts. During each iteration, an accumulated de Bruijn graph Hk for a fixed k is constructed from the input reads and the contigs constructed in previous iterations, i.e. those contigs constructed in Hk-s are treated as input reads for the construction of Hk. The depth information is used to separate de Bruijn graph into components. Ideally, transcripts from different genes are decomposed into different components. In each component, alternative splicing can be detected and transcripts can be reconstructed. To accumulate information, all reconstructed transcripts are used as input reads for the next iteration. Figure 3 shows the workflow of IDBA-Tran for assembling a set of paired-end reads. In the first iteration when k = kmin, Hk is equivalent to a de Bruijn graph for vertices whose corresponding k-mers have multiplicity of at least m (2 by default) times in all reads. During all subsequent iterations, sequencing errors are first removed according to the topological structure of Hk in a slightly different way to other assemblers (Section 2.1). The tips (dangling paths in Hk of length shorter than 2k) are likely to be false positives (Li et al., 2010; Simpson et al., 2009; Zerbino and Birney, 2008). Similar paths (bubbles) representing very similar contigs with the same starting vertex and ending vertex are likely to be caused by errors or SNPs and they should be merged (Li et al., 2010; Simpson et al., 2009; Zerbino and Birney, 2008). Then, the depth information for contigs and components is used to decompose the graph into components (Section 2.2). Paths with high support for the paired-end reads are reconstructed as transcripts in each component (Section 2.3). Errors in the assembled contigs are corrected by aligning reads to the contigs (Section 2.4). When constructing Hk+s from Hk, each length s + 1 path in Hk is converted into a vertex ((k + s)-mer) and there is an edge between two vertices if the corresponding (k + s + 1)-mer appears f (1 by default) times in reads or once in contigs in Ck∪LCk∪Tk, where Ck represents the set of contigs, LCk is the set of contigs constructed by local assembly using paired-end information (Section 2.5), and Tk is the set of transcripts when considering Hk. In the following subsections, we describe each step of IDBA-Tran in detail. Pruning short tips and merge similar path Many de novo assemblers remove tips (short simple paths leading to dead ends) in the de Bruijn graph as erroneous contigs. It would not be advisable to remove such tips in transcriptome assembly, because transcripts are very short (could be several hundred bases) when compared to genomes. Removing one hundred bases from the end of a genome may not be a problem, but removing one hundred bases from the end of a transcript may lose much important information. When constructing the accumulated de Bruijn graph in IDBA-Tran, the tip removal process will take place at each iteration. Instead of removing all tips and producing shorter transcripts, IDBA-Tran keeps the longest tip (with highest probability of being a correct path) and removes all other short tips. For each branch in the graph, IDBA-Tran checks each outgoing (and incoming) edge, keeps the branch which leads to the longest path, and removes all other branches (tips) which lead to paths shorter than 2k. Usually, the correct branch leads to longer paths than tips, and this method preserves correct branches. As transcriptome sequencing data contains more errors and insertions/deletions than genome sequencing data, IDBA-Tran identifies and merges paths with same starting point and end point and higher than 98% similarity (including insertions and deletions). 2.2 Decomposing the graph by iterating depth Recall that T-IDBA (Peng et al., 2011) also tries to decompose the de Bruijn graph into components. It is based on the observation that there are not many repeat patterns between two transcripts from different genes while isoforms from the same gene share common exons. Thus, it decomposes the graph into different components such that there are relatively more branches inside each component and relatively fewer branches between two components. However, erroneous k-mers (from high-expressed isoforms) still cannot be removed effectively since components representing isoforms from different genes may be connected by these erroneous k-mers to form a very large component preventing the assembler from determining isoforms in the component. Instead of considering the number of branches for decomposing the de Bruijn graph into components, IDBA-Tran detects and removes erroneous paths connecting two components by considering the lengths and sequencing depths (depths in short) of the paths using a probabilistic approach. The depth of a path (contig) is the average multiplicity of the k-mer on the path. Long contigs (simple paths in the de Bruijn graph) are usually correct, because long simple paths are unlikely to be formed by erroneous reads, and similarly for high-depth contigs which have supports from many reads. For a contig, whether its length is long or short and whether its depth is high or low cannot be judged by absolute values as the length of a contig depends on the value of k and the depth of a contig depends on the depths of neighboring contigs (contigs in the same component). Since erroneous contigs in high-depth regions may have higher depths than correct contigs in low-depth regions, short (<l) and relatively low-depth contigs are likely to be erroneous and can be removed. The removal takes place in an iterative manner (Chitsaz et al., 2011; Peng et al., 2012), because after some low-depth errors are removed, some short low-depth contigs may be connected together to form long contigs. Increasing depth cutoff progressively may help to preserve more low-depth correct contigs. IDBA-Tran removes contigs (simple paths) shorter than l with average sequencing depth lower than β where β is a threshold calculated based on value of l and the depth distribution of the connected component which contains the contig. When β is large, many correct contigs are removed and many true positive transcripts cannot be assembled. When β is small, many erroneous contigs are not removed and transcripts from different genes may form a large component such that correct transcripts are difficult to reconstruct in later steps (Section 2.3). Thus, we should select the largest threshold β such that not too many correct contigs are removed, say <1%. Consider a correct exon with length at least l. It should be represented by a simple path P in the de Bruijn graph. However, as there are sequencing errors in reads, there may be branches in P and simple path P may be broken into several shorter paths with length less than l. Consider a particular edge u→v in P with the corresponding k-mer v sampled x times (some may contain errors). There is another edge u→v’ in the de Bruijn graph if at least m (the multiplicity threshold used for removing erroneous k-mers) out of the x k-mers sampled from v having the same error at the last nucleotide, i.e. v and v’ differ by the last nucleotide, thus introduces branching at u. This probability can be calculated as follows. Assume the probability of a sequencing error per base is e and the probabilities that the erroneous base is changed to each other nucleotide are the same, i.e. 1/3. Although this simple assumption is not correct for real biological data, the calculation can be readily refined for different probabilities. The probability that v is sampled as v’ with the last nucleotide changed to a particular nucleotide, say ‘A’, is formula As v can be sampled with error as v’, i.e. at least m of the x samples have the same error at the last nucleotide. Since there are three possible v’, the existence probability of v’ (probability of branching at u) is formula In order to estimate the value of depth x, we use a multi-normal distribution to model the depth distribution of a component as there can be multiple isoforms, say t, in a component. Given a set of k-mers with different multiplicities in the same component, we assume the multiplicities of the k-mers are sampled from t normal distributions. Although the mean and standard deviation of each normal distribution can be estimated by expectation-maximization algorithm (Tanaseichuk et al., 2012), the time is too long because there are many k-mers and components. Thus IDBA-Tran applies an approximation by clustering the k-mers based on their multiplicities (the distance between two k-mers equals their difference in multiplicities) using K-means clustering method. The mean and standard deviation can then be calculated for each cluster. We set t = 3 in the experiments based on the assumption that there are at most 3 transcripts in each components (at the final step). Let N(μ, σ) be a normal distribution of depth with minimum mean depth value μ. The probability that we wrongly remove a correct contig with average depth ≤ β is at most formula Note that for an exon of length at least l of sequencing depth x, the probability of branching is forumla⁠. The value of l should be selected based on the length of exons. If a very large l is selected, true positive k-mers and paths are removed. If a very small l is selected, most true negative k-mers and paths cannot be removed. We should select different values for l depending on the properties of the data (we use l = 2k in the experiments). Once l is selected, we can calculate the largest β such that P(false positive) is lower than some value, say 1%, so as to remove most erroneous contigs without too many false positives. Algorithm 1 shows the pseudocode for the decomposing step. According to (Peng et al., 2011), when the size of the component is small (with ≤ γ = 30 contigs), the component is likely to represent isoforms from a single gene and we can use a very low depth threshold β = 0.1 × T(comp), where T(comp) is the average depth of connected component comp, to prevent removing correct contigs. The filtering depth cutoff threshold t is increased by a factor of α progressively (10% by default). In each iteration, short contig c is removed if its depth T(c) is lower than the minimum of cutoff threshold t and the depth threshold β. 2.3 Finding transcripts Algorithm 1. Progressive-Component-Depth(G, k) t ← 1 repeat until t > maxc∈GT(c) for each component comp in G if size(comp) > γ, then calculate β, else β ← 0.1 × T(comp) for each contig c in comp if len(c) < 2k and T(c) < min(t, β) remove c from G t ← t × (1 + α) For each connected component in the de Bruijn graph, IDBA-Tran discovers those paths starting from a vertex with zero in-degree to a vertex with zero out-degree with the highest support from paired-end reads. A path is supported by paired-end reads if the paired-end reads can be aligned to the path with the distance between the aligned positions matching the insert distance of the paired-end reads. The problem definition can be simplified as follows [Transcripts Discovering (TD) Problem]: given a de Bruijn graph G(V, E) with a set of vertices V and edges E, a set of paired-end reads P = {(vi, vj)}, vi, vj ∈ V, an insert distance d and error s, find t paths in G with the maximum number of supporting paired-end reads P’ ⊆ P. A path p has a supporting paired-end read (vi, vj) iff p contains vertices vi and vj and the distance between vi and vj in p is between d – s and d + s. Since the TD problem is a NP-hard problem (see Supplementary Appendix), IDBA-Tran performs a heuristic depth-first search to find paths from a zero in-degree vertex to a zero out-degree vertex with maximum supporting paired-end reads. At each branch, the path with many supporting paired-end reads will be considered before other paths. In practice, IDBA-Tran reports at most tmax (default 3) potential transcripts for each zero in-degree vertex in each connected component. IDBA-Tran applies a seed and extend method for aligning reads to contigs (paths in de Bruijn graph). k-mers in a read appearing in the de Bruijn graph is considering as potential aligned position and IDBA-Tran will try to extend both ends of alignment considering substitution error only. Note that insertion and deletion error can be implemented in IDBA-Tran easily. However, as the number of substitution errors appears much more than the insertion/deletion errors, IDBA-Tran considers substitution error only for speeding up the alignment process. 2.4 Error correction The error correction step is performed on reads and assembled contigs during the assembling process. At first, reads are aligned to each contig. The consensus of the aligned reads will replace the original contig, i.e. positions of the contig inconsistent with the majority of aligned reads will be corrected. Then aligned reads are corrected according to the aligned position in contigs, i.e. positions in the reads with nucleotides inconsistent with the consensus will be corrected. This error correction step can reduce the number of erroneous reads and branches in the de Bruijn graph. 2.5 Local assembly Let C be the set of contigs. We extract the beginning and end of each contig c in C to form a set of contigs C'. Assume the insert distances of paired-end reads satisfy the normal distribution N(d, δ). IDBA-Tran performs local assembly (Peng et al., 2012) on the last d + 3δ bases of each end of the contig and the paired-end read with one end aligned to it. Since those reads which are far away from contig c will not mix with reads with one end aligned to c, some missing k-mers can be reconstructed and the contigs can be extended longer. 2.6 Estimating expression levels Since IDBA-Tran is designed for assembling reads to reconstruct expressed transcripts, sophisticated algorithms can be then applied to estimate the expression levels of each transcript. IDBA-Tran also provides an estimated expression level for each transcript by aligning reads to the transcript. RPKM (Reads Per Kilobase per Million mapped reads) is estimated by dividing the total length of reads uniquely aligned to a transcript by the total length of regions of transcript uniquely aligned by reads.",Assembly,"idbatran   robust  novo  bruijn graph assembler  transcriptomes  uneven expression levels
similar  oasesm idbatran also adopt  idea  multiple   handle transcripts  different expression level however instead  generate   bruijn graph  find transcripts    value  accumulate  bruijn graph  build  capture  information   highexpressed  lowexpressed transcripts   iteration  accumulate  bruijn graph    fix   construct   input read   contigs construct  previous iterations   contigs construct  hks  treat  input read   construction    depth information  use  separate  bruijn graph  components ideally transcripts  different genes  decompose  different components   component alternative splice   detect  transcripts   reconstruct  accumulate information  reconstruct transcripts  use  input read   next iteration figure  show  workflow  idbatran  assemble  set  pairedend read   first iteration    kmin   equivalent    bruijn graph  vertices whose correspond kmers  multiplicity   least    default time   read   subsequent iterations sequence errors  first remove accord   topological structure     slightly different way   assemblers section   tip dangle paths    length shorter    likely   false positives     simpson    zerbino  birney  similar paths bubble represent  similar contigs    start vertex  end vertex  likely   cause  errors  snps     merge     simpson    zerbino  birney    depth information  contigs  components  use  decompose  graph  components section  paths  high support   pairedend read  reconstruct  transcripts   component section  errors   assemble contigs  correct  align read   contigs section   construct hks    length    path    convert   vertex   smer     edge  two vertices   correspond     mer appear    default time  read    contigs  ∪lck∪   represent  set  contigs lck   set  contigs construct  local assembly use pairedend information section      set  transcripts  consider    follow subsections  describe  step  idbatran  detail prune short tip  merge similar path many  novo assemblers remove tip short simple paths lead  dead end    bruijn graph  erroneous contigs  would   advisable  remove  tip  transcriptome assembly  transcripts   short could  several hundred base  compare  genomes remove one hundred base   end   genome may    problem  remove one hundred base   end   transcript may lose much important information  construct  accumulate  bruijn graph  idbatran  tip removal process  take place   iteration instead  remove  tip  produce shorter transcripts idbatran keep  longest tip  highest probability    correct path  remove   short tip   branch   graph idbatran check  outgo  incoming edge keep  branch  lead   longest path  remove   branch tip  lead  paths shorter   usually  correct branch lead  longer paths  tip   method preserve correct branch  transcriptome sequence data contain  errors  insertionsdeletions  genome sequence data idbatran identify  merge paths   start point  end point  higher   similarity include insertions  deletions  decompose  graph  iterate depth recall  tidba peng    also try  decompose   bruijn graph  components   base   observation     many repeat pattern  two transcripts  different genes  isoforms    gene share common exons thus  decompose  graph  different components     relatively  branch inside  component  relatively fewer branch  two components however erroneous kmers  highexpressed isoforms still cannot  remove effectively since components represent isoforms  different genes may  connect   erroneous kmers  form   large component prevent  assembler  determine isoforms   component instead  consider  number  branch  decompose   bruijn graph  components idbatran detect  remove erroneous paths connect two components  consider  lengths  sequence depths depths  short   paths use  probabilistic approach  depth   path contig   average multiplicity   kmer   path long contigs simple paths    bruijn graph  usually correct  long simple paths  unlikely   form  erroneous read  similarly  highdepth contigs   support  many read   contig whether  length  long  short  whether  depth  high  low cannot  judge  absolute value   length   contig depend   value     depth   contig depend   depths  neighbor contigs contigs    component since erroneous contigs  highdepth regions may  higher depths  correct contigs  lowdepth regions short   relatively lowdepth contigs  likely   erroneous    remove  removal take place   iterative manner chitsaz    peng       lowdepth errors  remove  short lowdepth contigs may  connect together  form long contigs increase depth cutoff progressively may help  preserve  lowdepth correct contigs idbatran remove contigs simple paths shorter    average sequence depth lower       threshold calculate base  value     depth distribution   connect component  contain  contig    large many correct contigs  remove  many true positive transcripts cannot  assemble    small many erroneous contigs   remove  transcripts  different genes may form  large component   correct transcripts  difficult  reconstruct  later step section  thus   select  largest threshold      many correct contigs  remove say  consider  correct exon  length  least     represent   simple path     bruijn graph however    sequence errors  read  may  branch    simple path  may  break  several shorter paths  length less   consider  particular edge →     correspond kmer  sample  time  may contain errors   another edge →    bruijn graph   least   multiplicity threshold use  remove erroneous kmers     kmers sample      error   last nucleotide     differ   last nucleotide thus introduce branch    probability   calculate  follow assume  probability   sequence error per base     probabilities   erroneous base  change    nucleotide      although  simple assumption   correct  real biological data  calculation   readily refine  different probabilities  probability    sample     last nucleotide change   particular nucleotide say   formula     sample  error     least     sample    error   last nucleotide since   three possible   existence probability   probability  branch    formula  order  estimate  value  depth   use  multinormal distribution  model  depth distribution   component     multiple isoforms say    component give  set  kmers  different multiplicities    component  assume  multiplicities   kmers  sample   normal distributions although  mean  standard deviation   normal distribution   estimate  expectationmaximization algorithm tanaseichuk     time   long    many kmers  components thus idbatran apply  approximation  cluster  kmers base   multiplicities  distance  two kmers equal  difference  multiplicities use kmeans cluster method  mean  standard deviation    calculate   cluster  set      experiment base   assumption       transcripts   components   final step let     normal distribution  depth  minimum mean depth value   probability   wrongly remove  correct contig  average depth ≤     formula note    exon  length  least   sequence depth   probability  branch  forumla⁠  value     select base   length  exons    large   select true positive kmers  paths  remove    small   select  true negative kmers  paths cannot  remove   select different value   depend   properties   data  use      experiment    select   calculate  largest    pfalse positive  lower   value say     remove  erroneous contigs without  many false positives algorithm  show  pseudocode   decompose step accord  peng      size   component  small  ≤    contigs  component  likely  represent isoforms   single gene    use   low depth threshold     tcomp  tcomp   average depth  connect component comp  prevent remove correct contigs  filter depth cutoff threshold   increase   factor   progressively   default   iteration short contig   remove   depth   lower   minimum  cutoff threshold    depth threshold   find transcripts algorithm  progressivecomponentdepthg     repeat    maxc∈gtc   component comp    sizecomp    calculate  else     tcomp   contig   comp  lenc      mint  remove             connect component    bruijn graph idbatran discover  paths start   vertex  zero indegree   vertex  zero outdegree   highest support  pairedend read  path  support  pairedend read   pairedend read   align   path   distance   align position match  insert distance   pairedend read  problem definition   simplify  follow transcripts discover  problem give   bruijn graph     set  vertices   edge   set  pairedend read   { }   ∈   insert distance   error  find  paths     maximum number  support pairedend read  ⊆   path    support pairedend read   iff  contain vertices      distance                since   problem   nphard problem see supplementary appendix idbatran perform  heuristic depthfirst search  find paths   zero indegree vertex   zero outdegree vertex  maximum support pairedend read   branch  path  many support pairedend read   consider   paths  practice idbatran report   tmax default  potential transcripts   zero indegree vertex   connect component idbatran apply  seed  extend method  align read  contigs paths   bruijn graph kmers   read appear    bruijn graph  consider  potential align position  idbatran  try  extend  end  alignment consider substitution error  note  insertion  deletion error   implement  idbatran easily however   number  substitution errors appear much    insertiondeletion errors idbatran consider substitution error   speed   alignment process  error correction  error correction step  perform  read  assemble contigs   assemble process  first read  align   contig  consensus   align read  replace  original contig  position   contig inconsistent   majority  align read   correct  align read  correct accord   align position  contigs  position   read  nucleotides inconsistent   consensus   correct  error correction step  reduce  number  erroneous read  branch    bruijn graph  local assembly let    set  contigs  extract  begin  end   contig     form  set  contigs ' assume  insert distance  pairedend read satisfy  normal distribution   idbatran perform local assembly peng      last    base   end   contig   pairedend read  one end align   since  read   far away  contig    mix  read  one end align    miss kmers   reconstruct   contigs   extend longer  estimate expression level since idbatran  design  assemble read  reconstruct express transcripts sophisticate algorithms    apply  estimate  expression level   transcript idbatran also provide  estimate expression level   transcript  align read   transcript rpkm read per kilobase per million map read  estimate  divide  total length  read uniquely align   transcript   total length  regions  transcript uniquely align  read",5
118,Meta-IDBA,"Meta-IDBA: a de Novo assembler for metagenomic data.
In this section, we will describe our algorithm, Meta-IDBA, for assembling reads from multiple genomes of subspecies in different species. There are two main steps in Meta-IDBA as shown in Figure 2. Initially (Step 1) sequencing reads are used to construct a de Bruijn graph using any de Bruijn graph-based assembler (Chaisson and Pevzner, 2008; Peng et al., 2010; Simpson et al., 2009; Zerbino and Birney, 2008). Each simple path in the de Bruijn graph might represent a contig of the genome of some species or subspecies. As there are some sequences appearing in multiple species, the de Bruijn graph of reads from different species are interconnected by cr-branches. In the second step (Step 2), based on the assumption that the genomes of subspecies from the same species share more similar regions than the genomes of subspecies from different species, Meta-IDBA divides the de Bruijn graph into many small connected components by removing cr-branches. As the genome sequences of subspecies even from the same species are not exactly the same, the consensus contig may not be represented by a simple path in the de Bruijn graph, but may be represented by a component with multiple paths (due to sp-branches) from a source vertex to a single sink vertex, so as to confine the variations of the similar regions in each genome. These small components are then merged into bigger components, which represent longer consensus contigs using paired-end reads (Step 3). In the last step of Meta-IDBA (Step 4), each component is transformed to a multiple alignment of similar contigs of different subspecies from the same species. The consensus contig, which represents the similar contigs of the subspecies in a species, is found. The steps of Meta-IDBA will be described in detail in the following sections. Construct de Bruijn graph Any genome assembler based on the de Bruijn graph approach (Chaisson et al., 2009; Peng et al., 2010; Pevzner et al., 2001; Simpson et al., 2009; Zerbino and Birney, 2008) can be used to assemble reads to form contigs as if the reads were from a single genome. The de Bruijn graph-based algorithm first divides each reads into length-k substrings called k-mers. Each k-mer is represented by a vertex in the de Bruijn graph. There is an edge from u to v if the length k −1 suffix of k-mer u is the same as the length k −1 prefix of k-mer v and k-mers u and v occur adjacently in at least one read. In the single genome assembly problem, each simple path in the de Bruijn graph represents a contig of the genome and each branch represents a repeat region in the genome or an error in the read. The IDBA genome assembler (Peng et al., 2010) is used in our implementation as IDBA iteratively increases the value of k and removes k-mers with few supports from reads, more branches can be resolved and longer contigs of the genome can be obtained. However, metagenomic data contains read from genomes of multiple species, and the sp- and cr-branches in the de Bruijn graph represent common regions occurring in genomes of different species or subspecies. These kinds of branches cannot be removed by the single genome assembler and thus very short contigs will be produced especially for samples with many subspecies of the same species. Thus, additional steps are required for resolving these branches. 2.2 Divide graph into connected components Based on the well-accepted idea (Pop, 2009) that genomes of subspecies in the same species are more similar than genomes of subspecies from different species, Meta-IDBA divides the de Bruijn graph into connected components such that each component represents a consensus contig of a species. With the assumption that genomes of subspecies from the same species are very similar, there are many similar regions along the genome, where common k-mers (e.g. k=50) do not appear too distinct and at least one common k-mer exists within a relative short region of length w (e.g. w=300 bp). On the other hand, the genomes of subspecies from different species seldom contain a common k-mer so frequently. Based on this assumption, we divide the de Bruijn graph into components by solving the following graph partitioning algorithm. Graph partition problem: given a directed graph G, the graph partition problem is to partition the graph into maximal connected components that satisfy the following property. For each vertex u in a component C, there is another vertex v in C such that: (i) starting from each out-going edge e of u, there is at least a path from u to v in C with length at most w; or (ii) for every in-coming edge e of u, there is at least a path from v to u in C with length at most w. The idea behind the formulation of the graph partition problem is as follows. If a k-mer u can reach another k-mer v in a de Bruijn graph through a path of length at most w starting from each of its out-going edges (or vice versa), k-mers u and v are likely to occur in the genomes of subspecies from the same species, which represent high similarity among the genomes of the subspecies. Otherwise, k-mers u and v may occur in genomes of different species and they should be separated. The graph partition problem can be solved by a greedy algorithm, which repeatedly checks all out-going (in-coming) edges of each vertex. If there are no paths of length at most w starting from (ending with) the out-going (in-coming) edges of a vertex u that ends with (start from) another vertex v, all the out-going (in-coming) edges of vertex u are removed from the de Bruijn graph. The removing process will be repeated until all components satisfy our requirements. The correctness of this greedy algorithm is proved in Theorem 1. The process will remove the cr-branches as well as some of the sp-branches, resulting in many small connected components with close common regions (k-mers) representing consensus contigs of a single species. By induction of the number of removing edges. When the first edge e1=(u1, u1′) is removed by the greedy algorithm, edge e1 should not be in any component, because either there is no vertex v such that all out-going edges of u1 can access v in a path in G with length at most w or there is no vertex u such that u can access u1′ by paths in G with length at most w that end with each in-coming edges of u1′. Consider the k-th edge ek=(uk,uk′) removed by the greedy algorithm. Either there is no vertex v such that all out-going edges of uk can access v in a path in G/{e1,…,ek−1} with length at most w or there is no vertex u such that u can access u1′ by paths in G/{e1,…,ek−1} with length at most w that end with each in-coming edges of uk′. Since the edges e1,…,ek−1 are not in any component, edge ek should not be in any component.    ▪ 2.3 Connect components by paired-end reads After solving the graph partition problem, larger components may be broken down into smaller components, because of the erroneous reads and common regions among different species and each component will represent a consensus contig of subspecies from a single species. Meta-IDBA merges the components into larger components using paired-end reads. A paired-end read represents two reads appearing in the same genome with known order and distance (insert distance). One end of a paired-end read is considered to appear in a component if a k-mer of the read appears in the component. If the two ends of a paired-end read appear in different components, the paired-end read is considered as a support that the two components should be merged into a larger component. Meta-IDBA merges the components if there are at least α (e.g. α=10) supports from paired-end reads and the two components are connected by the shortest path, which matches with the insert distance of the pair-end reads only if the merging is unambiguous (i.e. the merging will not be performed if a component has enough supports to connect to more than one component). Moreover, only the components with consensus contig longer than insert distance are considered in this procedure, because otherwise a component caused by a repeat region shorter than insert distance will be connected to multiple components with enough supports. 2.4 Construct multiple alignment and consensus contigs Since the genomes of the subspecies from the same species are similar with small variants, it is very difficult to determine long contigs (simple path) from each subspecies, because these contigs are interconnected in the de Bruijn graph. Instead of representing a contig of a species by a simple path, a consensus contig of the species (which has many subspecies) is represented by a component, which is usually a direct acyclic graph (with exception when there is repeat region in the contig) to capture the small variants among the genomes of different subspecies in the same species. Each simple path in the component represents a short contig of a subspecies and the relative positions of these short contigs can be obtained from the structure of the directed acyclic graph. In order to obtain a contig of a single species from the component, multiple alignment of the short contigs represented by simple paths is performed. Meta-IDBA starts with an alignment of some short simple subpaths at the source vertex of the component and progressively constructs the alignments of longer contigs with reference to the longest subpath, because it provides more information of the genomes of the subspecies than the short subpaths with deletions. As the relative position of each simple path can be determined, the alignment between each simple subpath can be found locally. Finally, each component is represented by a consensus contig obtained through multiple alignment.",Assembly,"metaidba   novo assembler  metagenomic data
  section   describe  algorithm metaidba  assemble read  multiple genomes  subspecies  different species   two main step  metaidba  show  figure  initially step  sequence read  use  construct   bruijn graph use   bruijn graphbased assembler chaisson  pevzner  peng    simpson    zerbino  birney   simple path    bruijn graph might represent  contig   genome   species  subspecies     sequence appear  multiple species   bruijn graph  read  different species  interconnect  crbranches   second step step  base   assumption   genomes  subspecies    species share  similar regions   genomes  subspecies  different species metaidba divide   bruijn graph  many small connect components  remove crbranches   genome sequence  subspecies even    species   exactly    consensus contig may   represent   simple path    bruijn graph  may  represent   component  multiple paths due  spbranches   source vertex   single sink vertex    confine  variations   similar regions   genome  small components   merge  bigger components  represent longer consensus contigs use pairedend read step    last step  metaidba step   component  transform   multiple alignment  similar contigs  different subspecies    species  consensus contig  represent  similar contigs   subspecies   species  find  step  metaidba   describe  detail   follow section construct  bruijn graph  genome assembler base    bruijn graph approach chaisson    peng    pevzner    simpson    zerbino  birney    use  assemble read  form contigs    read    single genome   bruijn graphbased algorithm first divide  read  lengthk substrings call kmers  kmer  represent   vertex    bruijn graph    edge       length   suffix  kmer       length   prefix  kmer   kmers    occur adjacently   least one read   single genome assembly problem  simple path    bruijn graph represent  contig   genome   branch represent  repeat region   genome   error   read  idba genome assembler peng     use   implementation  idba iteratively increase  value    remove kmers   support  read  branch   resolve  longer contigs   genome   obtain however metagenomic data contain read  genomes  multiple species     crbranches    bruijn graph represent common regions occur  genomes  different species  subspecies  kinds  branch cannot  remove   single genome assembler  thus  short contigs   produce especially  sample  many subspecies    species thus additional step  require  resolve  branch  divide graph  connect components base   wellaccepted idea pop   genomes  subspecies    species   similar  genomes  subspecies  different species metaidba divide   bruijn graph  connect components    component represent  consensus contig   species   assumption  genomes  subspecies    species   similar   many similar regions along  genome  common kmers     appear  distinct   least one common kmer exist within  relative short region  length        hand  genomes  subspecies  different species seldom contain  common kmer  frequently base   assumption  divide   bruijn graph  components  solve  follow graph partition algorithm graph partition problem give  direct graph   graph partition problem   partition  graph  maximal connect components  satisfy  follow property   vertex    component    another vertex       start   outgo edge       least  path        length       every incoming edge       least  path        length     idea behind  formulation   graph partition problem   follow   kmer   reach another kmer     bruijn graph   path  length    start     outgo edge  vice versa kmers     likely  occur   genomes  subspecies    species  represent high similarity among  genomes   subspecies otherwise kmers    may occur  genomes  different species     separate  graph partition problem   solve   greedy algorithm  repeatedly check  outgo incoming edge   vertex     paths  length    start  end   outgo incoming edge   vertex   end  start  another vertex    outgo incoming edge  vertex   remove    bruijn graph  remove process   repeat   components satisfy  requirements  correctness   greedy algorithm  prove  theorem   process  remove  crbranches  well     spbranches result  many small connect components  close common regions kmers represent consensus contigs   single species  induction   number  remove edge   first edge e1u1 ′  remove   greedy algorithm edge       component  either    vertex     outgo edge    access    path    length        vertex      access ′  paths    length     end   incoming edge  ′ consider  kth edge ekukuk′ remove   greedy algorithm either    vertex     outgo edge    access    path  {…}  length        vertex      access ′  paths  {…}  length     end   incoming edge  ′ since  edge …     component edge       component    ▪  connect components  pairedend read  solve  graph partition problem larger components may  break   smaller components    erroneous read  common regions among different species   component  represent  consensus contig  subspecies   single species metaidba merge  components  larger components use pairedend read  pairedend read represent two read appear    genome  know order  distance insert distance one end   pairedend read  consider  appear   component   kmer   read appear   component   two end   pairedend read appear  different components  pairedend read  consider   support   two components   merge   larger component metaidba merge  components     least    support  pairedend read   two components  connect   shortest path  match   insert distance   pairend read    merge  unambiguous   merge    perform   component  enough support  connect    one component moreover   components  consensus contig longer  insert distance  consider   procedure  otherwise  component cause   repeat region shorter  insert distance   connect  multiple components  enough support  construct multiple alignment  consensus contigs since  genomes   subspecies    species  similar  small variants    difficult  determine long contigs simple path   subspecies   contigs  interconnect    bruijn graph instead  represent  contig   species   simple path  consensus contig   species   many subspecies  represent   component   usually  direct acyclic graph  exception    repeat region   contig  capture  small variants among  genomes  different subspecies    species  simple path   component represent  short contig   subspecies   relative position   short contigs   obtain   structure   direct acyclic graph  order  obtain  contig   single species   component multiple alignment   short contigs represent  simple paths  perform metaidba start   alignment   short simple subpaths   source vertex   component  progressively construct  alignments  longer contigs  reference   longest subpath   provide  information   genomes   subspecies   short subpaths  deletions   relative position   simple path   determine  alignment   simple subpath   find locally finally  component  represent   consensus contig obtain  multiple alignment",5
119,IDBA-UD,"IDBA-UD: a de novo assembler for single-cell and metagenomic sequencing data with highly uneven depth
A flowchart of the major steps of IDBA-UD is shown in Figure 1. IDBA-UD iterates the value of k from kmin to kmax. In each iteration, an ‘accumulated de Bruijn graph’ Hk for a fixed k is constructed from the set of input reads and the contigs (Ck−s and LCk−s) constructed in previous iterations, i.e. these contigs are treated as input reads for constructing Hk. In each iteration, IDBA-UD also progressively increases the value of depth cutoff thresholds for removing some low-depth contigs so as to get longer confident contigs (Ck) in Hk. Error in reads are corrected by aligning the reads to some confident contigs. Some missing k-mers in reads can be recovered from those contigs (LCk) reconstructed by local assembling of a small set of paired-end reads with one end aligned to a confident contig. Information of these missing k-mers will be passed on to the next iteration through these contigs (LCk) for the construction of Hk+s. Finally, all outputted contigs are used to form scaffolds using paired-end reads information. Algorithm 1 shows the pseudocode of IDBA-UD for assembling a set of paired-end reads R with insert distance d and SD δ. In the first iteration when k=kmin, Hk is equivalent to a de Bruijn graph for vertices whose corresponding k-mers have multiplicity at least m (2 by default) times in all reads. During all the subsequent iterations, some sequencing errors are first removed according to the topological structure of Hk, e.g. dead-end contigs and bubbles [Steps (b) and (c)]. The dead-end contigs (tangling paths in Hk of lengths shorter than 2k) are likely to be false positives (Li et al., 2010; Simpson et al., 2009; Zerbino and Birney, 2008). Paths (bubbles) representing very similar contigs except at one position and with the same starting vertex and ending vertex are likely to be caused by an error or a single-nucleotide polymorphism (SNP) and they should be merged together into one contig (Hernandez et al., 2008; Simpson et al., 2009; Zerbino and Birney, 2008). When constructing Hk+s from Hk, each length s+1 path in Hk is converted into a vertex (k+s)-mer and there is an edge from between two vertices if the corresponding (k+s+1)-mer appears f (1 by default) times in reads or once in contigs in Ck∪ LCk. In the following subsections, we will describe the other steps of IDBA-UD in detail. 2.1 Progressive relative depth The sequencing depth, ‘depth’ in short, of each simple path (contig) in Hk (H′k which is a copy of Hk is used in Algorithm 1 so as to preserve Hk after the implementation of this step) is used to remove errors. The ‘depth of a contig’ is the average number of reads covering each k-mer in the contig. Note that long contigs are usually correct, because long simple paths can unlikely be formed by erroneous reads; similarly for high-depth contigs which have supports from many reads. For a contig, whether its length is long or short and whether its depth is high or low cannot be judged by its absolute values as the length of a contig depends on the value of k and the depth of a contig depends on the depths of its neighboring contigs (neighboring contigs can be identified by their adjacency in the de Bruijn graph). Even though wrong contigs in highdepth regions may have higher depths than correct contigs from low-depth regions, ‘short’ (<2k) and ‘relatively low depth’ (less than a fraction β of its neighboring contigs' average depth) contigs are likely to be erroneous and can be removed. There is still a risk of removing short and relatively low-depth correct contigs because some relatively low-depth correct contigs with high-depth neighbors may be broken into short contigs by some wrong contigs (as branches in Hk). Based on the observation that these short and relatively low-depth correct contigs usually have higher depths than the short wrong contigs, we can filter out these wrong contigs first by increasing the depth cutoff threshold progressively from low to high. After the wrong contigs or branches are removed by a low-depth cutoff threshold, the relative low-depth correct contigs will be linked together to form long confident contigs which will be considered as reads for the next iteration. The key idea to consider the depth progressively and relatively is shown in Algorithm 2. T(c) represents the depth of contig c and Tneighbor(c) represents the mean depth of c's neighboring contigs. The filtering depth cutoff threshold t is increased by a factor α progressively (α is ~10%). A geometric increase, instead of absolute increase (as used in Velvet-SC), in the depth cutoff threshold value improves implementation efficiency because the threshold difference is more sensitive at the low-depth values than the high-depth values. In each iteration, short contig c is removed if its depth T(c) is lower than the minimum of cutoff threshold t and the relative threshold β*Tneighbor(c) where β is in the range of 0.1–0.5. graphic 2.2 Local assembly IDBA makes use of the contigs (containing the information of some missing k-mers for larger k) constructed in each iteration for the construction of the de Bruijn graphs of larger k. These missing k-mers may not exist in any of the reads but they might help to fill the gaps in the de Bruijn graphs for larger k. This approach still has a limitation that not all the missing k-mers, i.e. contigs containing these k-mers, can be constructed (so not all the gaps can be filled) because of branches. The main contribution of local assembly is to construct these contigs for the missing k-mers, especially in the low-depth regions, based on the information of paired-end reads to eliminate the branches introduced from other parts of the genome. We shall illustrate this main idea of local assembly through an example (Fig 2). Let us consider the construction of a de Bruijn Graph for k=3, based on two reads, …AACT and ACTG…, we have a simple path connecting the 3mers, AAC, ACT and CTG. IDBA can reconstruct the missing 5mer AACTG (not appeared in any reads) by forming a simple path containing it. However, as given in Figure 2, when ACT is a length-3 repeat in the genome (the repeat regions are apart by more than the insert distance) and there are reads covering the region …TACTT… containing the other repeat. The 3mer ACT in the de Bruijn graph for k=3 now has two in-branches and two out-branches (refer to the left diagram of Figure 3 where vertex v represents the 3mer ACT; vertices u, w, u′ and w′ are for 3mers AAC, CTG, TAC and CTT, respectively). Under this situation, even when k is increased to 4 and 5 in IDBA (this part of graph will be disconnected in H4 and H5), the missing critical 5mers AACTG cannot be reconstructed because of the branches. However, when considering the de Bruijn graph when k=3, IDBA-UD will align the paired-end reads to the contig ACGATCGTAGCTGA (Fig. 2) whereas the reads of the other ends covering the repeat regions will only be …AACT and ACTG… (reads covering the other repeat region …TACTT… are not involved because they are far away). Thus, local assembly (by considering the reads locally) can produce a simple path containing the critical 5mer AACTG to resolve branches as if there were no repeats. graphic Fig. 2. Example of reconstructing missing k-mer in local assembly. ACT is a repeat region in the genome and no reads containing AACTG or TACTT for resolving repeat branches. In local assembly, ACT is no longer a repeat so that a simple path (local contig) covering AACTG can be reconstructed from local reads Open in new tabDownload slide Example of reconstructing missing k-mer in local assembly. ACT is a repeat region in the genome and no reads containing AACTG or TACTT for resolving repeat branches. In local assembly, ACT is no longer a repeat so that a simple path (local contig) covering AACTG can be reconstructed from local reads Fig. 3. Example of resolving repeats by iteration from k to k+1. The repeat region is a single k-mer, uvw and u′vw′ appear in the genome. After the iteration, repeat v is resolved Open in new tabDownload slide Example of resolving repeats by iteration from k to k+1. The repeat region is a single k-mer, uvw and u′vw′ appear in the genome. After the iteration, repeat v is resolved Let Ck be the set of contigs (simple paths) in Hk. The set of paired-end reads Rc are those with one read aligned with the ends of each long contig c (with length at least twice of read length) in Ck (crc stands for the reverse complement of contig c). The other unaligned ends of these aligned paired-end reads, which would cover the genome regions extended about an insert distance beyond each end of a long contig, are extracted separately. Assume the insert distances of paired-end reads satisfy the normal distribution N(d, δ). IDBA-UD groups the last d+3δ bases of c/crc and Rc/Rcrc together and then locally assembles them into the set of local contigs LCk using IDBA [Algorithm 1 without Steps (e), (g) and (h)] as shown in Algorithm 3. Since those reads which are far away from the contig c will not be mixed up with these unaligned ends, the contig c and these unaligned ends (reads) of Rc can be used to construct a smaller and simpler de Bruijn graph whose simple paths (represented by the set of contigs LCk) might reconstruct some of the missing k-mers and be considered as reads for the next iteration. Thus, the contigs can be extended longer and longer at each iteration. The expected number of resolved branches can be computed by Theorem 3 (Appendix). 2.3 Error correction To reduce the errors in reads, error correction on some erroneous bases is performed based on the alignment between reads and confident contigs. Errors in reads are corrected only if they can be aligned to contigs with certain similarity, say 95%. The reads which can be multi-aligned to different contigs will not be considered for corrections. This approach of error correction is especially effective for high-depth regions because the confident contigs are well-supported by many reads. A position of a contig is labeled as ‘confirmed’ if one base type appears over 80% in all reads aligned to that position. Each read, aligned to a contig region with all positions confirmed and the number of different bases no >3, will be corrected according to the confirmed bases. A pre-error-correction step for improved efficiency can be used to remove errors in high-depth regions as the first step in IDBA-UD if the sequencing depths are extremely uneven. A medium k-value and filtering threshold will be used to assemble reads to form contigs and errors in reads are corrected based on its alignment with the output contigs. 2.4 Scaffold The reads are finally aligned to contigs so as to build a scaffold graph in which each vertex u represents a contig and each edge (u, v) represents the connection between u and v with a support of >p (3 by default) paired-end reads. After the scaffold graph is built, scaffold algorithm (Li et al., 2010) will be applied to further connect contigs.",Assembly,"idbaud   novo assembler  singlecell  metagenomic sequence data  highly uneven depth
 flowchart   major step  idbaud  show  figure  idbaud iterate  value    kmin  kmax   iteration  accumulate  bruijn graph    fix   construct   set  input read   contigs cks  lcks construct  previous iterations   contigs  treat  input read  construct    iteration idbaud also progressively increase  value  depth cutoff thresholds  remove  lowdepth contigs    get longer confident contigs    error  read  correct  align  read   confident contigs  miss kmers  read   recover   contigs lck reconstruct  local assemble   small set  pairedend read  one end align   confident contig information   miss kmers   pass    next iteration   contigs lck   construction  hks finally  output contigs  use  form scaffold use pairedend read information algorithm  show  pseudocode  idbaud  assemble  set  pairedend read   insert distance       first iteration  kkmin   equivalent    bruijn graph  vertices whose correspond kmers  multiplicity  least    default time   read    subsequent iterations  sequence errors  first remove accord   topological structure    deadend contigs  bubble step     deadend contigs tangle paths    lengths shorter    likely   false positives     simpson    zerbino  birney  paths bubble represent  similar contigs except  one position     start vertex  end vertex  likely   cause   error   singlenucleotide polymorphism snp     merge together  one contig hernandez    simpson    zerbino  birney   construct hks    length  path    convert   vertex ksmer     edge   two vertices   correspond ksmer appear    default time  read    contigs  ∪ lck   follow subsections   describe   step  idbaud  detail  progressive relative depth  sequence depth depth  short   simple path contig   ′    copy    use  algorithm     preserve    implementation   step  use  remove errors  depth   contig   average number  read cover  kmer   contig note  long contigs  usually correct  long simple paths  unlikely  form  erroneous read similarly  highdepth contigs   support  many read   contig whether  length  long  short  whether  depth  high  low cannot  judge   absolute value   length   contig depend   value     depth   contig depend   depths   neighbor contigs neighbor contigs   identify   adjacency    bruijn graph even though wrong contigs  highdepth regions may  higher depths  correct contigs  lowdepth regions short   relatively low depth less   fraction    neighbor contigs' average depth contigs  likely   erroneous    remove   still  risk  remove short  relatively lowdepth correct contigs   relatively lowdepth correct contigs  highdepth neighbor may  break  short contigs   wrong contigs  branch   base   observation   short  relatively lowdepth correct contigs usually  higher depths   short wrong contigs   filter   wrong contigs first  increase  depth cutoff threshold progressively  low  high   wrong contigs  branch  remove   lowdepth cutoff threshold  relative lowdepth correct contigs   link together  form long confident contigs    consider  read   next iteration  key idea  consider  depth progressively  relatively  show  algorithm   represent  depth  contig   tneighborc represent  mean depth  ' neighbor contigs  filter depth cutoff threshold   increase   factor  progressively   ~  geometric increase instead  absolute increase  use  velvetsc   depth cutoff threshold value improve implementation efficiency   threshold difference   sensitive   lowdepth value   highdepth value   iteration short contig   remove   depth   lower   minimum  cutoff threshold    relative threshold *tneighborc      range   graphic  local assembly idba make use   contigs contain  information   miss kmers  larger  construct   iteration   construction    bruijn graph  larger   miss kmers may  exist     read   might help  fill  gap    bruijn graph  larger   approach still   limitation     miss kmers  contigs contain  kmers   construct     gap   fill   branch  main contribution  local assembly   construct  contigs   miss kmers especially   lowdepth regions base   information  pairedend read  eliminate  branch introduce   part   genome  shall illustrate  main idea  local assembly   example fig  let  consider  construction    bruijn graph   base  two read …aact  actg…    simple path connect  3mers aac act  ctg idba  reconstruct  miss 5mer aactg  appear   read  form  simple path contain  however  give  figure   act   length repeat   genome  repeat regions  apart     insert distance    read cover  region …tactt… contain   repeat  3mer act    bruijn graph     two inbranches  two outbranches refer   leave diagram  figure   vertex  represent  3mer act vertices   ′  ′   3mers aac ctg tac  ctt respectively   situation even    increase      idba  part  graph   disconnect      miss critical 5mers aactg cannot  reconstruct    branch however  consider   bruijn graph   idbaud  align  pairedend read   contig acgatcgtagctga fig  whereas  read    end cover  repeat regions    …aact  actg… read cover   repeat region …tactt…   involve    far away thus local assembly  consider  read locally  produce  simple path contain  critical 5mer aactg  resolve branch      repeat graphic fig  example  reconstruct miss kmer  local assembly act   repeat region   genome   read contain aactg  tactt  resolve repeat branch  local assembly act   longer  repeat    simple path local contig cover aactg   reconstruct  local read open  new tabdownload slide example  reconstruct miss kmer  local assembly act   repeat region   genome   read contain aactg  tactt  resolve repeat branch  local assembly act   longer  repeat    simple path local contig cover aactg   reconstruct  local read fig  example  resolve repeat  iteration      repeat region   single kmer uvw  ′′ appear   genome   iteration repeat   resolve open  new tabdownload slide example  resolve repeat  iteration      repeat region   single kmer uvw  ′′ appear   genome   iteration repeat   resolve let    set  contigs simple paths    set  pairedend read     one read align   end   long contig   length  least twice  read length   crc stand   reverse complement  contig    unaligned end   align pairedend read  would cover  genome regions extend   insert distance beyond  end   long contig  extract separately assume  insert distance  pairedend read satisfy  normal distribution   idbaud group  last d3δ base  ccrc  rcrcrc together   locally assemble    set  local contigs lck use idba algorithm  without step      show  algorithm  since  read   far away   contig     mix    unaligned end  contig    unaligned end read     use  construct  smaller  simpler  bruijn graph whose simple paths represent   set  contigs lck might reconstruct    miss kmers   consider  read   next iteration thus  contigs   extend longer  longer   iteration  expect number  resolve branch   compute  theorem  appendix  error correction  reduce  errors  read error correction   erroneous base  perform base   alignment  read  confident contigs errors  read  correct      align  contigs  certain similarity say   read    multialigned  different contigs    consider  corrections  approach  error correction  especially effective  highdepth regions   confident contigs  wellsupported  many read  position   contig  label  confirm  one base type appear     read align   position  read align   contig region   position confirm   number  different base     correct accord   confirm base  preerrorcorrection step  improve efficiency   use  remove errors  highdepth regions   first step  idbaud   sequence depths  extremely uneven  medium kvalue  filter threshold   use  assemble read  form contigs  errors  read  correct base   alignment   output contigs  scaffold  read  finally align  contigs    build  scaffold graph    vertex  represent  contig   edge   represent  connection       support     default pairedend read   scaffold graph  build scaffold algorithm       apply   connect contigs",5
120,T-IDBA,"T-IDBA: A de novo Iterative de Bruijn Graph Assembler for Transcriptome
Different from genome assembly whose input reads are sampled from a species genome, the input reads of mammalian transcriptome assembly are sampled from the (expressed) mRNAs of a mammal. As the total length of genes is much shorter than the genome, at first glance, mammalian transcriptome assembly problem seems easier than the genome assembly problem. However, because of alternative splicing, some long patterns representing exons may occur in multiple mRNAs (isoforms) from the same gene. Thus, the de Bruijn graph (i.e. the graph with each vertex representing a k-mer and an edge from u to v if u and v adjacently occur in a read) has more branches when constructed for the mammalian transcriptome assembly problem than those for the genome assembly problem. Therefore, traditional de novo genome assemblers [11] would not work well for the mammalian transcriptome assembly problem as they would usually stop at branches resulting in very short contigs which represent only part of the exons instead of the whole isoform. Besides, many de novo genomic assemblers [10, 11] remove both ends of a contig to increase the accuracy. As the mRNAs are usually relatively short, e.g. 500~5000nt, the lengths of contigs decrease significantly. In order to reconstruct the isoforms of different genes, we developed T-IDBA which first divides the de Bruijn graph into many connected components, most of which represent isoforms from a single gene. Then T-IDBA determines each isoform from each component using pair-end information. 2.1 Constructing Connected Components We observe that, although the repeat patterns in the whole genome can be very long, the number of genes that contain the same long repeated patterns is actually quite few. Table 1 shows the number of genes of mouse2 having repeated patterns of length at least 30. In particular, there are only 367 genes, out of 16,491 genes, containing repeated patterns of length at least 90bp. If we can construct a de Brujin graph with large k, most connected components should contain isoforms from single genes. We have also built de Bruijn graphs of the reference mRNA of the mouse (UCSC: mm9, NCBI build 37) for different values of k. Figure 1 shows the number of connected components. The number of connected components increases when k increases and there are 20,457 connected components for the de Bruijn graph with k = 90. As there are 46,104 mRNAs in the mouse database, each component contains on average 2 mRNAs. Table 2 shows the distribution of the numbers of mRNAs in components. About 91% of mRNAs are in components containing no more than 10 mRNAs, with the majority containing only one mRNA. The current next-generation sequencing technology usually only produces reads of length about 753 . Thus, it is impossible to construct a de Bruijn graph with k = 90 from single-end reads directly. Even if the reads were long enough, there would be a lot of gaps in the graph when constructed directly. In order to solve this problem, at the first step, T-IDBA applies the IDBA algorithm [17] to construct a de Bruijn graph with k = kmod < l, where l is the length of the input read. At the second step, T-IDBA aligns mate-pair reads (by exact match) to the graph for confident connection between nodes. Each connection of nodes is validated by finding a unique path in the graph connecting the pair of nodes with length matching the insert distance of the pair-end reads (within specified error). Note that, the connection between two nodes will be discarded if the number of paths between them is zero or more than one. Although this problem is NP-hard, since there are only a few loops in the graph when kmod = 50, the unique path can be found in practice. Table 3 shows the number of mRNAs with length-k repeats. As we can see, less than 5% (1,922 out of 46,104) of mRNAs contain loops (length-k repeats) in the graph. All the unique paths for validation are recorded for resolving branches and treated as extra long reads for IDBA to construct de Bruijn graph with kmax ≥ k ≥ l. Note that IDBA needs to be tuned specifically for transcriptome assembly. Tips removal in de Bruijn graph is performed using very short length to avoid removing too many k-mers, since transcripts are usually very short. In addition, a bigger threshold m is used for filtering those incorrect k-mers due to sequencing errors. Since this may filter out some low-coverage k-mers, it is unlikely that we can recover mRNAs with low converage (i.e. low expression level) and only those well-expressed mRNAs are considered. 2.2 Discovering Isoforms in Connected Components For each connected component in the de Bruijn graph with k = kmax, T-IDBA discovers those paths starting from a vertex with zero in-degree to a vertex with zero out-degree with the highest support from pair-end reads. A path is supported by a pair-end reads if the pair-end reads can be aligned (by exact match) to the path with the distance between the aligned positions matching the insert distance of the pair-end reads (with up to 10% error). T-IDBA performs depth-first search from a vertex with zero in-degree to a vertex with zero out-degree in decreasing order of support of the branches. In practice, instead of performing a complete depth-first search, T-IDBA reports at most 3 potential isoforms for each zero in-degree node in each connected component (note that 3 is a parameter set to be set by the user). 2.3 Expected Sensitivity of T-IDBA Given a length-n mRNA R with t length-w exons, i.e. n = tw. If the coverage of pair-end length-l reads on R is c and the error rate of each nucleotide in a read is e, we can evaluate in the following the probability that R can be recovered by T-IDBA as one contig. Thus, we can conclude that most mRNAs can be recovered by T-IDBA with high probability as long as the coverage of the mRNA exceeds a certain threshold. In order to recover R, all kmin-mers of R must exist in the de Bruijn graph when k = kmin, i.e. every kmin-mer must be sampled at least m times with no error. Since there are s = cn/(2l) pair of reads sampled from R and the probability that a particular kmin-mer contains in a pair of read is 2(l – kmin + 1)/(n – l + 1), the probability that a kmin-mer of R is sampled j times is j js n l l k n l l k j s −       − + − +  −      − + − +         1 (2 )1 1 1 (2 )1 min min However, some of the sampled kmin-mers may contain error. Since the probability that a kmin-mer contains error is min min 1 1( ) k k p = − − e , the probability that all n – kmin + 1 kmin-mers of R exist in the de Bruijn graph when k = kmod is ( ) 1 min min min min min 1 1 (2 )1 1 1 (2 )1 − + = − − =                 −               − + − +  −      − + − +         ∑ ∑ kn j mq q k qj k s j js mj p p q j n l l k n l l k j s (1) If R exists in the de Bruijn graph when k = kmod, R exists in a connected component if and only if for each pair of adjacent exons, there is no branch (with probability 1 – pb) or the branches can be resolved by pair-end reads. Similar to (1), the probability that α pair-end reads connecting two adjacent exons are sampled is j js n l l k n l l k j s −       − + − +  −      − + − +         1 ( )1 1 1 ( )1 mod mod And the probability that two kmod-mers, one from each one of pair-end reads, are sampled correctly is 1 2 1( ) mod mod − + = − kl c k p p , the probability that R exists in a connected component is 1 min min 1( ) 1 ( )1 1 1 ( )1 1( ) − = = − −                 −               − + − +  −      − + − +         − + ∑ ∑ t s j j q qj c q c j js b b p p q j n l l k n l l k j s p p α α (2) By multiplying the two probabilities given in (1) and (2), we can calculate the sensitivity of finding an mRNA with t length-m exons and coverage c. For example, given a length-2500 mRNA with 5 length-500 exons and 30X coverage, if the error rate is 1% and the threshold m = 4, α = 5 the mRNA can be discovered with probability 0.69.",Assembly,"tidba   novo iterative  bruijn graph assembler  transcriptome
different  genome assembly whose input read  sample   species genome  input read  mammalian transcriptome assembly  sample   express mrnas   mammal   total length  genes  much shorter   genome  first glance mammalian transcriptome assembly problem seem easier   genome assembly problem however   alternative splice  long pattern represent exons may occur  multiple mrnas isoforms    gene thus   bruijn graph   graph   vertex represent  kmer   edge         adjacently occur   read   branch  construct   mammalian transcriptome assembly problem     genome assembly problem therefore traditional  novo genome assemblers  would  work well   mammalian transcriptome assembly problem   would usually stop  branch result   short contigs  represent  part   exons instead   whole isoform besides many  novo genomic assemblers   remove  end   contig  increase  accuracy   mrnas  usually relatively short  ~5000nt  lengths  contigs decrease significantly  order  reconstruct  isoforms  different genes  develop tidba  first divide   bruijn graph  many connect components    represent isoforms   single gene  tidba determine  isoform   component use pairend information  construct connect components  observe  although  repeat pattern   whole genome    long  number  genes  contain   long repeat pattern  actually quite  table  show  number  genes  mouse2  repeat pattern  length  least   particular     genes    genes contain repeat pattern  length  least 90bp    construct   brujin graph  large   connect components  contain isoforms  single genes   also build  bruijn graph   reference mrna   mouse ucsc mm9 ncbi build   different value   figure  show  number  connect components  number  connect components increase   increase     connect components    bruijn graph         mrnas   mouse database  component contain  average  mrnas table  show  distribution   number  mrnas  components    mrnas   components contain     mrnas   majority contain  one mrna  current nextgeneration sequence technology usually  produce read  length    thus   impossible  construct   bruijn graph      singleend read directly even   read  long enough  would   lot  gap   graph  construct directly  order  solve  problem   first step tidba apply  idba algorithm   construct   bruijn graph    kmod       length   input read   second step tidba align matepair read  exact match   graph  confident connection  nod  connection  nod  validate  find  unique path   graph connect  pair  nod  length match  insert distance   pairend read within specify error note   connection  two nod   discard   number  paths    zero    one although  problem  nphard since      loop   graph  kmod    unique path   find  practice table  show  number  mrnas  lengthk repeat    see less        mrnas contain loop lengthk repeat   graph   unique paths  validation  record  resolve branch  treat  extra long read  idba  construct  bruijn graph  kmax ≥  ≥  note  idba need   tune specifically  transcriptome assembly tip removal   bruijn graph  perform use  short length  avoid remove  many kmers since transcripts  usually  short  addition  bigger threshold   use  filter  incorrect kmers due  sequence errors since  may filter   lowcoverage kmers   unlikely    recover mrnas  low converage  low expression level    wellexpressed mrnas  consider  discover isoforms  connect components   connect component    bruijn graph    kmax tidba discover  paths start   vertex  zero indegree   vertex  zero outdegree   highest support  pairend read  path  support   pairend read   pairend read   align  exact match   path   distance   align position match  insert distance   pairend read     error tidba perform depthfirst search   vertex  zero indegree   vertex  zero outdegree  decrease order  support   branch  practice instead  perform  complete depthfirst search tidba report    potential isoforms   zero indegree node   connect component note     parameter set   set   user  expect sensitivity  tidba give  lengthn mrna    lengthw exons       coverage  pairend lengthl read       error rate   nucleotide   read     evaluate   follow  probability     recover  tidba  one contig thus   conclude   mrnas   recover  tidba  high probability  long   coverage   mrna exceed  certain threshold  order  recover   kminmers   must exist    bruijn graph    kmin  every kminmer must  sample  least  time   error since     cn2l pair  read sample     probability   particular kminmer contain   pair  read    kmin        probability   kminmer    sample  time                                                   min min however    sample kminmers may contain error since  probability   kminmer contain error  min min             probability     kmin   kminmers   exist    bruijn graph    kmod     min min min min min                                                                     ∑ ∑                             exist    bruijn graph    kmod  exist   connect component       pair  adjacent exons    branch  probability      branch   resolve  pairend read similar    probability   pairend read connect two adjacent exons  sample                                                   mod mod   probability  two kmodmers one   one  pairend read  sample correctly      mod mod            probability   exist   connect component   min min                                                                         ∑ ∑                                  multiply  two probabilities give       calculate  sensitivity  find  mrna   lengthm exons  coverage   example give  length mrna   length exons   coverage   error rate     threshold        mrna   discover  probability ",5
121,Pasha,"Parallelized short read assembly of large genomes using de Bruijn graphs
Even though de Bruijn graph-based assemblers successfully alleviate the pressure on memory space and execution speed by substituting reads with k-mers (a contiguous sequence of k bases) as nodes, compared to the conventional overlap-layout-consensus approaches, the memory consumption and execution time is still prohibitive for large genomes. For example, for the genomic data of a Yoruban male individual, the total number of nodes in the preliminary de Bruijn graph (a node corresponds to a distinct 27-mer) is about 7.73 billion. This motivates us to design a scalable assembler for large genomes that is workable on modest and commonly used high-performance computing resources. PASHA is a parallelized algorithm for large genome assembly, which overcomes the memory and execution speed constraints by using hybrid computing architectures consisting of shared-memory multi-core CPUs and distributed-memory compute clusters. Figure 1 illustrates the pipeline of our assembler. The pipeline comprises four stages: (i) generating and distributing k-mers, (ii) constructing and simplifying the distributed preliminary de Bruijn graph, (iii) merging bubbles and generating contigs after constructing a Velvet-like de Bruijn graph, and (iv) scaffolding to join contigs into scaffolds. We have implemented Stages (i) and (ii), which are suitable for parallelization and the most memory-intensive, using MPI. This makes our program compatible with both shared-memory and distributed-memory computing systems. Each MPI process P i comprises two threads T0 and T1. T0 performs computations for the assembly pipeline, and T1 performs communications between different processes (see (i) and (ii) in Figure 1), as well as file I/O operations. By employing two threads in a single process, we intend to gain faster speed by overlapping the local computation and remote communications with processes (and file I/O). By distributing the de Bruijn graph over a network of computers, we get a partition of the graph with each part stored in a different computer. Hence, we do not need a large amount of memory in a single computer, making our algorithm workable even on a compute cluster comprised of commonplace workstations or personal computers. Since the size of a message is very small, sending messages one-by-one to remote processes will incur large communication overheads. Thus, for the messages that are not time-critical, we combine them into packets to improve the communication efficiency. T0 and T1 are connected through a bi-directional message queue. The maximal number of messages in the queue is controlled by a maximal capability threshold. Any thread, which tries to append a new message to the queue, will be blocked if the queue reaches the threshold, and will be resumed once the queue has spaces available. Any thread that tries to retrieve a message from an empty queue will be blocked until there is at least one message available. For Stages (iii) and (iv), which exhibit limited parallelism and are less memory-intensive, we use a multi-threaded design, only compatible with shared-memory systems. Figure 1 figure1 Schematic diagram of the PASHA assembly pipeline. (i) k-mer generation and distribution over a number of MPI processes; (ii) distributed preliminary de Bruijn graph construction and simplification over a number of MPI processes; (iii) Bubble merging and contig generation; and (iv) scaffolding. Full size image In our proof-of-concept implementation (the source code is available for download at http://sites.google.com/site/yongchaosoftware/pasha), we have used parts of the source code from Velvet for Stages (iii) and (iv) with some algorithmic and data structure modifications. The use of existing open-source code significantly reduces the development time for prototyping our algorithm, and more importantly, our modifications make the two stages feasible and practical to execute on a workstation with limited system memory (i.e. 72 GB RAM in our workstation), as well as providing better assembly qualities. PASHA supports the standard FASTA and FASTQ input formats for single-end and paired-end short reads with different insert sizes. While some other assemblers require users to tune a number of parameters to gain the best assembly, PASHA only needs a single parameter ""-k"" (i.e. the k-mer size), making it relatively user friendly. Before describing PASHA in details, we firstly define some terms to facilitate our discussion. Given a sequence S of length l, we define S[i] as the i-th base in the sequence, 𝑆[𝑖]⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯ as the complement of S[i], S i as the k-mer starting at position i (1≤i≤l-k+1) of S, 𝑆𝑖⎯⎯⎯⎯⎯⎯ as the reverse complement of S i , and 𝑆𝑐𝑖 as the canonical k-mer that is the lexicographically smaller of S i and 𝑆𝑖⎯⎯⎯⎯⎯⎯. A k-molecule of S i is a pair of complementary k-mer strands consisting of the canonical k-mer of S i (i.e. 𝑆𝑐𝑖) and the non-canonical k-mer of S i (i.e. the reverse complement of 𝑆𝑐𝑖). K-mer generation and distribution In a de Bruijn graph, a node corresponds to a k-mer and an edge between two nodes is created if and only if their corresponding k-mers have a suffix-prefix overlap of k-1 bases. Hence, PASHA starts the construction of its preliminary de Bruijn graph from the generation of all k-mers from the input read data. As mentioned above, PASHA employs an MPI-based approach to k-mer generation and distributes them among the processes. This distribution requires that the location of any k-mer is deterministic and can be efficiently computed from the k-mer itself. Since a k-molecule is a pair of complementary strands, the location of a k-mer and its reverse complement must be the same. Before calculating the location of S i and 𝑆𝑖⎯⎯⎯⎯⎯⎯, PASHA first transforms S i and 𝑆𝑖⎯⎯⎯⎯⎯⎯ to their corresponding base-4 representation by assigning numerical value {0, 1, 2, 3} to bases {A, C, G, T}. To determine the location of S i , unlike ABySS (which calculates the hash values of S i and 𝑆𝑖⎯⎯⎯⎯⎯⎯, and then performs an XOR operation on them), PASHA computes the location from the canonical k-mer. Since the base-4 representation of a k-mer is stored in a 64-bit integer (thus limiting the maximum allowable k-mer size to 31), the comparison can be theoretically completed in one clock cycle on a 64-bit computing system. A balanced distribution of k-mers among processes is critical to the performance of our algorithm in terms of both execution time and memory space. An unbalanced distribution would cause some processes to consume much more memory for k-mer storage, thus resulting in a system failure due to memory limitations on some compute nodes. In PASHA, we first calculate a hash value I k using a linear congruential hash function from the base-4 presentation of the canonical k-mer. Then, the ID of the process that owns this k-mer is computed as I k % N p , where N p is the number of processes. From our experiments, our location determination method is able to (roughly) balance the distribution of k-mers (e.g., for the Yoruban Male genome assembly using 16 processes, the average number of local 27-mers for each process is 483,194,335 ± 963,003). To achieve memory efficiency, we use the sparse_hash_set template class in the Google Sparse Hash library (http://code.google.com/p/google-sparsehash) to distinguish and store k-mers. In PASHA, each process holds a local sparse hash-set to store its k-mers. For each process, thread T1 loads the reads from disk and transfers the reads to thread T0 through the message queue, where the short reads are arranged into batches and a message contains a batch of reads. T0 receives batches of reads from the queue, calculates the hash values of all k-mers, and stores some of them in its local sparse hash-set depending on the hash values. The cooperation of the two threads overlaps the computation and the file I/O operations, thus reducing the execution time. Since a k-mer and its reverse complement are considered equivalent in a k-molecule, we only need to store the canonical k-mer into the sparse hash-set to represent the k-molecule. For any read containing non-A/C/G/T bases, PASHA converts those non-A/C/G/T bases to the base ""A"" (as Velvet does), not simply discarding the whole read as ABySS does [9]. After completing the generation of k-mers, each process writes its local k-mers to disk for future use when constructing the preliminary de Bruijn graph. Our distributed k-mer generation implementation can also be used (directly or after modification) by other tools, such as CUDA_EC [15] and Quake [16] to generate and count the occurrences of k-mers in genomic data. Distributed de Bruijn graph construction The preceding stage only generates k-mers, and does not record any graph-related information for a k-mer. However, to construct a de Bruijn graph, we need not only the k-mers themselves, but also multiplicity and linkage information. A common approach is to use a hash-map implementation, using k-mers as keys, to provide fast access to the graph-related information. However, this approach will result in a large memory overhead. Conway and Bromage [17] suggested a sparse bitmap data structure to represent the de Bruijn graph, achieving memory efficiency at the cost of execution time. However, their proof-of-concept assembler (functionally similar to our Stages (i) and (ii) ) takes about 50 hours and yields a highly fragmented assembly, with an N50 contig size of only 250, for the Yoruban male genome. Hence, we exclude it from the following assessments. In PASHA, we instead use a sorted vector data structure to store the k-mers and their graph-related information. Each process loads its local k-mers from disk and stores them in a sorted vector. The sorted vector is sorted using the k-mers as keys. We use the same approach as ABySS to represent the linkage information between nodes; i.e., the linkages are compacted into 8 bits with each bit representing the presence or absence of each of the eight edges in the two directions. However, to build the linkages between nodes, PASHA employs a different approach. For each k-mer (each node), ABySS checks the existence of all possible neighbours by doing all possible base extensions in each direction. If a neighbour exists, it sets its corresponding bit to represent the existence of the linkage. This approach is effective but has a probability of introducing spurious edges, which connect k-mers that are not adjacent in any read. Hence, PASHA builds linkages directly from the adjacency information of k-mers in the input reads, i.e., a linkage between two k-mers is created if and only if the two k-mers are adjacent in at least one read. While building linkages, for each process, thread T1 loads batches of reads from disk and transfers them to T0 as the previous stage does. For each read S, T0 iterates over each k-mer S i and identifies its location after calculating the base-4 representation of its canonical k-mer. If the k-mer belongs to it, T0 sets the corresponding linkage bits calculated from the bases S[i-1] and S[i+k], which are the extension bases in the left and the right directions of S i . When the index is out of the range, the corresponding extension base (and its complement) is an invalid base Ø, indicating that no linkage is created in that direction. Figure 2 shows all four cases of the linkage construction between two adjacent k-mers in a read. Because each process iterates all k-mers in all input reads, no communication between processes is required during the construction process. While constructing the linkages, we compute the multiplicity of each k-mer at the same time. In PASHA, two bytes are used to represent the multiplicity of a k-mer. After completing the linkage construction, we will get a distributed preliminary de Bruijn graph with each node corresponding to a k-mer. Figure 2 figure2 Linkage construction for two adjacent k-mers in a read. Full size image Graph simplification The preliminary de Bruijn graph contains many linear chains of nodes that can be merged to simplify the graph without loss of information. We start the simplification from the removal of spurious linkages. Generally, there are three major kinds of spurious linkages: tips, low-coverage paths and bubbles. A tip is a short and low-coverage dead end, which is likely to be caused by sequence errors at the beginning or the end of reads. A low-coverage path only covers one or a few reads and is likely to be a chimeric connection. Bubbles are redundant paths with minor differences, which might be due to heterozygosity, internal read errors or nearby tips connecting. At this stage, we only remove tips and low-coverage paths, leaving the removal of bubbles to the following stage. Prior to the removal of longer tips, we first remove low-frequency dead-end individual k-mers. This removal relies on the assumption that the majority of true k-mers should occur in several reads, i.e. the multiplicity of a true k-mer is supposed to be above a minimum multiplicity threshold M, which is automatically estimated from the multiplicities of all k-mers. This removal work is conducted round-by-round until no dead-end k-mers meet the removal conditions. For each process, T0 identifies the k-mers, which are dead-end and have a multiplicity less than M, in its local collection of k-mers, and then removes the k-mers and their linkages in the graph. When removing linkages to k-mers in another process, T0 packs a request message and lets T1 forward to the remote process. T1 forwards the request from T0 to other processes, and handles the requests on the removal of the specific linkages to its local k-mers, from the other processes. In PASHA, we simply remove tips that are shorter than 2k. For each process, T0 generates the current linear chain of k-mers, starting from a dead-end k-mer, by extending the chain in the left or right directions base-by-base. If the chain is longer than 2k, the chain is released; and otherwise, the k-mers in the chain will be removed as well as their linkages. If, during the extension of a chain, a linked k-mer exists locally, T0 gets the graph-related information of the k-mer directly from its local sorted vector. Otherwise, T0 packs a request message and lets T1 forward it to the remote process. T0 will be blocked until receiving the response, forwarded back by T1, from the remote process. After completing the removal of tips, the graph is split into different linear chains of nodes. All processes cooperate in parallel to generate the sequences corresponding to the linear chains. The linear chains are generated using two steps. The first step generates linear chains starting from dead-end k-mers, where a chain is extended in only one direction. The second step starts from an arbitrary k-mer, where a chain must be extended in two directions. In the first step each process P i extends a linear chain from each active local dead-end k-mer until another dead-end k-mer is met. In this case, P i checks the location process P j of the dead-end k-mer to avoid duplicates because P j (if i ≠ j) might be generating this linear chain at the same time. In our algorithm, P i keeps this linear chain only if P i ≤ P j , and releases it, otherwise. This process will be conducted iteratively until there are no local dead-end k-mers in each process. The second step is completed by assigning processes one-by-one to generate linear chains. At any time, only one process P i is allowed to generate linear chains and the other processes have to wait and process requests from P i . P i starts the two-directional extension from each local k-mer until a loop or a dead-end k-mer is found. In this case, the loop is simply broken up and output as a linear chain. For each sequence, the coverage is calculated by dividing the sum of multiplicities of the k-mers in its corresponding chain by the sequence length. If the coverage is lower than the minimum coverage threshold, the sequence should be given up since it is likely to be generated from linear chains containing spurious connections. The remaining sequences are written to disk for the use in the next stage. Bubble merging and contig generation This stage consists of three steps. First, a Velvet-like de Bruijn graph is constructed from the sequences produced from the previous stage. To build the Velvet-like graph, we form a node, as well as its twin node, from a sequence and create linkages between nodes by aligning reads to nodes. If two adjacent k-mers in a read belong to two different nodes, we create an edge connecting them if there is no edge between them, and otherwise, update the information of the existing edge. While aligning reads to nodes, we do not record any mapping information about the reads. We employ a multi-threaded implementation to accelerate the alignment of reads to nodes on multi-core CPUs, where a single read is aligned to the graph nodes by a thread and locks are carefully employed to guarantee the mutual exclusive access to critical sections (e.g. the creation and updating of links between nodes). Secondly, we detect and merge bubbles in the graph. The ""Tour-bus"" method in Velvet is employed to detect bubbles. The detected bubbles are merged into a single path if the sequences of the parallel paths meet the user-specified similarity requirement. In PASHA, we directly use the source code of the ""Tour-bus"" method, but modified the conditions to merge the two paths. Two paths are merged if they have at most a two-base-pair difference in length with ≥ 90% identity. Finally, we simplify the graph after further removal of short tips and low-coverage nodes, and then generate contigs from nodes. Scaffolding The scaffolding work aims to find the correct ordering of the assembled contigs, and then joins them into scaffolds. The determination of the ordering of contigs relies on the mapping information of paired-end reads onto the contigs, and then the mapping information is transferred to scaffolding linkages between contigs. In PASHA, the scaffolding work starts from the construction of a Velvet-like de Bruijn graph from the assembled contigs. While aligning paired-end reads to the graph nodes (we extend the k-mer based alignment algorithm in Velvet to provide support for multi-threading), the mapping information of a read pair, such as mapping locations and node identifiers, is recorded into an in-disk database if the two reads successfully map onto the graph. A similar multi-threaded design, as in the previous stage, is employed to accelerate the graph construction on multi-core CPUs. Having completed the read mapping, the median insert size, as well as its standard deviation, for each library is estimated from the mapping information of paired-end reads whose two reads map onto the same nodes. We employ a modified Pebble algorithm [13] to do the scaffolding work. The scaffolding linkages are built from the mapping information of paired-end reads in the in-disk database. Velvet constructs linkages from the mapping information of reads and read pairs. For a single read, if it overlaps with more than one node, linkages will be created between these nodes. For a read pair, both of which have overlaps with nodes, a linkage is created between two nodes that respectively have overlaps with the two reads. In PASHA, we only use the mapping information of read pairs to construct linkages, where a linkage is considered reliable if we have at least three read pairs to form the linkage. Speed optimizations In the representation of the preliminary de Bruijn graph, PASHA employs a sorted vector data structure, instead of a hash-map, to store k-mers and their graph-related information. While reducing memory overhead, a sorted vector causes an increase in the average search time of k-mers. Given N k-mers stored in a sorted vector, the average search time is about logN, generally longer than the (nearly) constant search time of a hash-map. In this case, we build an acceleration table using the most significant r bits (r = 24 by default) of a k-mer to speed up the search. This acceleration table only results in a memory increase of 2rtimes the size of type integer bytes, but is expected to reduce the average search time to log(N/2r). For Stages (i) and (ii), each MPI process has two threads T0 and T1: one for communication and the other for local computation. This mechanism is expected to improve the execution speed by overlapping communication and computation. However, when the two threads communicate frequently, along with memory allocations and de-allocations at the same time, the overhead incurred by system calls on memory operations may offset the performance obtained from the overlapping. Thus, we use the tbb_allocator template class in the Intel Threading Building Blocks library to manage the memory allocation and de-allocation for the communication between T0 and T1. The tbb_allocator template class does improve the execution speed through its smart management of user memory allocation and de-allocation.",Assembly,"parallelize short read assembly  large genomes use  bruijn graphs
even though  bruijn graphbased assemblers successfully alleviate  pressure  memory space  execution speed  substitute read  kmers  contiguous sequence   base  nod compare   conventional overlaplayoutconsensus approach  memory consumption  execution time  still prohibitive  large genomes  example   genomic data   yoruban male individual  total number  nod   preliminary  bruijn graph  node correspond   distinct mer    billion  motivate   design  scalable assembler  large genomes   workable  modest  commonly use highperformance compute resources pasha   parallelize algorithm  large genome assembly  overcome  memory  execution speed constraints  use hybrid compute architectures consist  sharedmemory multicore cpus  distributedmemory compute cluster figure  illustrate  pipeline   assembler  pipeline comprise four stag  generate  distribute kmers  construct  simplify  distribute preliminary  bruijn graph iii merge bubble  generate contigs  construct  velvetlike  bruijn graph   scaffold  join contigs  scaffold   implement stag      suitable  parallelization    memoryintensive use mpi  make  program compatible   sharedmemory  distributedmemory compute systems  mpi process   comprise two thread     perform computations   assembly pipeline   perform communications  different process see     figure   well  file  operations  employ two thread   single process  intend  gain faster speed  overlap  local computation  remote communications  process  file   distribute   bruijn graph   network  computers  get  partition   graph   part store   different computer hence    need  large amount  memory   single computer make  algorithm workable even   compute cluster comprise  commonplace workstations  personal computers since  size   message   small send message onebyone  remote process  incur large communication overheads thus   message    timecritical  combine   packets  improve  communication efficiency     connect   bidirectional message queue  maximal number  message   queue  control   maximal capability threshold  thread  try  append  new message   queue   block   queue reach  threshold    resume   queue  space available  thread  try  retrieve  message   empty queue   block     least one message available  stag iii    exhibit limit parallelism   less memoryintensive  use  multithreaded design  compatible  sharedmemory systems figure  figure1 schematic diagram   pasha assembly pipeline  kmer generation  distribution   number  mpi process  distribute preliminary  bruijn graph construction  simplification   number  mpi process iii bubble merge  contig generation   scaffold full size image   proofofconcept implementation  source code  available  download     use part   source code  velvet  stag iii     algorithmic  data structure modifications  use  exist opensource code significantly reduce  development time  prototyping  algorithm   importantly  modifications make  two stag feasible  practical  execute   workstation  limit system memory    ram   workstation  well  provide better assembly qualities pasha support  standard fasta  fastq input format  singleend  pairedend short read  different insert size    assemblers require users  tune  number  parameters  gain  best assembly pasha  need  single parameter """"   kmer size make  relatively user friendly  describe pasha  detail  firstly define  term  facilitate  discussion give  sequence   length   define    ith base   sequence ⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯   complement       kmer start  position  ≤≤   ⎯⎯⎯⎯⎯⎯   reverse complement      𝑆𝑐𝑖   canonical kmer    lexicographically smaller     ⎯⎯⎯⎯⎯⎯  kmolecule      pair  complementary kmer strand consist   canonical kmer     𝑆𝑐𝑖   noncanonical kmer      reverse complement  𝑆𝑐𝑖 kmer generation  distribution    bruijn graph  node correspond   kmer   edge  two nod  create      correspond kmers   suffixprefix overlap   base hence pasha start  construction   preliminary  bruijn graph   generation   kmers   input read data  mention  pasha employ  mpibased approach  kmer generation  distribute  among  process  distribution require   location   kmer  deterministic    efficiently compute   kmer  since  kmolecule   pair  complementary strand  location   kmer   reverse complement must     calculate  location     ⎯⎯⎯⎯⎯⎯ pasha first transform    ⎯⎯⎯⎯⎯⎯   correspond base representation  assign numerical value {   }  base {   }  determine  location     unlike aby  calculate  hash value     ⎯⎯⎯⎯⎯⎯   perform  xor operation   pasha compute  location   canonical kmer since  base representation   kmer  store   bite integer thus limit  maximum allowable kmer size    comparison   theoretically complete  one clock cycle   bite compute system  balance distribution  kmers among process  critical   performance   algorithm  term   execution time  memory space  unbalance distribution would cause  process  consume much  memory  kmer storage thus result   system failure due  memory limitations   compute nod  pasha  first calculate  hash value   use  linear congruential hash function   base presentation   canonical kmer      process    kmer  compute             number  process   experiment  location determination method  able  roughly balance  distribution  kmers    yoruban male genome assembly use  process  average number  local mers   process   ±   achieve memory efficiency  use  sparse_hash_set template class   google sparse hash library   distinguish  store kmers  pasha  process hold  local sparse hashset  store  kmers   process thread  load  read  disk  transfer  read  thread    message queue   short read  arrange  batch   message contain  batch  read  receive batch  read   queue calculate  hash value   kmers  store      local sparse hashset depend   hash value  cooperation   two thread overlap  computation   file  operations thus reduce  execution time since  kmer   reverse complement  consider equivalent   kmolecule   need  store  canonical kmer   sparse hashset  represent  kmolecule   read contain nonacgt base pasha convert  nonacgt base   base """"  velvet   simply discard  whole read  aby    complete  generation  kmers  process write  local kmers  disk  future use  construct  preliminary  bruijn graph  distribute kmer generation implementation  also  use directly   modification   tool   cuda_ec   quake   generate  count  occurrences  kmers  genomic data distribute  bruijn graph construction  precede stage  generate kmers    record  graphrelated information   kmer however  construct   bruijn graph  need    kmers   also multiplicity  linkage information  common approach   use  hashmap implementation use kmers  key  provide fast access   graphrelated information however  approach  result   large memory overhead conway  bromage  suggest  sparse bitmap data structure  represent   bruijn graph achieve memory efficiency   cost  execution time however  proofofconcept assembler functionally similar   stag     take   hours  yield  highly fragment assembly   n50 contig size      yoruban male genome hence  exclude    follow assessments  pasha  instead use  sort vector data structure  store  kmers   graphrelated information  process load  local kmers  disk  store    sort vector  sort vector  sort use  kmers  key  use   approach  aby  represent  linkage information  nod   linkages  compact   bits   bite represent  presence  absence     eight edge   two directions however  build  linkages  nod pasha employ  different approach   kmer  node aby check  existence   possible neighbour    possible base extensions   direction   neighbour exist  set  correspond bite  represent  existence   linkage  approach  effective    probability  introduce spurious edge  connect kmers    adjacent   read hence pasha build linkages directly   adjacency information  kmers   input read   linkage  two kmers  create      two kmers  adjacent   least one read  build linkages   process thread  load batch  read  disk  transfer      previous stage    read   iterate   kmer    identify  location  calculate  base representation   canonical kmer   kmer belong    set  correspond linkage bits calculate   base   sik    extension base   leave   right directions       index     range  correspond extension base   complement   invalid base  indicate   linkage  create   direction figure  show  four case   linkage construction  two adjacent kmers   read   process iterate  kmers   input read  communication  process  require   construction process  construct  linkages  compute  multiplicity   kmer    time  pasha two bytes  use  represent  multiplicity   kmer  complete  linkage construction   get  distribute preliminary  bruijn graph   node correspond   kmer figure  figure2 linkage construction  two adjacent kmers   read full size image graph simplification  preliminary  bruijn graph contain many linear chain  nod    merge  simplify  graph without loss  information  start  simplification   removal  spurious linkages generally   three major kinds  spurious linkages tip lowcoverage paths  bubble  tip   short  lowcoverage dead end   likely   cause  sequence errors   begin   end  read  lowcoverage path  cover one    read   likely    chimeric connection bubble  redundant paths  minor differences  might  due  heterozygosity internal read errors  nearby tip connect   stage   remove tip  lowcoverage paths leave  removal  bubble   follow stage prior   removal  longer tip  first remove lowfrequency deadend individual kmers  removal rely   assumption   majority  true kmers  occur  several read   multiplicity   true kmer  suppose     minimum multiplicity threshold    automatically estimate   multiplicities   kmers  removal work  conduct roundbyround   deadend kmers meet  removal condition   process  identify  kmers   deadend    multiplicity less     local collection  kmers   remove  kmers   linkages   graph  remove linkages  kmers  another process  pack  request message  let  forward   remote process  forward  request     process  handle  request   removal   specific linkages   local kmers    process  pasha  simply remove tip   shorter     process  generate  current linear chain  kmers start   deadend kmer  extend  chain   leave  right directions basebybase   chain  longer    chain  release  otherwise  kmers   chain   remove  well   linkages    extension   chain  link kmer exist locally  get  graphrelated information   kmer directly   local sort vector otherwise  pack  request message  let  forward    remote process    block  receive  response forward back     remote process  complete  removal  tip  graph  split  different linear chain  nod  process cooperate  parallel  generate  sequence correspond   linear chain  linear chain  generate use two step  first step generate linear chain start  deadend kmers   chain  extend   one direction  second step start   arbitrary kmer   chain must  extend  two directions   first step  process   extend  linear chain   active local deadend kmer  another deadend kmer  meet   case   check  location process     deadend kmer  avoid duplicate      ≠  might  generate  linear chain    time   algorithm   keep  linear chain     ≤     release  otherwise  process   conduct iteratively     local deadend kmers   process  second step  complete  assign process onebyone  generate linear chain   time  one process    allow  generate linear chain    process   wait  process request       start  twodirectional extension   local kmer   loop   deadend kmer  find   case  loop  simply break   output   linear chain   sequence  coverage  calculate  divide  sum  multiplicities   kmers   correspond chain   sequence length   coverage  lower   minimum coverage threshold  sequence   give  since   likely   generate  linear chain contain spurious connections  remain sequence  write  disk   use   next stage bubble merge  contig generation  stage consist  three step first  velvetlike  bruijn graph  construct   sequence produce   previous stage  build  velvetlike graph  form  node  well   twin node   sequence  create linkages  nod  align read  nod  two adjacent kmers   read belong  two different nod  create  edge connect      edge    otherwise update  information   exist edge  align read  nod    record  map information   read  employ  multithreaded implementation  accelerate  alignment  read  nod  multicore cpus   single read  align   graph nod   thread  lock  carefully employ  guarantee  mutual exclusive access  critical section   creation  update  link  nod secondly  detect  merge bubble   graph  ""tourbus"" method  velvet  employ  detect bubble  detect bubble  merge   single path   sequence   parallel paths meet  userspecified similarity requirement  pasha  directly use  source code   ""tourbus"" method  modify  condition  merge  two paths two paths  merge       twobasepair difference  length  ≥  identity finally  simplify  graph   removal  short tip  lowcoverage nod   generate contigs  nod scaffold  scaffold work aim  find  correct order   assemble contigs   join   scaffold  determination   order  contigs rely   map information  pairedend read onto  contigs    map information  transfer  scaffold linkages  contigs  pasha  scaffold work start   construction   velvetlike  bruijn graph   assemble contigs  align pairedend read   graph nod  extend  kmer base alignment algorithm  velvet  provide support  multithreading  map information   read pair   map locations  node identifiers  record   indisk database   two read successfully map onto  graph  similar multithreaded design    previous stage  employ  accelerate  graph construction  multicore cpus  complete  read map  median insert size  well   standard deviation   library  estimate   map information  pairedend read whose two read map onto   nod  employ  modify pebble algorithm     scaffold work  scaffold linkages  build   map information  pairedend read   indisk database velvet construct linkages   map information  read  read pair   single read   overlap    one node linkages   create   nod   read pair     overlap  nod  linkage  create  two nod  respectively  overlap   two read  pasha   use  map information  read pair  construct linkages   linkage  consider reliable     least three read pair  form  linkage speed optimizations   representation   preliminary  bruijn graph pasha employ  sort vector data structure instead   hashmap  store kmers   graphrelated information  reduce memory overhead  sort vector cause  increase   average search time  kmers give  kmers store   sort vector  average search time   logn generally longer   nearly constant search time   hashmap   case  build  acceleration table use   significant  bits     default   kmer  speed   search  acceleration table  result   memory increase  2rtimes  size  type integer bytes   expect  reduce  average search time  logn2r  stag     mpi process  two thread    one  communication     local computation  mechanism  expect  improve  execution speed  overlap communication  computation however   two thread communicate frequently along  memory allocations  deallocations    time  overhead incur  system call  memory operations may offset  performance obtain   overlap thus  use  tbb_allocator template class   intel thread build block library  manage  memory allocation  deallocation   communication      tbb_allocator template class  improve  execution speed   smart management  user memory allocation  deallocation",5
122,Ray,"Ray: Simultaneous Assembly of Reads from a Mix of High-Throughput Sequencing Technologies
Ray is derived from the work by Pevzner et al. (2001) in the sense that we make use of a de Bruijn graph. However, our framework does not rely on Eulerian walks. To generate an assembly, we define specific subsequences, called seeds, and for each of them, the algorithm extends it into a contig. We define heuristics that control the extension process in such a way that the process is stopped if, at some point, the family of reads does not clearly indicate the direction of the extension. In example, consider the graph of Figure 1, and suppose that the seed is z2 … z3, then if most of the reads that overlap the seed also contain z5, then we will proceed by adding z5 to the contig. On the contrary, we will choose an another direction, and in the case where there is no obvious winner, the process will stop. Note that our heuristics will give a higher importance to reads that heavily overlap a contig than to reads that only intersect a small number of the last vertices of it. Indeed, we consider that reads that overlap strongly in the contig we are constructing are more informative to assign the direction in which the extension should proceed. An external file that holds a picture, illustration, etc. Object name is fig-1.jpg FIG. 1. A subgraph of a de Bruijn graph. This figure shows a part of a de Bruijn graph. In this example, short reads are not enough for the assembly problem. Suppose that the true genome sequence is of the form equation M5. If the length of the reads (or paired reads) is smaller than the equation M6 subsequence, no hints will help an assembly algorithm to differentiate the true sequence from the following one equation M7. On the other hand, if there is a read that starts before z1 and ends after z2, there will be a possibility to solve this branching problem. We measure the overlapping level with functions called offseti and equation M8. The proposed heuristics will therefore limit the length of the obtained contigs, but will give better guarantees against assembly errors. Moreover, those heuristics allow us to construct greedy algorithms in the sense that at each point, we extend the contig that we are constructing, or we end the construction. Such a greedy choice that will never be reconsidered is inevitable if one wants to have an algorithm that runs in polynomial time. The choice in favor of greedy optimization is motivated by the NP-hard nature of the problem (Pop, 2009). To minimize running time and memory utilization, we use the sequence data provided by the reads through a de Bruijn graph annotation, each read being annotated only once and attached to a single vertex of the graph. 6.1. Coverage distribution Given a family of reads equation M9, we denote by D+ the family consisting of the reads of D and the reverse-complement sequences of D, or more precisely, equation M10. Moreover, for any integer c, a k-mer is c-confident if it occurs at least c times in the reads of D+. Note that a k-mer that appears twice in the same read is counted twice. We define equation M11 as the function that associates to each integer value c the number of k-mers that are covered c times according to D+. As proposed by Chaisson and Pevzner (2008), this function is the sum of an exponentially-decreasing curve and a Poisson distribution. The first represents the errors contained in the reads, and the second is a Poisson distribution because each read is a subsequence picked among all subsequences around a particular length with some almost uniform probability. The graph of the function equation M12 has the shape of the function drawn in Figure 2—the coverage distributions for the A. baylyi ADP1 dataset are shown in this figure, with Roche/454 reads, Illumina reads, and a mix of those. equation M13 has a local minimum, that we will call cmin, followed by a local maximum, that we will denote cpeak. Basically, cpeak is an estimate of the average coverage depth of the genome, and cmin is an estimation of the value where, with high probability, the amount of incorrect data is lower than the amount of correct data. An external file that holds a picture, illustration, etc. Object name is fig-2.jpg FIG. 2. Coverage distributions. This figure shows the coverage distributions of k-mers for the A. baylyi ADP1 dataset with Roche/454, Illumina, and Roche/454 and Illumina, k = 21. The minimum coverage and the peak coverage are identified for the Roche/454, Illumina, and Roche/454 and Illumina coverage distributions. The peak coverage of Roche/454+Illumina is greater than the sum of the peak coverage of Roche/454 and the peak coverage of Illumina, which suggests that the mixed approach allows one to recover low-coverage regions. 6.2. Annotated de Bruijn graph Given a family of reads D, a de Bruijn parameter k, and a coverage cutoff c, the associated annotated de Bruijn graph, noted Bruijn(D+, k, c) is defined as follows. The vertices V of the graph are the c-confident k-mers and the set of arcs A is composed of all arcs between two vertices of V that correspond to a 1-confident (k + 1)-mer. For example, for k = 5 and c = 2, the two k-mers ATGCT and TGCTG will be vertices of the graph if and only if they both appear at least twice in the reads of D+. Moreover, 〈ATGCT,TGCTG〉 will be an arc of the graph if and only if ATGCTG appears at least once in the reads of D+. For the annotation, we consider the first c-confident k-mer of each read r; we will denote this k-mer xr. We then remove from D+ every read r for which the associated xr has a confidence value of 255 or more. A vertex with a confidence value of 255 or more is repeated because any region of a genome is usually covered 30–50 times. We consider that any xr for which we have such coverage will belong to a repeated region. These regions are more susceptible to generate misassemblies. We avoid these regions, and do not consider the corresponding reads for the annotation. This choice implies that the algorithm may output contigs of shorter length, but we will gain on the quality of these contigs. For the remaining xr's, first note that xr is a subsequence of the read r that is of length k and such that any subsequence of length k that starts before xr in r is not c-confident. To each such remaining xr, we complete the annotation (r, s), where s is the starting position of subsequence xr in the read r. Hence, on a given vertex x, one can recover any read for which x is the first c-confident k-mer. Note that this annotation system is not memory intensive since each read is annotated only once. Moreover, the other positions of the read are recovered by the algorithm using the following functions. 6.2.1. Offset functions Given a walk equation M14, and an arc 〈xl, y〉, we define the ith offset value of y according to w, noted offseti(w, y), as the number of annotations (r, s) on the vertex xi, such that y is a subsequence of r, and such that the distance between the starting point of xi and the starting point of y are equal in both the read r and the walk w concatenated with the vertex y. The starting position of y in the read r is l − i + s + 1. As an example, put k = 5 and c = 1, and consider that we have a single read r = ATGCATCG in D. Since c = 1, all the 5-mers of r belong to the de Bruijn graph. Also, on the vertex xr = ATGCA, we will have the annotation (r, 1). Now consider the walk w = 〈ATGCA, TGCAT, GCATC〉, and the vertex y = CATCG. Then, we have that offset1(w, y) = 1 because the read r starts at position 1 in w, and the distance between y and xr is the same in the read and in the concatenation of the walk w with the new arc 〈GCATC, CATCG〉. Note that y is a subsequence of r, whose starting position is 4, which is exactly l − i + s + 1, since the length of the walk w is l = 3, the position xr in w is i = 1, and the position of xr in the read r is s = 1. We will have the same type of functions for paired reads, except that since the fragment length is approximated by dμ for each library, we will tolerate a certain difference between the length d of a particular pair of reads and the average length dμ for the corresponding fragment library—namely dσ. More formally, for a read pair (r1, r2, dμ, dσ), let us define the meta read r12 which has length d, begins with r1, ends with revcomp(r2), and is composed of dummy letters in the middle. Then, given a walk equation M15, and an arc 〈xl, y〉, we define the ith paired offset value of y according to w, noted equation M16 (w, y), as the number of annotations (r1, s1) on the vertex xi, for which there exists a paired read (r1, r2, dμ, dσ), such that y is a subsequence of revcomp(r2), and the distances between the starting point of xi and the starting point of y in the meta read r12 and in the walk w concatenated with the vertex y differ by a value that is at most dσ. Mathematically, the starting position of y in the read r2 is dμ − |r2| − (l − i + s1 + 1) ± dσ. As stated above, we want to give more importance to reads that have a large overlap with the contig we are constructing. Let us point out that when the value of i is getting smaller in the offset function, the magnitude of the overlap is increasing. So given a partially constructed contig equation M17, and two possible arcs 〈xl, y〉 and 〈xl, y′〉 that we can add to it, we will compare the offset values of y and y′ according to w, if there is a clear winner, the algorithm will choose it, otherwise it will stop the ongoing extension. 6.3. Seeds The proposed algorithm starts on some walks, for which we have a strong confidence that they are subsequences of the genome sequence. Such particular walks are referred to as seeds. To obtain them, we first consider a de Bruijn graph with a very high cutoff value. We fix this cutoff to equation M18. cpeak corresponds to the average coverage and cmin to where the number of errors is lower than the number of true sequences. We consider that by taking the average of those two, a very small amount of errors will be in the de Bruijn graph equation M19. The resulting stringent de Bruijn graph suffers from a huge loss of the information contained in the reads, but this procedure is only used to determine the seeds. More specifically, the seeds are the set of maximum walks in equation M20 that are composed of vertices of indegree and outdegree at most one. 6.4. The heuristics In this section, we present the heuristics on which our algorithm is based. In these rules, m is a multiplicative value indicating the confidence required to perform a choice—as the coverage increases, its value decreases. For a vertex, m = 3.0 when its coverage is at least 2, but no more than 19, m = 2.0 when its coverage is at least 20, but no more than 24, and m = 1.3 when its coverage is at least 25. For any vertex, the relation between its coverage and the value of the multiplicator m is not a parameter, and works well on various tested datasets (see Section 7). The values taken by m are motivated by the distribution of reads on a seeds—two reads starting consecutively on a seed will be nearer if more local coverage is available. Given a walk equation M21, and two arcs in the de Bruijn graph Bruijn(D+, k, c) that start at xl, namely 〈xl, y〉 and 〈xl, y′〉, we say that Rule 1 y wins over y′ if the three following inequalities hold. equation M22 So, according to Rule 1, y wins over y′ if 1) the number of paired reads that strongly overlap y and w is more than m times the corresponding value for y′, 2) the total amount of paired reads that overlap y and w is more than m times than the same amount for y′, and 3) the paired read that has the maximal overlap over w and y is more than m times the same value for y′. Recall that the smallest is the value of i, the biggest is the overlap for the corresponding reads. Rule 2: By replacing the functions equation M23 by their corresponding functions offseti in the three equations above, we get Rule 2. Hence, Rule 2 is doing exactly the same type of measurement as Rule 1, except that it works on single reads. These rules are utilized to extend seeds in the Ray algorithm. The Ray algorithm is presented in Figure 3. An external file that holds a picture, illustration, etc. Object name is fig-3.jpg FIG. 3. The Ray algorithm. Ray is a greedy algorithm on a de Bruijn graph. The extension of seeds is carried out by the subroutine GrowSeed. Each seed is extended using the set of Rules 1 and 2. Afterwards, each extended seed is extended in the opposite direction using the reverse-complement path of the extended seed. Given two seeds s1 and s2, the reachability of s1 from s2 is not a necessary and sufficient condition of the reachability of s2 from s1. Owing to this property of reachability between seeds, a final merging step is necessary to remove things appearing twice in the assembly. 6.5. Technical points about the implementation The data structure selected to extract k-mer information in reads is the splay tree (Sleator and Tarjan, 1985)—a balanced binary search tree based on the splay operation. k-mers are stored on 64 bits—thus, the maximum value for k is 32.",Assembly,"ray simultaneous assembly  read   mix  highthroughput sequence technologies
ray  derive   work  pevzner      sense   make use    bruijn graph however  framework   rely  eulerian walk  generate  assembly  define specific subsequences call seed       algorithm extend    contig  define heuristics  control  extension process    way   process  stop    point  family  read   clearly indicate  direction   extension  example consider  graph  figure   suppose   seed   …       read  overlap  seed also contain     proceed  add    contig   contrary   choose  another direction    case     obvious winner  process  stop note   heuristics  give  higher importance  read  heavily overlap  contig   read   intersect  small number   last vertices   indeed  consider  read  overlap strongly   contig   construct   informative  assign  direction    extension  proceed  external file  hold  picture illustration etc object name  figjpg fig   subgraph    bruijn graph  figure show  part    bruijn graph   example short read   enough   assembly problem suppose   true genome sequence    form equation    length   read  pair read  smaller   equation  subsequence  hint  help  assembly algorithm  differentiate  true sequence   follow one equation     hand     read  start    end       possibility  solve  branch problem  measure  overlap level  function call offseti  equation   propose heuristics  therefore limit  length   obtain contigs   give better guarantee  assembly errors moreover  heuristics allow   construct greedy algorithms   sense    point  extend  contig    construct   end  construction   greedy choice   never  reconsider  inevitable  one want    algorithm  run  polynomial time  choice  favor  greedy optimization  motivate   nphard nature   problem pop   minimize run time  memory utilization  use  sequence data provide   read    bruijn graph annotation  read  annotate    attach   single vertex   graph  coverage distribution give  family  read equation   denote    family consist   read     reversecomplement sequence     precisely equation m10 moreover   integer   kmer  cconfident   occur  least  time   read   note   kmer  appear twice    read  count twice  define equation m11   function  associate   integer value   number  kmers   cover  time accord    propose  chaisson  pevzner   function   sum   exponentiallydecreasing curve   poisson distribution  first represent  errors contain   read   second   poisson distribution   read   subsequence pick among  subsequences around  particular length   almost uniform probability  graph   function equation m12   shape   function draw  figure — coverage distributions    baylyi adp1 dataset  show   figure  roche read illumina read   mix   equation m13   local minimum    call cmin follow   local maximum    denote cpeak basically cpeak   estimate   average coverage depth   genome  cmin   estimation   value   high probability  amount  incorrect data  lower   amount  correct data  external file  hold  picture illustration etc object name  figjpg fig  coverage distributions  figure show  coverage distributions  kmers    baylyi adp1 dataset  roche illumina  roche  illumina     minimum coverage   peak coverage  identify   roche illumina  roche  illumina coverage distributions  peak coverage  rocheillumina  greater   sum   peak coverage  roche   peak coverage  illumina  suggest   mix approach allow one  recover lowcoverage regions  annotated  bruijn graph give  family  read    bruijn parameter    coverage cutoff   associate annotate  bruijn graph note bruijnd    define  follow  vertices    graph   cconfident kmers   set  arc   compose   arc  two vertices    correspond   confident   mer  example          two kmers atgct  tgctg   vertices   graph       appear  least twice   read   moreover 〈atgcttgctg〉    arc   graph     atgctg appear  least    read     annotation  consider  first cconfident kmer   read    denote  kmer    remove   every read     associate    confidence value      vertex   confidence value      repeat   region   genome  usually cover  time  consider         coverage  belong   repeat region  regions   susceptible  generate misassemblies  avoid  regions    consider  correspond read   annotation  choice imply   algorithm may output contigs  shorter length    gain   quality   contigs   remain ' first note     subsequence   read     length      subsequence  length   start       cconfident    remain   complete  annotation       start position  subsequence    read  hence   give vertex  one  recover  read      first cconfident kmer note   annotation system   memory intensive since  read  annotate   moreover   position   read  recover   algorithm use  follow function  offset function give  walk equation m14   arc 〈 〉  define  ith offset value   accord   note offsetiw    number  annotations     vertex       subsequence       distance   start point     start point    equal    read    walk  concatenate   vertex   start position     read            example put         consider     single read   atgcatcg   since      mers   belong    bruijn graph also   vertex   atgca     annotation    consider  walk   〈atgca tgcat gcatc〉   vertex   catcg     offset1w      read  start  position      distance          read    concatenation   walk    new arc 〈gcatc catcg〉 note     subsequence   whose start position     exactly        since  length   walk       position          position     read           type  function  pair read except  since  fragment length  approximate     library   tolerate  certain difference   length    particular pair  read   average length    correspond fragment library—namely   formally   read pair     let  define  meta read r12   length  begin   end  revcompr2   compose  dummy letter   middle  give  walk equation m15   arc 〈 〉  define  ith pair offset value   accord   note equation m16     number  annotations     vertex     exist  pair read          subsequence  revcompr2   distance   start point     start point     meta read r12    walk  concatenate   vertex  differ   value      mathematically  start position     read              ±   state   want  give  importance  read    large overlap   contig   construct let  point     value    get smaller   offset function  magnitude   overlap  increase  give  partially construct contig equation m17  two possible arc 〈 〉  〈 ′〉    add     compare  offset value    ′ accord       clear winner  algorithm  choose  otherwise   stop  ongoing extension  seeds  propose algorithm start   walk      strong confidence    subsequences   genome sequence  particular walk  refer   seed  obtain   first consider   bruijn graph    high cutoff value  fix  cutoff  equation m18 cpeak correspond   average coverage  cmin    number  errors  lower   number  true sequence  consider   take  average   two   small amount  errors      bruijn graph equation m19  result stringent  bruijn graph suffer   huge loss   information contain   read   procedure   use  determine  seed  specifically  seed   set  maximum walk  equation m20   compose  vertices  indegree  outdegree   one   heuristics   section  present  heuristics    algorithm  base   rule    multiplicative value indicate  confidence require  perform  choice—  coverage increase  value decrease   vertex      coverage   least            coverage   least             coverage   least    vertex  relation   coverage   value   multiplicator     parameter  work well  various test datasets see section   value take    motivate   distribution  read   seeds—two read start consecutively   seed   nearer   local coverage  available give  walk equation m21  two arc    bruijn graph bruijnd    start   namely 〈 〉  〈 ′〉  say  rule   win  ′   three follow inequalities hold equation m22  accord  rule   win  ′    number  pair read  strongly overlap        time  correspond value  ′   total amount  pair read  overlap        time    amount  ′    pair read    maximal overlap         time   value  ′ recall   smallest   value    biggest   overlap   correspond read rule   replace  function equation m23   correspond function offseti   three equations   get rule  hence rule    exactly   type  measurement  rule  except   work  single read  rule  utilize  extend seed   ray algorithm  ray algorithm  present  figure   external file  hold  picture illustration etc object name  figjpg fig   ray algorithm ray   greedy algorithm    bruijn graph  extension  seed  carry    subroutine growseed  seed  extend use  set  rule    afterwards  extend seed  extend   opposite direction use  reversecomplement path   extend seed give two seed     reachability        necessary  sufficient condition   reachability     owe   property  reachability  seed  final merge step  necessary  remove things appear twice   assembly  technical point   implementation  data structure select  extract kmer information  read   splay tree sleator  tarjan — balance binary search tree base   splay operation kmers  store   bits—thus  maximum value    ",5
123,Ray-meta,"Ray Meta: scalable de novo metagenome assembly and profiling
Thorough documentation and associated scripts to reproduce our studies are available in Additional file 3 on the publisher website or on https://github.com/sebhtml/Paper-Replication-2012. Memory model Ray Meta uses the message-passing interface. As such, a 1,024-core job has 1,024 processes running on many computers. In the experiments, each node had 8 processor cores and 24 GB, or 3 GB per core. With the message-passing paradigm, each core has its own virtual memory, which is protected from any other process. Because the data are distributed uniformly using a distributed hash table, memory usage for a single process is very low. For the 1,024-core job, the maximum memory usage of any process was on average 1.5 GB. Assemblies Metagenome assemblies with profiling were computed with Ray v2.0.0 (Additional file 4) on Colosse, a Compute Canada resource. Ray is open source software - the license is the GNU General Public License, version 3 (GPLv3) - and is freely available from http://denovoassembler.sourceforge.net/ or http://github.com/sebhtml/ray. Ray can be deployed on public compute infrastructure or in the cloud (see [45] for a review). The algorithms implemented in the software Ray were heavily modified for metagenome de novo assembly and these changes were called Ray Meta. Namely, the coverage distribution for k-mers in the de Bruijn graph is not utilized to infer the average coverage depth for unique genomic regions. Instead, this value is derived from local coverage distributions during the parallel assembly process. Therefore, unlike MetaVelvet [39], Ray Meta does not attempt to calculate or use any global k-mer coverage depth distribution. Simulated metagenomes with a power law Two metagenomes (100 and 1,000 genomes, respectively) were simulated with abundances following a power law (Tables S1 and S2 in Additional file 1). Power laws are commonly found in biological systems [46]. Simulated sequencing errors were randomly distributed, the error rate was set at 0.25% and the average insert length was 400. The second simulated metagenome was assembled with 128 8-core computers (1,024 processor cores) interconnected with a Mellanox ConnectX QDR Infiniband fabric (Mellanox, Inc.). For the 1,000-genome dataset, messages were routed with a de Bruijn graph of degree 32 and diameter 2 to reduce the latency. Validation of assemblies Assembled contigs were aligned onto reference genomes using the MUMmer bioinformatics software suite [44]. More precisely, deltas were generated with nucmer. Using show-coords, any contig not aligning as one single maximum with at least 98% breadth of coverage was marked as misassembled. Contigs aligning in two parts at the beginning and end of a reference were not counted as misassembled owing to the circular nature of bacterial genomes. Finally, small insertions/deletions and mismatches were obtained with show-SNPs. Colored and distributed de Bruijn graphs The vertices of a de Bruijn graph are distributed across processes called ranks. Here, graph coloring means labeling the vertices of a graph. A different color is added to the graph for each reference sequence. Each k-mer in any reference sequence is colored with the reference sequence color if it occurs in the distributed de Bruijn graph. Therefore, any k-mer in the graph has zero, one or more colors. First, a k-mer with no colors indicates that the k-mer does not exist in the databases provided. Second, a k-mer with one color means that this k-mer is specific to one and only one reference genome in the databases provided while at least two colors indicates that the k-mer is not specific to one single reference sequence. These reference sequences are assigned to leaves in a taxonomic tree. Reference sequences can be grouped in independent name spaces. Genome assembly is independent of graph coloring. Demultiplexing signals from similar bacterial strains Biological abundances were estimated using the product of the number of k-mers matched in the distributed de Bruijn graph by the mode coverage of k-mers that were uniquely colored. This number is called the number of k-mer observations. The total number of k-mer observations is the sum of coverage depth values of all colored k-mers. A proportion is calculated by dividing the number of k-mer observations by the total. Taxonomic profiling All bacterial genomes available in GenBank [47] were utilized for coloring the distributed de Bruijn graphs (Table S4 in Additional file 1). Each k-mer was assigned to a taxon in the taxonomic tree. When a k-mer has more than one taxon color, the coverage depth was assigned to the nearest common ancestor. Gene ontology profiling The de Bruijn graph was colored with coding sequences from the EMBL nucleotide sequence database [48] (EMBL_CDS), which are mapped to gene ontology by transitivity using the uniprot mapping to gene ontology [49]. For each ontology term, coverage depths of colored k-mers were added to obtain the total number of k-mer observations. Principal component analysis Principal component analysis was used to group taxonomic profiles to produce enterotypes. Data were prepared in a matrix using the genera as rows and the samples as columns. Singular values and left and right singular vectors of this matrix were obtained using singular value decomposition implemented in R. The right singular vectors were sorted by singular values. The sorted right singular vectors were used as the new base for the re-representation of the genus proportions. The two first dimensions were plotted. Software implementation Ray Meta is distributed software that runs on connected computers by transmitting messages over a network using the message-passing interface (MPI) and is implemented in C++. The MPI standard is implemented in libraries such as Open-MPI [50] and MPICH2 [51]. On each processor core, tasks are divided into smaller ones and given to a pool of 32,768 workers (thread pool), which are similar to chares in CHARM++ [52]. Each of these sends messages to a virtual communicator. The latter implements a message aggregation strategy in which messages are automatically multiplexed and demultiplexed. The k-mers are stored in a distributed sparse hash table which utilizes open addressing (double hashing) for collisions. Incremental resizing is utilized in this hash table when the occupancy exceeds 90% to grow tables locally. Smart pointers are utilized in this table to perform real-time memory compaction. The software is implemented on top of RayPlatform, a development framework used to ease the creation of massively distributed high-performance computing applications. Comparison with MetaVelvet Software versions used were: MetaVelvet 1.2.01, Velvet 1.2.07 and Ray 2.0.0 (with Ray Meta). MetaVelvet was run on one processor core. Ray Meta was run on 64 processor cores for Human Microbiome Project samples (SRS011098, SRS017227 and SRS018661) and on 48, 32 and 32 processor cores for MetaHIT samples (ERS006494, ERS006497 and ERS006592), respectively. There were eight processor cores per node. The running time for MetaVelvet is the sum of running times for velveth, velvetg and meta-velvetg. For MetaVelvet, sequence files were filtered to remove any sequence with more than 10N symbols. The resulting files were shuffled to create files with interleaved sequences. The insert size was manually provided to MetaVelvet and the k-mer length was set to 51 as suggested in its documentation. Peak coverages were determined automatically by MetaVelvet. Ray Meta was run with a k-mer length of 31. No other parameters were required for Ray Meta and sequence files were provided without modification to Ray Meta. The overlaps of assemblies produced by MetaVelvet and by Ray Meta were evaluated with Ray using the graph coloring features. No mismatches were allowed in k-mers. Overlaps were computed for scaffolds with at least 500 nucleotides. Comparison with MetaPhlAn Taxonomic profiles calculated with MetaPhlAn [27] for samples from the Human Microbiome Project were obtained [24]. Taxonomic profiles were produced by Ray Communities for 313 samples (Additional file 2). Pearson's correlation was calculated for each body site by combining taxon proportions for both methods for each taxonomic rank.",Assembly,"ray meta scalable  novo metagenome assembly  profiling
thorough documentation  associate script  reproduce  study  available  additional file    publisher website    memory model ray meta use  messagepassing interface    core job   process run  many computers   experiment  node   processor core       per core   messagepassing paradigm  core    virtual memory   protect    process   data  distribute uniformly use  distribute hash table memory usage   single process   low   core job  maximum memory usage   process   average   assemblies metagenome assemblies  profile  compute  ray  additional file   colosse  compute canada resource ray  open source software   license   gnu general public license version  gplv3    freely available     ray   deploy  public compute infrastructure    cloud see    review  algorithms implement   software ray  heavily modify  metagenome  novo assembly   change  call ray meta namely  coverage distribution  kmers    bruijn graph   utilize  infer  average coverage depth  unique genomic regions instead  value  derive  local coverage distributions   parallel assembly process therefore unlike metavelvet  ray meta   attempt  calculate  use  global kmer coverage depth distribution simulate metagenomes   power law two metagenomes    genomes respectively  simulate  abundances follow  power law table     additional file  power laws  commonly find  biological systems  simulate sequence errors  randomly distribute  error rate  set     average insert length    second simulate metagenome  assemble   core computers  processor core interconnect   mellanox connectx qdr infiniband fabric mellanox inc   genome dataset message  rout    bruijn graph  degree   diameter   reduce  latency validation  assemblies assemble contigs  align onto reference genomes use  mummer bioinformatics software suite   precisely deltas  generate  nucmer use showcoords  contig  align  one single maximum   least  breadth  coverage  mark  misassembled contigs align  two part   begin  end   reference   count  misassembled owe   circular nature  bacterial genomes finally small insertionsdeletions  mismatch  obtain  showsnps color  distribute  bruijn graph  vertices    bruijn graph  distribute across process call rank  graph color mean label  vertices   graph  different color  add   graph   reference sequence  kmer   reference sequence  color   reference sequence color   occur   distribute  bruijn graph therefore  kmer   graph  zero one   color first  kmer   color indicate   kmer   exist   databases provide second  kmer  one color mean   kmer  specific  one   one reference genome   databases provide   least two color indicate   kmer   specific  one single reference sequence  reference sequence  assign  leave   taxonomic tree reference sequence   group  independent name space genome assembly  independent  graph color demultiplexing signal  similar bacterial strain biological abundances  estimate use  product   number  kmers match   distribute  bruijn graph   mode coverage  kmers   uniquely color  number  call  number  kmer observations  total number  kmer observations   sum  coverage depth value   color kmers  proportion  calculate  divide  number  kmer observations   total taxonomic profile  bacterial genomes available  genbank   utilize  color  distribute  bruijn graph table   additional file   kmer  assign   taxon   taxonomic tree   kmer    one taxon color  coverage depth  assign   nearest common ancestor gene ontology profile   bruijn graph  color  cod sequence   embl nucleotide sequence database  embl_cds   map  gene ontology  transitivity use  uniprot map  gene ontology    ontology term coverage depths  color kmers  add  obtain  total number  kmer observations principal component analysis principal component analysis  use  group taxonomic profile  produce enterotypes data  prepare   matrix use  genera  row   sample  columns singular value  leave  right singular vectors   matrix  obtain use singular value decomposition implement    right singular vectors  sort  singular value  sort right singular vectors  use   new base   rerepresentation   genus proportion  two first dimension  plot software implementation ray meta  distribute software  run  connect computers  transmit message   network use  messagepassing interface mpi   implement    mpi standard  implement  libraries   openmpi   mpich2    processor core task  divide  smaller ones  give   pool   workers thread pool   similar  char  charm     send message   virtual communicator  latter implement  message aggregation strategy   message  automatically multiplexed  demultiplexed  kmers  store   distribute sparse hash table  utilize open address double hash  collisions incremental resize  utilize   hash table   occupancy exceed   grow table locally smart pointers  utilize   table  perform realtime memory compaction  software  implement  top  rayplatform  development framework use  ease  creation  massively distribute highperformance compute applications comparison  metavelvet software versions use  metavelvet  velvet   ray   ray meta metavelvet  run  one processor core ray meta  run   processor core  human microbiome project sample srs011098 srs017227  srs018661       processor core  metahit sample ers006494 ers006497  ers006592 respectively   eight processor core per node  run time  metavelvet   sum  run time  velveth velvetg  metavelvetg  metavelvet sequence file  filter  remove  sequence     symbols  result file  shuffle  create file  interleave sequence  insert size  manually provide  metavelvet   kmer length  set    suggest   documentation peak coverages  determine automatically  metavelvet ray meta  run   kmer length     parameters  require  ray meta  sequence file  provide without modification  ray meta  overlap  assemblies produce  metavelvet   ray meta  evaluate  ray use  graph color feature  mismatch  allow  kmers overlap  compute  scaffold   least  nucleotides comparison  metaphlan taxonomic profile calculate  metaphlan   sample   human microbiome project  obtain  taxonomic profile  produce  ray communities   sample additional file  pearson' correlation  calculate   body site  combine taxon proportion   methods   taxonomic rank",5
124,MegaHIt,"MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph
MEGAHIT makes use of succinct de Bruijn graphs (SdBG; Bowe et al., 2012), which are compressed representation of de Bruijn graphs. A SdBG encodes a graph with m edges in O(m) bits, and supports O(1) time traversal from a vertex to its neighbors. Our implementation has added a bit-vector of length m to mark the validity of each edge (so as to support dynamic removal of edges efficiently), and an auxiliary vector of 2kt bits (where k is the k-mer size and t is the number of zero-indegree vertices) to store the sequence of zero-indegree vertices to ensure the graph being lossless. Despite its advantages, constructing a SdBG efficiently is non-trivial. MEGAHIT is rooted in a fast parallel algorithm for SdBG construction; the bottleneck is sorting a set of (k+1)-mers that are the edges of an SdBG in reverse lexicographical order of their length-k prefixes (k-mers). MEGAHIT exploits the parallelism of a graphics processing unit (GPU, CUDA-enabled) by adapting the recent BWT-construction algorithm CX1 (Liu et al., 2014), which takes advantage of a GPU to sort the suffices of a set of reads very efficiently. Limited by the relatively small size of GPU’s on-board memory, we adopt a block-wise strategy that partitions the k-mers according to their length-l prefix (l = 8 in our implementation). The k-mers in consecutive partitions that fit within the GPU memory are sorted together. Leveraging the parallelism of GPU, MEGAHIT speeds up the construction by 3–5 times over its CPU-only counterpart. Notably, sequencing error is problematic, because a single base of sequencing error leads to k erroneous k-mer singletons, which increases the memory consumption of MEGAHIT significantly. To cope with the problem, before graph construction, all (k + 1)-mers from the input reads are sorted and counted, and only (k + 1)-mers that appear at least d (2 by default) times are kept as solid-kmer. This method removes many spurious edges, but may be risky for metagenomics assembly since many low-abundance species may have been sequenced at very low depth. Thus we introduce a mercy-kmer strategy to recover these low-depth edges. Given two solid (k + 1)-mers x and y from the same read, where x has no outdegree and y has no indegree. If all (k + 1)-mers between x and y in that read are not solid, they will be added to the de Bruijn graph as mercy-kmers. Mercy-kmers strengthen the contiguity of low-depth regions. Without this approach, many authentic low-depth edges would be incorrectly identified as tips and removed. Based on SdBG, we implemented a multiple k-mer size strategy in MEGAHIT (Peng et al., 2012). The method iteratively builds multiple SdBGs from a small k to a large k. While a small k-mer size is favourable for filtering erroneous edges and filling gaps in low-coverage regions, a large k-mer size is useful for resolving repeats. In each iteration, MEGAHIT cleans potentially erroneous edges by removing tips, merging bubbles and removing low local coverage edges. The last approach is especially useful for metagenomics, which suffers from non-uniform sequencing depths.",Assembly,"megahit  ultrafast singlenode solution  large  complex metagenomics assembly via succinct  bruijn graph
megahit make use  succinct  bruijn graph sdbg bowe      compress representation   bruijn graph  sdbg encode  graph   edge   bits  support  time traversal   vertex   neighbor  implementation  add  bitvector  length   mark  validity   edge    support dynamic removal  edge efficiently   auxiliary vector  2kt bits     kmer size     number  zeroindegree vertices  store  sequence  zeroindegree vertices  ensure  graph  lossless despite  advantage construct  sdbg efficiently  nontrivial megahit  root   fast parallel algorithm  sdbg construction  bottleneck  sort  set  kmers    edge   sdbg  reverse lexicographical order   lengthk prefix kmers megahit exploit  parallelism   graphics process unit gpu cudaenabled  adapt  recent bwtconstruction algorithm cx1 liu     take advantage   gpu  sort  suffice   set  read  efficiently limit   relatively small size  gpus onboard memory  adopt  blockwise strategy  partition  kmers accord   lengthl prefix      implementation  kmers  consecutive partition  fit within  gpu memory  sort together leverage  parallelism  gpu megahit speed   construction   time   cpuonly counterpart notably sequence error  problematic   single base  sequence error lead   erroneous kmer singletons  increase  memory consumption  megahit significantly  cope   problem  graph construction    mers   input read  sort  count     mers  appear  least    default time  keep  solidkmer  method remove many spurious edge  may  risky  metagenomics assembly since many lowabundance species may   sequence   low depth thus  introduce  mercykmer strategy  recover  lowdepth edge give two solid   mers       read     outdegree     indegree     mers       read   solid    add    bruijn graph  mercykmers mercykmers strengthen  contiguity  lowdepth regions without  approach many authentic lowdepth edge would  incorrectly identify  tip  remove base  sdbg  implement  multiple kmer size strategy  megahit peng     method iteratively build multiple sdbgs   small    large    small kmer size  favourable  filter erroneous edge  fill gap  lowcoverage regions  large kmer size  useful  resolve repeat   iteration megahit clean potentially erroneous edge  remove tip merge bubble  remove low local coverage edge  last approach  especially useful  metagenomics  suffer  nonuniform sequence depths",5
125,Omega,"Omega: an overlap-graph de novo assembler for metagenomics
The performance of assemblers on Illumina HiSeq 100-bp data was benchmarked using a real-world sequencing dataset of genomic DNA mixture of 64 diverse bacterial and archaeal microorganisms ( Shakya et al. , 2013 ). The dataset is available at National Center for Biotechnology Information (NCBI) Sequence Read Archive (SRA) (accession number: SRX200676). The 64 microorganisms are listed in Supplementary Table S1 . Fastq sequences were extracted from the SRA format raw dataset using NCBI SRA Toolkit (version 2.3.4). This dataset contains 108.7 million paired-end and 0.4 million single-end 100-bp reads. Sickle ( https://github.com/najoshi/sickle ) was used to trim reads using a 20-Phred quality threshold, to filter out reads shorter than 60 bp and to discard reads containing many Ns. BBNorm ( https://sourceforge.net/projects/bbmap/ ) was then used for error correction with default settings. The HiSeq 100-bp dataset was assembled using SOAPdenovo, IDBA-UD, MetaVelvet and Omega. The k- mer length or minimum overlap length was optimized for each assembler based on the N50 size: SOAPdenovo (best k- mer length = 51 of 31, 41, 51 and 61), IDBA-UD ( k- mer length range = 30–60 with a step size of 10), MetaVelvet (best k- mer length = 51 of 31, 41, 51 and 61) and Omega (best minimum overlap length = 50 of 30, 40, 50, 60 and 70). SOAPdenovo was run in a metagenome configuration as described ( Pop, 2011 ). IDBA-UD, MetaVelvet and Omega were run with default parameters. The performance of assemblers on Illumina MiSeq 300-bp data was tested using a simulated dataset of a nine-genome synthetic community. Ten million paired-end 300-bp reads with an average insert size of 900 bp were simulated based on an empirical error model using MetaSim ( Richter et al. , 2008 ). Supplementary Table S2 lists the nine genomes and their relative abundances ranging from 3 to 20%. The simulated reads were preprocessed using Sickle and error-corrected using BBNorm as described above. The maximum k- mer length of MetaVelvet was increased to 171 by changing a constant parameter in its source code. We were unable to increase the maximum k- mer lengths of SOAPdenovo and IDBA-UD, which were hard-coded at 127 and 124, respectively. The k- mer length or minimum overlap length was optimized for each assembler: SOAPdenovo (best k- mer length = 121 of 41, 61, 81, 101, 121 and 127), IDBA-UD ( k- mer length range = 40–120 with a step size of 20), MetaVelvet (best k- mer length = 151 of 121, 131, 141, 151 and 161) and Omega (best minimum overlap length = 150 of 120, 130, 140, 150 and 160). The assemblers were run as described above. Celera was also tested for the MiSeq 300-bp dataset using a default setting and a metagenomics setting ( http://sourceforge.net/apps/mediawiki/wgs-assembler/index.php?title=RunCA_Examples_-_454_%2B_Sanger_Metagenomic ). The Celera assembly using the default setting was substantially better than that using the metagenomics setting and, therefore, was used for performance comparison. To measure assembly accuracy, contigs and scaffolds >200 bp produced by each assembler were aligned with reference genomes using Burrow–Wheeler Aligner (BWA) ( Li and Durbin, 2010 ). The alignments were used to generate a list of correct contigs containing <5% of substitutions and indels. The scaffolding of two adjacent contigs was considered to be correct if their alignments to the same reference genome were in correct orientation and separated apart by less than the mean plus one standard deviation (SD) of the mate inner distances of the paired-end sequencing data. The performance of assemblers was compared by N80, N50, N20, largest contig length and genome sequence coverage for each reference genome. N80, N50 and N20 are the minimum size thresholds for length-sorted contigs that covers 80, 50 and 20% of the total length of all contigs, respectively. Genome sequence coverage is the percentage of a reference genome sequence covered by the assembled contigs. N80, N50, N20 and largest contig length measure the contiguity of the correct contigs at different levels. Genome sequence coverage measures the completeness of the correct contigs. Different types of assembly errors were identified based on the BWA alignment between the contigs and the reference genomes, including the number of base pairs of insertion, deletion and substitution and the number of chimeric contigs. Chimeric contigs were identified by their fragmented alignments to multiple non-contiguous regions of a reference genome or multiple reference genomes. 3 ALGORITHM Omega was developed in C++ using object-oriented programming. Omega can accept multiple input datasets with different insert sizes and variable read lengths in fasta or fastq format. The assembly and scaffolding were performed in eight steps ( Fig. 1 ): Hash table construction. All unique reads are loaded to the memory and indexed in a hash table. Let K be the user-defined minimum overlap length. The keys of the hash table are DNA sequence substrings of length K − 1. Each read is inserted to the hash table with four keys: prefix and suffix of length K − 1 of both forward sequence and reverse complement sequence of the read. A value in the hash table is an array of pointers to the reads associated with the corresponding key. The hash table is initialized to be eight times of the total read number. Hash collision is resolved using linear probing. The hash table allows a nearly constant time search of all reads by their prefixes or suffixes. A read that is a substring of another read is called a contained read . To identify all contained reads of read r , every proper substring s of length K in read r is searched in the hash table. This produces a short list of reads that contains s as a prefix or suffix, which is then compared with read r to identify the contained reads of read r . The contained reads are used for coverage depth calculation and mate-pair linkage analysis below. Overlap graph construction. Each read is represented by a vertex in a bi-directed overlap graph. An edge is inserted between two vertices if the two corresponding reads have an exact-match overlap of at least K bp. The bi-directed edges represent the four different orientations in which two reads can overlap: suffix with prefix ( •→−−→• ), suffix of the reverse complement with prefix ( •←−−→• ), suffix with prefix of the reverse complement ( •→−−←• ) and suffix of the reverse complement with prefix of the reverse complement ( •←−−←• ). To efficiently find all reads overlapping with a read r , every proper substring s of length K − 1 in read r is searched in the hash table, and all retrieved reads are compared with the read r . If a read has the exact match with read r for their remaining overlap, an edge is inserted between the two reads’ corresponding vertices. After inserting all edges of read r , all transitive edges incident on read r are removed using a linear algorithm as described ( Haider, 2012 ; Myers, 2005 ). Briefly, suppose that r is connected with two other reads, a and b . If there is also an edge between a and b to form a triangle with r and the sequence represented by the edge ( r, b ) is the same as the sequence represented by the path through ( r, a ) and ( a, b ), then ( r, b ) is identified as a transitive edge and is deleted. Removing all transitive edges significantly simplifies the overlap graph without losing any information. Composite edge contraction. While the bi-directed edges can be traversed in both directions, the vertices can be traversed only by entering a vertex in an in-arrow and exiting in an out-arrow ( −→•→− ) or by entering a vertex in an out-arrow and exiting in an in-arrow ( −←•←− ). A valid path in the overlap graph represents an assembled DNA sequence containing proper overlapping reads with appropriate orientation and sufficient overlap length. After removing transitive edges, simple vertices have exactly one in-arrow and one out-arrow, representing only one possible way to traverse such simple vertices. A read in a simple vertex uniquely overlaps with one other read in either direction. To simplify the overlap graph, a simple vertex, r , along with its in-arrow edge ( u, r ) and out-arrow edge ( r, w ), are replaced by a composite edge ( u, w ) in the overlap graph. The composite edge ( u, w ) contains the read r and all ordered reads in edge ( u, r ) and ( r, w ). The edge ( u, w ) has the same arrow types to u and w as the original edges, ( u, r ) and ( r, w ), respectively. Simple vertices are merged into composite edges iteratively, until there is no simple vertex remaining in the overlap graph. Sequence variation removal. Sequence variations originate from uncorrected sequencing errors and natural sequence polymorphisms in microbial communities. Many reads with sequence variations do not overlap with any other reads and are represented as isolated vertices in the overlap graph. Reads with the same sequence variation may overlap with one another, which creates small branches and bubbles in the overlap graph. Small branches are short dead-end paths that contain <10 reads. Bubbles are two edges that connect the same two vertices with the same arrow types. The overlap graph is systematically traversed to trim off small branches and remove the edges containing less reads in bubbles. This may create new simple vertices that are then removed by repeating the composite edge contraction. Minimum cost flow analysis. Each edge in the overlap graph is associated with a string copy number, representing how many times the edge’s sequence is present in the metagenome. String copy numbers of edges are estimated based on the topology of the overlap graph using minimum cost flow analysis as described ( Haider, 2012 ; Myers, 2005 ). Composite edges with sequences >1000 bp are set to have a minimum flow of 1, requiring such edges’ sequences to be present in the metagenome at least once. The minimum flow for short edges (<1000 bp) is set to 0. The CS2 algorithm ( Goldberg, 1997 ) is used to optimize the amount of flow passing through every edge such that the total cost of the flow network in the overlap graph is minimized. Edges with more than one unit of flow correspond to repeat regions shared among multiple genomes or multiple places in a single genome. Edges with zero flow represent short sequences that are not needed to connect long sequences together and are ignored. Tree structures in the overlap graph are simplified using the flows. A tree comprises two edges, ( p, t ) and ( q, t ), merging to a third edge ( t, r ), and the flow on ( t, r ) is equal to the total flow on ( p, t ) and ( q, t ). Such a tree is reduced to two new edges ( p, r ) and ( q, r ) that both contain the reads in vertex t and edge ( t, r ). Merging of adjacent edges with mate-pair support. The insert size of each paired-end dataset is estimated separately to accommodate a mixture of datasets with different insert sizes. The overlap graph at this stage has long composite edges that contain both reads of many mate-pairs. The insert sizes of such pairs are determined from their relative locations on the long edges and are pooled to estimate the mean μ and SD σ of all mate-pairs’ insert sizes in each dataset. Mate-pairs that span multiple edges are used to merge adjacent edges in the overlap graph. For each of such mate-pairs, all possible paths of length within range ( μ − 3 σ , μ + 3 σ ) are enumerated. If all paths of a mate-pair travel through two adjacent edges, ( m, r ) and ( r, n ), the connection between these two edges is considered to be supported by this mate-pair. After processing all mate-pairs, if the connection between ( m, r ) and ( r, n ) is supported by more than three mate-pairs, these two edges are merged to one edge ( m, n ) containing a duplicated r . Scaffolding of long edges with mate-pair support. Scaffolding uses mate-pairs that have no valid path between their paired reads in the overlap graph because of a gap in genome coverage. Scaffolding is attempted for every pair of non-adjacent edges >1000 bp. A mate-pair is considered to support the scaffolding of two edges if its two reads are uniquely mapped to the two edges at an appropriate distance apart. After processing all mate-pairs, the scaffolds of long edges with support of more than three mate-pairs are accepted. Resolving ambiguity by coverage depth. Many unresolved vertices in the overlap graph have two incoming edges and two outgoing edges, which often originate from a short repeat region between two different genomes. The two genomes may have different coverage depths to separate their edges. The coverage depth is calculated for every position along an edge to estimate the mean δ and SD θ of coverage depth along the edge. Only unique reads in an edge are considered for coverage depth calculation. A pair of adjacent edges on an unresolved vertex are merged if | δ1 – δ2 | < θ1 + θ2 . Fig. 1. Overview of Omega. The prefix and suffix (red sections) of every read are indexed in a hash table. As reads are aligned using the hash table, transitive edges (green arrows) are removed. In the unitig graph, edges (blue arrows) represent unambiguous series of overlapping reads, vertices (red dots) represent branching points and flows (green numbers) estimate the copy numbers of strings in the edges. The mate-pair linkages (orange dotted lines) are used to build contigs and then scaffolds containing gaps (blue dotted arrows). The repeat region between two different genomes (the edge with 2 units of flow) may be resolved using mate-pair supports (as shown here) or coverage depth information Open in new tabDownload slide Overview of Omega. The prefix and suffix (red sections) of every read are indexed in a hash table. As reads are aligned using the hash table, transitive edges (green arrows) are removed. In the unitig graph, edges (blue arrows) represent unambiguous series of overlapping reads, vertices (red dots) represent branching points and flows (green numbers) estimate the copy numbers of strings in the edges. The mate-pair linkages (orange dotted lines) are used to build contigs and then scaffolds containing gaps (blue dotted arrows). The repeat region between two different genomes (the edge with 2 units of flow) may be resolved using mate-pair supports (as shown here) or coverage depth information Finally, Omega reports contigs and scaffolds based on the edges of the overlap graph.",Assembly,"omega  overlapgraph  novo assembler  metagenomics
 performance  assemblers  illumina hiseq  data  benchmarked use  realworld sequence dataset  genomic dna mixture   diverse bacterial  archaeal microorganisms  shakya       dataset  available  national center  biotechnology information ncbi sequence read archive sra accession number srx200676   microorganisms  list  supplementary table   fastq sequence  extract   sra format raw dataset use ncbi sra toolkit version   dataset contain  million pairedend   million singleend  read sickle     use  trim read use  phred quality threshold  filter  read shorter      discard read contain many  bbnorm      use  error correction  default settings  hiseq  dataset  assemble use soapdenovo idbaud metavelvet  omega   mer length  minimum overlap length  optimize   assembler base   n50 size soapdenovo best  mer length         idbaud   mer length range     step size   metavelvet best  mer length          omega best minimum overlap length          soapdenovo  run   metagenome configuration  describe  pop   idbaud metavelvet  omega  run  default parameters  performance  assemblers  illumina miseq  data  test use  simulate dataset   ninegenome synthetic community ten million pairedend  read   average insert size     simulate base   empirical error model use metasim  richter      supplementary table  list  nine genomes   relative abundances range      simulate read  preprocessed use sickle  errorcorrected use bbnorm  describe   maximum  mer length  metavelvet  increase    change  constant parameter   source code   unable  increase  maximum  mer lengths  soapdenovo  idbaud   hardcoded     respectively   mer length  minimum overlap length  optimize   assembler soapdenovo best  mer length           idbaud   mer length range     step size   metavelvet best  mer length           omega best minimum overlap length           assemblers  run  describe  celera  also test   miseq  dataset use  default set   metagenomics set     celera assembly use  default set  substantially better   use  metagenomics set  therefore  use  performance comparison  measure assembly accuracy contigs  scaffold   produce   assembler  align  reference genomes use burrowwheeler aligner bwa    durbin    alignments  use  generate  list  correct contigs contain   substitutions  indels  scaffold  two adjacent contigs  consider   correct   alignments    reference genome   correct orientation  separate apart  less   mean plus one standard deviation    mate inner distance   pairedend sequence data  performance  assemblers  compare  n80 n50 n20 largest contig length  genome sequence coverage   reference genome n80 n50  n20   minimum size thresholds  lengthsorted contigs  cover       total length   contigs respectively genome sequence coverage   percentage   reference genome sequence cover   assemble contigs n80 n50 n20  largest contig length measure  contiguity   correct contigs  different level genome sequence coverage measure  completeness   correct contigs different type  assembly errors  identify base   bwa alignment   contigs   reference genomes include  number  base pair  insertion deletion  substitution   number  chimeric contigs chimeric contigs  identify   fragment alignments  multiple noncontiguous regions   reference genome  multiple reference genomes  algorithm omega  develop   use objectoriented program omega  accept multiple input datasets  different insert size  variable read lengths  fasta  fastq format  assembly  scaffold  perform  eight step  fig   hash table construction  unique read  load   memory  index   hash table let    userdefined minimum overlap length  key   hash table  dna sequence substrings  length     read  insert   hash table  four key prefix  suffix  length      forward sequence  reverse complement sequence   read  value   hash table   array  pointers   read associate   correspond key  hash table  initialize   eight time   total read number hash collision  resolve use linear probe  hash table allow  nearly constant time search   read   prefix  suffix  read    substring  another read  call  contain read   identify  contain read  read   every proper substring   length   read   search   hash table  produce  short list  read  contain    prefix  suffix    compare  read   identify  contain read  read    contain read  use  coverage depth calculation  matepair linkage analysis  overlap graph construction  read  represent   vertex   bidirected overlap graph  edge  insert  two vertices   two correspond read   exactmatch overlap   least    bidirected edge represent  four different orientations   two read  overlap suffix  prefix  •→→•  suffix   reverse complement  prefix  •→•  suffix  prefix   reverse complement  •→•   suffix   reverse complement  prefix   reverse complement  ••   efficiently find  read overlap   read   every proper substring   length     read   search   hash table   retrieve read  compare   read     read   exact match  read    remain overlap  edge  insert   two read correspond vertices  insert  edge  read    transitive edge incident  read   remove use  linear algorithm  describe  haider   myers   briefly suppose    connect  two  read        also  edge      form  triangle     sequence represent   edge          sequence represent   path                 identify   transitive edge   delete remove  transitive edge significantly simplify  overlap graph without lose  information composite edge contraction   bidirected edge   traverse   directions  vertices   traverse   enter  vertex   inarrow  exit   outarrow  →•→    enter  vertex   outarrow  exit   inarrow  •   valid path   overlap graph represent  assemble dna sequence contain proper overlap read  appropriate orientation  sufficient overlap length  remove transitive edge simple vertices  exactly one inarrow  one outarrow represent  one possible way  traverse  simple vertices  read   simple vertex uniquely overlap  one  read  either direction  simplify  overlap graph  simple vertex   along   inarrow edge      outarrow edge      replace   composite edge       overlap graph  composite edge     contain  read    order read  edge           edge        arrow type       original edge          respectively simple vertices  merge  composite edge iteratively     simple vertex remain   overlap graph sequence variation removal sequence variations originate  uncorrected sequence errors  natural sequence polymorphisms  microbial communities many read  sequence variations   overlap    read   represent  isolate vertices   overlap graph read    sequence variation may overlap  one another  create small branch  bubble   overlap graph small branch  short deadend paths  contain  read bubble  two edge  connect   two vertices    arrow type  overlap graph  systematically traverse  trim  small branch  remove  edge contain less read  bubble  may create new simple vertices    remove  repeat  composite edge contraction minimum cost flow analysis  edge   overlap graph  associate   string copy number represent  many time  edge sequence  present   metagenome string copy number  edge  estimate base   topology   overlap graph use minimum cost flow analysis  describe  haider   myers   composite edge  sequence    set    minimum flow   require  edge sequence   present   metagenome  least   minimum flow  short edge    set    cs2 algorithm  goldberg    use  optimize  amount  flow pass  every edge    total cost   flow network   overlap graph  minimize edge    one unit  flow correspond  repeat regions share among multiple genomes  multiple place   single genome edge  zero flow represent short sequence    need  connect long sequence together   ignore tree structure   overlap graph  simplify use  flow  tree comprise two edge          merge   third edge       flow       equal   total flow             tree  reduce  two new edge            contain  read  vertex   edge     merge  adjacent edge  matepair support  insert size   pairedend dataset  estimate separately  accommodate  mixture  datasets  different insert size  overlap graph   stage  long composite edge  contain  read  many matepairs  insert size   pair  determine   relative locations   long edge   pool  estimate  mean       matepairs insert size   dataset matepairs  span multiple edge  use  merge adjacent edge   overlap graph     matepairs  possible paths  length within range             enumerate   paths   matepair travel  two adjacent edge           connection   two edge  consider   support   matepair  process  matepairs   connection            support    three matepairs  two edge  merge  one edge     contain  duplicate   scaffold  long edge  matepair support scaffold use matepairs    valid path   pair read   overlap graph    gap  genome coverage scaffold  attempt  every pair  nonadjacent edge    matepair  consider  support  scaffold  two edge   two read  uniquely map   two edge   appropriate distance apart  process  matepairs  scaffold  long edge  support    three matepairs  accept resolve ambiguity  coverage depth many unresolved vertices   overlap graph  two incoming edge  two outgo edge  often originate   short repeat region  two different genomes  two genomes may  different coverage depths  separate  edge  coverage depth  calculate  every position along  edge  estimate  mean      coverage depth along  edge  unique read   edge  consider  coverage depth calculation  pair  adjacent edge   unresolved vertex  merge            fig  overview  omega  prefix  suffix red section  every read  index   hash table  read  align use  hash table transitive edge green arrows  remove   unitig graph edge blue arrows represent unambiguous series  overlap read vertices red dot represent branch point  flow green number estimate  copy number  string   edge  matepair linkages orange dot line  use  build contigs   scaffold contain gap blue dot arrows  repeat region  two different genomes  edge   units  flow may  resolve use matepair support  show   coverage depth information open  new tabdownload slide overview  omega  prefix  suffix red section  every read  index   hash table  read  align use  hash table transitive edge green arrows  remove   unitig graph edge blue arrows represent unambiguous series  overlap read vertices red dot represent branch point  flow green number estimate  copy number  string   edge  matepair linkages orange dot line  use  build contigs   scaffold contain gap blue dot arrows  repeat region  two different genomes  edge   units  flow may  resolve use matepair support  show   coverage depth information finally omega report contigs  scaffold base   edge   overlap graph",5
126,Genovo,"Genovo: de novo assembly for metagenomes
To assist the reader, Table 3 summarizes the notation used in the following sections. An assembly consists of a list of contigs, and a mapping of each read to a contiguous area in a contig. The contigs are GENOVO 437 represented each as a list of DNA letters {bso}, where bso is the letter at position o of contig s. For each read xi, we have its contig number si, and its starting location oi within the contig. We denote by yi the alignment (orientation, insertions and deletions) required to match xi base-for-base with the contig. Bold-face letters, such as b or s, represent the set of variables of that type. The subscript i excludes the variable indexed i from the set. One way to assign a probability to every possible assembly is to describe the full process that generated the reads, from the creation of the originating sequences up to the sequencing noise copying the reads from the sequences. Such model is called a generative model. In this model, the observed variables are the reads xi, and the hidden variables are the sequences bso and the location from which each read was copied si, oi plus the alignment yi. An assembly is hence an assignment to the hidden variables of the model, and once we have a full assignment we can plug it into the model and get a probability. In our generative process, we first construct a potentially unbounded number of contigs (each has potentially unbounded length), then assign place holders for the beginning of reads in a coordinate system of contigs and offsets, and finally copy each read’s letters (with some noise) from the place to which it is mapped in the contig. We deal with the challenge of unbounded quantities by assuming that we have infinitely many of them. Since the number of reads is finite, only a finite number of infinitely many contigs will have any reads assigned to them, and these are the contigs we report. Hence, instead of first deciding how many contigs there are and then assigning the reads to them, we do the opposite—first partition the reads to clusters, and then assign each cluster of reads to a contig. Hence the number of reported contigs will be determined by the number of clusters in the partition generated for the reads. In order to randomly partition the reads we need a prior over partitions. The Chinese Restaurant Process (CRP) (Aldous, 1983) is such a prior. CRP(a, N) can generate any partition of N items by assigning the items to clusters incrementally. If the first i  1 items are assigned to clusters s1::si  1, then item i joins an existing cluster with a probability proportional to the number of items already assigned to that cluster, or it joins a new cluster with a probability proportional to a. The assignment probability for the last item, given the partition of the previous items is hence given by: p(sN ¼ s j s  N) ¼ 1 N  1 þ a  Ns s is an existing cluster a s represents a new cluster where Ns counts the number of items, not including item N, that are in cluster s. One property of the CRP is that the likelihood of a partition under this construction is invariant to the order of the items, and thus this last conditional probability is true for any item i, as we can assume that it is the last one. This conditional probability illustrates another desired property of the CRP, in which items are more likely to join clusters that already have a lot of items. The parameter a controls the expected number of clusters, which in our case represent contigs. In Supplementary Material (Additional File 1), we show how to set it correctly. The same idea is used to deal with the unbounded length of the contigs. We treat the contigs as infinite in length, stretching from minus infinity to infinity, and then draw the starting points of the reads from a distribution over the integers. The length of the contig is then simply determined by the distance between Table 3. Notation Table Variables xi vector of letters of read i (observed) yi alignment (insertions/deletions) of read i si contig index of read i oi starting offset of read i within contig bso DNA letter in offset o of contig s rs controls contig s length Other parameters a controls the number of contigs pins, pdel, pmis probability for base insertion, deletion, mismatch N number of reads Ns number of reads in contig s B the DNA letters alphabet (typically {A, C, G, T}) 438 LASERSON ET AL. the two most extreme reads. Since we cannot put a uniform prior over a countably infinite set, and since we want the reads to overlap with each other, we chose to use a symmetric geometric distribution G over the integers that pushes the reads towards the arbitrary ‘‘zero.’’ This center has no meaning except for marking the region of the integer line where reads are more likely to cluster. Formally, this is defined as follows: G(o; q) ¼ 0:5(1  q) jotj q o 6¼ 0 q o ¼ 0 The parameter r controls the length of the region from which reads are generated. The full generative model is described as: 1. Infinitely many letters in infinitely many contigs are sampled uniformly: bso~Uniform(B) 8s ¼ 1 ... 1, 8o ¼ 1... 1 where B is the alphabet of the bases (typically B¼fA,C, G, Tg). 2. N empty reads are randomly partitioned between these contigs: s~CRP(a, N) 3. The reads are assigned a starting point oi within each contig: qs~Beta(1, b) 8s oi~G(qs) 8i ¼ 1::N The distribution Beta(1, b) is over [0, 1] and has mean 1/(1 þ b). We set b ¼ 100. 4. We assume that the lengths li of the reads are already given. The read letters xi are copied (with some mismatches) from its contig si starting from position oi and according to the alignment yi (encoding orientation, insertions and deletions): xi, yi~A(li,si, oi, b, pins, pdel, pmis) 8i ¼ 1::N The distribution A represents the noise model known for the sequencing technology (454, Illumina, etc.). In particular, if each read letter has a pmis probability to be copied incorrectly, and the probabilities for insertions and deletions are pins and pdel respectively, then the log-probability log p(xi, yijoi, si, li, b) of generating a read in a specific orientation with nhit matches, nmis mismatches, nins insertions and ndel deletions is log 0:5 þ nhit log (1  pmis) þ nmis log pmis jBj  1  þ nins log (pins) þ ndel log (pdel) assuming an equal chance (0.5) to appear in each orientation and an independent noise model. Given an assembly, we denote the above quantity as scorei READ, where i is the read index. This model includes an infinite number of bso variables, which clearly cannot all be represented in the algorithm. The trick is to treat most of these variables as ‘‘unobserved,’’ effectively integrating them out during likelihood computations. The only observed bso letters are those that are supported by reads, i.e., have at least one read letter aligned to location (s, o). Hence, in the algorithm detailed below, if a contig letter loses its read support, it immediately becomes ‘‘unobserved.’’ 5.2. Algorithm Our algorithm starts from any initial assembly and takes steps in the space of all assemblies. Recall that each assembly is an assignment to the hidden variables in our model, namely the mapping of every read to the set of contigs (oi, si and yi for all i ¼ 1::N) and the contig letters bso in the locations that are supported by at least one read. The steps we take are designed to lead us to high probability regions in the space of assemblies. Each step either maximizes or samples a local conditional probability distribution over some GENOVO 439 hidden variables given the current values for all the other hidden variables. We list below the moves used to explore the space: 5.2.1. Consensus sequence. This type of move (Fig. 1B) updates the sequence letters bso by maximizing p(bsojx, y, s, o, bso) sequentially for each letter. Let ab so be the number of reads in the current assembly that align the letter b 2 B to location (s, o). Since we assumed a uniform prior over the contig letters, it is easy to see that the above conditional probability is maximized by setting bso ¼ arg maxb2B ab so (ties broken randomly), or in other words, by setting the sequence to be the consensus sequence of the reads in their current mapping. 5.2.2. Read mapping. This move (Fig. 1C) updates the read variables si, oi, yi, sequentially for each i, by sampling from the probability p(si ¼ s, oi ¼ o, yi ¼ yjx, yi, si, oi, b, r). First, we remove read i completely from the assembly. The above conditional probability decomposes to: p(si ¼ s, oi ¼ o, yi ¼ y j ) / p(si ¼ s j s  i)p(oi ¼ o j si ¼ s, qs)p(xi, yi ¼ y j si ¼ s, oi ¼ o, b) In order to make the sampling tractable, we reduce the space by considering for every location (s, o) only the best alignment y so as a possible value for yi: y so ¼ arg max y p(xi, yi ¼ y j si ¼ s, oi ¼ o, b): We compute y so using the banded Smith-Waterman algorithm, applied to both read orientations. This includes locations where the read only partially overlaps with the contig, in which case aligning a read letter to an unobserved contig letter entails a probabilistic price of 1=jBj per letter. Given the vector y*, we can now sample from a simpler distribution over all possible locations (s, o): p(si ¼ s, oi ¼ o j y ,  ) / p(si ¼ s j s  i)p(oi ¼ o j si ¼ s, qs)p(xi, y so j si ¼ s, oi ¼ o, b) / Ns  G(o; qs)  p(xi, y so j si ¼ s, oi ¼ o, b) The weights {Ns}, which count the number of reads in each sequence, encourage the read to join large contigs. As dictated by the CRP, we also include the case where s represents an empty contig, in which case we simply replace Ns with a in the formula above. In that case, the p(xi, y so j ) term also simplifies to 1=jBjli , where li is the length of the read. We set yi ¼ y sioi . As bad alignments render most (s, o) combinations extremely unlikely, we significantly speed up the above computation by filtering out combinations with implausible alignments. A very fast computation can detect locations that have at least one 10-mer in common with the read. This weak requirement is enough to filter out all but a few locations, making the optimization process efficient and scalable. A further speedup is achieved by caching common alignments. 5.2.3. Merge. During the run of the algorithm, as contigs grow and accumulate more reads, it is common to find two contigs that have overlapping ends. Even though the assembly that merges two such contigs into one would have a higher probability, it could take many iterations to reach that assembly if we rely only on the ‘‘Read Mapping’’ step. Reads are likely to move back and forth between the two contigs, especially if they contain a similar number of reads, although eventually chance will make one contig have many more reads than the other, and then the CRP will push the rest of the reads of the smaller contig to the larger one. To speed up this process, we designed a global move (Fig. 1D) where we detect such cases specifically and commit a merge if it increases the likelihood. Specifically, if there are more than 15 k-mers in common between the end of one contig and the beginning of another (we include all possible orientations), we align those ends, re-align the reads in the overlapping interval, and continue with this merged assembly if the overall likelihood had increased. 5.2.4. Fix indels. If the first read that mapped to a previously unsupported area of a contig has an insertion error, then that error is going to propagate into the contig representation. Hence, the ‘‘Read Mapping’’ step will make all the other reads that map to the same location align with an unnecessary insertion. In such locations, this step will propose to delete the corresponding letter in the contig and realign the reads, and accept the proposal if that improves the likelihood (Fig. 1E). We have a similar move for deletions. 440 LASERSON ET AL. 5.2.5. Geometric variables. The only hidden variables that are not represented specifically in the assembly are the rs parameters that control the length of the contigs. We set rs to the value that maximizes the probability p(rsj  ) (see Supplementary Material, Additional File 1, for details). Also, any shift by a constant of the starting locations oi of the reads in a particular contig does not change the assembly. Hence we simply use the shift that maximizes the overall probability. Both the above updates are local optimization moves that do not effect the current assembly. 5.2.6. Chimeric reads. Chimeric reads (Lasken and Stockwell, 2007) are reads with a prefix and a suffix matching distant locations in the genome. In our algorithm, these rare corrupted reads often find their way to the edge of an assembled contig, thus interfering with the assembly process. To deal with this problem we occasionally (every 5 iterations) disassemble the reads sitting in the edge of a contig, thus allowing other correct reads or contigs to merge with it and increase the likelihood beyond that of the original state. If such a disassembled read was not chimeric, it will reassemble correctly in the next iteration, thus keeping the likelihood the same as before. 5.3. Computation of scoredenovo We adopt a view of the metagenomic sequence assembly problem in which two competing constraints are traded off. First, we wish a compact summary of the read dataset. The reconstructed sequences should be reasonably covered and small in number. Second, each read should have a reasonable point of origin in our assembly. scoredenovo coherently trades off these constraints. Given an assembly, denote by S the number of contigs, and by L the total length of all the contigs. We measure the quality of an assembly using the expression X i scorei READ  log (jBj)L þ log (jBj)V0S: The first term in the above score penalizes for read errors and the second for contig length, embodying the trade off required for a good assembly. For example, the first term will be optimized by a naive assembly that lays each read in its own contig (so that it is an exact copy of it), but the large number of total bases will incur a severe penalty from the second term. These two terms interact well since they represent probabilities—the first term is the (log) probability for generating each noisy read from the contig bases it aligns to, and the second term is the (log) probability for generating (uniformly) each contig letter. In other words, if you put a read in a region that already has the support of other reads, you pay only for the disagreements between the read and the contig. But if you put that read in an unsupported region, that is, the read is the first one to cover this contig region, then you pay log(0.25) for generating each new letter. If the read does not align well to any supported region in the current assembly, it will be more beneficial to use the read to extend existing contigs or create new contigs than to pay the high ‘‘disagreement’’ cost due to bad alignment. The third term in the score ensures a minimal overlap of V0 bases between two consecutive reads. To understand this, assume two reads have an overlap of V bases. If you split the contig into two at this position, the third term gives you a ‘‘bonus’’ of log (jBj)V0, while the second term penalizes you for log (jBj)V for adding V new bases to the assembly. Hence, we will prefer to merge the sequences if V > V0. We set V0 to 20. To be able to compare the above score across different datasets, we normalized it by first subtracting from it the score of a naive assembly that puts each read in its own contig, and then dividing this difference by the total length of all the reads in the dataset. We define scoredenovo to be this normalized score. See Supplementary Material, Additional File 1, for another derivation of scoredenovo, based on our model.",Assembly,"genovo  novo assembly  metagenomes
 assist  reader table  summarize  notation use   follow section  assembly consist   list  contigs   map   read   contiguous area   contig  contigs  genovo  represent    list  dna letter {bso}  bso   letter  position   contig    read     contig number    start location  within  contig  denote    alignment orientation insertions  deletions require  match  baseforbase   contig boldface letter      represent  set  variables   type  subscript  exclude  variable index    set one way  assign  probability  every possible assembly   describe  full process  generate  read   creation   originate sequence    sequence noise copy  read   sequence  model  call  generative model   model  observe variables   read    hide variables   sequence bso   location    read  copy   plus  alignment   assembly  hence  assignment   hide variables   model      full assignment   plug    model  get  probability   generative process  first construct  potentially unbounded number  contigs   potentially unbounded length  assign place holders   begin  read   coordinate system  contigs  offset  finally copy  read letter   noise   place     map   contig  deal   challenge  unbounded quantities  assume    infinitely many   since  number  read  finite   finite number  infinitely many contigs    read assign       contigs  report hence instead  first decide  many contigs     assign  read      opposite—first partition  read  cluster   assign  cluster  read   contig hence  number  report contigs   determine   number  cluster   partition generate   read  order  randomly partition  read  need  prior  partition  chinese restaurant process crp aldous     prior crpa   generate  partition   items  assign  items  cluster incrementally   first    items  assign  cluster s1si    item  join  exist cluster   probability proportional   number  items already assign   cluster   join  new cluster   probability proportional    assignment probability   last item give  partition   previous items  hence give  psn                   exist cluster   represent  new cluster   count  number  items  include item     cluster  one property   crp    likelihood   partition   construction  invariant   order   items  thus  last conditional probability  true   item     assume     last one  conditional probability illustrate another desire property   crp   items   likely  join cluster  already   lot  items  parameter  control  expect number  cluster    case represent contigs  supplementary material additional file   show   set  correctly   idea  use  deal   unbounded length   contigs  treat  contigs  infinite  length stretch  minus infinity  infinity   draw  start point   read   distribution   integers  length   contig   simply determine   distance  table  notation table variables  vector  letter  read  observe  alignment insertionsdeletions  read   contig index  read   start offset  read  within contig bso dna letter  offset   contig   control contig  length  parameters  control  number  contigs pin pdel pmis probability  base insertion deletion mismatch  number  read  number  read  contig    dna letter alphabet typically {   }  laserson    two  extreme read since  cannot put  uniform prior   countably infinite set  since  want  read  overlap     choose  use  symmetric geometric distribution    integers  push  read towards  arbitrary zero  center   mean except  mark  region   integer line  read   likely  cluster formally   define  follow       jotj          parameter  control  length   region   read  generate  full generative model  describe   infinitely many letter  infinitely many contigs  sample uniformly bso~uniformb              alphabet   base typically b¼fac     empty read  randomly partition   contigs ~crpa    read  assign  start point  within  contig ~beta   ~gqs     distribution beta        mean     set      assume   lengths    read  already give  read letter   copy   mismatch   contig  start  position   accord   alignment  encode orientation insertions  deletions  ~alisi   pin pdel pmis     distribution  represent  noise model know   sequence technology  illumina etc  particular   read letter   pmis probability   copy incorrectly   probabilities  insertions  deletions  pin  pdel respectively   logprobability log pxi yijoi     generate  read   specific orientation  nhit match nmis mismatch nins insertions  ndel deletions  log   nhit log   pmis  nmis log pmis jbj     nins log pin  ndel log pdel assume  equal chance   appear   orientation   independent noise model give  assembly  denote   quantity  scorei read     read index  model include  infinite number  bso variables  clearly cannot   represent   algorithm  trick   treat    variables  unobserved effectively integrate    likelihood computations   observe bso letter     support  read    least one read letter align  location   hence   algorithm detail    contig letter lose  read support  immediately become unobserved  algorithm  algorithm start   initial assembly  take step   space   assemblies recall   assembly   assignment   hide variables   model namely  map  every read   set  contigs            contig letter bso   locations   support   least one read  step  take  design  lead   high probability regions   space  assemblies  step either maximize  sample  local conditional probability distribution   genovo  hide variables give  current value     hide variables  list   move use  explore  space  consensus sequence  type  move fig  update  sequence letter bso  maximize pbsojx     sequentially   letter let     number  read   current assembly  align  letter     location   since  assume  uniform prior   contig letter   easy  see    conditional probability  maximize  set bso  arg maxb2b   tie break randomly    word  set  sequence    consensus sequence   read   current map  read map  move fig  update  read variables    sequentially     sample   probability psi        yjx      first  remove read  completely   assembly   conditional probability decompose  psi            psi      ipoi       qspxi             order  make  sample tractable  reduce  space  consider  every location     best alignment     possible value      arg max  pxi             compute   use  band smithwaterman algorithm apply   read orientations  include locations   read  partially overlap   contig   case align  read letter   unobserved contig letter entail  probabilistic price  jbj per letter give  vector *    sample   simpler distribution   possible locations   psi            psi      ipoi       qspxi                 pxi            weight {}  count  number  read   sequence encourage  read  join large contigs  dictate   crp  also include  case   represent  empty contig   case  simply replace      formula    case  pxi     term also simplify  jbjli      length   read  set    sioi   bad alignments render    combinations extremely unlikely  significantly speed    computation  filter  combinations  implausible alignments   fast computation  detect locations    least one mer  common   read  weak requirement  enough  filter      locations make  optimization process efficient  scalable   speedup  achieve  cache common alignments  merge   run   algorithm  contigs grow  accumulate  read   common  find two contigs   overlap end even though  assembly  merge two  contigs  one would   higher probability  could take many iterations  reach  assembly   rely    read map step read  likely  move back  forth   two contigs especially   contain  similar number  read although eventually chance  make one contig  many  read       crp  push  rest   read   smaller contig   larger one  speed   process  design  global move fig    detect  case specifically  commit  merge   increase  likelihood specifically       kmers  common   end  one contig   begin  another  include  possible orientations  align  end realign  read   overlap interval  continue   merge assembly   overall likelihood  increase  fix indels   first read  map   previously unsupported area   contig   insertion error   error  go  propagate   contig representation hence  read map step  make    read  map    location align   unnecessary insertion   locations  step  propose  delete  correspond letter   contig  realign  read  accept  proposal   improve  likelihood fig     similar move  deletions  laserson    geometric variables   hide variables    represent specifically   assembly    parameters  control  length   contigs  set    value  maximize  probability prsj   see supplementary material additional file   detail also  shift   constant   start locations    read   particular contig   change  assembly hence  simply use  shift  maximize  overall probability    update  local optimization move    effect  current assembly  chimeric read chimeric read lasken  stockwell   read   prefix   suffix match distant locations   genome   algorithm  rare corrupt read often find  way   edge   assemble contig thus interfere   assembly process  deal   problem  occasionally every  iterations disassemble  read sit   edge   contig thus allow  correct read  contigs  merge    increase  likelihood beyond    original state    disassemble read   chimeric   reassemble correctly   next iteration thus keep  likelihood      computation  scoredenovo  adopt  view   metagenomic sequence assembly problem   two compete constraints  trade  first  wish  compact summary   read dataset  reconstruct sequence   reasonably cover  small  number second  read    reasonable point  origin   assembly scoredenovo coherently trade   constraints give  assembly denote    number  contigs     total length    contigs  measure  quality   assembly use  expression   scorei read  log jbjl  log jbjv0s  first term    score penalize  read errors   second  contig length embody  trade  require   good assembly  example  first term   optimize   naive assembly  lay  read    contig      exact copy     large number  total base  incur  severe penalty   second term  two term interact well since  represent probabilities— first term   log probability  generate  noisy read   contig base  align    second term   log probability  generate uniformly  contig letter   word   put  read   region  already   support   read  pay    disagreements   read   contig    put  read   unsupported region    read   first one  cover  contig region   pay log  generate  new letter   read   align well   support region   current assembly     beneficial  use  read  extend exist contigs  create new contigs   pay  high disagreement cost due  bad alignment  third term   score ensure  minimal overlap   base  two consecutive read  understand  assume two read   overlap   base   split  contig  two   position  third term give   bonus  log jbjv0   second term penalize   log jbjv  add  new base   assembly hence   prefer  merge  sequence      set      able  compare   score across different datasets  normalize   first subtract    score   naive assembly  put  read    contig   divide  difference   total length    read   dataset  define scoredenovo    normalize score see supplementary material additional file   another derivation  scoredenovo base   model",5
127,metaSPAdes,"metaSPAdes: a new versatile metagenomic assembler
Detecting and masking strain variation Genomic differences between related strains often result in “bulges” and “tips” in the de Bruijn graphs that are not unlike artifacts caused by sequencing errors in genome assembly (Pevzner et al. 2004; Zerbino and Birney 2008). For example, a sequencing error often results in a bulge formed by two short alternative paths between the same vertices in the de Bruijn graph, a “correct” path with high coverage and an “erroneous” path with low coverage. Similarly, a substitution or a small indel in a rare strain (compared with an abundant strain) often results in a bulge formed by a high-coverage path corresponding to the abundant strain and an alternative low-coverage path corresponding to the rare strain. Aiming at the consensus assembly of a strain mixture, metaSPAdes masks the majority of strain differences using a modification of the SPAdes procedures for masking sequencing errors (the algorithms for removal of tips, “simple” bulges [Bankevich et al. 2012], and “complex” bulges [Nurk et al. 2013]). metaSPAdes uses more aggressive settings than the ones used in assemblies of isolates; for example, it collapses larger bulges and removes longer tips than SPAdes. We note that the bulge projection approach in SPAdes improves on the originally proposed bulge removal approach (Pevzner et al. 2004; Zerbino and Birney 2008) used in most existing assemblers since it stores valuable information about the processed bulges (see “Bulge Projection Approach” in the Supplemental Material). This feature is important for the repeat resolution approach in metaSPAdes described below. Analyzing filigree edges in the assembly graph In addition to single-nucleotide variants and small indels, strain variation is often manifested as highly diverged regions, insertions of mobile elements, rearrangements, large deletions, parallel gene transfer, etc. The green edges in the assembly graph shown in Figure 3 result from an additional copy of a mobile element in a rare strain2 (compared with the abundant strain1), while the blue edge corresponds to a horizontally transferred gene (or a highly diverged genomic region) in a rare strain3 (compared to the abundant strain1). Such edges fragment contigs corresponding to the abundant strain1; for example, the green edges in Figure 3 (bottom right) break the edge c into three shorter edges. We note that the edges in the assembly graph are condensed; that is, they represent nonbranching paths formed by k-mers. An external file that holds a picture, illustration, etc. Object name is 824f03.jpg Figure 3. The de Bruijn graphs of three strains and their strain mixture. The figure shows only a small subgraph of the de Bruijn graph. The abundant strain (strain1) is shown by thick lines, and the rare strains (strain2 and strain3) are shown by thin lines. The genomic repeat R is shown in red. (Top left) The de Bruijn graph of the abundant strain1. (Top right) The rare strain2 differs from the abundant strain1 by an insertion of an additional copy or repeat R. The two breakpoint edges resulting from this insertion are shown in green. These filigree edges are not removed by the graph simplification procedures in the standard assembly tools aimed at isolates. (Bottom left) The rare strain3 differs from the abundant strain1 by an insertion of a horizontally transferred gene (or a highly diverged genomic region). (Bottom right) The de Bruijn graph of the mixture of three strains. We refer to edges originating from rare strain variants within the assembly graph of a strain mixture as filigree edges. Traditional genome assemblers use a global threshold on read coverage to remove the low-coverage edges (that typically result from sequencing errors) from the assembly graph during the graph simplification step. However, this approach does not work well for metagenomic assemblies, since there is no global threshold that (1) removes edges corresponding to rare strains and (2) preserves edges corresponding to rare species. Similarly to IDBA-UD and MEGAHIT, metaSPAdes analyzes the coverage ratios between adjacent edges in the assembly graph, classifying edges with low-coverage ratios as potential filigree edges. We denote the coverage of an edge e in the assembly graph as cov(e) and define the coverage cov(v) of a vertex v as the maximum of cov(e) over all edges e incident to v. Given an edge e incident to a vertex v and a threshold ratio (the default value is 10), a vertex v predominates an edge e if its coverage is significantly higher than the coverage of the edge e; that is, if ratio · cov(e) < cov(v). An edge (v,w) is weak if it is predominated by either v or w. Note that filigree edges are often classified as weak since their coverage is much lower than the coverage of adjacent edges resulting from abundant strains. metaSPAdes disconnects all weak edges from their predominating vertices in the assembly graph. Disconnection of a weak edge (v,w) in the assembly graph from its starting vertex v (ending vertex w) is simply a removal of its first (last) k-mer rather than removal of the entire condensed edge. We emphasize that, in contrast to IDBA-UD and MEGAHIT, we disconnect rather than remove weak edges in the assembly graph since our goal is to preserve the information about rare strains whenever possible, that is, when it does not lead to a deterioration of the consensus backbone. Repeat resolution with exSPAnder exSPAnder (Prjibelski et al. 2014; Vasilinetc et al. 2015; Antipov et al. 2016) is a module of SPAdes that combines various sources of information (e.g., paired reads or long error-prone reads) for resolving repeats and scaffolding in the assembly graph. Starting from a path consisting of a single condensed edge in the assembly graph, exSPAnder iteratively attempts to extend it to a longer path that represents a contiguous segment of the genome (genomic path). To extend a path, exSPAnder selects one of its extension edges (edges that start at the terminal vertex of this path). Choice of the extension edge is controlled by the decision rule that evaluates whether a particular extension edge is sufficiently supported by the data, while other extension edges are not (given the existing path). exSPAnder further removes overlaps between generated genomic paths (overlap reduction step) and outputs the strings spelled by the resulting paths as a set of contigs. metaSPAdes modifies the decision rule of exSPAnder to account for the local read coverage, denoted localCov, of the specific genomic region that is being reconstructed during the path extension process. For details, see “Modifying the Decision Rule in exSPAnder for Metagenomic Data” in the Supplemental Material. The value localCov is estimated as the minimum across the average coverages of the edges in the path that is being extended. Taking minimum (rather than the average) coverage excludes the repetitive edges in the path from consideration and typically underestimates the real coverage of the region, making the decision rule more conservative. A new metagenomic decision rule in metaSPAdes Some intergenomic repeats between species of different abundances can be resolved based on the differences in the depth of read coverage (Haider et al. 2014; Namiki et al. 2012). metaSPAdes introduces an additional metagenomics-specific decision rule that filters out unlikely path extensions using the coverage estimate of the region that is being reconstructed (Fig. 4). It often allows metaSPAdes to pass through long inter-species repeats during reconstruction of abundant species. metaSPAdes applies a new decision rule described below only if the paired reads failed to provide sufficient evidence to discriminate between extension edges. An external file that holds a picture, illustration, etc. Object name is 824f04.jpg Figure 4. Applying the metagenomics-specific decision rule for repeat resolution. The figure shows only a small subgraph of the assembly graph. (A) The path that is currently being extended (formed by green edges) along with its blue extension edges e and e′. (B) The short-edge traversal from the end of the extension edge e. The dotted curve shows the boundary frontier(e) of the traversal. The edges in the set next(e) are shown in red with low-coverage edges represented as dashed arrows (other edges in next(e) are represented as solid arrows). Since all edges in next(e) have low coverage, the edge e is ruled out as an unlikely extension candidate. (C) The short-edge traversal from the end of the extension edge e′. (D) Since e′ is a single extension edge that was not ruled out (there is a solid edge in next(e′)), it is added to the growing path and the extension process continues. An edge in the assembly graph is called long if its length exceeds a certain threshold (1500 bp by default) and short otherwise. We say that a long edge e2 follows a long edge e1 in a genomic path if all edges between the end of e1 and the start of e2 in this path are short. While considering an extension edge e, metaSPAdes performs a directed traversal of the graph (Fig. 4B), starting from the end of e and walking along the short edges. We define the set of all vertices that are reached by this traversal as frontier(e) and consider the set next(e) of all long edges starting in frontier(e). This procedure is aimed at finding nonrepetitive long edges that can follow e in the (unknown) genomic path. We classify an edge in the set next(e) as a low-coverage edge if the coverage estimate of the region that is being reconstructed, localCov, exceeds its coverage at least by a factor β (the default value β = 2). If all edges in next(e) are low-coverage edges, then e is considered an unlikely candidate for an extension of the current path. If all but a single edge e′ represent unlikely extensions, the path is extended by e′ (Fig. 4C). Utilizing strain differences for repeat resolution in metaSPAdes Safonova et al. (2015) showed that differences between haplomes can be used to improve the quality of consensus assembly of a highly polymorphic diploid genome. metaSPAdes capitalizes on the similar observation that the differences between strains can be, somewhat counter-intuitively, used to improve the quality of consensus assembly of a strain mixture. In particular, contigs generated prior to masking strain differences in assembly graph and thus representing genomic fragments of individual strains (strain-contigs) often provide additional long-range information for reconstruction of a strain-mixture backbone. Inspired by dipSPAdes (Safonova et al. 2015), metaSPAdes uses the following pipeline that includes two launches of exSPAnder (Fig. 5). Generating strain-contigs. After constructing the assembly graph (that encodes both abundant and rare strains), we launch exSPAnder to generate a set of strain-contigs representing both rare and abundant strains (Fig. 5C). Strain-contigs are not subjected to the default overlap reduction step in exSPAnder. Transforming assembly graph into consensus assembly graph. metaSPAdes identifies and masks rare strain variants, resulting in the consensus assembly graph (Fig. 5D). Generating strain-paths in the consensus assembly graph. Capitalizing on the bulge projection approach (see “Bulge Projection Approach” in the Supplemental Material), metaSPAdes reconstructs paths in the consensus assembly graph corresponding to strain-contigs, referred to as strain-paths (Fig. 5E). Repeat resolution using strain-paths. This step utilizes the hybrid mode of exSPAnder originally developed to incorporate long error-prone Pacific Biosciences and Oxford Nanopore reads in the repeat resolution process (Ashton et al. 2014; Labonté et al. 2015; Antipov et al. 2016). Instead of working with long error-prone reads, we modified exSPAnder to work with virtual reads spelled by the strain-paths to facilitate resolution of repeats in the consensus assembly graph (Fig. 5F). An external file that holds a picture, illustration, etc. Object name is 824f05.jpg Figure 5. Repeat resolution in metagenomic assembly. (A) One of two identical copies of a long (longer than the insert size) repeat R (red) in the abundant strain has mutated into a unique genomic “green” region R′ in the rare strain. (B) The assembly graph resulting from a mixture of reads from the abundant and rare strains. Two alternative paths between the start and the end of the green edge (one formed by a single green edge and another formed by two black and one red edge) form a bulge. (C) The strain-contig spanning R′ (shown by green dashed line) constructed by exSPAnder at the “generating strain-contigs” step. (D) Masking of the strain variation at the “transforming assembly graph into consensus assembly graph” step leads to a projection of a bulge (formed by red and green edges) and results in the consensus assembly graph shown in E. The blue arrows emphasize that SPAdes projects rather than deletes bulges, facilitating the subsequent reconstruction of strain-paths in the consensus assembly graph. (E) Reconstruction of the strain-path (green dotted line), corresponding to a strain-contig (green dashed line) at the “generating strain-paths in the consensus assembly graph” step. (F) At the “repeat resolution using strain-paths” step, metaSPAdes utilizes both strain-paths and paired reads to resolve repeats in the consensus graph. The green dotted strain-path from E is used as additional information to reconstruct the consensus contig cRd spanning the long repeat. Note that in the example in Figure 5, the long red repeat with multiplicity 2 in the abundant strain is resolved because of the variants (diverged green copy of the repeat) in the rare strain. Scaling metaSPAdes Since some metagenomic data sets contain billions of reads, metagenomic assemblers have to be optimized with respect to both speed and memory footprint (Nagarajan and Pop 2013). “Reducing Running Time and Memory Footprint of metaSPAdes” in the Supplemental Material describes efforts to scale metaSPAdes for assembling large metagenomic data sets.",Assembly,"metaspades  new versatile metagenomic assembler
detecting  mask strain variation genomic differences  relate strain often result  “bulges”  “tips”    bruijn graph    unlike artifacts cause  sequence errors  genome assembly pevzner    zerbino  birney   example  sequence error often result   bulge form  two short alternative paths    vertices    bruijn graph  “correct” path  high coverage   “erroneous” path  low coverage similarly  substitution   small indel   rare strain compare   abundant strain often result   bulge form   highcoverage path correspond   abundant strain   alternative lowcoverage path correspond   rare strain aim   consensus assembly   strain mixture metaspades mask  majority  strain differences use  modification   spade procedures  mask sequence errors  algorithms  removal  tip “simple” bulge bankevich     “complex” bulge nurk    metaspades use  aggressive settings   ones use  assemblies  isolate  example  collapse larger bulge  remove longer tip  spade  note   bulge projection approach  spade improve   originally propose bulge removal approach pevzner    zerbino  birney  use   exist assemblers since  store valuable information   process bulge see “bulge projection approach”   supplemental material  feature  important   repeat resolution approach  metaspades describe  analyze filigree edge   assembly graph  addition  singlenucleotide variants  small indels strain variation  often manifest  highly diverge regions insertions  mobile elements rearrangements large deletions parallel gene transfer etc  green edge   assembly graph show  figure  result   additional copy   mobile element   rare strain2 compare   abundant strain1   blue edge correspond   horizontally transfer gene   highly diverge genomic region   rare strain3 compare   abundant strain1  edge fragment contigs correspond   abundant strain1  example  green edge  figure  bottom right break  edge   three shorter edge  note   edge   assembly graph  condense    represent nonbranching paths form  kmers  external file  hold  picture illustration etc object name  824f03jpg figure    bruijn graph  three strain   strain mixture  figure show   small subgraph    bruijn graph  abundant strain strain1  show  thick line   rare strain strain2  strain3  show  thin line  genomic repeat   show  red top leave   bruijn graph   abundant strain1 top right  rare strain2 differ   abundant strain1   insertion   additional copy  repeat   two breakpoint edge result   insertion  show  green  filigree edge   remove   graph simplification procedures   standard assembly tool aim  isolate bottom leave  rare strain3 differ   abundant strain1   insertion   horizontally transfer gene   highly diverge genomic region bottom right   bruijn graph   mixture  three strain  refer  edge originate  rare strain variants within  assembly graph   strain mixture  filigree edge traditional genome assemblers use  global threshold  read coverage  remove  lowcoverage edge  typically result  sequence errors   assembly graph   graph simplification step however  approach   work well  metagenomic assemblies since    global threshold   remove edge correspond  rare strain   preserve edge correspond  rare species similarly  idbaud  megahit metaspades analyze  coverage ratios  adjacent edge   assembly graph classify edge  lowcoverage ratios  potential filigree edge  denote  coverage   edge    assembly graph  cove  define  coverage covv   vertex    maximum  cove   edge  incident   give  edge  incident   vertex    threshold ratio  default value    vertex  predominate  edge    coverage  significantly higher   coverage   edge     ratio  cove  covv  edge   weak    predominate  either    note  filigree edge  often classify  weak since  coverage  much lower   coverage  adjacent edge result  abundant strain metaspades disconnect  weak edge   predominate vertices   assembly graph disconnection   weak edge    assembly graph   start vertex  end vertex   simply  removal   first last kmer rather  removal   entire condense edge  emphasize   contrast  idbaud  megahit  disconnect rather  remove weak edge   assembly graph since  goal   preserve  information  rare strain whenever possible       lead   deterioration   consensus backbone repeat resolution  exspander exspander prjibelski    vasilinetc    antipov      module  spade  combine various source  information  pair read  long errorprone read  resolve repeat  scaffold   assembly graph start   path consist   single condense edge   assembly graph exspander iteratively attempt  extend    longer path  represent  contiguous segment   genome genomic path  extend  path exspander select one   extension edge edge  start   terminal vertex   path choice   extension edge  control   decision rule  evaluate whether  particular extension edge  sufficiently support   data   extension edge   give  exist path exspander  remove overlap  generate genomic paths overlap reduction step  output  string spell   result paths   set  contigs metaspades modify  decision rule  exspander  account   local read coverage denote localcov   specific genomic region    reconstruct   path extension process  detail see “modifying  decision rule  exspander  metagenomic data”   supplemental material  value localcov  estimate   minimum across  average coverages   edge   path    extend take minimum rather   average coverage exclude  repetitive edge   path  consideration  typically underestimate  real coverage   region make  decision rule  conservative  new metagenomic decision rule  metaspades  intergenomic repeat  species  different abundances   resolve base   differences   depth  read coverage haider    namiki    metaspades introduce  additional metagenomicsspecific decision rule  filter  unlikely path extensions use  coverage estimate   region    reconstruct fig   often allow metaspades  pass  long interspecies repeat  reconstruction  abundant species metaspades apply  new decision rule describe     pair read fail  provide sufficient evidence  discriminate  extension edge  external file  hold  picture illustration etc object name  824f04jpg figure  apply  metagenomicsspecific decision rule  repeat resolution  figure show   small subgraph   assembly graph   path   currently  extend form  green edge along   blue extension edge   ′   shortedge traversal   end   extension edge   dot curve show  boundary frontiere   traversal  edge   set nexte  show  red  lowcoverage edge represent  dash arrows  edge  nexte  represent  solid arrows since  edge  nexte  low coverage  edge   rule    unlikely extension candidate   shortedge traversal   end   extension edge ′  since ′   single extension edge    rule     solid edge  nexte′   add   grow path   extension process continue  edge   assembly graph  call long   length exceed  certain threshold    default  short otherwise  say   long edge  follow  long edge    genomic path   edge   end     start     path  short  consider  extension edge  metaspades perform  direct traversal   graph fig  start   end    walk along  short edge  define  set   vertices   reach   traversal  frontiere  consider  set nexte   long edge start  frontiere  procedure  aim  find nonrepetitive long edge   follow    unknown genomic path  classify  edge   set nexte   lowcoverage edge   coverage estimate   region    reconstruct localcov exceed  coverage  least   factor   default value      edge  nexte  lowcoverage edge    consider  unlikely candidate   extension   current path     single edge ′ represent unlikely extensions  path  extend  ′ fig  utilize strain differences  repeat resolution  metaspades safonova    show  differences  haplomes   use  improve  quality  consensus assembly   highly polymorphic diploid genome metaspades capitalize   similar observation   differences  strain   somewhat counterintuitively use  improve  quality  consensus assembly   strain mixture  particular contigs generate prior  mask strain differences  assembly graph  thus represent genomic fragment  individual strain straincontigs often provide additional longrange information  reconstruction   strainmixture backbone inspire  dipspades safonova    metaspades use  follow pipeline  include two launch  exspander fig  generate straincontigs  construct  assembly graph  encode  abundant  rare strain  launch exspander  generate  set  straincontigs represent  rare  abundant strain fig  straincontigs   subject   default overlap reduction step  exspander transform assembly graph  consensus assembly graph metaspades identify  mask rare strain variants result   consensus assembly graph fig  generate strainpaths   consensus assembly graph capitalize   bulge projection approach see “bulge projection approach”   supplemental material metaspades reconstruct paths   consensus assembly graph correspond  straincontigs refer   strainpaths fig  repeat resolution use strainpaths  step utilize  hybrid mode  exspander originally develop  incorporate long errorprone pacific biosciences  oxford nanopore read   repeat resolution process ashton    labonté    antipov    instead  work  long errorprone read  modify exspander  work  virtual read spell   strainpaths  facilitate resolution  repeat   consensus assembly graph fig   external file  hold  picture illustration etc object name  824f05jpg figure  repeat resolution  metagenomic assembly  one  two identical copy   long longer   insert size repeat  red   abundant strain  mutate   unique genomic “green” region ′   rare strain   assembly graph result   mixture  read   abundant  rare strain two alternative paths   start   end   green edge one form   single green edge  another form  two black  one red edge form  bulge   straincontig span ′ show  green dash line construct  exspander   “generating straincontigs” step  mask   strain variation   “transforming assembly graph  consensus assembly graph” step lead   projection   bulge form  red  green edge  result   consensus assembly graph show    blue arrows emphasize  spade project rather  delete bulge facilitate  subsequent reconstruction  strainpaths   consensus assembly graph  reconstruction   strainpath green dot line correspond   straincontig green dash line   “generating strainpaths   consensus assembly graph” step    “repeat resolution use strainpaths” step metaspades utilize  strainpaths  pair read  resolve repeat   consensus graph  green dot strainpath    use  additional information  reconstruct  consensus contig crd span  long repeat note    example  figure   long red repeat  multiplicity    abundant strain  resolve    variants diverge green copy   repeat   rare strain scale metaspades since  metagenomic data set contain billions  read metagenomic assemblers    optimize  respect   speed  memory footprint nagarajan  pop  “reducing run time  memory footprint  metaspades”   supplemental material describe efforts  scale metaspades  assemble large metagenomic data set",5
128,VirusTap,"VirusTAP: Viral Genome-Targeted Assembly Pipeline
Clinical Specimens and NGS Short Reads for Viral Genome Assembly Sample NGS short reads were prepared using stool specimens from a patient with rotavirus gastroenteritis and analyzed as described previously (Mizukoshi et al., 2014). In brief, total RNA was prepared from patient feces, followed by RNA-seq library preparation using a ScriptSeq v2 RNA-seq library preparation kit (Epicentre, Madison, WI, USA). Deep sequencing was performed to obtain 120-mer paired-end (PE) short reads with a MiSeq Reagent kit v2 (Illumina, San Diego, CA, USA). The sample short-read sequences have been deposited in the DNA Data Bank of Japan (DDBJ; accession number: DRA004165). The study protocol was approved by the institutional medical ethics committee of the National Institute of Infectious Diseases in Japan (Approval No. 576), and it was conducted according to the Declaration of Helsinki Principles. NGS Read Processing VirusTAP accepts single- and PE reads (#1 in Figure 1). Read quality trimming was performed using the skewer (Jiang et al., 2014) or fastq-mcf program in the ea-utils package1, with an additional trimming filter for unreliable sequences after a user specified quality score (#2 in Figure 1). Host read subtraction by read-mapping was performed with the bwasw program (version 0.7.9a-r786; Li and Durbin, 2009) against ribosomal RNAs (16, 18, 23, 28, 5S and internal transcribed spacers rRNA were retrieved from the following ftp site2), bacterial genome sequences3 and the latest host organism genome sequences4 (#3–1 in Figure 1). FIGURE 1 www.frontiersin.org FIGURE 1. Schematic representation of the VirusTAP procedures. To further remove residual non-virus sequence reads, the above subtracted PE reads are assembled with IDBA-UD (Peng et al., 2012), followed by the filtering of non-virus PE reads with megablast (v. 2.2.26) (Altschul et al., 1997) and RAPSearch2 (v. 2.16) (Zhao et al., 2012) search against the customized viral nucleotide/protein sequences from the NCBI nt/nr database based on virus taxonomy, excluding bacteriophages5 (#3–2 in Figure 1). No significant e-value less than either 1e – 10 for megablast or e – 30 for RAPSearch2 was determined for non-virus contigs, and residual PE reads were further subtracted for the following process. This non-virus filtering step will be repeated up to 10 times or until non-virus contigs disappear. After removing the non-virus sequencing reads, broken PE reads are also removed for the following de novo assembly. The de novo assembly pipeline can be selected from the following four pipelines (#4–1 in Figure 1): A5-miseq (Coil et al., 2015), Platanus (version 1.2.1) (Kajitani et al., 2014) with PriceTI (Ruby et al., 2013), and IDBA-UD (Peng et al., 2012) with PriceTI (Ruby et al., 2013). PriceTI (v. 1.2) (Ruby et al., 2013) performs PE iterative contig extension from pre-assembly contigs by Platanus or IDBA-UD. A5-miseq is one of the most recommended de novo assemblers for general NGS PE reads because assemble accuracy for the resultant scaffolds can be checked using the resulting “XXX.scaffolds.fastq” file based on the read-mapping score. The contigs were divided at the nucleotide position, where the Phred score is less than 93 and the resulting sequences shorter than 100 bp are removed. The final scaffolds were subjected to bwasw read mapping and a megablast homology search against the NCBI nt database; the alignment results are visualized at the web page. Results and Discussion Basic VirusTAP Procedure Briefly, a schematic representation of VirusTAP is shown in Figure 1. The VirusTAP process starts with the quality trimming, followed by host- and bacteria-related read subtraction. The read subtraction is performed in the following two steps: first, read-mapping with the bwasw program to rRNA sequences, bacterial genome sequences, and selected virus-host genomes, such as human, mouse, or monkey, was performed; second, non-virus filtering against a virus nucleotide/protein database, excluding bacteriophages, can be selected for further subtraction to extract possible virus-related NGS reads. The above-described highly extensive non-virus read subtraction facilitates effective de novo assembly and prompt analysis, achieving both time savings and accuracy. VirusTAP Performance Comparison with Direct Metagenomic Assembly Methods Using metagenomic RNA-seq short reads obtained from stool specimens from a patient with rotavirus gastroenteritis (Mizukoshi et al., 2014), the performance of VirusTAP was compared with that of other de novo assembly methods without subtraction treatment. For instance, direct metagenomic assembly without subtraction using a CLC genome workbench v.8.5.1 (QIAGEN, Aarhus Denmark) or A5-miseq produced 23,937 or 13,365 scaffolds, respectively; VirusTAP significantly reduced the results to 15 scaffolds (Figure 2A step 4). FIGURE 2 www.frontiersin.org FIGURE 2. Performance comparison of VirusTAP with non-subtraction methods. (A) Sample metagenomic RNA-seq reads obtained from the rotavirus gastroenteritis patient were analyzed by VirusTAP and non-subtraction methods. (B) Pair-wise alignment view of the most similar hit for each scaffold. Eleven scaffolds showed similarity to eleven segments of rotavirus genome sequences. NCBI Web blast search (BLASTN or BLASTX) can be directly performed for each scaffold by clicking the button to reconfirm the similarity. (C) Visualization of the read-mapping result in the scaffold 4. Viral quasispecies and read coverage on each scaffold can be confirmed by the read-mapping viewer using the bam file in the downloaded results package. Those direct metagenomic assembly methods generated unexpectedly abundant scaffolds related to multiple organisms at step 4 in Figure 2A. In particular, over 80% of scaffolds were composed of bacterial sequences (19,525 and 11,266 scaffolds by CLC genome and A5-miseq, respectively; Figure 2A), suggesting that subsequent homology searches and taxonomy classifications are very laborious and time-consuming procedures to identify the final viral genome sequence. Indeed, the entire VirusTAP process for these sample reads required ∼17 min to obtain 11 segments of the rotavirus genome sequence, whereas the other two methods required more than 10 h, because of the time-consuming de novo assembly and homology search for all abundant scaffolds (Figure 2A). In addition to the stool sample, human serum specimen of Dengue fever patient was investigated by RNA-seq, followed by VirusTAP analysis. Since bacterial contamination is not common in serum, plasma, spinal fluid, and urine specimens, VirusTAP efficiently removed the more than 90% of host-related sequence in total reads to obtain possible virus sequence reads (data not shown). Only human genome subtraction could be sufficient for the serum specimen, but fecal or pharyngeal specimens contain unfavorable bacterial reads for virus genome assembly. This study demonstrated that rather laborious sample such as bacteria rich stool sample can be acceptable to determine a virus genome by VirusTAP, it includes non-virus filtering (see step #3–2 in Figure 1) to facilitate the virus genome assembly. The results of the homology search of pair-wise alignment for each scaffold can be visualized on the web (Figure 2B) or by downloading the full-results package, although VirusTAP provides the results against the latest NCBI nt database. Thus, users can reconfirm the obtained scaffolds by NCBI BLASTN or BLASTX web search with a direct web link (Figure 2B). The downloaded results package includes the bwa-mapping “sorted.bam” file, which can be imported into the reads-mapping viewer to visualize nucleotide polymorphisms, insertion-deletion and read coverage depth. In this sample study, a T/C heterogeneous mixture was identified at the 1,729 nt position of scaffold 4 (Figure 2C), indicating a possible quasispecies with a valine or alanine residue at the 578 aa position of the VP4 protein. VP4 is a spike protein at the outer capsid and has receptor binding activities and functions in membrane penetration (Trask et al., 2012), suggesting that the quasispecies could emerge during rotavirus gastroenteritis, contributing to the antigenic drift during rotavirus infection.",Assembly,"virustap viral genometargeted assembly pipeline
clinical specimens  ngs short read  viral genome assembly sample ngs short read  prepare use stool specimens   patient  rotavirus gastroenteritis  analyze  describe previously mizukoshi     brief total rna  prepare  patient feces follow  rnaseq library preparation use  scriptseq  rnaseq library preparation kit epicentre madison  usa deep sequence  perform  obtain mer pairedend  short read   miseq reagent kit  illumina san diego  usa  sample shortread sequence   deposit   dna data bank  japan ddbj accession number dra004165  study protocol  approve   institutional medical ethics committee   national institute  infectious diseases  japan approval      conduct accord   declaration  helsinki principles ngs read process virustap accept single   read #  figure  read quality trim  perform use  skewer jiang     fastqmcf program   eautils package1   additional trim filter  unreliable sequence   user specify quality score #  figure  host read subtraction  readmapping  perform   bwasw program version 9ar786   durbin   ribosomal rnas       internal transcribe spacers rrna  retrieve   follow ftp site2 bacterial genome sequences3   latest host organism genome sequences4 #  figure  figure  wwwfrontiersinorg figure  schematic representation   virustap procedures   remove residual nonvirus sequence read   subtract  read  assemble  idbaud peng    follow   filter  nonvirus  read  megablast   altschul     rapsearch2   zhao    search   customize viral nucleotideprotein sequence   ncbi ntnr database base  virus taxonomy exclude bacteriophages5 #  figure   significant evalue less  either     megablast      rapsearch2  determine  nonvirus contigs  residual  read   subtract   follow process  nonvirus filter step   repeat    time   nonvirus contigs disappear  remove  nonvirus sequence read break  read  also remove   follow  novo assembly   novo assembly pipeline   select   follow four pipelines #  figure  a5miseq coil    platanus version  kajitani     priceti ruby     idbaud peng     priceti ruby    priceti   ruby    perform  iterative contig extension  preassembly contigs  platanus  idbaud a5miseq  one    recommend  novo assemblers  general ngs  read  assemble accuracy   resultant scaffold   check use  result “xxxscaffoldsfastq” file base   readmapping score  contigs  divide   nucleotide position   phred score  less     result sequence shorter     remove  final scaffold  subject  bwasw read map   megablast homology search   ncbi  database  alignment result  visualize   web page result  discussion basic virustap procedure briefly  schematic representation  virustap  show  figure   virustap process start   quality trim follow  host  bacteriarelated read subtraction  read subtraction  perform   follow two step first readmapping   bwasw program  rrna sequence bacterial genome sequence  select virushost genomes   human mouse  monkey  perform second nonvirus filter   virus nucleotideprotein database exclude bacteriophages   select   subtraction  extract possible virusrelated ngs read  abovedescribed highly extensive nonvirus read subtraction facilitate effective  novo assembly  prompt analysis achieve  time save  accuracy virustap performance comparison  direct metagenomic assembly methods use metagenomic rnaseq short read obtain  stool specimens   patient  rotavirus gastroenteritis mizukoshi     performance  virustap  compare      novo assembly methods without subtraction treatment  instance direct metagenomic assembly without subtraction use  clc genome workbench  qiagen aarhus denmark  a5miseq produce    scaffold respectively virustap significantly reduce  result   scaffold figure  step  figure  wwwfrontiersinorg figure  performance comparison  virustap  nonsubtraction methods  sample metagenomic rnaseq read obtain   rotavirus gastroenteritis patient  analyze  virustap  nonsubtraction methods  pairwise alignment view    similar hit   scaffold eleven scaffold show similarity  eleven segment  rotavirus genome sequence ncbi web blast search blastn  blastx   directly perform   scaffold  click  button  reconfirm  similarity  visualization   readmapping result   scaffold  viral quasispecies  read coverage   scaffold   confirm   readmapping viewer use  bam file   download result package  direct metagenomic assembly methods generate unexpectedly abundant scaffold relate  multiple organisms  step   figure   particular    scaffold  compose  bacterial sequence    scaffold  clc genome  a5miseq respectively figure  suggest  subsequent homology search  taxonomy classifications   laborious  timeconsuming procedures  identify  final viral genome sequence indeed  entire virustap process   sample read require  min  obtain  segment   rotavirus genome sequence whereas   two methods require        timeconsuming  novo assembly  homology search   abundant scaffold figure   addition   stool sample human serum specimen  dengue fever patient  investigate  rnaseq follow  virustap analysis since bacterial contamination   common  serum plasma spinal fluid  urine specimens virustap efficiently remove      hostrelated sequence  total read  obtain possible virus sequence read data  show  human genome subtraction could  sufficient   serum specimen  fecal  pharyngeal specimens contain unfavorable bacterial read  virus genome assembly  study demonstrate  rather laborious sample   bacteria rich stool sample   acceptable  determine  virus genome  virustap  include nonvirus filter see step #  figure   facilitate  virus genome assembly  result   homology search  pairwise alignment   scaffold   visualize   web figure    download  fullresults package although virustap provide  result   latest ncbi  database thus users  reconfirm  obtain scaffold  ncbi blastn  blastx web search   direct web link figure   download result package include  bwamapping “sortedbam” file    import   readsmapping viewer  visualize nucleotide polymorphisms insertiondeletion  read coverage depth   sample study   heterogeneous mixture  identify     position  scaffold  figure  indicate  possible quasispecies   valine  alanine residue     position   vp4 protein vp4   spike protein   outer capsid   receptor bind activities  function  membrane penetration trask    suggest   quasispecies could emerge  rotavirus gastroenteritis contribute   antigenic drift  rotavirus infection",5
129,SearchSmallRNA,"SearchSmallRNA: a graphical interface tool for the assemblage of viral genomes using small RNA libraries data
Software SearchSmallRNA was developed in JAVA language version 7 using NetBeans IDE 7.1 software. Biojava3-core-3.0.2.jar and Biojava3-alignment-3.0.2.jar packages were used as additional tool, but the search engine doesn’t use traditional alignment to find similar strings. It searches reads (small DNA strings with 18-24nt length) in a reference genome comparing their by Hamming distance [5]. Only reads presenting Hamming value equal or smaller than the allowed mismatches value are selected for assemblage. Each read mapped generates a list of index values based on the reference genome and the first character of the read. The nucleotides of each position in the mapped genome are choosed taking in count the amount of reads matching it. The most abundant nucleotide is used to the assemblage of the new genome. The software is freely available in http://www.microbiologia.ufrj.br/ssrna. It is only necessary to install Java virtual machine 7 (free available in http://www.java.com) or higher. Sequencing of CLRDV genome by deep-sequencing Leaves of cotton (Gossypium hirsutum) plants (cultivar Fibermax966) infected in green house conditions with Cotton Leafroll Dwarf Virus (CLRDV) by viruliferous aphids inoculation were used for total RNA extraction. The CLRDV virus correspond to a PV1 isolate that suffered more than 5 years of passages in green house conditions at the UFRJ, Rio de Janeiro, RJ, Brazil. Leaves from mock-infected plants were used as negative control. Total RNAs was extracted using the Invisorb Spin Plant RNA Mini Kit (Invisorb®). The quantity and quality of RNA samples obtained were determined by spectrophotometry (Nanodrop ND-1000, Thermo Fisher Scientific) and agarose gel electrophoresis. Systemic infections were confirmed using nested (RT)-PCR assays to detect the viral capsid protein-encoding gene as previously described [6]. The procedures for obtaining the viral small sRNA library was already described [6]. In brief, RNA samples were precipitated in ethanol and sequenced at Fasteris Co. (Geneve, Switzerland) with an Illumina Genome Analyzer (Illumina, San Diego, USA). Small RNAs of 15–30 nt were purified from acrylamide gel; the 3′ IDT miRNA cloning linker (Integrated DNA Technologies, San Diego, USA) and then the 5′ Illumina adapters were single-stranded ligated with T4 RNA ligase to the purified small RNAs. The constructs were purified again on an acrylamide gel to remove empty adapters and then reverse-transcribed and PCR-amplified. The primers used for cDNA synthesis and PCR were designed to insert an index in the 3′ adapter. The libraries were quality controlled by cloning an aliquot into a TOPO plasmid and capillary sequencing 4–8 clones. High-throughput sequencing was performed on a Genome Analyzer GAIIx for 38 cycles plus 7 cycles to read the indexes. After demultiplexing and adapter removal, 10.5 million pass filter reads were obtained in the library. The deep sequencing libraries were deposited at GEO (Gene Expression Omnibus) under the number GSE311062 (http://www.ncbi.nlm.nih.gov/geo/info/submission.html). Datasets and accession number Human immunodeficiency virus 1 (HIV-1) reads were downloaded from SRA database (http://www.ncbi.nlm.nih.gov/sra) using Aspera software. The accessions numbers of HIV-1 dataset was SRP007924. SPFMV reads were downloaded from https://research.cip.cgiar.org/confluence/display/cpx/CIP.sweetpotato.2008.PER.CIPHQ.siRNA-1.tables. The accessions numbers of the complete genome virus used in this paper were: HIV - NC_001802.1; Human herpesvirus 1 (HHV-1) - NC_001806.1; Sweet potato feathery mottle virus (SPFMV) - NC_001841.1; Sweet potato feathery mottle virus isolate Piu3 (SPFMV – Piu3) - FJ155666; CLRDV-ARG isolate - GU167940; CLRDV-PV1 - HQ827780.1. All alignments were performed using MultiAlign",Assembly,"searchsmallrna  graphical interface tool   assemblage  viral genomes use small rna libraries data
software searchsmallrna  develop  java language version  use netbeans ide  software biojava3corejar  biojava3alignmentjar package  use  additional tool   search engine doesnt use traditional alignment  find similar string  search read small dna string  24nt length   reference genome compare   ham distance   read present ham value equal  smaller   allow mismatch value  select  assemblage  read map generate  list  index value base   reference genome   first character   read  nucleotides   position   map genome  choose take  count  amount  read match    abundant nucleotide  use   assemblage   new genome  software  freely available      necessary  install java virtual machine  free available    higher sequence  clrdv genome  deepsequencing leave  cotton gossypium hirsutum plant cultivar fibermax966 infect  green house condition  cotton leafroll dwarf virus clrdv  viruliferous aphids inoculation  use  total rna extraction  clrdv virus correspond   pv1 isolate  suffer    years  passages  green house condition   ufrj rio  janeiro  brazil leave  mockinfected plant  use  negative control total rnas  extract use  invisorb spin plant rna mini kit invisorb®  quantity  quality  rna sample obtain  determine  spectrophotometry nanodrop  thermo fisher scientific  agarose gel electrophoresis systemic infections  confirm use nest rtpcr assay  detect  viral capsid proteinencoding gene  previously describe   procedures  obtain  viral small srna library  already describe   brief rna sample  precipitate  ethanol  sequence  fasteris  geneve switzerland   illumina genome analyzer illumina san diego usa small rnas     purify  acrylamide gel  ′ idt mirna clone linker integrate dna technologies san diego usa    ′ illumina adapters  singlestranded ligate   rna ligase   purify small rnas  construct  purify    acrylamide gel  remove empty adapters   reversetranscribed  pcramplified  primers use  cdna synthesis  pcr  design  insert  index   ′ adapter  libraries  quality control  clone  aliquot   topo plasmid  capillary sequence  clone highthroughput sequence  perform   genome analyzer gaiix   cycles plus  cycles  read  index  demultiplexing  adapter removal  million pass filter read  obtain   library  deep sequence libraries  deposit  geo gene expression omnibus   number gse311062  datasets  accession number human immunodeficiency virus  hiv read  download  sra database  use aspera software  accession number  hiv dataset  srp007924 spfmv read  download    accession number   complete genome virus use   paper  hiv  nc_001802 human herpesvirus  hhv  nc_001806 sweet potato feathery mottle virus spfmv  nc_001841 sweet potato feathery mottle virus isolate piu3 spfmv  piu3  fj155666 clrdvarg isolate  gu167940 clrdvpv1  hq827780  alignments  perform use multialign",5
130,drVM,"drVM: a new tool for efficient genome assembly of known eukaryotic viruses from metagenomes.
Reference databases Viral nucleotide sequences were obtained from the Nucleotide database of the National Center for Biotechnology Information (NCBI) using the query term “(complete[title]) AND (viridae[organism])”; this query resulted in 642 079 hits (as of 16 March 2016) and increased to 705 577 hits (as of 20 October 2016). Based on each sequence identifier (accession number) of the viral sequence, its taxonomic ID, scientific name, and genus-level annotation were separately obtained from the NCBI Taxonomy database. Viral sequences with taxonomy ID not included in the virus division (division id = 9), those with taxonomic information absent, and those lacking genus-level annotation were labeled “nonViral, “noTax,” and “noGenus,” respectively. The noGenus sequences can be used by adding “-kn on” option in the creation of databases. With the exception of noGenus sequences, those sequences with the nonViral or noTax labels were excluded from database construction. The remaining viral sequences were utilized as rawDB. Three hundred and seventy-seven viral genomes were obtained by filtering with host “human” from the NCBI viral genome resource to retrieve 684 sequences (1 March 2016) [1]. The viral sequences were segmented into their corresponding genus level for refDB; the corresponding refDB was uploaded to SourceForge for later use (https://sourceforge.net/projects/sb2nhri/files/drVM/refDB.tar.gz). To increase read alignments and reduce memory requirements, rawDB was split into eight sub-databases, and each contained sequences with total length <200 Mbp. SNAP (version 0.15.4) [30], possessing a default seed size of 20, was used to generate index tables (snap_index_virus) for the viral sub-databases. In addition, a BLAST database (blast_index_virus) was created by makeblastdb (BLAST 2.2.28+) [31] from rawDB to enable contig annotation. Implementation of drVM The drVM pipeline is implemented in Python and incorporates several open-source tools, including BLAST, SNAP, and SPAdes [32]. The package comprises two modules: CreateDB.py and drVM.py. A schematic flowchart of drVM is shown in Fig. 1. In CreateDB.py, viral sequences (provided by the user) are processed to produce rawDB for SNAP and BLAST database construction, and refDB is downloaded from the SourceForge link. drVM.py takes single- or paired-end reads as input. It aligns reads to SNAP DB and, accordingly, identifies viral reads using the following parameters: snap single -x -h 250 -d 24 -n 25 -F a. Based on taxonomic information (genus-level annotation) of the aligned sequence, the viral reads are partitioned into genus-level groups. Each group is labeled according to its corresponding genus. By examining the aligned sequences with reads, sequence coverage and read depth are calculated. Maximum coverage and average depth for a genus are estimated by taking all aligned sequences corresponding to that genus into consideration. Viral reads in a genus with average depth more than or equal to min depth (default = 1) undergo de novo assembly via SPAdes (v.3.6.1). Prior to assembly, digital normalization [33] in khmer [34] is employed to remove reads with high-abundance (default = 100) k-mers. Using BLAST, each genus-level assembly is annotated to its closest reference against refDB using blastn (identity ≧80%). Each reference is examined to confirm that it contains at least a 50% alignment rate. If this procedure returns no hits, the assembly is annotated by BLAST against rawDB. Note that, although the viral sequences in refDB are restricted to human viruses, there is no filter by host option in rawDB. Reads are aligned to contigs by means of SOAP2 [35] and the read-covered contigs are placed into a coverage plot with reference-guide coordinates. To increase contig continuity, reads aligned to contigs corresponding to a reference are extracted for pairs and those paired reads are re-assembled. This re-assembly process is only performed for paired-end reads. Figure 1. A schematic flowchart of drVM. CreateDB.py processes viral sequences to produce SNAP and BLAST databases. drVM.py analyzes NGS reads to produce viral genomes. Open in new tabDownload slide A schematic flowchart of drVM. CreateDB.py processes viral sequences to produce SNAP and BLAST databases. drVM.py analyzes NGS reads to produce viral genomes. Simulation datasets A reference genome of the hepatitis C virus (HCV; NC_004102.1) was used to simulate metagenomes with 10×, 15×, 20×, 30×, 40×, and 50× viral reads using VirtualNextGenSequencer [36]. The simulated reads were concatenated to 1-Gbp sequencing reads (3.5 million paired-end 150-bp reads) of liver cancer cells (SRR3031107 in SRA) and 328-Mbp reads of a human microbiome sample (SRR062412, 100-bp paired-end reads containing any Ns were removed). The reads of liver cancer cells and the reads from the human microbiome project (HMP) were taken as host and bacterial metagenomes. Three reference genomes of respiratory viruses including human enterovirus (NC_001612 for CA16), human rhinovirus (NC_001617 for HRV), and human respiratory syncytial virus (NC_001781 for HRSV) were used separately to simulate 20×, 20× and 40×, and 20× and 80× viral sequences using VirtualNextGenSequencer. The simulated reads (all 20×, and 20×-40×-80×) and randomly selected reads (1 Gbp, 5 million paired-end 100-bp reads), as a background, from an influenza-negative sample (ERR690488) [9] were used to assess the ability of drVM to assemble viral genomes within the same genus (human enterovirus and rhinovirus). Metagenomic datasets A total of 349 sequencing runs in the SRA were downloaded (see Table S1). The metagenome datasets based on prior studies [8–10, 13, 16–29] are summarized in Table 1. Each run was analyzed by drVM: drVM.py -1 read1.fastq -2 read2.fastq -t 16 (-type iontorrent for Ion Torrent datasets) on a server (Intel Xeon E7-4820, 2.00GHz with 256 GB of RAM).",Assembly,"drvm  new tool  efficient genome assembly  know eukaryotic viruses  metagenomes
reference databases viral nucleotide sequence  obtain   nucleotide database   national center  biotechnology information ncbi use  query term “completetitle  viridaeorganism”  query result    hit    march   increase    hit    october  base   sequence identifier accession number   viral sequence  taxonomic  scientific name  genuslevel annotation  separately obtain   ncbi taxonomy database viral sequence  taxonomy   include   virus division division      taxonomic information absent   lack genuslevel annotation  label “nonviral “notax”  “nogenus” respectively  nogenus sequence   use  add “ ” option   creation  databases   exception  nogenus sequence  sequence   nonviral  notax label  exclude  database construction  remain viral sequence  utilize  rawdb three hundred  seventyseven viral genomes  obtain  filter  host “human”   ncbi viral genome resource  retrieve  sequence  march    viral sequence  segment   correspond genus level  refdb  correspond refdb  upload  sourceforge  later use   increase read alignments  reduce memory requirements rawdb  split  eight subdatabases   contain sequence  total length  mbp snap version   possess  default seed size    use  generate index table snap_index_virus   viral subdatabases  addition  blast database blast_index_virus  create  makeblastdb blast    rawdb  enable contig annotation implementation  drvm  drvm pipeline  implement  python  incorporate several opensource tool include blast snap  spade   package comprise two modules createdbpy  drvmpy  schematic flowchart  drvm  show  fig   createdbpy viral sequence provide   user  process  produce rawdb  snap  blast database construction  refdb  download   sourceforge link drvmpy take single  pairedend read  input  align read  snap   accordingly identify viral read use  follow parameters snap single          base  taxonomic information genuslevel annotation   align sequence  viral read  partition  genuslevel group  group  label accord   correspond genus  examine  align sequence  read sequence coverage  read depth  calculate maximum coverage  average depth   genus  estimate  take  align sequence correspond   genus  consideration viral read   genus  average depth    equal  min depth default   undergo  novo assembly via spade  prior  assembly digital normalization   khmer   employ  remove read  highabundance default   kmers use blast  genuslevel assembly  annotate   closest reference  refdb use blastn identity ≧  reference  examine  confirm   contain  least   alignment rate   procedure return  hit  assembly  annotate  blast  rawdb note  although  viral sequence  refdb  restrict  human viruses    filter  host option  rawdb read  align  contigs  mean  soap2    readcovered contigs  place   coverage plot  referenceguide coordinate  increase contig continuity read align  contigs correspond   reference  extract  pair   pair read  reassemble  reassembly process   perform  pairedend read figure   schematic flowchart  drvm createdbpy process viral sequence  produce snap  blast databases drvmpy analyze ngs read  produce viral genomes open  new tabdownload slide  schematic flowchart  drvm createdbpy process viral sequence  produce snap  blast databases drvmpy analyze ngs read  produce viral genomes simulation datasets  reference genome   hepatitis  virus hcv nc_004102  use  simulate metagenomes         viral read use virtualnextgensequencer   simulate read  concatenate  gbp sequence read  million pairedend  read  liver cancer cells srr3031107  sra  mbp read   human microbiome sample srr062412  pairedend read contain    remove  read  liver cancer cells   read   human microbiome project hmp  take  host  bacterial metagenomes three reference genomes  respiratory viruses include human enterovirus nc_001612  ca16 human rhinovirus nc_001617  hrv  human respiratory syncytial virus nc_001781  hrsv  use separately  simulate         viral sequence use virtualnextgensequencer  simulate read      randomly select read  gbp  million pairedend  read   background   influenzanegative sample err690488   use  assess  ability  drvm  assemble viral genomes within   genus human enterovirus  rhinovirus metagenomic datasets  total   sequence run   sra  download see table   metagenome datasets base  prior study     summarize  table   run  analyze  drvm drvmpy  read1fastq  read2fastq   type iontorrent  ion torrent datasets   server intel xeon  00ghz     ram",5
131,InteMap,"InteMAP: Integrated metagenomic assembly pipeline for NGS short reads
Simulated dataset To construct the simulated metagenome dataset, we used the MetaSim simulator [40] to generate a collection of synthetic metagenomic reads which reflect the specified abundance distribution of the species. In the current study, we mainly target the Illumina reads although our pipeline is also able to be applied on other short reads. To imitate Illumina sequencing, the read length was set 100 bp, with the average and the standard deviation of paired-end insert size as 300 bp and 20 bp. To model the specific pattern of sequencing error of Illumina technology, we used NGSfy [41] to generate sequencing errors in reads, which uses a fourth-degree polynomial model to describe the frequency of errors in Illumina reads. We used the default settings for NGSfy, and the average error rate was 1.5 %, as in previous studies [41, 42]. We constructed the sim-113sp dataset by mixing the synthetic reads from 113 complete microbial genomes downloaded from NCBI RefSeq database. The collection of the 113 species is exactly the same as used by Mavromatis et al. [43] to generate simulated metagenomic datasets, except for a few unfinished genomes, which we chose close relatives (usually a different strain) instead (see Additional file 2: File S1). For the abundance setting, we used the logarithmic distribution, which has been used in Mende et al. [30] to model the microbial abundance distribution. The highest sequencing coverage of species in this dataset is 177.5×, followed by 97.6×, and the coverage drops to below 10× after the top 20 species (see Additional file 1: Figure S7). We totally generated 4 million reads of totally 400 Mbp, which are able to be downloaded from http://cqb.pku.edu.cn/ZhuLab/InteMAP/index.html. In order to calculate the number of genes contained by the assembly in the evaluation on simulated metagenomic dataset sim-113sp, we downloaded the gene annotation files for each reference genome, which record the start and ending positions for each gene on the genome sequence, from the NCBI database. According to the mappings between the contigs and the reference genomes, if one gene was completely contained in an aligned region of the references, this gene was deemed completely contained by the contigs. Running the assemblers The pre-processing of correcting the sequencing errors in raw reads has been reported to substantially enlarge the contig size for some assemblers in the single genome assembly [34]. In the current study, error correction also improved the metagenome-assembly quality, especially on the high-depth species. The InteMAP pipeline employs a module of error correction to pre-process the reads before assembling. For a fair comparison, in our experiments, reads were pre-processed by the error correction process before being assembled by all assemblers (see Additional file 1: Supplemental Methods for the detailed process of error correction). For each assembler, we ran multiple times with different parameter settings and selected the ones that appeared optimal or near optimal results (the parameters selected and details describing how to run each of the assembler are in Additional file 1: Supplemental Methods). The contigs by ABySS, IDBA-UD, MetaVelvet and SOAPdenovo were extracted from their scaffold sequences after splitting the scaffold sequences at gaps. SOAPdenovo has an additional module named GapCloser as a post-processor. Our test demonstrated that the GapCloser's post-processing actually increases the assembling errors dramatically. Therefore, we selected the assemblies without the post-processing of GapCloser for SOAPdenovo. Evaluating the assemblies Because extremely short contigs, in many cases as small as the k-mer size used to build the k-mer graph, can hardly support information for further analysis, for all the assemblies, contigs with length <200 bp were excluded before evaluation, as did in previous study [34]. Following the way used in the previous study [34], we used MUMmer package v3.23 [44] to evaluate the contigs, in which nucmer [44] was used to align the contigs against the references with the options “-maxmatch -l 30 -banded -D 5.” Then delta-filter with the option “-i 95 -o 95” and dnadiff [45] were used to obtain a globally optimal mappings between contigs and references, from which we could directly get the structural assembly errors (segmental indels (>5 bp) and misjoins of two non-adjacent segments). From the output of dnadiff, we also got the identity of the aligned contigs. In the evaluation of the assemblies on simulated dataset, we used corrected contigs to evaluate the contiguity of the contigs by computing the correct N50 size, correct N-len size, and correct E-size. We extracted the aligned contig fragments as the corrected contigs from the mappings between contigs and references generated by dnadiff (aligned fragments end at the ends of contigs or assembly errors). The N-len size is computed as defined in the RESULTS section. Herein it is should be noted that N50 size corresponds to the N -len(x) for x = L/2, where L denotes the reference genome length. The computation of the E-value follows the formula 𝐸=∑𝐶𝐿𝐶2𝐺, where L C is the length of contig C, and G is the total reference genome length. On real NGS dataset MH0012, as the total length of reference genomes is not available, the G in the E-size formula was assigned the maximal total contigs length among all the assemblies run on this dataset (we used 280,000,000 here). Algorithm of the InteMAP pipeline We then describe the procedures of the InteMAP pipeline step by step as follows (see Fig. 6): Fig. 6 figure6 The flowchart of the main procedures of the InteMAP pipeline Full size image Step 1. Correcting errors of the input reads. By default, input reads are first pre-processed by the Quake tool [46], which aims to correct the sequencing errors in high-coverage sequences (see Additional file 1: Supplemental Methods for details). Two sets of reads are then generated: one named “correct read set” consisting of reads corrected or validated by Quake, and the other consisting of the remaining ones (see Additional file 1: Figure S8 for the ratio of the correct reads to total reads for each coverage of species). Two sets are mixed together and the mixed read set is termed “total read set”. Step 2. Running IDBA-UD and estimating reads coverage. After the assembly by IDBA-UD, reads are mapped into the contigs using the read alignment tool, Bowtie 2 [47]. According to the mappings between reads and contigs, the coverage of each read is then estimated by the times of the total length of reads to the length of the assembled contig. Furthermore, the contigs are separated along with the mapped reads by different coverage. The pipeline then picks up both the reads mapping to contigs with the coverage < L (this can be designated by users, 50× by default) and the unmapped reads, and assembles them using CABOG. Step 3. Checking the total length of high coverage contigs. The pipeline first checks the total length of the contigs with the coverage >30×. If the length is shorter than a designated value (e.g. 1 Mbp by default), the pipeline goes to Step 6 for the condition of the low-coverage reads assembly. Otherwise, goes to the next step. Step 4. Assembling reads of the “correct read set” using ABySS and IDBA-UD respectively. The “correct read set” has been generated in Step 1, and using this set can largely increase the quality of assembly for the high-depth species by both ABySS and IDBA-UD. After assemblies are generated by ABySS and IDBA-UD, the pipeline maps the reads into the contigs and picks up the high-depth contigs for both assemblies. The minimal coverage of high-depth contigs for ABySS and IDBA-UD is set 20× by default. Step 5. Merging the assemblies at high and low coverage levels iteratively. First, the pipeline merges the high-depth contigs from the IDBA-UD and ABySS assemblies generated in Step 4, using the algorithm described below. Then filter out the high-coverage contigs (≥50×) by IDBA-UD in Step 2 and merge the remaining contigs by CABOG in Step 2. Since the pipeline has merged contigs both on the high and low coverage sequences, finally, the pipeline merges them together as the final contigs. Note that the ranges of the high and low coverage overlap each other to ensure that the pipeline covers all the sequences, so the default cut-off for high-depth and low-depth are set to make the range broader than those used for the assembly evaluation. The pipeline ends at this step. Step 6. Merging the assemblies by IDBA-UD and CABOG. If Step 4 and 5 are skipped, which means that there are no high coverage species within the metagenomes, merge the contigs generated by both IDBA-UD and CABOG in Step 2 as the final contigs. The pipeline ends at this step. The algorithm for merging two sets of contigs The steps of merging contigs in the InteMAP pipeline are described as follows. Step 1. Comparing the two sets of contigs. It begins with comparing the two sets of contigs using nucmer, delta-filter, and dnadiff in MUMmer package to get the mappings between the two sets of contigs. From the mappings, the same segments shared by two sets of contigs are marked. Then, the breakpoints are marked inside the contigs where the bifurcations appear (Type 1 breakpoint) or the segments end in the contigs of the different assembly (Type 2 breakpoint), as illustrated in Fig. 7a. The breakpoints may indicate assembly errors. Fig. 7 figure7 Illustration of merging two assemblies. a Two types of breakpoints caused by differences between two assemblies are illustrated. Suppose two assemblies asm1 and asm2 have the same assembled sequence segment seg1. The first type of difference is that in asm1, seg1 is extended with seg2 on the right, while in asm1, seg1 ends on the right. The second type of difference is that in ass1, seg1 is extended with seg2 on the right, while in asm2, seg1 is extended with seg3 on the right. b Split the contigs at the breakpoint within the suspicious region. c An example of merging the segments at the breakpoints which are not broken Full size image Step 2. Validating the contigs. The contigs are validated by mapping the paired-end reads against the contigs, which has been used in previous work [45]. The basic idea is that if one contig is correctly assembled, each pair of paired-end reads is located and mated correctly. If a region has ≥5 mis-mated reads located, this region will be marked as suspicious which indicates potential mis-assembly within the region. Thus, if one breakpoint is located within the suspicious region, it is more likely to be an assembly error. Therefore, the algorithm breaks all the breakpoints which are located within the suspicious regions (Fig. 7b). Step 3. Breaking the contradictory breakpoints. The remaining breakpoints are treated differently according to their types. As Type 1 breakpoints are mated and in different assemblies respectively, if both mated breakpoints are not broken in Step 2, both of them will be broken at this step. In contrast, if only one Type 1 breakpoint is broken in Step 2, the other breakpoint will automatically transform to the Type 2 breakpoint. The survived Type 2 breakpoints are reserved. Step 4. Connecting the segments at reserved breakpoints (Fig. 7c). Because there are no conflicting breakpoints remained after Step 2 and 3, each segment has at most one segment linked by the reserved breakpoint in either the left or the right direction. Then, beginning from any segment which ends at one side, the pipeline extends the segment by connecting the adjacent one until the extending segment ends at the extending direction. The final contigs are generated by reading the bases from the merged segments. If the segment is shared by two assemblies, randomly select one strand of sequences to read the bases.",Assembly,"intemap integrate metagenomic assembly pipeline  ngs short reads
simulated dataset  construct  simulate metagenome dataset  use  metasim simulator   generate  collection  synthetic metagenomic read  reflect  specify abundance distribution   species   current study  mainly target  illumina read although  pipeline  also able   apply   short read  imitate illumina sequence  read length  set     average   standard deviation  pairedend insert size        model  specific pattern  sequence error  illumina technology  use ngsfy   generate sequence errors  read  use  fourthdegree polynomial model  describe  frequency  errors  illumina read  use  default settings  ngsfy   average error rate      previous study    construct  sim113sp dataset  mix  synthetic read   complete microbial genomes download  ncbi refseq database  collection    species  exactly    use  mavromatis     generate simulate metagenomic datasets except    unfinished genomes   choose close relatives usually  different strain instead see additional file  file    abundance set  use  logarithmic distribution    use  mende     model  microbial abundance distribution  highest sequence coverage  species   dataset   follow     coverage drop      top  species see additional file  figure   totally generate  million read  totally  mbp   able   download    order  calculate  number  genes contain   assembly   evaluation  simulate metagenomic dataset sim113sp  download  gene annotation file   reference genome  record  start  end position   gene   genome sequence   ncbi database accord   mappings   contigs   reference genomes  one gene  completely contain   align region   reference  gene  deem completely contain   contigs run  assemblers  preprocessing  correct  sequence errors  raw read   report  substantially enlarge  contig size   assemblers   single genome assembly    current study error correction also improve  metagenomeassembly quality especially   highdepth species  intemap pipeline employ  module  error correction  preprocess  read  assemble   fair comparison   experiment read  preprocessed   error correction process   assemble   assemblers see additional file  supplemental methods   detail process  error correction   assembler  run multiple time  different parameter settings  select  ones  appear optimal  near optimal result  parameters select  detail describe   run    assembler   additional file  supplemental methods  contigs  aby idbaud metavelvet  soapdenovo  extract   scaffold sequence  split  scaffold sequence  gap soapdenovo   additional module name gapcloser   postprocessor  test demonstrate   gapcloser' postprocessing actually increase  assemble errors dramatically therefore  select  assemblies without  postprocessing  gapcloser  soapdenovo evaluate  assemblies  extremely short contigs  many case  small   kmer size use  build  kmer graph  hardly support information   analysis    assemblies contigs  length    exclude  evaluation    previous study  follow  way use   previous study   use mummer package    evaluate  contigs   nucmer   use  align  contigs   reference   options “maxmatch   band  ”  deltafilter   option “   ”  dnadiff   use  obtain  globally optimal mappings  contigs  reference    could directly get  structural assembly errors segmental indels    misjoins  two nonadjacent segment   output  dnadiff  also get  identity   align contigs   evaluation   assemblies  simulate dataset  use correct contigs  evaluate  contiguity   contigs  compute  correct n50 size correct nlen size  correct esize  extract  align contig fragment   correct contigs   mappings  contigs  reference generate  dnadiff align fragment end   end  contigs  assembly errors  nlen size  compute  define   result section herein     note  n50 size correspond    lenx       denote  reference genome length  computation   evalue follow  formula ∑𝐶𝐿𝐶2𝐺      length  contig      total reference genome length  real ngs dataset mh0012   total length  reference genomes   available     esize formula  assign  maximal total contigs length among   assemblies run   dataset  use   algorithm   intemap pipeline   describe  procedures   intemap pipeline step  step  follow see fig  fig  figure6  flowchart   main procedures   intemap pipeline full size image step  correct errors   input read  default input read  first preprocessed   quake tool   aim  correct  sequence errors  highcoverage sequence see additional file  supplemental methods  detail two set  read   generate one name “correct read set” consist  read correct  validate  quake    consist   remain ones see additional file  figure    ratio   correct read  total read   coverage  species two set  mix together   mix read set  term “total read set” step  run idbaud  estimate read coverage   assembly  idbaud read  map   contigs use  read alignment tool bowtie   accord   mappings  read  contigs  coverage   read   estimate   time   total length  read   length   assemble contig furthermore  contigs  separate along   map read  different coverage  pipeline  pick    read map  contigs   coverage      designate  users   default   unmapped read  assemble  use cabog step  check  total length  high coverage contigs  pipeline first check  total length   contigs   coverage    length  shorter   designate value   mbp  default  pipeline go  step    condition   lowcoverage read assembly otherwise go   next step step  assemble read   “correct read set” use aby  idbaud respectively  “correct read set”   generate  step   use  set  largely increase  quality  assembly   highdepth species   aby  idbaud  assemblies  generate  aby  idbaud  pipeline map  read   contigs  pick   highdepth contigs   assemblies  minimal coverage  highdepth contigs  aby  idbaud  set   default step  merge  assemblies  high  low coverage level iteratively first  pipeline merge  highdepth contigs   idbaud  aby assemblies generate  step  use  algorithm describe   filter   highcoverage contigs ≥  idbaud  step   merge  remain contigs  cabog  step  since  pipeline  merge contigs    high  low coverage sequence finally  pipeline merge  together   final contigs note   range   high  low coverage overlap    ensure   pipeline cover   sequence   default cutoff  highdepth  lowdepth  set  make  range broader   use   assembly evaluation  pipeline end   step step  merge  assemblies  idbaud  cabog  step     skip  mean     high coverage species within  metagenomes merge  contigs generate   idbaud  cabog  step    final contigs  pipeline end   step  algorithm  merge two set  contigs  step  merge contigs   intemap pipeline  describe  follow step  compare  two set  contigs  begin  compare  two set  contigs use nucmer deltafilter  dnadiff  mummer package  get  mappings   two set  contigs   mappings   segment share  two set  contigs  mark   breakpoints  mark inside  contigs   bifurcations appear type  breakpoint   segment end   contigs   different assembly type  breakpoint  illustrate  fig   breakpoints may indicate assembly errors fig  figure7 illustration  merge two assemblies  two type  breakpoints cause  differences  two assemblies  illustrate suppose two assemblies asm1  asm2    assemble sequence segment seg1  first type  difference    asm1 seg1  extend  seg2   right   asm1 seg1 end   right  second type  difference    ass1 seg1  extend  seg2   right   asm2 seg1  extend  seg3   right  split  contigs   breakpoint within  suspicious region   example  merge  segment   breakpoints    break full size image step  validate  contigs  contigs  validate  map  pairedend read   contigs    use  previous work   basic idea    one contig  correctly assemble  pair  pairedend read  locate  mat correctly   region  ≥ mismate read locate  region   mark  suspicious  indicate potential misassembly within  region thus  one breakpoint  locate within  suspicious region    likely    assembly error therefore  algorithm break   breakpoints   locate within  suspicious regions fig  step  break  contradictory breakpoints  remain breakpoints  treat differently accord   type  type  breakpoints  mat   different assemblies respectively   mat breakpoints   break  step       break   step  contrast   one type  breakpoint  break  step    breakpoint  automatically transform   type  breakpoint  survive type  breakpoints  reserve step  connect  segment  reserve breakpoints fig      conflict breakpoints remain  step     segment    one segment link   reserve breakpoint  either  leave   right direction  begin   segment  end  one side  pipeline extend  segment  connect  adjacent one   extend segment end   extend direction  final contigs  generate  read  base   merge segment   segment  share  two assemblies randomly select one strand  sequence  read  base",5
132,DBG2OLC,"DBG2OLC: Efficient Assembly of Large Genomes Using Long Erroneous Reads of the Third Generation Sequencing Technologies
Our algorithm starts with linear unambiguous regions of a de Bruijn graph (DBG) and ends up with linear unambiguous regions in an overlap graph (used in the Overlap-Layout-Consensus framework). Due to this property, we dub our software DBG2OLC. The whole algorithm consists of the following five procedures and we implement them as a pipeline in DBG2OLC. Each piece of the pipeline can be carried out efficiently. 1 Construct a de Bruijn graph (DBG) and output contigs from highly accurate NGS short reads. 2 Map the contigs to each long read to anchor the long reads. The long reads are compressed into a list of contig identifiers to reduce the cost of processing the long reads (Fig. 1A). Figure 1 figure1 (A) Map de Bruijn graph contigs to the long reads. The long reads are in red, the de Bruijn graph contigs are in other colors. Each long read is converted into an ordered list of contigs, termed compressed reads. (B) Calculate overlaps between the compressed reads. The alignment is calculated using the anchors. Contained reads are removed and the reads are chained together in the best-overlap fashion. (C) Layout: construct the assembly backbone from the best overlaps. (D) Consensus: align all related reads to the backbone and calculate the most likely sequence as the consensus output. Full size image 3 Use multiple sequence alignment to clean the compressed reads and remove reads with structural errors (or so-called chimeras) (Fig. 2). Figure 2 figure2 Reads correction by multiple sequence alignment. The left portion shows removing a false positive anchoring contig (brown) that appears only once in the multiple alignment. The right portion shows detection of a chimeric read by aligning it to multiple reads. A breakpoint is detected as all the reads can be aligned with the left portion of the target read are not consistent with all the reads that can be aligned with the right portion of the target read. Full size image 4 Construct a best overlap graph using the cleaned compressed long reads (Fig. 1B). 5 Uncompress and chain together the long reads (Fig. 1C) and resort to a consensus algorithm to convert them into DNA sequences (Fig. 1D). Details for procedure (2–5) are explained below. The explanation of procedure (1) can be found in our previous SparseAssembler for NGS technology5 and is omitted here.",Assembly,"dbg2olc efficient assembly  large genomes use long erroneous read   third generation sequence technologies
 algorithm start  linear unambiguous regions    bruijn graph dbg  end   linear unambiguous regions   overlap graph use   overlaplayoutconsensus framework due   property  dub  software dbg2olc  whole algorithm consist   follow five procedures   implement    pipeline  dbg2olc  piece   pipeline   carry  efficiently  construct   bruijn graph dbg  output contigs  highly accurate ngs short read  map  contigs   long read  anchor  long read  long read  compress   list  contig identifiers  reduce  cost  process  long read fig  figure  figure1  map  bruijn graph contigs   long read  long read   red   bruijn graph contigs    color  long read  convert   order list  contigs term compress read  calculate overlap   compress read  alignment  calculate use  anchor contain read  remove   read  chain together   bestoverlap fashion  layout construct  assembly backbone   best overlap  consensus align  relate read   backbone  calculate   likely sequence   consensus output full size image  use multiple sequence alignment  clean  compress read  remove read  structural errors  socalled chimeras fig  figure  figure2 read correction  multiple sequence alignment  leave portion show remove  false positive anchor contig brown  appear     multiple alignment  right portion show detection   chimeric read  align   multiple read  breakpoint  detect    read   align   leave portion   target read   consistent    read    align   right portion   target read full size image  construct  best overlap graph use  clean compress long read fig   uncompress  chain together  long read fig   resort   consensus algorithm  convert   dna sequence fig  detail  procedure   explain   explanation  procedure    find   previous sparseassembler  ngs technology5   omit ",5
133,MaSuRcA,"The MaSuRCA genome assembler
The key idea driving the development of the MaSuRCA assembler is to reduce the complexity of the data by transforming the high coverage (typically 50–100× or deeper) of the paired-end reads into 2–3× coverage by fairly long (maximal) super-reads. The reduced data could then be efficiently assembled by a modified Celera Assembler. Here we want to describe some of the modules in MaSuRCA. In this section, we simply say super-read for maximal super-reads, as non-maximal super-reads are not used in assembly. QuorUM error correction. We use the QuorUM error corrector due to its stability and high performance (Marçais et al., 2013). One may substitute another error corrector, such as Quake (Kelley et al., 2010), as long as the output is processed in such a way that read names are preserved and the mates are reported together. Creation of k-unitigs. Extending each read base by base is computationally inefficient. Therefore, we use error-corrected reads to construct k-unitigs as follows. We create a k-mer count look-up table, using a hash constructed by the Jellyfish program (Marçais and Kingsford, 2011), which allows us to determine quickly how many times each k-mer occurs in the reads. Given any k-mer, there are four possible k-mers that may follow it, one for each possible extension by A,C,G or T. We look up how often each of these occurs, and if we find only one of these four extensions, we say that our original k-mer has a unique following k-mer. We similarly check whether there is a unique preceding k-mer, analogously defined. We call a k-mer simple if it has a unique preceding k-mer and a unique following k-mer. A k-unitig is a string of maximal length such that every k-mer in it is simple except for the first and the last. There are alternative names for quite similar concepts, such as ‘unipaths’ in Allpaths-LG (Gnerre et al., 2011). By construction, no k-mer can belong to more than one k-unitig. (But note that a k-unitig itself can occur at multiple sites in the genome, as will happen for exact repeats.) Hence, if a k-mer from a k-unitig U is encountered in the genome, then the whole sequence of the k-unitig U must appear at that location in the genome. We will use this property of k-unitigs when creating super-reads. Super-reads from paired-end reads. Because no k-mer can belong to more than one k-unitig, if a read has a k-mer that occurs in a k-unitig, the read and the k-unitig can be aligned to one another. In the simplest case, when a read is a substring of a k-unitig, then that k-unitig is the read’s super-read. In other cases, we use individual reads to merge the k-unitigs that overlap them into a single longer super-read. Figure 2 shows an example of this process, in which a read R is extended to a super-read that consists of two k-unitigs that overlap by k-1 bases. K-mers M1 and M2 belong to R and also to k-unitigs K1 and K2, which may extend beyond the ends of R. Two reads that differ even at only one base will map and be extended by different sets of k-unitigs. The k-unitigs by construction can be connected by the exact k-1 end sequence overlaps. We call them k-unitig overlaps. An external file that holds a picture, illustration, etc. Object name is btt476f2p.jpg Open in a separate window Fig. 2. An example of a read whose super-read has two k-unitigs. Read R contains k-mers M1 and M2 on its ends. M1 and M2 each belong to k-unitigs K1 and K2, respectively. K-unitigs K1 and K2 are shown in blue, and the matching k-mers M1 and M2 are shown in red and green. K1 and K2 overlap by k-1 bases. We extend read R on both ends producing a super-read, also depicted in blue. A super-read can consist of one k-unitig or can contain many k-unitigs If the reads are paired, we examine each pair of reads (we also call them mated reads or mate pairs) and map each read to the k-unitigs, and then look for a unique path of k-unitigs connected by k-unitig overlaps that connects the two reads. If we find such a path, then we extend both paired-end reads to a new super-read, created by merging the k-unitigs on this unique path. Often this process of creating a super-read from a pair fails, e.g. when there is a repeated sequence or a gap in coverage between the mates. In this case, we form a super-read for each of the mates separately, and the mate pair is submitted to the assembler as linking mate pair along with the corresponding super-reads. Jumping library filter. Although not required for MaSuRCA, long ‘jumping’ libraries (i.e. in which each pair of reads are several kilobases apart) are often part of genome sequencing projects, where they provide valuable long-range connectivity data for the scaffolding. However, these libraries sometimes contain reads that are chimeric, i.e. they derive from two distant parts of the genome, or they may be mis-oriented because of problems in library construction. In particular, some jumping libraries use a circularization protocol, in which the DNA fragment is circularized to bring together its ends. The resulting read pairs face outward, meaning that the opposite ends of the original DNA fragment occur on the 3′ ends of the two reads. Misoriented mates from these libraries are ‘innies’ that originated from one ‘side’ of the original circle that did not contain a junction site. These ‘innies’ should be treated as regular short insert (300–400 bp) paired-end reads. Because these kinds of errors create many problems in the scaffolding phase, it is imperative that most of them be removed before giving the data to the assembly program. We use QuORUM error correction and the super-reads procedures to perform library cleaning. First, during error correction we create a k-mer database from the paired-end reads only, excluding the jumping libraries. Note that circularized reads include a linker sequence, and the concatenation of the linker and the source DNA represents sequence that should not appear in the genome. If one of the reads in a jumping library mate pair contains a junction site (identified by the linker), then k-mers that span the junction site in that read will not be found in the k-mer database. The error corrector then trims the read at the junction site. We then create super-reads from the jumping library reads. Here we introduce a modification to the algorithm that accepts any path of overlapping k-unitigs in building a super-read. By doing aggressive joining, we are able to identify most of the non-junction ‘innie’ pairs, because they end up in the same super-read. Next we look for redundant jumping library pairs, where the same DNA fragment was amplified before circularization, producing two or more pairs of reads that represent the same fragment. Because the assembly process assumes that each mate pair is an independent sample from the genome, such redundant pairs can lead to assembly errors later. We examine the positions of reads in the resulting super-reads and look for redundant pairs, where both sets of forward and reverse reads end up in the same one or two super-reads with the same offset. We reduce such pairs to a single copy. Contiging and scaffolding with the CABOG assembler. We keep track of the number of reads that generated each maximal super-read and the positions of those reads in the maximal super-read; thus allowing us to precisely report positions of all reads in the assembly. After creating the super-reads, we assemble the data with a modified version of the CABOG assembler (Miller et al., 2008, 2010), updated to allow long super-reads as well as short reads in the same assembly. We supply the following four types of data to CABOG: super-reads linking mates cleaned and de-duplicated jumping library mate pairs (if available) other available LR We note that the modified version of CABOG 6.1 used in MaSuRCA is not capable of supporting the long high-error-rate reads generated by the PacBio technology. CABOG uses read coverage statistics (Myers et al., 2000) to distinguish between unique and repetitive regions of the assembly. Each super-read typically represents multiple reads, and we keep track of how many reads belong to each super-read and the position of these reads. We modified CABOG to incorporate these data, using counts of all the original reads in its computation of coverage statistics. This major modification allowed us to use CABOG to assemble super-read data together with Illumina, 454 and Sanger reads. Gap filling. The final major step in the MaSuRCA assembler is gap filling. This step is aimed at filling gaps in scaffolds that are relatively short and do not contain complicated repetitive structures. Such gaps may occur because the assembler software over-trimmed the reads in the error correction step, or because it misestimated the coverage statistics for some contigs or because of other reasons. Our gap-filling technique is again centered on our super-reads algorithm. For each gap in each scaffold, we create faux 100-bp paired-end reads from the ends of the contigs surrounding the gap. We call these contig-end reads. From each pair of contig-end reads, we then use the 21mers whose counts are globally below a threshold of 1000 (to avoid highly repetitive sequences) to pull in the original uncorrected reads where either the read or its mate contains one of those k-mers. From this set, we create a local bin of reads corresponding to the gap in question. We then create k-unitigs from the reads in the bin for k ranging from 17 to 85, and see if we can create a unique super-read that contains both contig-end reads. If we are able to achieve that for some value of k, then the resulting super-read is used as a local patch of sequence to fill the gap. We found that, depending on the genome, 10–20% of the gaps in the scaffolds can be filled using this approach",Assembly," masurca genome assembler
 key idea drive  development   masurca assembler   reduce  complexity   data  transform  high coverage typically   deeper   pairedend read   coverage  fairly long maximal superreads  reduce data could   efficiently assemble   modify celera assembler   want  describe    modules  masurca   section  simply say superread  maximal superreads  nonmaximal superreads   use  assembly quorum error correction  use  quorum error corrector due   stability  high performance marçais    one may substitute another error corrector   quake kelley     long   output  process    way  read name  preserve   mat  report together creation  kunitigs extend  read base  base  computationally inefficient therefore  use errorcorrected read  construct kunitigs  follow  create  kmer count lookup table use  hash construct   jellyfish program marçais  kingsford   allow   determine quickly  many time  kmer occur   read give  kmer   four possible kmers  may follow  one   possible extension  acg    look   often    occur    find  one   four extensions  say   original kmer   unique follow kmer  similarly check whether    unique precede kmer analogously define  call  kmer simple     unique precede kmer   unique follow kmer  kunitig   string  maximal length   every kmer    simple except   first   last   alternative name  quite similar concepts   unipaths  allpathslg gnerre     construction  kmer  belong    one kunitig  note   kunitig   occur  multiple sit   genome   happen  exact repeat hence   kmer   kunitig   encounter   genome   whole sequence   kunitig  must appear   location   genome   use  property  kunitigs  create superreads superreads  pairedend read   kmer  belong    one kunitig   read   kmer  occur   kunitig  read   kunitig   align  one another   simplest case   read   substring   kunitig   kunitig   read superread   case  use individual read  merge  kunitigs  overlap    single longer superread figure  show  example   process    read   extend   superread  consist  two kunitigs  overlap   base kmers    belong    also  kunitigs     may extend beyond  end   two read  differ even   one base  map   extend  different set  kunitigs  kunitigs  construction   connect   exact  end sequence overlap  call  kunitig overlap  external file  hold  picture illustration etc object name  btt476f2pjpg open   separate window fig   example   read whose superread  two kunitigs read  contain kmers      end     belong  kunitigs    respectively kunitigs     show  blue   match kmers     show  red  green    overlap   base  extend read    end produce  superread also depict  blue  superread  consist  one kunitig   contain many kunitigs   read  pair  examine  pair  read  also call  mat read  mate pair  map  read   kunitigs   look   unique path  kunitigs connect  kunitig overlap  connect  two read   find   path   extend  pairedend read   new superread create  merge  kunitigs   unique path often  process  create  superread   pair fail      repeat sequence   gap  coverage   mat   case  form  superread     mat separately   mate pair  submit   assembler  link mate pair along   correspond superreads jump library filter although  require  masurca long jump libraries     pair  read  several kilobases apart  often part  genome sequence project   provide valuable longrange connectivity data   scaffold however  libraries sometimes contain read   chimeric   derive  two distant part   genome   may  misoriented   problems  library construction  particular  jump libraries use  circularization protocol    dna fragment  circularize  bring together  end  result read pair face outward mean   opposite end   original dna fragment occur   ′ end   two read misoriented mat   libraries  innies  originate  one side   original circle    contain  junction site  innies   treat  regular short insert   pairedend read   kinds  errors create many problems   scaffold phase   imperative      remove  give  data   assembly program  use quorum error correction   superreads procedures  perform library clean first  error correction  create  kmer database   pairedend read  exclude  jump libraries note  circularize read include  linker sequence   concatenation   linker   source dna represent sequence    appear   genome  one   read   jump library mate pair contain  junction site identify   linker  kmers  span  junction site   read    find   kmer database  error corrector  trim  read   junction site   create superreads   jump library read   introduce  modification   algorithm  accept  path  overlap kunitigs  build  superread   aggressive join   able  identify    nonjunction innie pair   end     superread next  look  redundant jump library pair    dna fragment  amplify  circularization produce two   pair  read  represent   fragment   assembly process assume   mate pair   independent sample   genome  redundant pair  lead  assembly errors later  examine  position  read   result superreads  look  redundant pair   set  forward  reverse read end     one  two superreads    offset  reduce  pair   single copy contiging  scaffold   cabog assembler  keep track   number  read  generate  maximal superread   position   read   maximal superread thus allow   precisely report position   read   assembly  create  superreads  assemble  data   modify version   cabog assembler miller     update  allow long superreads  well  short read    assembly  supply  follow four type  data  cabog superreads link mat clean  deduplicated jump library mate pair  available  available   note   modify version  cabog  use  masurca   capable  support  long higherrorrate read generate   pacbio technology cabog use read coverage statistics myers     distinguish  unique  repetitive regions   assembly  superread typically represent multiple read   keep track   many read belong   superread   position   read  modify cabog  incorporate  data use count    original read   computation  coverage statistics  major modification allow   use cabog  assemble superread data together  illumina   sanger read gap fill  final major step   masurca assembler  gap fill  step  aim  fill gap  scaffold   relatively short    contain complicate repetitive structure  gap may occur   assembler software overtrimmed  read   error correction step    misestimate  coverage statistics   contigs     reason  gapfilling technique   center   superreads algorithm   gap   scaffold  create faux  pairedend read   end   contigs surround  gap  call  contigend read   pair  contigend read   use  21mers whose count  globally   threshold    avoid highly repetitive sequence  pull   original uncorrected read  either  read   mate contain one   kmers   set  create  local bin  read correspond   gap  question   create kunitigs   read   bin   range      see    create  unique superread  contain  contigend read    able  achieve    value     result superread  use   local patch  sequence  fill  gap  find  depend   genome    gap   scaffold   fill use  approach",5
134,TIGER,"TIGER: tiled iterative genome assembler
Most assemblers can deal with small genomes (such as E. coli) very well using a small amount of computation resource and time. For very large genomes (such as mammalian-size genomes), most assemblers either cannot produce good results or require tremendous amount of resources and/or time. Besides, assemblers usually have their own design characteristics targeting at specific types of genomes [21]. Our approach aims to substantially reduce the computational complexity and resources needed for large genome assembly. The key innovation is to more effectively divide the genome assembly problem into smaller sub-problems and assemble them individually (without inter-node communication) with the flexibility of using various off-the-shelf assemblers. We use an iterative refinement approach to gradually improve the quality of problem partitioning and the overall solution. Key ideas Tiled genome assembly The rationale of our method follows our belief that the genome assembly could be done part-by-part instead of as a whole, namely the input reads can be divided into multiple tiles (or clusters) and the assembly results of all tiles can be merged as the final assembly. We call this approach tiled genome assembly. We observed that if we can have all related information (e.g., reads) for only a short fragment of a target genome, most assemblers would get excellent results and require much less memory. Including more reads that correspond to larger regions increases memory requirement and potentially makes assembly results worse. The main reason is that de novo assemblers cannot tell which part of the genome the reads belong to. However, if we can partition the reads in an effective way, assemblers can produce better results while requiring much less memory. Taking the DBG-based assemblers as an example, ideally a contig (from a specific region in the target genome) is supposed to be built using only the k-mers extracted by the reads contributing to that region. However, most assemblers extract the k-mers from all the input reads and mix them together when constructing the DBG. More specifically, the k-mers whose source reads are not contributing to a specific region in the target genome may still be used in the DBG construction process. For such k-mers, we call them ambiguous k-mers. For genomes that are less repetitive, the ambiguous k-mers could be few. But for genomes that are highly repetitive, they can be significant enough to confuse the assembly process. Therefore, we designed a new approach to partition the input reads into multiple tiles. Our goal is to have each tile contain only those reads contributing to a specific region of the target genome. The reads in such read tiles are called well-clustered reads. Thus the effect from ambiguous k-mers can be dramatically reduced. Since each read tile has all the necessary information, no communication would be needed among the assemblies of different read tiles. Since regions in the target genome can be assembled independently and as a result, each assembly will need less memory to complete. Read clustering based on clustered contigs A well-clustered read tile should contribute to a continuous region of the target genome. The region can be composed of one long contig or a set of contigs covering the whole region without gaps in-between. A set of such contigs is called well-clustered and can be obtained by sorting or clustering closely related contigs together. Therefore, by aligning the input reads against a well-clustered contig set, the reads having high similarity with sub-sections in the contigs can be collected as a well-clustered read tile. This process is called read clustering. The collected reads can then be assembled to produce a similar set of the contigs but with improved contig lengths and quality. Intermediate reference genome A target genome can be considered as a combination of multiple continuous regions (the minimum number is the number of chromosomes in the target genome), where each region can be contributed completely by one or many contigs. Therefore, ultimately there would be multiple well-clustered contig sets corresponding to multiple regions in the target genome. In our approach, we treat the contigs from assembly as the intermediate reference genome and arrange the contigs in multiple clustered contig sets. For de novo assembly, we start from random partitions of the reads in tiles, assemble the reads in each tile, and merge all contig sets into one as the intermediate reference. In this case, the reads in an initial, randomly partitioned tile will correspond to random regions in the target genome. As a result, the initial contig sets that serve as the intermediate reference will likely be fragmented and will have errors. Our approach iteratively improves the clustering and thus the quality of the intermediate reference genome. In the end, the intermediate reference genome converges to the final target genome. Iterative assembly The transformation from reads to contigs and from contigs to reads forms a cycle. Thus the whole assembly flow can be iterative. As more transformation iterations are performed, contigs become longer with higher quality since read clustering improves, and each tile contains less irrelevant information that may confuse the assembly process. The Tiger algorithm Based on aforementioned ideas, we developed ""Tiled Iterative GEnome assembleR,"" or Tiger. Here ""tile"" is a synonym of ""set"" or ""cluster"", representing the tiled computation nature in the assembly process. The conceptual flow is illustrated in Figure 1. Figure 1 figure1 Schematic view of our iterative framework for genome assembly. Full size image Step 1. Reads partitioning: We first partition the input reads into multiple read tiles. In our current implementation, the input reads are randomly partitioned evenly into N subsets, which can be determined by users based on the available resources and the total size of the input reads. Step 2. Read assembly: Read tiles are assembled individually, using an off-the-shelf assembler, such as Velvet, embedded into Tiger. Depending on the available system memory, the assembly of read tiles can be done independently in serial or in parallel on a shared or distributed memory computer cluster. There is no communication between the assemblies of different read tiles. For the embedded assembler requiring specifying a k-mer size, k-mer sizes are decided either manually by users or automatically through the auto-k-mer scheme in Tiger. For the manual k-mer designation, a k-mer size is used in all read tile assemblies for all Tiger iterations. Otherwise, the auto-k-mer scheme randomly picks k-mer sizes within a given range and records the best k-mer size in the assembly results. The best k-mer size and the randomly picked ones will be considered in the subsequent assemblies. User-specified k-mer size can be introduced into this k-mer history database but may not be used again if the first attempt is not good. The number of used reads in the assembly, the total length of the contigs, and the resultant N50s are used to evaluate whether a k-mer size can help produce the best result without knowing the target genome. This avoids the problem of picking a contig set with high N50 and low coverage and enables Tiger to find a good direction in the iterative process and to converge to high quality results. Since Step 2 is the first time to assemble the initial read tiles, the contigs can be short and may cause long running time in the later iterations. We address this issue by merging the contig sets and feed the merged contig set to Velvet with the LONGSEQUENCE flag enabled. Velvet may further elongate the contigs by treating the input contigs as long reads. The new contig set is used when it is better than the merged contig set. The output contig set is scaffolded by SSPACE [22]. The scaffolded contig set is the input to Step 3. The purpose of this scaffolding process is to leverage paired-end information to bridge contigs which may be from different assemblies. This is beneficial for better clustered contigs at Step 3. The scaffolding process also helps resolve duplicated contigs from different assemblies. Step 3. Contig clustering: The overall contig clustering algorithm is depicted in Figure 2. A graph that models the contig connectivity intensity is built from the merged contig set. This graph is called the contig connectivity graph. Graph vertices are the contigs. Vertex weights are contig lengths. Edge weights are defined based on the contig overlapping degree with each other. Note that the contig connectivity graph is much smaller than the DBG so it uses much smaller amount of memory. We then apply a graph-partitioning tool, METIS [23], to partition the graph into contig clusters. METIS is known to be fast and memory-efficient in processing millions of graph vertexes. Figure 2 figure2 Contig clustering algorithm. Words are extracted from contigs. The number of common words between two contigs is used as the edge weight in the graph. Contig lengths are modeled as vertex weights. The contig connectivity graph is thus built, followed by the METIS partitioning process. The partitioned sub-graphs are clustered contig sets. Full size image The contig lengths (vertex weights) are given less importance than the contig overlapping degrees (edge weights) in the graph partitioning process. This is because we want the partitioned contig connectivity sub-graphs to be more edge-oriented instead of being vertex-oriented. But we still need to consider the vertex weights for the situations where there exist many short contigs with little connectivity in-between. This is very common for the assembly results in the first few iterations on assembling randomly partitioned read tiles. These short contigs ought to be distributed to all clusters evenly. This not only preserves their existence in the following Tiger iterations but also reduces their influence on the rest of the clustered contigs. By focusing graph partitioning on edge intensity, overlapping contigs will be grouped together and would be rebuilt as one long complete contig later at Step 5. These contigs are used to produce well-clustered reads in the read clustering process at Step 4. That is, this contig clustering step makes crucial contribution to the quality of results of the later steps. Building of a contig connectivity graph can be time-consuming if a traditional sequence alignment method is used, like the Smith-Waterman algorithm [24]. Since the degree of overlap between contigs need not be determined exactly for our purposes, we apply a heuristic algorithm based on the image recognition algorithm using vocabulary trees in [25], with inverse document frequency scoring dropped. We begin by extracting consecutive sequences of equal length (called words) from each of the contigs in the set. The extracted words are used to build a map (or inverted file) from the words to the contigs containing them. A contig connectivity graph is then built from the map with the edge weights being set to the number of words in common between the vertices (contigs) connected by that edge. Since the connectivity graph stores only the contig connectivity information, the memory usage of this step is much lower than that at the read assembly step. Regarding runtime, building a connectivity graph dominates the whole step. Building the word map can be done in parallel but we leave it as future work. Overall the runtime is still much shorter than Step 4 and 5. Step 4. Read clustering: The entire input read set is aligned to the contig sets from Step 3. The reads having high similarity to each contig set are collected as one read cluster. Each read is collected once for each cluster. This means a read can appear in multiple read clusters if similar contigs are not clustered together. This process guarantees any read potentially contributing to a contig set will be collected. The read-to-contig alignment is done by Bowtie [26]. For paired-end reads, if one of a read pair aligns to the given contig cluster, both reads are collected. This step provides the opportunity to extend and/or bridge the contigs. This clustering process can be done in parallel or in serial on a shared or distributed memory computer cluster. No communication is needed between read tiles. The required memory is also proportional to the size of a read tile. Step 5. Read assembly: We assemble the read tiles, the same as we do at Step 2. But the assembly of the merged contigs from all read tile assemblies may not be performed. If the assembly of the merged contigs is not improving, it is skipped in later iterations to save time. Based on our experience, we found it is useful to have this additional assembly in the first few iterations. Step 6. Post processing: If we have reached the given number of iterations, we will just exit. Otherwise, go to Step 3. Step 3, 4, and 5 form an iterative process. To sum up, the rationale behind our framework is that the improvement on contig quality of the current iteration can be carried over to the next iteration through more accurate read clustering. An optimal clustering solution will be achieved if only reads contributing to a contig are clustered for assembling the contig. This approach differentiates our algorithm from the previous work and provides our framework the capability of improving an existing contig set further.",Assembly,"tiger tile iterative genome assembler
 assemblers  deal  small genomes    coli  well use  small amount  computation resource  time   large genomes   mammaliansize genomes  assemblers either cannot produce good result  require tremendous amount  resources andor time besides assemblers usually    design characteristics target  specific type  genomes   approach aim  substantially reduce  computational complexity  resources need  large genome assembly  key innovation    effectively divide  genome assembly problem  smaller subproblems  assemble  individually without internode communication   flexibility  use various offtheshelf assemblers  use  iterative refinement approach  gradually improve  quality  problem partition   overall solution key ideas tile genome assembly  rationale   method follow  belief   genome assembly could   partbypart instead    whole namely  input read   divide  multiple tile  cluster   assembly result   tile   merge   final assembly  call  approach tile genome assembly  observe       relate information  read    short fragment   target genome  assemblers would get excellent result  require much less memory include  read  correspond  larger regions increase memory requirement  potentially make assembly result worse  main reason    novo assemblers cannot tell  part   genome  read belong  however    partition  read   effective way assemblers  produce better result  require much less memory take  dbgbased assemblers   example ideally  contig   specific region   target genome  suppose   build use   kmers extract   read contribute   region however  assemblers extract  kmers    input read  mix  together  construct  dbg  specifically  kmers whose source read   contribute   specific region   target genome may still  use   dbg construction process   kmers  call  ambiguous kmers  genomes   less repetitive  ambiguous kmers could     genomes   highly repetitive    significant enough  confuse  assembly process therefore  design  new approach  partition  input read  multiple tile  goal     tile contain   read contribute   specific region   target genome  read   read tile  call wellclustered read thus  effect  ambiguous kmers   dramatically reduce since  read tile    necessary information  communication would  need among  assemblies  different read tile since regions   target genome   assemble independently    result  assembly  need less memory  complete read cluster base  cluster contigs  wellclustered read tile  contribute   continuous region   target genome  region   compose  one long contig   set  contigs cover  whole region without gap inbetween  set   contigs  call wellclustered    obtain  sort  cluster closely relate contigs together therefore  align  input read   wellclustered contig set  read  high similarity  subsections   contigs   collect   wellclustered read tile  process  call read cluster  collect read    assemble  produce  similar set   contigs   improve contig lengths  quality intermediate reference genome  target genome   consider   combination  multiple continuous regions  minimum number   number  chromosomes   target genome   region   contribute completely  one  many contigs therefore ultimately  would  multiple wellclustered contig set correspond  multiple regions   target genome   approach  treat  contigs  assembly   intermediate reference genome  arrange  contigs  multiple cluster contig set   novo assembly  start  random partition   read  tile assemble  read   tile  merge  contig set  one   intermediate reference   case  read   initial randomly partition tile  correspond  random regions   target genome   result  initial contig set  serve   intermediate reference  likely  fragment    errors  approach iteratively improve  cluster  thus  quality   intermediate reference genome   end  intermediate reference genome converge   final target genome iterative assembly  transformation  read  contigs   contigs  read form  cycle thus  whole assembly flow   iterative   transformation iterations  perform contigs become longer  higher quality since read cluster improve   tile contain less irrelevant information  may confuse  assembly process  tiger algorithm base  aforementioned ideas  develop ""tiled iterative genome assembler""  tiger  ""tile""   synonym  ""set""  ""cluster"" represent  tile computation nature   assembly process  conceptual flow  illustrate  figure  figure  figure1 schematic view   iterative framework  genome assembly full size image step  read partition  first partition  input read  multiple read tile   current implementation  input read  randomly partition evenly   subsets    determine  users base   available resources   total size   input read step  read assembly read tile  assemble individually use  offtheshelf assembler   velvet embed  tiger depend   available system memory  assembly  read tile    independently  serial   parallel   share  distribute memory computer cluster    communication   assemblies  different read tile   embed assembler require specify  kmer size kmer size  decide either manually  users  automatically   autokmer scheme  tiger   manual kmer designation  kmer size  use   read tile assemblies   tiger iterations otherwise  autokmer scheme randomly pick kmer size within  give range  record  best kmer size   assembly result  best kmer size   randomly pick ones   consider   subsequent assemblies userspecified kmer size   introduce   kmer history database  may   use    first attempt   good  number  use read   assembly  total length   contigs   resultant n50s  use  evaluate whether  kmer size  help produce  best result without know  target genome  avoid  problem  pick  contig set  high n50  low coverage  enable tiger  find  good direction   iterative process   converge  high quality result since step    first time  assemble  initial read tile  contigs   short  may cause long run time   later iterations  address  issue  merge  contig set  fee  merge contig set  velvet   longsequence flag enable velvet may  elongate  contigs  treat  input contigs  long read  new contig set  use    better   merge contig set  output contig set  scaffold  sspace   scaffold contig set   input  step   purpose   scaffold process   leverage pairedend information  bridge contigs  may   different assemblies   beneficial  better cluster contigs  step   scaffold process also help resolve duplicate contigs  different assemblies step  contig cluster  overall contig cluster algorithm  depict  figure   graph  model  contig connectivity intensity  build   merge contig set  graph  call  contig connectivity graph graph vertices   contigs vertex weight  contig lengths edge weight  define base   contig overlap degree    note   contig connectivity graph  much smaller   dbg   use much smaller amount  memory   apply  graphpartitioning tool metis   partition  graph  contig cluster metis  know   fast  memoryefficient  process millions  graph vertexes figure  figure2 contig cluster algorithm word  extract  contigs  number  common word  two contigs  use   edge weight   graph contig lengths  model  vertex weight  contig connectivity graph  thus build follow   metis partition process  partition subgraphs  cluster contig set full size image  contig lengths vertex weight  give less importance   contig overlap degrees edge weight   graph partition process     want  partition contig connectivity subgraphs    edgeoriented instead   vertexoriented   still need  consider  vertex weight   situations   exist many short contigs  little connectivity inbetween    common   assembly result   first  iterations  assemble randomly partition read tile  short contigs ought   distribute   cluster evenly    preserve  existence   follow tiger iterations  also reduce  influence   rest   cluster contigs  focus graph partition  edge intensity overlap contigs   group together  would  rebuild  one long complete contig later  step   contigs  use  produce wellclustered read   read cluster process  step     contig cluster step make crucial contribution   quality  result   later step build   contig connectivity graph   timeconsuming   traditional sequence alignment method  use like  smithwaterman algorithm  since  degree  overlap  contigs need   determine exactly   purpose  apply  heuristic algorithm base   image recognition algorithm use vocabulary tree    inverse document frequency score drop  begin  extract consecutive sequence  equal length call word     contigs   set  extract word  use  build  map  invert file   word   contigs contain   contig connectivity graph   build   map   edge weight  set   number  word  common   vertices contigs connect   edge since  connectivity graph store   contig connectivity information  memory usage   step  much lower     read assembly step regard runtime build  connectivity graph dominate  whole step build  word map     parallel   leave   future work overall  runtime  still much shorter  step    step  read cluster  entire input read set  align   contig set  step   read  high similarity   contig set  collect  one read cluster  read  collect    cluster  mean  read  appear  multiple read cluster  similar contigs   cluster together  process guarantee  read potentially contribute   contig set   collect  readtocontig alignment    bowtie   pairedend read  one   read pair align   give contig cluster  read  collect  step provide  opportunity  extend andor bridge  contigs  cluster process     parallel   serial   share  distribute memory computer cluster  communication  need  read tile  require memory  also proportional   size   read tile step  read assembly  assemble  read tile       step    assembly   merge contigs   read tile assemblies may   perform   assembly   merge contigs   improve   skip  later iterations  save time base   experience  find   useful    additional assembly   first  iterations step  post process    reach  give number  iterations    exit otherwise   step  step     form  iterative process  sum   rationale behind  framework    improvement  contig quality   current iteration   carry    next iteration   accurate read cluster  optimal cluster solution   achieve   read contribute   contig  cluster  assemble  contig  approach differentiate  algorithm   previous work  provide  framework  capability  improve  exist contig set ",5
135,PANDAseq,"PANDAseq: paired-end assembler for illumina sequences
PANDAseq aligns each set of paired-end sequence reads in a three-step process. First, it determines the locations of the amplification primers, if they are specified and were sequenced. Then, it identifies the optimal overlap. Finally, it reconstructs the complete sequence, correcting any errors, and checks for various constraints, such as length and quality. To score alignments, we calculate the probability that the true nucleotides, 𝑋ˆ and Ŷ, are the same, given the observed nucleotides, X and Y. We estimate this with the included quality information found in the Illumina reads. For each base, CASAVA provides an encoded quality score, which is the probability of the base being miscalled. This probability (ϵ) is approximated by 10−𝐴0−6410=10−𝐴1−3310 where A0 is the ASCII quality value in the Illumina analysis pipeline versions before CASAVA 1.8 and A 1 is the ASCII value used in CASAVA 1.8. [7] Assuming all nucleotides are equally likely (i.e., the prior probability that the true bases match is 14), and that sequencing errors are independent and result in equiprobable choices over the other three nucleotides, the probability that the true bases match, given that the sequenced bases match, is: Pr[𝑋ˆ=Ŷ∣∣𝑋=𝑌]=(1−𝜀𝑋)(1−𝜀𝑌)+𝜀𝑋𝜀𝑌3 and the probability that the true bases match, given the sequenced bases mismatch, is: Pr[𝑋ˆ=Ŷ∣∣𝑋≠𝑌]+13(1−𝜀𝑌)𝜀𝑋+29𝜀𝑋𝜀𝑌=13(1−𝜀𝑋)𝜀𝑌 If one of the bases is an uncalled base, N, then the probability that the bases match is: Pr[𝑋ˆ=Ŷ∣∣𝑌=𝑁]=14 Using these probabilities, PANDAseq begins the assembly process by determining the positions of forward and reverse primers, if supplied. To accomplish this, the program finds the first offset, x, where the primer aligns. For a primer P and a sequence S, the program calculates ∏𝑖=0∣∣𝑃∣∣−1Pr[Ŝ𝑖+𝑥=𝑃𝑖] while assuming that 𝜀𝑃𝑖=1−10−4.1, which is the highest value score assigned by Illumina [8] and, intuitively, assuming that 𝑃ˆ is P. The program then finds the best overlap greater than a specified threshold for the forward and reverse sequences, F and R, respectively. If no suitable overlap is found, then the read pair is discarded. This is done for the entire read, even if there are primers to be removed, as it is possible for the overlap to be sufficiently long to be in the primer region. A schematic is shown in Figure 1. The value of c ∈ [1, min(|F|, |R|)) is chosen which maximises this formula: Pr[𝐹,𝑅|𝑐]⋅∏𝑖=1…𝑐Pr[𝐹ˆ𝑖+𝑓=𝑅ˆ𝑖]⋅∏𝑖=1…𝑟Pr[𝑅𝑖+𝑐]=∏𝑖=1…𝑓Pr[𝐹𝑖] where Pr[𝐹𝑖]=14 and Pr[𝑅𝑖]=14 and the remainder is as above with e fixed at a value determined empirically to be the average error rate. This value of ϵ was calculated by counting the mismatch rate in known index tags in a defined community data set (described below). This parameter need not be retuned as it is only an estimate of the error. Because the index read is short and sequenced earlier in the process, it likely has fewer errors and, therefore, its error rate should underestimate the true error rate. Regardless, the error rate specified for this step should not negatively affect the ability of PANDAseq to identify the best overlap for the forward and reverse reads. Once the overlap is selected, the output sequence is constructed and an overall quality score is calculated. During this process, the primer regions are disregarded if primers were specified. The unpaired regions are copied from the available strands and the quality score for these regions is the product of the probability of those bases being correct. For the overlapping region, the decision-making process is more complex. If the bases agree, the base is included and the quality of this base is assumed to be Pr[𝑋ˆ=Ŷ∣∣𝑋=𝑌]. If the bases disagree, the base with the higher quality score is chosen and the quality of this base is assumed to be Pr[𝑋ˆ=Ŷ∣∣𝑋≠𝑌]. If either or both bases are uncalled, they are considered to always match, noting that unassigned bases are always associated with the lowest quality score by CASAVA [8]. In certain cases, the CASAVA pipeline masks the quality score at the end of the read, replacing all quality scores with the lowest quality score [8]. In this case, special quality scoring is used by PANDAseq. If one base is masked, the probability of the other base is used if the bases match or uniformly random, 14, is used if they do not match. If both are in the masked region, the quality is assumed to be uniformly random, 14. The constructed sequence can then be validated against user-specified criteria. The quality score assigned to the assembled sequence is the geometric mean of the quality scores calculated above, which compensates for the variable lengths of the final sequences. PANDAseq enables users to reject sequences based on low quality score, lengths that are too short or too long, or the presence of uncalled bases. A module system is also available within PANDAseq to allow more sophisticated validation of user sequences, such as verification of known secondary structure or conserved regions. Note that there is a detailed manual included with the software that describes example usage scenarios.",Assembly,"pandaseq pairedend assembler  illumina sequences
pandaseq align  set  pairedend sequence read   threestep process first  determine  locations   amplification primers    specify   sequence   identify  optimal overlap finally  reconstruct  complete sequence correct  errors  check  various constraints   length  quality  score alignments  calculate  probability   true nucleotides       give  observe nucleotides     estimate    include quality information find   illumina read   base casava provide  encode quality score    probability   base  miscall  probability   approximate  𝐴0𝐴1     ascii quality value   illumina analysis pipeline versions  casava       ascii value use  casava   assume  nucleotides  equally likely   prior probability   true base match     sequence errors  independent  result  equiprobable choices    three nucleotides  probability   true base match give   sequence base match  pr𝑋ˆŷ∣∣𝑋𝑌𝜀𝑋𝜀𝑌𝜀𝑋𝜀𝑌3   probability   true base match give  sequence base mismatch  pr𝑋ˆŷ∣∣≠𝑌𝜀𝑌𝜀𝑋29𝜀𝑋𝜀𝑌𝜀𝑋𝜀𝑌  one   base   uncalled base    probability   base match  pr𝑋ˆŷ∣∣ use  probabilities pandaseq begin  assembly process  determine  position  forward  reverse primers  supply  accomplish   program find  first offset    primer align   primer    sequence   program calculate ∏∣∣∣∣1prŝ𝑖𝑥𝑃𝑖  assume  𝜀𝑃𝑖    highest value score assign  illumina   intuitively assume      program  find  best overlap greater   specify threshold   forward  reverse sequence    respectively   suitable overlap  find   read pair  discard      entire read even    primers   remove    possible   overlap   sufficiently long     primer region  schematic  show  figure   value   ∈  minf   choose  maximise  formula pr𝐹𝑅𝑐⋅∏…𝑐pr𝐹ˆ𝑖𝑓𝑅ˆ𝑖⋅∏…𝑟pr𝑅𝑖𝑐∏…𝑓pr𝐹𝑖  pr𝐹𝑖  pr𝑅𝑖   remainder      fix   value determine empirically    average error rate  value    calculate  count  mismatch rate  know index tag   define community data set describe   parameter need   retuned      estimate   error   index read  short  sequence earlier   process  likely  fewer errors  therefore  error rate  underestimate  true error rate regardless  error rate specify   step   negatively affect  ability  pandaseq  identify  best overlap   forward  reverse read   overlap  select  output sequence  construct   overall quality score  calculate   process  primer regions  disregard  primers  specify  unpaired regions  copy   available strand   quality score   regions   product   probability   base  correct   overlap region  decisionmaking process   complex   base agree  base  include   quality   base  assume   pr𝑋ˆŷ∣∣   base disagree  base   higher quality score  choose   quality   base  assume   pr𝑋ˆŷ∣∣≠  either   base  uncalled   consider  always match note  unassigned base  always associate   lowest quality score  casava   certain case  casava pipeline mask  quality score   end   read replace  quality score   lowest quality score    case special quality score  use  pandaseq  one base  mask  probability    base  use   base match  uniformly random   use     match      mask region  quality  assume   uniformly random   construct sequence    validate  userspecified criteria  quality score assign   assemble sequence   geometric mean   quality score calculate   compensate   variable lengths   final sequence pandaseq enable users  reject sequence base  low quality score lengths    short   long   presence  uncalled base  module system  also available within pandaseq  allow  sophisticate validation  user sequence   verification  know secondary structure  conserve regions note     detail manual include   software  describe example usage scenarios",5
136,ARACHNE,"ARACHNE: A Whole-Genome Shotgun Assembler
Input Data ARACHNE is designed to analyze sequence reads obtained from both ends of plasmid clones—that is, paired forward and reverse reads (although it can also be used with unpaired reads). The mean and standard deviation of the insert length of each plasmid clone can be specified individually. In practice, it is most convenient to describe each clone as belonging to one of several libraries characterized by these parameters. Each base in each read has an associated quality score, such as that produced by the PHRED computer program (Ewing et al. 1998). A quality score of q corresponds to a probability of 10−q/10 that the base is incorrect; a quality score of 40 thus corresponds to 99.99% accuracy. As an initial step, ARACHNE trims reads to eliminate terminal regions of extremely low quality and to eliminate reads containing very little high-quality sequence (see the later section on technical details, “Details of Assembly Algorithm”). The program also trims known vector sequences and eliminates known contaminants (e.g., sequence from the bacterial host or cloning vector). Overlap Detection and Alignment: Sort and Extend ARACHNE starts by detecting and aligning pairs of apparently overlapping reads, referred to here simply as overlaps. Some of these are false overlaps resulting from repeated sequences in the genome and will be eliminated in subsequent steps. Overlap detection is performed in an efficient manner. Rather than comparing all pairs of reads (which requires N2/2 comparisons, where N is the number of reads), the program uses a sort and extend strategy that scales approximately linearly. This strategy involves producing a sorted table of each k-letter subword (k-mer) together with its source—that is, the read in which it occurs and its position within the read (in practice, we use k = 24). The table is sorted so that identical k-mers appear consecutively (Batzoglou 2000). The program then excludes k-mers that occur with extremely high frequency, which typically correspond to high-copy, high-fidelity repeated sequences in the genome. We eliminate them to increase the efficiency of the overlap detection process rather than to eliminate repeated sequences, which do not cause a problem per se. The program then identifies all instances of read pairs that share one or more overlapping k-mer (which can be readily recognized from the sorted k-mer table), and uses a three-step process to align the reads in an efficient manner. The first step merges overlapping shared k-mers, the second step extends the shared k-mers to alignments, and the third step refines the alignments by dynamic programming (details provided in the section below entitled “The Alignment Module”). The overall process bears some resemblance to the FASTA algorithm (Pearson and Lipman 1988). This three-step process yields the most valid alignments between read pairs. Some valid alignments may be missed if the overlaps are short, of low quality, or consist solely of high-copy, high-fidelity repeats. Some invalid alignments may result from low-copy repeats that produce apparently overlapping sequence. Error Correction ARACHNE next detects and corrects sequencing errors by generating multiple alignments among overlapping reads. The program then identifies instances in which a base is overwhelmingly outvoted by bases aligned to it and corrects the base (Fig. ​(Fig.1).1). In practice, this occurs where there is a disagreement at an isolated position involving a single base (or occasionally two bases); the process takes into account the quality scores of the bases. Based on the cases examined, the majority of these instances appear to be sequencing errors rather than slightly different copies of a repeated sequence. In the latter instance, this is due to the fact that there are typically multiple occurrences of each alternative sequence and, thus, there is not an overwhelming vote for one alternative. ARACHNE similarly corrects occasional insertions and deletions resulting from what appear to be sequencing errors. As the reads are corrected, corresponding changes are made to the alignments. An external file that holds a picture, illustration, etc. Object name is 42244-16f1_L1TT.jpg Open in a separate window Figure 1 Correcting errors in reads. A portion of a multiple alignment between five reads is shown. In the highlighted column of the alignment, a base T of quality 30 is aligned only to bases C, some of which are of quality greater than 30. The base T is changed to a base C of quality 0. Evaluation of Alignments ARACHNE assigns a penalty score to each aligned pair of overlapping reads. The program first assigns a penalty score to each discrepant base, based on the sequence quality score at the base and flanking bases on either side. Discrepancies in high-quality sequence are assigned a high penalty, whereas discrepancies in low-quality sequence are penalized less heavily. The penalty scores for the individual discrepancies are then combined to yield an overall penalty score for the alignment; overlaps incurring too high a penalty are discarded (see The “Alignment Module” section for details). This step is stringent, and false overlaps that remain are typically caused by highly conserved repeats. At this point, ARACHNE also detects and discards likely chimeric reads (see the section below entitled “Detection of Chimeric Reads” for details). Identification of Paired Pairs ARACHNE searches for instances of two plasmids of similar insert size with sequence overlaps occurring at both ends (Fig. ​(Fig.2A).2A). Such instances are referred to as paired pairs. These instances can be extended by iteratively building complexes of such paired pairs (Fig. ​(Fig.2B).2B). An external file that holds a picture, illustration, etc. Object name is 42244-16f2_L1TT.jpg Open in a separate window Figure 2 Using paired pairs of overlaps to merge reads. (A) A paired pair of overlaps. The top two reads are end sequences from one insert, and the bottom two reads are end sequences from another. The two overlaps must not imply too large a discrepancy between the insert lengths. (B) Initially, the top two pairs of reads are merged. Then the third pair of reads (from the top) is merged in, based on having an overlap with one of the top two left reads, an overlap with one of the top two right reads, and consistent insert lengths. The bottom pair is similarly merged. Empirically, we find that these initial assemblies are almost always correct. The only significant exception occurs when the paired pairs come from within a large repeat (larger than the insert size of the plasmid). The program attempts to detect and eliminate such cases by virtue of their having too high a depth of coverage (see below). Such instances are discarded. The collections of paired pairs (Fig. ​(Fig.2B)2B) are merged together into contigs, and consensus sequences are formed (see below). The resulting contigs are then treated as large reads. In practice, their length varies from slightly more than the size of a typical read to ~30 kb. Contig Assembly In the absence of repeats, producing the correct layout of reads is straightforward. Any two reads with substantial sequence overlap must truly overlap in the genome, and thus a correct assembly could be obtained simply by merging all overlapping reads. However, false overlaps may arise between reads derived from different copies of a repeat. For example, reads entering different copies of a repeat may have overlapping sequence and could give rise to misassemblies (Fig. ​(Fig.3A).3A). An external file that holds a picture, illustration, etc. Object name is 42244-16f3_L1TT.jpg Open in a separate window Figure 3 Contig assembly. (A) How merging reads across the boundary of a repeat may result in a misassembly. Regions A, B, C, and D are unique regions, and region R is a repeat occurring twice in the genome. Reads x and y overlap in region R. Thus, regions A and D are wrongly joined after merging reads x and y. (B) A potential repeat boundary. Read r overlaps both reads x and y, but reads x and y do not overlap each other; they disagree in their rightmost ends. Here, a repeat R starting inside reads x and y and including the full read r is shown. In practice, sequencing errors rather than repeats often cause such patterns of overlap. (C) Contigs are created by merging reads up to the potential boundaries of repeats. A potential repeat boundary is any place where a read may be extended with two nonoverlapping reads. Two regions of the genome covered with reads are shown here. One region (A-R-D) is covered with solid line reads and the second region (C-R-B) with dotted line reads. The two regions meet in the repeat R creating five contigs: these are the unique contigs corresponding to unique sequences A, B, C, and D, and the repeat contig corresponding to the repeat R, where reads from both copies of R are overcollapsed into one contig. According to the algorithm used to construct contigs, the contig corresponding to R would have exactly the reads that are fully included in the boundaries of R. All the other reads would be assigned to contigs A, B, C, and D. (D) Sequencing errors. Read r dominates read y because the neighbors of y are all neighbors of r. This is caused by a sequencing error on y, which is marked in the figure. Note that if y represented correct sequence, it would likely be extended to the right by some read that did not overlap r, and thus r would not dominate y. ARACHNE identifies potential repeat boundaries and avoids assembling contigs across such boundaries. The program marks a potential repeat boundary whenever a read r can be extended to the right (or left) by reads x and y, but x and y do not overlap each other (Fig. ​(Fig.3B).3B). Read r is not merged with any read on its right, because it is not clear which read it should be merged with. By contrast, when all neighbors to the right of r overlap each other, then r can be safely merged with its closest neighbor. As in Myers (1995), the program thus merges, at this stage, read pairs that do not cross a marked repeat boundary (Fig. ​(Fig.33C). This criterion for merging pairs of reads is very conservative, and based on the cases examined, it rarely produces misassemblies. In fact, it is overly conservative because some potential repeat boundaries reflect instances in which reads x and y (Fig. ​(Fig.3D)3D) fail to overlap owing to several sequencing errors. ARACHNE employs a technique to eliminate some of these spurious repeat boundaries (dropping dominated reads, as described in the “Contig Assembly” section below) and performs a second round of contig merger. In addition to dominated reads, another category of reads that introduces repeat boundaries that can be merged across safely are reads fully included in other reads (the subreads). During the above procedure of constructing contigs, subreads are ignored. They are not used to detect repeat boundaries and they are not included in any contig. After creating the contigs, if read a is fully included in reads b1, …, bk which all belong to contig C, then a is inserted in C. Details are described in the “Contig Assembly” section. Detection of Repeat Contigs In the previous step, ARACHNE merges reads into contigs up to potential repeat boundaries. Some of these are repeat contigs—defined as contigs in which nearly identical sequence from distinct regions are collapsed together. Such repeat contigs can be identified in two ways. First, repeat contigs will typically have an unusually high depth of coverage. This can be assessed by using the log-odds ratio—that a contig of a given length and density of reads represents unique sequence versus being composed of reads derived from two copies of a repeat (Myers et al. 2000). Second, they will typically have conflicting links to other contigs. Repeat contigs are usually linked to multiple, distinct, nonoverlapping contigs, reflecting the multiple regions that flank the repeat in the genome (Fig. ​(Fig.4).4). An external file that holds a picture, illustration, etc. Object name is 42244-16f4_L1TT.jpg Open in a separate window Figure 4 Detection of repeat contigs. Contig R is linked to contigs A and B to the right. The distances estimated between R and A and R and B are such that A and B cannot be positioned without substantial overlap between them. If there is no corresponding detected overlap between A and B (if their reads do not overlap), then R is probably a repeat linking to two unique regions to the right. Creation of Supercontigs If the repeat contigs have been correctly marked, the remaining contigs should represent correctly assembled sequence. The unmarked contigs are referred to as unique contigs (similar to unitigs, (Myers et al. 2000)), because they usually represent unique sequence in the genome (or genomic repeated sequence that has diverged sufficiently to allow it to assemble as if it were unique). ARACHNE then uses the forward-reverse links from plasmid reads to order and orient the unique contigs in longer layouts called supercontigs (or scaffolds). Supercontigs are created incrementally. At the beginning, each unique contig is considered a supercontig. Pairs of supercontigs are then merged into a large supercontig if they are joined by at least two forward-reverse links (Fig. ​(Fig.5A).5A). The program requires the presence of at least two links to avoid links due to a single chimeric plasmid. It is unlikely that there will be two independent links supporting the same incorrect merger. Priority is given to those mergers supported by the most links and involving the shortest distances (see the “Supercontig Assembly” section below for details). An external file that holds a picture, illustration, etc. Object name is 42244-16f5_L1TT.jpg Open in a separate window Figure 5 Supercontig creation and gap filling. (A) A supercontig is constructed by successively linking pairs of contigs that share at least two forward-reverse links. Here, three contigs are joined into one supercontig. (B) ARACHNE attempts to fill gaps by using paths of contigs. The first gap in the supercontig shown here is filled with one contig, and the second gap is filled by a path consisting of two contigs. Filling Gaps in Supercontigs The layout now consists of a number of supercontigs, each of which is an ordered list of contigs with interleaved gaps. Most of these gaps correspond to regions marked as repeat contigs and are thus omitted from the supercontig construction. Some gaps will also correspond to regions in which there is an insufficient number of shotgun reads to allow assembly. ARACHNE attempts to fill gaps by using the repeat contigs. For every pair of consecutive contigs with an interleaving gap in a supercontig S, the program tries to find a path of pairwise overlapping contigs that fill the gap. Forward-reverse links from S guide the construction of the path by identifying contigs likely to fall in the gap (Fig. ​(Fig.5B).5B). Further details are given in the section below entitled “Filling Gaps in Supercontigs”. Consensus Derivation and Postconsensus Merger The layout of overlapping reads is converted to a consensus sequence with quality scores. This is done by converting pairwise alignments of reads (computed during the overlap detection phase) into multiple alignments. Because multiple sequence alignment is traditionally a time-consuming step, we developed an efficient algorithm, described below under the heading “Consensus Derivation”. After forming consensus sequence, ARACHNE merges overlapping adjacent contigs in supercontigs. In this final stage, it accepts overlaps that have significant numbers of discrepancies, thereby merging across spurious repeat boundaries that were not detected during layout. Comparison with Other Assemblers ARACHNE was developed from an earlier version of the program described in a Ph.D. thesis (Batzoglou 2000). In broad outline, the program follows the overlap-layout-consensus approach that has been used by nearly all assemblers. Many of the algorithmic aspects of ARACHNE are novel, while other aspects are similar to existing assemblers. Myers and colleagues produced a sophisticated assembly program, called the Celera assembler (Myers et al. 2000), based on various layout algorithms previously developed by Myers (1995). ARACHNE shares some significant similarities with the Celera assembler, most notably in the algorithms for merging reads into contigs up to the boundaries of repeats (Myers 1995). However, the two assemblers also have many significant differences. The Celera assembler screens for predefined repeats; ARACHNE does not and instead uses k-mer frequencies to identify repeats. ARACHNE uses sorting of k-mers to detect overlaps; the initial version of the Celera assembler employed a different approach (although the program has been subsequently revised (G. Myers, pers. comm.). Like PHRAP (Green 1994) and CAP3 (Huang and Madan 1999), ARACHNE creates, refines, and evaluates read alignments using quality scores; the Celera assembler does not use quality scores subsequent to trimming and prior to consensus. ARACHNE uses error correction to more accurately evaluate read overlaps. Both assemblers detect potential repeat boundaries, merge reads up to these boundaries (as in Myers 1995), and use read density to detect repeat contigs. ARACHNE also detects repeat contigs via conflicting links. Moreover, it uses paired pairs and other techniques to produce longer contigs during layout. Both ARACHNE and the Celera assembler require at least two links to order a pair of contigs in a supercontig, but apart from that they use different algorithms to build supercontigs and fill in gaps within supercontigs. Simulation of WGS Data We tested ARACHNE on simulated WGS data from known genomes. The data were generated in two steps: (1) selection of random reads from the target genome and (2) assignment of quality scores and introduction of errors. In the first step, plasmids were randomly chosen from the genome by selecting a random location for the start of the insert and a random length from a given length distribution. The corresponding forward and reverse reads were then read from the genome. In the second step, each simulated read was assigned realistic quality scores and errors by pairing it with a real read taken from a finished BAC sequencing project from the Whitehead/MIT Center for Genome Research. For each read, the length, quality scores, and correctness of each base in the read were known (from comparison with the finished sequence). On each simulated read, we imposed the pattern of quality scores and errors seen in its corresponding read. The simulated reads thus contain fairly realistic error patterns. Nonetheless, they fall short of being completely realistic in several respects (in decreasing order of significance). First, because the plasmids are randomly positioned along the genome, the simulation does not reflect potential cloning bias. In other words, there is a nonuniform distribution of clones across the genome. Second, because the simulated reads are assigned quality scores by randomly pairing them with actual reads, the simulation does not reflect regions of systematically poor sequence quality. For example, sequence following a long poly(A)-stretch may have poor quality, whereas an ARACHNE simulation will pair each such simulated read with an independent real read. Third, because our reads are taken from published genomic sequences, we necessarily omitted regions absent from the published sequence and concatenated sequences flanking small gaps. Consequently, regions that were difficult to clone, sequence, or assemble in the original published sequence will likely also be missing from our results. We generated full coverage and half coverage in the simulations. Full coverage corresponds to ~10.3-fold coverage based on Whitehead's definition of trimmed read length and ~8.3-fold coverage based on the number of bases with a PHRED quality score ≥20. Half coverage is half of this level. We generated data from small plasmids (mean insert size 4 kb) and large plasmids (mean insert size 40 kb) in a ratio of 20:1 in the case of full coverage, and 10:1 in the case of half coverage. In each case, the insert sizes were normally distributed around the mean with a standard deviation of 10%. Only 85% of the reads were successfully paired, the other 15% being single reads (reflecting a realistic sequence failure rate). Moreover, 0.7% of the inserts were chimeric—that is, corresponding forward-reverse linked reads that come from unrelated, random locations on the genome. (This rate was chosen to be higher than the observed rate of chimeric inserts at Whitehead.)",Assembly,"arachne  wholegenome shotgun assembler
input data arachne  design  analyze sequence read obtain   end  plasmid clones—  pair forward  reverse read although   also  use  unpaired read  mean  standard deviation   insert length   plasmid clone   specify individually  practice    convenient  describe  clone  belong  one  several libraries characterize   parameters  base   read   associate quality score    produce   phred computer program ewing     quality score   correspond   probability     base  incorrect  quality score   thus correspond   accuracy   initial step arachne trim read  eliminate terminal regions  extremely low quality   eliminate read contain  little highquality sequence see  later section  technical detail “details  assembly algorithm”  program also trim know vector sequence  eliminate know contaminants  sequence   bacterial host  clone vector overlap detection  alignment sort  extend arachne start  detect  align pair  apparently overlap read refer   simply  overlap     false overlap result  repeat sequence   genome    eliminate  subsequent step overlap detection  perform   efficient manner rather  compare  pair  read  require  comparisons     number  read  program use  sort  extend strategy  scale approximately linearly  strategy involve produce  sort table   kletter subword kmer together   source—   read    occur   position within  read  practice  use     table  sort   identical kmers appear consecutively batzoglou   program  exclude kmers  occur  extremely high frequency  typically correspond  highcopy highfidelity repeat sequence   genome  eliminate   increase  efficiency   overlap detection process rather   eliminate repeat sequence    cause  problem per   program  identify  instance  read pair  share one   overlap kmer    readily recognize   sort kmer table  use  threestep process  align  read   efficient manner  first step merge overlap share kmers  second step extend  share kmers  alignments   third step refine  alignments  dynamic program detail provide   section  entitle “ alignment module”  overall process bear  resemblance   fasta algorithm pearson  lipman   threestep process yield   valid alignments  read pair  valid alignments may  miss   overlap  short  low quality  consist solely  highcopy highfidelity repeat  invalid alignments may result  lowcopy repeat  produce apparently overlap sequence error correction arachne next detect  correct sequence errors  generate multiple alignments among overlap read  program  identify instance    base  overwhelmingly outvote  base align    correct  base fig ​fig  practice  occur     disagreement   isolate position involve  single base  occasionally two base  process take  account  quality score   base base   case examine  majority   instance appear   sequence errors rather  slightly different copy   repeat sequence   latter instance   due   fact    typically multiple occurrences   alternative sequence  thus     overwhelm vote  one alternative arachne similarly correct occasional insertions  deletions result   appear   sequence errors   read  correct correspond change  make   alignments  external file  hold  picture illustration etc object name  16f1_l1ttjpg open   separate window figure  correct errors  read  portion   multiple alignment  five read  show   highlight column   alignment  base   quality   align   base       quality greater    base   change   base   quality  evaluation  alignments arachne assign  penalty score   align pair  overlap read  program first assign  penalty score   discrepant base base   sequence quality score   base  flank base  either side discrepancies  highquality sequence  assign  high penalty whereas discrepancies  lowquality sequence  penalize less heavily  penalty score   individual discrepancies   combine  yield  overall penalty score   alignment overlap incur  high  penalty  discard see  “alignment module” section  detail  step  stringent  false overlap  remain  typically cause  highly conserve repeat   point arachne also detect  discard likely chimeric read see  section  entitle “detection  chimeric reads”  detail identification  pair pair arachne search  instance  two plasmids  similar insert size  sequence overlap occur   end fig ​fig2a2a  instance  refer   pair pair  instance   extend  iteratively build complexes   pair pair fig ​fig2b2b  external file  hold  picture illustration etc object name  16f2_l1ttjpg open   separate window figure  use pair pair  overlap  merge read   pair pair  overlap  top two read  end sequence  one insert   bottom two read  end sequence  another  two overlap must  imply  large  discrepancy   insert lengths  initially  top two pair  read  merge   third pair  read   top  merge  base    overlap  one   top two leave read  overlap  one   top two right read  consistent insert lengths  bottom pair  similarly merge empirically  find   initial assemblies  almost always correct   significant exception occur   pair pair come  within  large repeat larger   insert size   plasmid  program attempt  detect  eliminate  case  virtue     high  depth  coverage see   instance  discard  collections  pair pair fig ​fig2b2b  merge together  contigs  consensus sequence  form see   result contigs   treat  large read  practice  length vary  slightly    size   typical read  ~  contig assembly   absence  repeat produce  correct layout  read  straightforward  two read  substantial sequence overlap must truly overlap   genome  thus  correct assembly could  obtain simply  merge  overlap read however false overlap may arise  read derive  different copy   repeat  example read enter different copy   repeat may  overlap sequence  could give rise  misassemblies fig ​fig3a3a  external file  hold  picture illustration etc object name  16f3_l1ttjpg open   separate window figure  contig assembly   merge read across  boundary   repeat may result   misassembly regions       unique regions  region    repeat occur twice   genome read    overlap  region  thus regions     wrongly join  merge read      potential repeat boundary read  overlap  read     read      overlap    disagree   rightmost end   repeat  start inside read     include  full read   show  practice sequence errors rather  repeat often cause  pattern  overlap  contigs  create  merge read    potential boundaries  repeat  potential repeat boundary   place   read may  extend  two nonoverlapping read two regions   genome cover  read  show  one region ard  cover  solid line read   second region crb  dot line read  two regions meet   repeat  create five contigs    unique contigs correspond  unique sequence        repeat contig correspond   repeat   read   copy    overcollapsed  one contig accord   algorithm use  construct contigs  contig correspond   would  exactly  read   fully include   boundaries      read would  assign  contigs       sequence errors read  dominate read    neighbor     neighbor     cause   sequence error     mark   figure note    represent correct sequence  would likely  extend   right   read    overlap   thus  would  dominate  arachne identify potential repeat boundaries  avoid assemble contigs across  boundaries  program mark  potential repeat boundary whenever  read    extend   right  leave  read          overlap   fig ​fig3b3b read    merge   read   right     clear  read    merge   contrast   neighbor   right   overlap       safely merge   closest neighbor   myers   program thus merge   stage read pair    cross  mark repeat boundary fig ​fig33c  criterion  merge pair  read   conservative  base   case examine  rarely produce misassemblies  fact   overly conservative   potential repeat boundaries reflect instance   read    fig ​fig3d3d fail  overlap owe  several sequence errors arachne employ  technique  eliminate    spurious repeat boundaries drop dominate read  describe   “contig assembly” section   perform  second round  contig merger  addition  dominate read another category  read  introduce repeat boundaries    merge across safely  read fully include   read  subreads    procedure  construct contigs subreads  ignore    use  detect repeat boundaries     include   contig  create  contigs  read   fully include  read  …    belong  contig     insert   detail  describe   “contig assembly” section detection  repeat contigs   previous step arachne merge read  contigs   potential repeat boundaries     repeat contigs—defined  contigs   nearly identical sequence  distinct regions  collapse together  repeat contigs   identify  two ways first repeat contigs  typically   unusually high depth  coverage    assess  use  logodds ratio—  contig   give length  density  read represent unique sequence versus  compose  read derive  two copy   repeat myers    second   typically  conflict link   contigs repeat contigs  usually link  multiple distinct nonoverlapping contigs reflect  multiple regions  flank  repeat   genome fig ​fig  external file  hold  picture illustration etc object name  16f4_l1ttjpg open   separate window figure  detection  repeat contigs contig   link  contigs      right  distance estimate               cannot  position without substantial overlap       correspond detect overlap       read   overlap    probably  repeat link  two unique regions   right creation  supercontigs   repeat contigs   correctly mark  remain contigs  represent correctly assemble sequence  unmarked contigs  refer   unique contigs similar  unitigs myers      usually represent unique sequence   genome  genomic repeat sequence   diverge sufficiently  allow   assemble     unique arachne  use  forwardreverse link  plasmid read  order  orient  unique contigs  longer layouts call supercontigs  scaffold supercontigs  create incrementally   begin  unique contig  consider  supercontig pair  supercontigs   merge   large supercontig    join   least two forwardreverse link fig ​fig5a5a  program require  presence   least two link  avoid link due   single chimeric plasmid   unlikely     two independent link support   incorrect merger priority  give   mergers support    link  involve  shortest distance see  “supercontig assembly” section   detail  external file  hold  picture illustration etc object name  16f5_l1ttjpg open   separate window figure  supercontig creation  gap fill   supercontig  construct  successively link pair  contigs  share  least two forwardreverse link  three contigs  join  one supercontig  arachne attempt  fill gap  use paths  contigs  first gap   supercontig show   fill  one contig   second gap  fill   path consist  two contigs fill gap  supercontigs  layout  consist   number  supercontigs      order list  contigs  interleave gap    gap correspond  regions mark  repeat contigs   thus omit   supercontig construction  gap  also correspond  regions      insufficient number  shotgun read  allow assembly arachne attempt  fill gap  use  repeat contigs  every pair  consecutive contigs   interleave gap   supercontig   program try  find  path  pairwise overlap contigs  fill  gap forwardreverse link   guide  construction   path  identify contigs likely  fall   gap fig ​fig5b5b  detail  give   section  entitle “filling gap  supercontigs” consensus derivation  postconsensus merger  layout  overlap read  convert   consensus sequence  quality score     convert pairwise alignments  read compute   overlap detection phase  multiple alignments  multiple sequence alignment  traditionally  timeconsuming step  develop  efficient algorithm describe    head “consensus derivation”  form consensus sequence arachne merge overlap adjacent contigs  supercontigs   final stage  accept overlap   significant number  discrepancies thereby merge across spurious repeat boundaries    detect  layout comparison   assemblers arachne  develop   earlier version   program describe   phd thesis batzoglou   broad outline  program follow  overlaplayoutconsensus approach    use  nearly  assemblers many   algorithmic aspects  arachne  novel   aspects  similar  exist assemblers myers  colleagues produce  sophisticate assembly program call  celera assembler myers    base  various layout algorithms previously develop  myers  arachne share  significant similarities   celera assembler  notably   algorithms  merge read  contigs    boundaries  repeat myers  however  two assemblers also  many significant differences  celera assembler screen  predefined repeat arachne    instead use kmer frequencies  identify repeat arachne use sort  kmers  detect overlap  initial version   celera assembler employ  different approach although  program   subsequently revise  myers pers comm like phrap green   cap3 huang  madan  arachne create refine  evaluate read alignments use quality score  celera assembler   use quality score subsequent  trim  prior  consensus arachne use error correction   accurately evaluate read overlap  assemblers detect potential repeat boundaries merge read    boundaries   myers   use read density  detect repeat contigs arachne also detect repeat contigs via conflict link moreover  use pair pair   techniques  produce longer contigs  layout  arachne   celera assembler require  least two link  order  pair  contigs   supercontig  apart    use different algorithms  build supercontigs  fill  gap within supercontigs simulation  wgs data  test arachne  simulate wgs data  know genomes  data  generate  two step  selection  random read   target genome   assignment  quality score  introduction  errors   first step plasmids  randomly choose   genome  select  random location   start   insert   random length   give length distribution  correspond forward  reverse read   read   genome   second step  simulate read  assign realistic quality score  errors  pair    real read take   finish bac sequence project   whiteheadmit center  genome research   read  length quality score  correctness   base   read  know  comparison   finish sequence   simulate read  impose  pattern  quality score  errors see   correspond read  simulate read thus contain fairly realistic error pattern nonetheless  fall short   completely realistic  several respect  decrease order  significance first   plasmids  randomly position along  genome  simulation   reflect potential clone bias   word    nonuniform distribution  clone across  genome second   simulate read  assign quality score  randomly pair   actual read  simulation   reflect regions  systematically poor sequence quality  example sequence follow  long polyastretch may  poor quality whereas  arachne simulation  pair   simulate read   independent real read third   read  take  publish genomic sequence  necessarily omit regions absent   publish sequence  concatenate sequence flank small gap consequently regions   difficult  clone sequence  assemble   original publish sequence  likely also  miss   result  generate full coverage  half coverage   simulations full coverage correspond  ~fold coverage base  whitehead' definition  trim read length  ~fold coverage base   number  base   phred quality score ≥ half coverage  half   level  generate data  small plasmids mean insert size    large plasmids mean insert size     ratio     case  full coverage     case  half coverage   case  insert size  normally distribute around  mean   standard deviation       read  successfully pair     single read reflect  realistic sequence failure rate moreover    insert  chimeric—  correspond forwardreverse link read  come  unrelated random locations   genome  rate  choose   higher   observe rate  chimeric insert  whitehead",5
137,Minimus,"Minimus: a fast, lightweight genome assembler
Implementation details of Minimus The Minimus assembler was built in a modular fashion from software modules available within the AMOS assembly package [25] and is released as one of the components of this package. AMOS is an open-source software package that provides researchers with a collection of modules and software libraries that are useful in the development of genome assembly and analysis software. A full description of the AMOS package is beyond the scope of this paper and will be published elsewhere (M.Pop, manuscript in preparation). Minimus consists of the combination of three AMOS modules, following the traditional overlap-layout-consensus paradigm [26]. These modules interact with each other through a central AMOS data-structure (called a bank) as shown in Figure 1. The three modules are: Figure 1 figure1 Overview of Minimus pipeline. Several independent modules of the AMOS package (shown as ovals) interact through the AMOS API to a central data-structure (called a Bank). The order of execution of the individual modules is shown by the arrow. Note that the inputs and outputs to minimus follow the AMOS file format (AMOS message files). The AMOS package provides converters between this file format and virtually all commonly used formats for representing sequence data and genome assemblies. Full size image 1. hash-overlap – a sequence overlapper that uses minimizers [27] to increase speed and decrease memory usage. 2. tigger – a unitigger, i.e. tool that identifies clusters of reads which can be uniquely assembled based on algorithms developed by Myers [28, 29]; in graph theoretic terms a unitigger identifies maximal interval subgraphs of the overlap graph. 3. make-consensus – a progressive multiple alignment program that refines the read layout generated by the unitigger to build a precise multiple alignment of these reads. Note that sequence quality values are only used during the generation of the multiple alignment consensus (step 3). Other assemblers, such as phrap, use the quality values as an integral component of the assembly algorithm. We found that, due to the high quality of data produced by modern sequencing instruments, the explicit consideration of quality values during the overlap and unitigging steps is unnecessary. Instead we only use the quality data to trim the poor quality flanks of each read (see below under Sequence trimming), and to compute the consensus (and associated quality values) for the multiple alignment of co-assembled reads. An execution of Minimus consists of the following stages, described in detail below. Input stage The shotgun reads are loaded into the AMOS bank. The inputs are presented as an AMOS message file, whose format is modeled on the format used by Celera Assembler [4]. Virtually any existing format for representing shotgun data can be easily converted to this message format with the help of conversion tools distributed with the AMOS package. Overlap stage The hash-overlap program is used to compute all pair-wise alignments between the reads provided in the input. Unitigger stage The tigger module constructs a graph representation of the set of overlaps determined in the overlap stage. The overlap graph contains a node for each shotgun read, and an edge connects two nodes if the corresponding reads overlap. The unitigger then uses several reduction steps to simplify this graph, and generate a set of unitigs, based on algorithms originally developed by Myers [28, 29]. Briefly, these reduction steps are: 1. Removal of containment edges. Reads completely contained within other reads in the input are removed from the graph. 2. Transitive reduction. For any set of three reads (A, B, and C), if the overlap between A and C can be inferred from the overlaps between reads A and B, and B and C, this overlap (i.e. the edge corresponding to this overlap) is removed from the graph. 3. Unique-join collapsing. Every simple path in the graph (paths that contain no branches, i.e. all the nodes have in- and out-degrees equal to 1) are collapsed into a single vertex. Each such vertex represents an individual unitig. Consensus stage The final stage of Minimus constructs the full multiple alignment of the reads aligned within each unitig, using as a guide the approximate placement of the reads inferred from the overlap information. Sequence trimming The criteria used for trimming the vector sequence and the poor quality flanks of shotgun reads vary significantly depending on the source of the data and on the protocols employed during sequencing. In designing Minimus we, thus, opted to perform the trimming of the data with external software tools that can be customized to the specific characteristics of the data. For the examples described in this paper we followed two different approaches for sequence trimming: 1. For data where we had confidence that the Trace Archive clipping coordinates were correct (i.e. the two bacterial genomes) we simply used the coordinates provided to us. 2. For the other data-sets (zebrafish gene and mouse BACs) we followed the protocol described at [30], specifically we used the program Lucy [31] for quality trimming, followed by a k-mer based vector trimming protocol. Note that while phrap performs some trimming based on quality values, in order to ensure consistent trimming of the data, we provided phrap with sequences already trimmed according to the protocol described above. Extraction of GPC3 homologues from zebrafish shotgun data To extract the set of zebrafish shotgun reads that map to the human GPC3 gene, we built an NCBI Blast database containing the high-quality region of the zebrafish reads (obtained by removing the sequencing vector and the poor quality regions). We then aligned the protein sequence of the human GPC3 gene using tblastn with an E-value cutoff of 0.01. All reads matching GPC3 under these extremely relaxed criteria were then provided to Minimus for assembly.",Assembly,"minimus  fast lightweight genome assembler
implementation detail  minimus  minimus assembler  build   modular fashion  software modules available within  amos assembly package    release  one   components   package amos   opensource software package  provide researchers   collection  modules  software libraries   useful   development  genome assembly  analysis software  full description   amos package  beyond  scope   paper    publish elsewhere mpop manuscript  preparation minimus consist   combination  three amos modules follow  traditional overlaplayoutconsensus paradigm   modules interact      central amos datastructure call  bank  show  figure   three modules  figure  figure1 overview  minimus pipeline several independent modules   amos package show  ovals interact   amos api   central datastructure call  bank  order  execution   individual modules  show   arrow note   input  output  minimus follow  amos file format amos message file  amos package provide converters   file format  virtually  commonly use format  represent sequence data  genome assemblies full size image  hashoverlap   sequence overlapper  use minimizers   increase speed  decrease memory usage  tigger   unitigger  tool  identify cluster  read    uniquely assemble base  algorithms develop  myers    graph theoretic term  unitigger identify maximal interval subgraphs   overlap graph  makeconsensus   progressive multiple alignment program  refine  read layout generate   unitigger  build  precise multiple alignment   read note  sequence quality value   use   generation   multiple alignment consensus step   assemblers   phrap use  quality value   integral component   assembly algorithm  find  due   high quality  data produce  modern sequence instrument  explicit consideration  quality value   overlap  unitigging step  unnecessary instead   use  quality data  trim  poor quality flank   read see   sequence trim   compute  consensus  associate quality value   multiple alignment  coassembled read  execution  minimus consist   follow stag describe  detail  input stage  shotgun read  load   amos bank  input  present   amos message file whose format  model   format use  celera assembler  virtually  exist format  represent shotgun data   easily convert   message format   help  conversion tool distribute   amos package overlap stage  hashoverlap program  use  compute  pairwise alignments   read provide   input unitigger stage  tigger module construct  graph representation   set  overlap determine   overlap stage  overlap graph contain  node   shotgun read   edge connect two nod   correspond read overlap  unitigger  use several reduction step  simplify  graph  generate  set  unitigs base  algorithms originally develop  myers   briefly  reduction step   removal  containment edge read completely contain within  read   input  remove   graph  transitive reduction   set  three read       overlap       infer   overlap  read         overlap   edge correspond   overlap  remove   graph  uniquejoin collapse every simple path   graph paths  contain  branch    nod    outdegrees equal    collapse   single vertex   vertex represent  individual unitig consensus stage  final stage  minimus construct  full multiple alignment   read align within  unitig use   guide  approximate placement   read infer   overlap information sequence trim  criteria use  trim  vector sequence   poor quality flank  shotgun read vary significantly depend   source   data    protocols employ  sequence  design minimus  thus opt  perform  trim   data  external software tool    customize   specific characteristics   data   examples describe   paper  follow two different approach  sequence trim   data    confidence   trace archive clip coordinate  correct   two bacterial genomes  simply use  coordinate provide       datasets zebrafish gene  mouse bacs  follow  protocol describe   specifically  use  program lucy   quality trim follow   kmer base vector trim protocol note   phrap perform  trim base  quality value  order  ensure consistent trim   data  provide phrap  sequence already trim accord   protocol describe  extraction  gpc3 homologues  zebrafish shotgun data  extract  set  zebrafish shotgun read  map   human gpc3 gene  build  ncbi blast database contain  highquality region   zebrafish read obtain  remove  sequence vector   poor quality regions   align  protein sequence   human gpc3 gene use tblastn   evalue cutoff    read match gpc3   extremely relax criteria   provide  minimus  assembly",5
138,SSPACE,"Scaffolding pre-assembled contigs using SSPACE.
Materials Part of the test data was taken from the NCBI Short Read Archive (SRA). For E.coli (strain K-12, MG1655) we used two Illumina paired-end libraries corresponding to an insert size of 200 bp (SRR001665) and 500 bp (SRR001666), respectively. For G.clavigera (strain kw1407; DiGuistini et al., 2009), we used a combination of single-end 454 reads (SRR023307 and SRR023517 to SRR023533) and two Illumina paired-end libraries corresponding to insert sizes of 200 bp (SRR018008 to SRR018011) and 700 bp (SRR018012). For both genomes, all sequence reads were assembled as single-ends with Abyss and subsequently scaffolded into supercontigs using the Abyss and SSPACE scaffolders. The minimum number of links required for matching unambiguous contig links was set to 5. For the giant panda genome, we retrieved sequence scaffolds constructed with SOAP from http://panda.genomics.org.cn (Li et al., 2010) and split these on gapped positions larger than 100 bp. The resulting 187.742 contigs (>100 bp) were scaffolded with SSPACE using all available paired-read libraries of highquality reads (in total 35 libraries with inserts ranging from 0.1 to 12 kb). The minimum number of links required for matching unambiguous contigs was set to 3. All analyses were performed on a 32 GB Linux machine. 2.2 SSPACE algorithm An overview of the SSPACE algorithm is given in Supplementary Figure S1. First, short-paired DNA reads are filtered by removing sequences containing non-ACTG characters. The remaining read pairs are mapped against the preassembled contigs using Bowtie (Langmead et al., 2009). The position and orientation of each pair that could be mapped is stored in a hash. Hereafter a post-filtering step is applied to remove duplicate read-pairs. Optionally, the pre-assembled contigs can be extended using sequence reads that could not be mapped. This feature is especially designed to incorporate the paired-read datasets that were not used in the pre-assembly. The next step in the SSPACE protocol is scaffolding, which is modified and extended from the SSAKE short-read assembler (Warren et al., 2007). In brief, putative contig pairs (pre-scaffolding stage) are computed based on the position of the paired reads on different contigs. Contig pairs are only considered if the calculated distances between them satisfy the userdefined distance range. After pairing the contigs, scaffolds are formed by iteratively combining contigs if a minimum number of read pairs (k) support the connection (default k = 5), starting with the largest contig. We have introduced a novel step to deal with contigs that have alternative connections (see also Supplementary Figure S2). If connections are also found between the alternatives themselves, the algorithm seeks to place all alternatives in the correct order using the estimated insertion. Otherwise a ratio is calculated between the two best alternatives. If this ratio is below a threshold (default a = 0.7), a connection with the best scoring alternative is established. Extension of scaffolds is aborted if either a contig has no links with other contigs or the ratio for alternatives is exceeded. The scaffolding process is repeated until all contigs are incorporated into linear scaffolds. Our program has been designed to allow for multiple library input sets that are scaffolded in a hierarchical manner (starting with small insert libraries). SSPACE provides the following output files: the final scaffolds (FASTA format) and a complementary file listing the contigs that build up each scaffold. Also a summary file containing useful statistics such as the total number of scaffolds, their (average) size and N50 statistics is provided. Optionally the connections within each scaffold can be graphically viewed.",Assembly,"scaffold preassemble contigs use sspace
materials part   test data  take   ncbi short read archive sra  ecoli strain  mg1655  use two illumina pairedend libraries correspond   insert size    srr001665    srr001666 respectively  gclavigera strain kw1407 diguistini     use  combination  singleend  read srr023307  srr023517  srr023533  two illumina pairedend libraries correspond  insert size    srr018008  srr018011    srr018012   genomes  sequence read  assemble  singleends  aby  subsequently scaffold  supercontigs use  aby  sspace scaffolders  minimum number  link require  match unambiguous contig link  set     giant panda genome  retrieve sequence scaffold construct  soap        split   gap position larger     result  contigs    scaffold  sspace use  available pairedread libraries  highquality read  total  libraries  insert range       minimum number  link require  match unambiguous contigs  set    analyse  perform     linux machine  sspace algorithm  overview   sspace algorithm  give  supplementary figure  first shortpaired dna read  filter  remove sequence contain nonactg character  remain read pair  map   preassemble contigs use bowtie langmead     position  orientation   pair  could  map  store   hash hereafter  postfiltering step  apply  remove duplicate readpairs optionally  preassemble contigs   extend use sequence read  could   map  feature  especially design  incorporate  pairedread datasets    use   preassembly  next step   sspace protocol  scaffold   modify  extend   ssake shortread assembler warren     brief putative contig pair prescaffolding stage  compute base   position   pair read  different contigs contig pair   consider   calculate distance   satisfy  userdefined distance range  pair  contigs scaffold  form  iteratively combine contigs   minimum number  read pair  support  connection default    start   largest contig   introduce  novel step  deal  contigs   alternative connections see also supplementary figure   connections  also find   alternatives   algorithm seek  place  alternatives   correct order use  estimate insertion otherwise  ratio  calculate   two best alternatives   ratio    threshold default     connection   best score alternative  establish extension  scaffold  abort  either  contig   link   contigs   ratio  alternatives  exceed  scaffold process  repeat   contigs  incorporate  linear scaffold  program   design  allow  multiple library input set   scaffold   hierarchical manner start  small insert libraries sspace provide  follow output file  final scaffold fasta format   complementary file list  contigs  build   scaffold also  summary file contain useful statistics    total number  scaffold  average size  n50 statistics  provide optionally  connections within  scaffold   graphically view",5
139,SSPACE-LongRead,"SSPACE-LongRead: scaffolding bacterial draft genomes using long read sequence information
The SSPACE-LongRead methodology can be summarized in a few steps which are described below and summarized in Figure 2. The pseudocode is summarized in Additional file 1: Figure S1. Figure 2 figure2 Overview of the SSPACE-LongRead scaffolding algorithm. A) The input consists of a set of pre-assembled contigs (or scaffolds) in FASTA format and a set of PacBio CLR reads (in FASTA or FASTQ format). B) The PacBio CLR reads are aligned against the contigs using BLASR and only the best alignment matches are kept. In red a repeated element is indicated. C) Contig pairings and multi-contig linkage information is stored, from this information also repeated elements are detected. D) Based on the pairing and linkage information, contigs are ordered, oriented and connected into scaffolds. E) A post-processing step performs the final linearization and circularization. Full size image Software input The user needs to create a draft assembly using a de novo assembly method of choice (e.g. Velvet [1], SOAPdenovo [2], Ray [18], CLCbio (CLC bio, Aarhus, Denmark) or Newbler (Roche)). Optionally the user may also provide scaffold sequences generated with dedicated software (e.g. SSPACE [5] or SOPRA [4]). The resulting contigs or scaffolds (in FASTA format) are to be provided as input to SSPACE-LongRead software together with a set of long reads in FASTQ or FASTA format (e.g. PacBio CLR reads). Note that in our study we observe that SSPACE-LongRead obtains the best results if the draft assembly is constructed with CLCbio or Newbler as these tend to better split contigs at repeat boundaries (see the Results and Discussion section for additional explanations). Alignment of long reads against the pre-assembled contigs (or scaffolds) Each long read is aligned against the pre-assembled contigs with BLASR [25] resulting in a local alignment and corresponding similarity score. For gap-estimation purposes, the local alignments are extended to generate a full contig match. In order to remove false-positive alignments, contigs that display a (partial) overlap with a contig that has obtained a higher alignment score are iteratively removed from the dataset (Figure 2, step B). The minimum overlap (in bp) required to remove a contig from the alignment is defined by parameter -g (default value = 200). Computation of contig linkage from the alignment order The remaining contigs are sorted based on their alignment position on the long reads. Subsequently the contig distance and orientation is computed. Each contig-pairing and multi-contig linkage is stored in a hash: the preferred pairings are retained by removing contig-links which are also found within a multi-contig path of another contig-link. For example, if there are two paths, A- > B- > C- > D and A- > C- > D, the linkage of A- > C is removed. The ambiguous paths are mainly explained by the fact that the BLASR alignment tool generally can not resolve (align) low-quality alignment regions of PacBio reads. If multiple paths remain, a ratio is calculated between the two best alternatives. Ambiguous pairings are solved using the multi-contig linkage information, the unsolved pairings are flagged as repeated elements. Scaffolding contigs into scaffolds The contigs are now connected into linear stretches where repeated elements are placed based on the multi-contig linkage information. Repeated elements on the edges of the linear stretches are removed. As a post-processing step, the non-repeated edges of the preliminary scaffolds are reused to find connections between other preliminary scaffolds. Finally the gap-size between each contig is calculated: if this value is negative, and an overlap is found, contigs are merged; if this value is positive, a gap is inserted between contigs (the gap is represented by one or more undefined ‘N’ nucleotides depending on the gap-size). Software output The final linear assembly is represented in an easily interpretable FASTA file. In addition a AGP (Accessioned Golden Path) file is generated which describes the contig order within the scaffolds. The latter files can be readily used for NCBI genome submissions. Also a summary file containing statistics of the assembly process and final assembly structure is provided in TEXT format. Datasets In total six bacterial datasets were used for testing the performance of the software. These comprise Illumina MiSeq, Roche-454 and PacBio RS reads from Escherichia coli (K12 MG1655), Escherichia coli (O157:H7 F8092B), Bibersteina trehalosi (USDA-ARS-USMARC-192), Mannheimia haemolytica (USDA-ARS-USMARC-2286), Francisella tularensis (99A-2628) and Salmonella enterica (Newport SN31241). Datasets are downloaded from http://www.cbcb.umd.edu/software/PBcR/closure/index.html and further described in Koren et al. (2013). Dataset statistics are displayed in Table 1. To assess the assembly correctness we used close reference genomes deposited in the NCBI database (E. coli K12 MG1655 = NC_000913, E. coli O157:H7 = NC_002127, NC_002128, NC_002695, F. tularensis = NC_008369, S. enterica = NC_011079, NC_011080, NC_009140). For B. trehalosi and M. haemolytica no reference genome is currently available. Assembly procedure Draft assemblies of Illumina MiSeq data were constructed using Ray version 2.3.0 [18] and the CLCbio de novo assembler version 6.5.1 (CLC bio, Aarhus, Denmark) using for each program a k-mer setting of 31. Draft assemblies of Roche-454 data were constructed using Newbler version 2.8 (Roche) as described in Koren et al. [19]. Scaffolding was performed using AHA [14] which is part of the SMRT Analysis Package version 2.0 and SSPACE-LongRead. For the latter software we required a minimal estimated overlap of 200 bp (option -g) between the contigs in order to avoid false positive alignments. For Ray the minimal estimated overlap was set to 500 bp since Ray tends to include repeated elements on contigs edges: as a result a larger overlap between the contigs is observed.",Assembly,"sspacelongread scaffold bacterial draft genomes use long read sequence information
 sspacelongread methodology   summarize    step   describe   summarize  figure   pseudocode  summarize  additional file  figure  figure  figure2 overview   sspacelongread scaffold algorithm   input consist   set  preassemble contigs  scaffold  fasta format   set  pacbio clr read  fasta  fastq format   pacbio clr read  align   contigs use blasr    best alignment match  keep  red  repeat element  indicate  contig pair  multicontig linkage information  store   information also repeat elements  detect  base   pair  linkage information contigs  order orient  connect  scaffold   postprocessing step perform  final linearization  circularization full size image software input  user need  create  draft assembly use   novo assembly method  choice  velvet  soapdenovo  ray  clcbio clc bio aarhus denmark  newbler roche optionally  user may also provide scaffold sequence generate  dedicate software  sspace   sopra   result contigs  scaffold  fasta format    provide  input  sspacelongread software together   set  long read  fastq  fasta format  pacbio clr read note    study  observe  sspacelongread obtain  best result   draft assembly  construct  clcbio  newbler   tend  better split contigs  repeat boundaries see  result  discussion section  additional explanations alignment  long read   preassemble contigs  scaffold  long read  align   preassemble contigs  blasr  result   local alignment  correspond similarity score  gapestimation purpose  local alignments  extend  generate  full contig match  order  remove falsepositive alignments contigs  display  partial overlap   contig   obtain  higher alignment score  iteratively remove   dataset figure  step   minimum overlap   require  remove  contig   alignment  define  parameter  default value   computation  contig linkage   alignment order  remain contigs  sort base   alignment position   long read subsequently  contig distance  orientation  compute  contigpairing  multicontig linkage  store   hash  prefer pair  retain  remove contiglinks   also find within  multicontig path  another contiglink  example    two paths               linkage      remove  ambiguous paths  mainly explain   fact   blasr alignment tool generally   resolve align lowquality alignment regions  pacbio read  multiple paths remain  ratio  calculate   two best alternatives ambiguous pair  solve use  multicontig linkage information  unsolved pair  flag  repeat elements scaffold contigs  scaffold  contigs   connect  linear stretch  repeat elements  place base   multicontig linkage information repeat elements   edge   linear stretch  remove   postprocessing step  nonrepeated edge   preliminary scaffold  reuse  find connections   preliminary scaffold finally  gapsize   contig  calculate   value  negative   overlap  find contigs  merge   value  positive  gap  insert  contigs  gap  represent  one   undefined  nucleotides depend   gapsize software output  final linear assembly  represent   easily interpretable fasta file  addition  agp accession golden path file  generate  describe  contig order within  scaffold  latter file   readily use  ncbi genome submissions also  summary file contain statistics   assembly process  final assembly structure  provide  text format datasets  total six bacterial datasets  use  test  performance   software  comprise illumina miseq roche  pacbio  read  escherichia coli k12 mg1655 escherichia coli o157h7 f8092b bibersteina trehalosi usdaarsusmarc mannheimia haemolytica usdaarsusmarc francisella tularensis   salmonella enterica newport sn31241 datasets  download     describe  koren    dataset statistics  display  table   assess  assembly correctness  use close reference genomes deposit   ncbi database  coli k12 mg1655  nc_000913  coli o157h7  nc_002127 nc_002128 nc_002695  tularensis  nc_008369  enterica  nc_011079 nc_011080 nc_009140   trehalosi   haemolytica  reference genome  currently available assembly procedure draft assemblies  illumina miseq data  construct use ray version     clcbio  novo assembler version  clc bio aarhus denmark use   program  kmer set   draft assemblies  roche data  construct use newbler version  roche  describe  koren    scaffold  perform use aha    part   smrt analysis package version   sspacelongread   latter software  require  minimal estimate overlap    option    contigs  order  avoid false positive alignments  ray  minimal estimate overlap  set    since ray tend  include repeat elements  contigs edge   result  larger overlap   contigs  observe",5
140,ccTSA,"ccTSA: a coverage-centric threaded sequence assembler
Execution Flow of ccTSA ccTSA reads input files, each of which is composed of the short fragments (reads) of an original DNA sequence, and generates an output file that contains the result of sequence assembly. Sequencing machines [3] occasionally make mistakes in reading base-pairs, which are called base-calling errors, and some k-mers are mapped to the multiple regions of the original sequence, which are called repeats. As a result, it is not always possible for a sequence assembler to perfectly reconstruct the original sequence. So the output file of ccTSA typically consists of multiple DNA sequences called contigs and none of the contigs might be mapped to some regions of the original sequence. Currently, ccTSA can read FASTA and FASTQ files and writes the generated contigs to a FASTA file. Figure 7 illustrates an overview of the execution flow of ccTSA, which consists of multiple phases. First, it reads the series of short reads and extracts k-mers from each read. Because a k-mer consists of k nucleotides, a read that has fewer than k nucleotides is discarded. ccTSA also discards k-mers that have ambiguous or unidentified nucleotides. It checks a dictionary called a k-mer coverage table, which has a k-mer as a key and its coverage as a value, to see if the extracted k-mer exists in the table. If so, its coverage value is incremented by one. If not, the k-mer is added to the table with the coverage value 1. Note that k-mer coverage is different from the sequence coverage of the original DNA sequence. The former is the number of a k-mer instance from the sequenced reads, while the latter stands for how many times a nucleotide in the original sequence appears at the reads. After all the reads are processed, ccTSA optionally prunes k-mers with too low or high coverage values. Assuming that the original sequence consists of g nucleotides, the k-mer coverage table would have (g–k+1) entries if the sequence has no repeats and the reads have no base-calling errors. If a k-mer generated from a sequenced read contains one or more base-calling errors, the k-mer typically has very low coverage because it is unlikely that the original DNA sequence includes the k-mer. When the base-calling error rate of the reads is high, the k-mer coverage table has much more than (g–m+1) entries. If the coverage table has more entries, more memory space is required and it takes more time to access and update the table. Assuming that the coverage of the original DNA sequence is sufficiently high, most of low coverage k-mers are due to base-calling errors and most of high coverage k-mers are from the original sequence. As a result, pruning these low coverage k-mers can be useful for removing the base-calling errors, saving memory usage and improving sequencing speed. However, because the coverage of the original sequence is not uniform over all the nucleotides, some of the low coverage k-mers could be from the original sequence hence pruned incorrectly. This lowers the average length of the generated contigs, but it would be possible to restore them during phases after assembly, such as the scaffolding phase, which will be further discussed later in this section. k-mers with very high coverage are typically from repeats, so we can optionally mark them as repeats and exclude them hereafter. Remaining k-mers become k-mer nodes, among which the nodes that share k-1 nucleotides are linked together through edges building a de Bruijn graph. Because there are 4 types (Adenine, Thymine, Guanine, and Cytosine) of nucleotides in DNA, a k-mer node has up to 8 neighbors, 4 to the left side that share the first k-1 nucleotides and 4 to the right side that share the last k-1 ones. A node that has multiple neighbors to either side is called a junction node. After linking, the k-mers that are connected without any junction are merged, forming a contig node. Then, for each side of a node, the weights of the edges are computed and the neighbor with the highest weight is called a preferred neighbor. The weight of an edge represents the likelihood of the neighbor, which is highly correlated to the coverage of the neighbor nodes. As of now, the weight of an edge on each side is computed by adding the coverage of the neighbor k-mer connected through the edge with the maximum coverage value among the k-mers connected to the neighbor k-mer on the same side (Figure 8). This gives a priority to the neighbor node with higher k-mer coverage, at the same time prefers a longer path and enables ccTSA not to miss a strong or more likely path that is connected through a low coverage k-mer. ccTSA is designed to easily implement other ways to calculate weights. After finding preferred neighbors, we check each junction node JN1 whether its preferred neighbor JN2 also points JN1 back as a preferred neighbor. If not, we call that there is a conflict between JN1 and JN2, which is resolved as follows: if the coverage of JN1 is higher, we enforce JN2 to point JN1 as a preferred neighbor; if the coverage of JN2 is higher, we disconnect the edge between JN1 and JN2, find the preferred neighbor among the remaining edges, and repeat the above steps until there is still a conflict. Our conflict resolution algorithm (Figure 9) is simpler than those of other assemblers such as tip removal and tour bus algorithms [6] in Velvet, ABySS, and SOAPdenovo. It is a future work to refine the conflict resolution algorithm. After all conflicts are resolved, finally, contigs are generated by traversing the nodes connected through preferred neighbors. Unlike other assemblers, ccTSA does not exploit paired-end reads to orient and align multiple contigs into a single super-contig or scaffold. ccTSA can leverage a separate tool, such as SSPACE [19], or the part of other assemblers to perform this scaffolding and finishing phase. Optimizations Continuous improvement in semiconductor process technology enables a single chip to integrate billions of transistors and a rack server to have dozens of computing cores and terabytes of shared memory [11]. We assume that the entire working set of ccTSA fits in a shared memory space. This simplifies programming and provides better performance than the systems that distribute k-mer entries across a cluster of computers connected over a network such as InfiniBand or Ethernet [11]. Any computing core can access any k-mer entry through low latency (tens of nanoseconds) memory loads and stores in a shared memory system, while the k-mer information must be encapsulated by request and reply packets and transferred over a high latency (a few microseconds or more) network. Because the size of a k-mer entry is rather small, the overhead of packing and unpacking the entry is relatively high, further reducing program speed. We apply several optimization techniques to ccTSA. To reduce execution time, we parallelize the phases where we construct the k-mer coverage table, populate and link k-mer nodes, and merge consecutive k-mers without junction, which take 99% of the single threaded execution of ccTSA on average over the Illumina-75 bp-80x datasets from 4 organism explained in the Results and Discussion section. When each phase is started, we first divide workload into many small chunks, each having the same size, and spawn multiple worker threads. Each worker repeats the process of receiving a chunk, processing it, and asking for another chunk that is not processed yet until all the chunks are processed. Because time for a thread to access data heavily depends on the address, the internal status of a complicated memory system within a processor, and interaction with concurrent accesses from other threads, time to process a chunk is not the same either [12]. As a result, statically dividing the workload into the worker threads suffers from the load balancing problem, while dynamically assigning chunks to idle threads leads to better performance [12]. As more worker threads are used, the performance advantage of the dynamic load balancing method becomes even higher. At the k-mer coverage construction phase, the workload is the sequences of short reads. We compose the k-mer coverage table of thousands of hash maps and use two different hash functions to identify a hash map and an entry in the hash map. Each hash map is protected by a mutex to prevent a simultaneous access to a hash map by multiple worker threads from destroying the data structure. Because there are much more hash maps than the worker threads and hash-map update is a simple operation, the worker threads rarely access the same hash map at the same time. As a result, the mutex operations do not incur significant performance overheads. Still, it is possible to further alleviate the overheads. Because a mutex is designed to protect a block of memory, not just a single word, it is heavier than an atomic CPU operation, which reads, modifies, and writes a word atomically. When the length of a k-mer is shorter than 32 base pairs, it can be represented as a single 64-bit word. Jellyfish [22] exploited this to replace the mutex operations into atomic memory operations, such as compare-and-swaps, in building concurrent hash maps and updating k-mer coverage values, and achieved a higher k-mer coverage construction performance for short k-mers. At the k-mer node populating, linking, and merging phases, each hash map becomes a chunk. Mutexes are not needed for these phases because no data is updated concurrently by the multiple threads. To save memory usage, ccTSA compares a k-mer with its reverse complement and only stores the value which is earlier in the lexicographical order. It utilizes bit fields extensively and has different data structures for the k-mer nodes with and without junctions. It includes a custom memory allocator [14], which provides multiple allocation classes. Each class is implemented as a chain of memory blocks. When the custom allocator is used to allocate an object, the object is categorized into a class and stored at the last block of the class. If the block does not have enough free space, the default memory allocator in C++ is used to allocate a block to the class. It cannot deallocate a single object, but can quickly deallocate all the objects of a certain class simply by freeing the blocks of the class. ccTSA utilizes this custom allocator in pruning low coverage k-mers by having separate tables for low coverage and high coverage k-mers, assigning the low coverage k-mer objects and the k-mer coverage table for them to a same class, and deallocating the class. The remaining k-mer coverage table has fewer entries than the table without pruning, which has fast access time. So pruning also helps reducing execution time. We can even prune low coverage k-mers in the middle of building the k-mer coverage table, not just at the end, which provides an interesting tradeoff between the memory footprint and assembly quality, which is evaluated in the Results and Discussion section. Availability and Future Directions ccTSA is written in C++ and can be run on Unix-like systems. Source code is freely available from http://code.google.com/p/cctsa/. ccTSA can be extended to multiple directions. First, alternative data structures and algorithms can be explored in search of better sequencing speed and lower memory usage. Second, ccTSA does not target current General-Purpose computing on Graphics Processing Units (GPGPUs) [11] because they do not provide enough memory capacity. However, it would be interesting to see if ccTSA can take advantage of their high computation power and memory bandwidth once future GPGPUs or many integrated core systems address the memory capacity issue. Third, ccTSA can be integrated with other scaffolding tools or extended to exploit paired-end reads to further orient and align the contigs.",Assembly,"cctsa  coveragecentric thread sequence assembler
execution flow  cctsa cctsa read input file     compose   short fragment read   original dna sequence  generate  output file  contain  result  sequence assembly sequence machine  occasionally make mistake  read basepairs   call basecalling errors   kmers  map   multiple regions   original sequence   call repeat   result    always possible   sequence assembler  perfectly reconstruct  original sequence   output file  cctsa typically consist  multiple dna sequence call contigs  none   contigs might  map   regions   original sequence currently cctsa  read fasta  fastq file  write  generate contigs   fasta file figure  illustrate  overview   execution flow  cctsa  consist  multiple phase first  read  series  short read  extract kmers   read   kmer consist   nucleotides  read   fewer   nucleotides  discard cctsa also discard kmers   ambiguous  unidentified nucleotides  check  dictionary call  kmer coverage table    kmer   key   coverage   value  see   extract kmer exist   table    coverage value  incremented  one    kmer  add   table   coverage value  note  kmer coverage  different   sequence coverage   original dna sequence  former   number   kmer instance   sequence read   latter stand   many time  nucleotide   original sequence appear   read    read  process cctsa optionally prune kmers   low  high coverage value assume   original sequence consist   nucleotides  kmer coverage table would   entries   sequence   repeat   read   basecalling errors   kmer generate   sequence read contain one   basecalling errors  kmer typically   low coverage    unlikely   original dna sequence include  kmer   basecalling error rate   read  high  kmer coverage table  much    entries   coverage table   entries  memory space  require   take  time  access  update  table assume   coverage   original dna sequence  sufficiently high   low coverage kmers  due  basecalling errors    high coverage kmers    original sequence   result prune  low coverage kmers   useful  remove  basecalling errors save memory usage  improve sequence speed however   coverage   original sequence   uniform    nucleotides    low coverage kmers could    original sequence hence prune incorrectly  lower  average length   generate contigs   would  possible  restore   phase  assembly    scaffold phase     discuss later   section kmers   high coverage  typically  repeat    optionally mark   repeat  exclude  hereafter remain kmers become kmer nod among   nod  share  nucleotides  link together  edge build   bruijn graph     type adenine thymine guanine  cytosine  nucleotides  dna  kmer node     neighbor    leave side  share  first  nucleotides     right side  share  last  ones  node   multiple neighbor  either side  call  junction node  link  kmers   connect without  junction  merge form  contig node    side   node  weight   edge  compute   neighbor   highest weight  call  prefer neighbor  weight   edge represent  likelihood   neighbor   highly correlate   coverage   neighbor nod     weight   edge   side  compute  add  coverage   neighbor kmer connect   edge   maximum coverage value among  kmers connect   neighbor kmer    side figure   give  priority   neighbor node  higher kmer coverage    time prefer  longer path  enable cctsa   miss  strong   likely path   connect   low coverage kmer cctsa  design  easily implement  ways  calculate weight  find prefer neighbor  check  junction node jn1 whether  prefer neighbor jn2 also point jn1 back   prefer neighbor    call     conflict  jn1  jn2   resolve  follow   coverage  jn1  higher  enforce jn2  point jn1   prefer neighbor   coverage  jn2  higher  disconnect  edge  jn1  jn2 find  prefer neighbor among  remain edge  repeat   step    still  conflict  conflict resolution algorithm figure   simpler     assemblers   tip removal  tour bus algorithms   velvet aby  soapdenovo    future work  refine  conflict resolution algorithm   conflict  resolve finally contigs  generate  traverse  nod connect  prefer neighbor unlike  assemblers cctsa   exploit pairedend read  orient  align multiple contigs   single supercontig  scaffold cctsa  leverage  separate tool   sspace    part   assemblers  perform  scaffold  finish phase optimizations continuous improvement  semiconductor process technology enable  single chip  integrate billions  transistors   rack server   dozens  compute core  terabytes  share memory   assume   entire work set  cctsa fit   share memory space  simplify program  provide better performance   systems  distribute kmer entries across  cluster  computers connect   network   infiniband  ethernet   compute core  access  kmer entry  low latency tens  nanoseconds memory load  store   share memory system   kmer information must  encapsulate  request  reply packets  transfer   high latency   microseconds   network   size   kmer entry  rather small  overhead  pack  unpack  entry  relatively high  reduce program speed  apply several optimization techniques  cctsa  reduce execution time  parallelize  phase   construct  kmer coverage table populate  link kmer nod  merge consecutive kmers without junction  take    single thread execution  cctsa  average   illumina bp80x datasets   organism explain   result  discussion section   phase  start  first divide workload  many small chunk     size  spawn multiple worker thread  worker repeat  process  receive  chunk process   ask  another chunk    process yet    chunk  process  time   thread  access data heavily depend   address  internal status   complicate memory system within  processor  interaction  concurrent access   thread time  process  chunk     either    result statically divide  workload   worker thread suffer   load balance problem  dynamically assign chunk  idle thread lead  better performance    worker thread  use  performance advantage   dynamic load balance method become even higher   kmer coverage construction phase  workload   sequence  short read  compose  kmer coverage table  thousands  hash map  use two different hash function  identify  hash map   entry   hash map  hash map  protect   mutex  prevent  simultaneous access   hash map  multiple worker thread  destroy  data structure    much  hash map   worker thread  hashmap update   simple operation  worker thread rarely access   hash map    time   result  mutex operations   incur significant performance overheads still   possible   alleviate  overheads   mutex  design  protect  block  memory    single word   heavier   atomic cpu operation  read modify  write  word atomically   length   kmer  shorter   base pair    represent   single bite word jellyfish  exploit   replace  mutex operations  atomic memory operations   compareandswaps  build concurrent hash map  update kmer coverage value  achieve  higher kmer coverage construction performance  short kmers   kmer node populate link  merge phase  hash map become  chunk mutexes   need   phase   data  update concurrently   multiple thread  save memory usage cctsa compare  kmer   reverse complement   store  value   earlier   lexicographical order  utilize bite field extensively   different data structure   kmer nod   without junctions  include  custom memory allocator   provide multiple allocation class  class  implement   chain  memory block   custom allocator  use  allocate  object  object  categorize   class  store   last block   class   block    enough free space  default memory allocator    use  allocate  block   class  cannot deallocate  single object   quickly deallocate   object   certain class simply  free  block   class cctsa utilize  custom allocator  prune low coverage kmers   separate table  low coverage  high coverage kmers assign  low coverage kmer object   kmer coverage table      class  deallocating  class  remain kmer coverage table  fewer entries   table without prune   fast access time  prune also help reduce execution time   even prune low coverage kmers   middle  build  kmer coverage table     end  provide  interest tradeoff   memory footprint  assembly quality   evaluate   result  discussion section availability  future directions cctsa  write      run  unixlike systems source code  freely available   cctsa   extend  multiple directions first alternative data structure  algorithms   explore  search  better sequence speed  lower memory usage second cctsa   target current generalpurpose compute  graphics process units gpgpus      provide enough memory capacity however  would  interest  see  cctsa  take advantage   high computation power  memory bandwidth  future gpgpus  many integrate core systems address  memory capacity issue third cctsa   integrate   scaffold tool  extend  exploit pairedend read   orient  align  contigs",5
141,Readjoiner,"Readjoiner: a fast and memory efficient string graph-based sequence assembler
Basic definitions Let w be a string of length n of symbols over an alphabet Σ . w[i] denotes the ith symbol of w and w[i…j] the substring of w from position i to j, 1 ≤ i, j ≤ n. w[1…i] is the prefix of w ending at position i and w[j…n] is the suffix of w starting at position j. A substring of w is proper if it is different from w. A substring of w is internal if it is neither a prefix nor a suffix of w. A read r is a string over the alphabet {A, C, G, T} which is assumed to be sorted by the alphabetical order < such that A < C < G < T. [left triangle, eq] denotes the lexicographic order of all substrings of the reads induced by the alphabetical order < . Let n be the length of r. The reverse complement of r, denoted by – r , is the sequence –––– r [ n ] … –––– r [ 1 ] , where –– a indicates the Watson-Crick complement of base a. Computing suffix- and prefix-free read sets The first step of our approach for assembling a collection of reads is to eliminate reads that are prefixes or suffixes of other reads. Here we describe a method to recognize these reads. Consider an ordered set R = ( r 1 , … , r m ) of reads, possibly of variable length, in which some reads may occur more than once (so R is indeed a multiset). We assume that, for all i, 1 ≤ i ≤ m, the ith read ri in R is virtually padded by a sentinel symbol $i at the right end and that the alphabetical order < is extended such that A < C < G < T < $1 < $2 < · · · < $m. We define a binary relation [precedes] on R such that ri [precedes] rj if and only if i < j. That is, [precedes] reflects the order of the reads in the input. R is prefix-free if for all reads r in R there is no r′ in R ∖ { r } such that r is a prefix of r′. R is suffix-free if for all r in R there is no read r′ in R ∖ { r } such that r is a suffix of r′. To obtain a prefix- and suffix-free set of reads we lexicographically sort all reads using a modified radixsort for strings, as described in [14]. In this algorithm, the strings to be sorted are first inserted into buckets according to their first character. Each bucket is then sorted recursively according to the next character of all reads in the bucket. A bucket always consists of reads which have a common prefix. Once a bucket is smaller than some constant, the remaining suffixes of the reads in the bucket are sorted by insertion sort [15]. During the sorting process, the length of the longest common prefix (lcp) of two lexicographically consecutive reads is calculated as a byproduct. For two lexicographically consecutive reads r and r′ with an lcp of length [ell] = |r|, we can conclude that r is a prefix of r′. If [ell] < |r′|, then r is a proper prefix of r′ and we mark r. If [ell] = |r′|, then r and r′ are identical and we mark the read which is larger according to the binary relation [precedes] . To handle reverse complements and to mark reads which are suffixes of other reads, one simply applies this method to the multiset –– R = ( r 1 , … , r m , r m + 1 , … , r 2 m ) where r m + i = ¯¯¯ r i for all i, 1 ≤ i ≤ m. As –– R includes the reverse complements of the reads, the method also marks reads which are suffixes of other reads. This is due to the observation that if read r is a suffix of read r′, then – r is a prefix of –– r ′ . In a final step of the algorithm one eliminates all reads from R which have been marked. The remaining unmarked reads from R are processed further. The algorithm to compute a suffix- and prefix-free set of reads runs in O ( m λ max ω ) time, where λmax is the maximum length of a read and ω is the machine’s word size. As we consider λmax to be a constant (which does not imply that the reads are all of the same length), the algorithm runs in O(m) time. Computing suffix-prefix matches Suppose that R is a suffix- and prefix-free set of m reads. Let [ell]min > 0 be the minimum length parameter. The set S P M ( R ) of suffix-prefix matches (SPMs, for short) is the smallest set of triples [left angle bracket]r, r′, [ell][right angle bracket] such that r , r ' ∈ R and strings u, v, w exist such that r = uv, r′ = vw, and |v| =[ell] ≥ [ell]min. [ell] is the length of a suffix-prefix match [left angle bracket]r, r′, [ell][right angle bracket] . The suffix-prefix matching problem is to find all suffix-prefix matches. As the reads of length smaller than [ell]min cannot, by definition, contribute any SPM, we can ignore them and thus we assume that R only contains reads of length at least [ell]min. The method to solve the suffix-prefix matching problem presented here consists of two main algorithms. The first algorithm identifies and lexicographically sorts all SPM-relevant suffixes, i.e. a subset of all suffixes of all reads from which one can compute all suffix-prefix matches. The second algorithm enumerates these matches given the sorted list of all SPM-relevant suffixes. Consider a suffix-prefix match [left angle bracket]r, r′, [ell][right angle bracket] . By definition, the suffix of length [ell] of r exactly matches the prefix of length [ell] of r′. Obviously, the suffix of r involved in the match starts at some position j, 2 ≤ j ≤ | r | − ℓ min + 1 in r. This implies that r must be at least of length [ell]min + 1. The suffix cannot start at the first position in r, as otherwise r would be a prefix of some other read, contradicting our assumption that R is prefix-free. To enumerate the set of all suffix-prefix matches of length at least [ell]min, we preprocess all reads and determine all proper suffixes of the reads which may be involved in a suffix-prefix match. More precisely, for all reads r we determine all matching candidates, i.e. all proper suffixes s of r such that the length of s is at least [ell]min and there is a read r′ such that s and r′ have a common prefix of length at least k, where k is an user-defined parameter satisfying k ≤ m i n { ℓ min , ω 2 } . There are two reasons for imposing this constraint on k: First, we want to represent a string of length k over an alphabet of size 4 in one machine word, thus k ≤ ω 2 . Second, the suffixes of the reads from which we take the prefixes of length k have minimum length [ell]min, thus we choose k ≤ [ell]min. The set of all matching candidates and all reads forms the set of all ([ell]min, k)-SPM-relevant suffixes. For simplicity sake, we use the notion SPM-relevant suffixes if [ell]min and k are clear from the context. While all SPMs can be constructed from the SPM-relevant suffixes, not all SPM-relevant suffixes lead to an SPM. An efficient algorithm for identifying and sorting all SPM-relevant suffixes The first two phases of our algorithm follow a strategy that is borrowed from the counting sort algorithm [15]. Like this, our algorithm has a counting phase and an insertion phase. However, in our problem, the elements (i.e. SPM-relevant suffixes) to be sorted are only determined during the algorithm. Moreover, the number of keys (i.e. initial k-mers) whose occurrences are counted is on the order of the number of elements to be sorted. Therefore, in a space efficient solution, it is not trivial to access a counter given a key. We have developed a time and space efficient method to access the counter for a key, exploiting the fact that counting and inserting the SPM-relevant suffixes does not have to be done immediately. Instead, the items to be counted/inserted are first buffered and then sorted. A linear scan then performs the counting or inserting step. In contrast to counting sort, our algorithm uses an extra sorting step to obtain the final order of elements pre-sorted in the insertion phase. Under the assumption that the maximum read length is a constant (which does not imply that the reads are all of the same length), our algorithm runs in O(n) time and space, where n is the total length of all reads. To the best of our knowledge a method employing a similar strategy has not yet been developed for the suffix-prefix matching problem. We first give a description of our algorithm using string notation. In a separate section, we explain how to efficiently implement the algorithm. In the following, we only consider the reads in the forward direction. However, it is not difficult to extend our method to also incorporate the reverse complements of the reads and we comment on this issue at the end of the methods section. The initial kmer of some sequence s is the prefix of s of length k. To determine the matching candidates efficiently, we first enumerate the initial k-mers of all reads and store them in a table of size m. This can be done in O(m) time. The notion table size always refers to the number of entries in the table. The next step lexicographically sorts the k-mers in the table in ascending order. This string sorting problem can be transformed into an integer sorting problem (see Implementation) which can be solved by radixsort [15] in O(m) time and O(m) extra working space. In the next step, a linear scan of the sorted k-mers removes duplicates from the table and counts how many times each k-mer occurs in the table. This scan requires O(m) time. Let d ≤ m be the number of different k-mers. These can be stored in a table K of size d. The counts for the elements in K require another table C of size d. In addition to the duplicate removal and counting, the linear scan of the sorted k-mers constructs two sets P and Q, the size of which depends on two user defined parameters k′ ≤ k and k ″≤ k. P is the set of all initial k′-mers of the reads. Q is the set of all k-mers r [ k − k ' ' + 1 … k ] for some r ∈ R . We assume that elements can be added to P and Q in constant time and that membership in these sets can be decided in constant time. Thus the linear scan constructs P and Q in O(m) time. As P is a subset of a set of size 4 k ′ , P can be stored in 4 k ′ bits. Q requires 4 k ' ' bits. Up until now, only the initial k-mers of the reads were considered, resulting in a sorted table K of d non-redundant keys (i.e. initial k-mers of reads), a table C of size d for counting k-mers and two sets P and Q. By construction, each count in C is at least 1 and the sum of the counts is m. The next task is to enumerate, for all reads r, the suffixes of r at all positions j, 2 ≤ j ≤ | r | − ℓ min + 1 . r has |r| − [ell]min such suffixes. For each such suffix s (which by construction is of length ≥ [ell]min), one extracts two strings v = s[1…k′] and w = s [ k − k ' ' + 1 … k ] . If v does not occur in P, then v is not a prefix of any read in R and thus s is not a matching candidate and can be discarded. If w does not occur in Q,  then w ≠ r [ k − k ' ' + 1 … k ] for all reads r ∈ R and thus s is not a matching candidate and can be discarded. Thus P and Q serve as filters to efficiently detect suffixes which can be discarded. For read r the suffixes s and corresponding strings v and w can be enumerated in O(|r| − [ell]min) time. Checking membership in P and in Q requires constant time. Therefore, each read r is processed in O(|r| − [ell]min) time. Thus the enumeration and checking requires O ( ∑ r ∈ R ∣ ∣ r ∣ ∣ − m ℓ min ) time altogether. The next task is to process a suffix, say s which has passed the P-filter and the Q-filter. That is, v = s[1…k′] [set membership] P and w = s [ k − k ' ' + 1 … k ] ∈ Q holds. One now has to check if u = s[1…k] occurs in K to verify if s is a matching candidate. If the latter is true, the appropriate counter needs to be incremented. Hence this is the counting phase of our algorithm. The simplest way to check the occurrence of u in K, is to perform a binary search, taking u as the key. However, this would require O(log2d) time for each k-mer passing the filters. This is too slow. Using a hash table turned out to be too slow as well and would require too much extra space, which we do not want to afford. We propose an efficient method that works as follows: Store each k-mer s[1.k] passing the P and the Q-filter in a buffer B of fixed size b = d γ for some constant γ > 1. Once B is full or all k-mers have been added to B, sort the elements in B in ascending lexicographic order. Then perform a binary search in K, but only for the first element in B, say x. As B is sorted, x is the smallest element. The binary search for x in K finds the smallest element in K greater than or equal to x using O(log2 d) time. If such an element occurs in K, say at index e, then simultaneously scan B beginning with the first index and K beginning at index e. For any element in B that is equal to an element in K, say at index i in K, increment the counter in C at the same index. This simultaneous linear scan of B and (a part of) K takes O(b + d) time and finds all k-mers from B occurring in K. Once the scan and the associated increments are done, the buffer is emptied for the next round. Suppose that there are in total b*k-mers that have passed B. Thus there are ⌈ b ∗ b ⌉ rounds filling the buffer. Each round is associated with a sorting step, a binary search and a linear scan. Sorting requires O(b) time using radixsort. This gives a running time of O ( b ∗ b ( b + l o g 2 d + ( b + d ) ) ) = O ( b ∗ b ( b + d ) ) = O ( b ∗ b ( b + b γ ) ) = O ( b ∗ ( 1 + γ ) ) = O ( b ∗ ) . As b ∗ ≤ n : = ∑ r ∈ R ∣ ∣ r ∣ ∣ , the running time is linear in the total length of the reads. Once all reads have been processed, for any initial k-mer u of any read, the following holds: If u is the ith initial k-mer in K, then C[i] is the number of SPM-relevant suffixes of which u is a prefix. To prepare for the insertion phase, compute the partial sums of C in an array π of size d + 1, such that π[0] = C[0], π [ i ] = π [ i − 1 ] + C [ i ] for all i, 1 ≤ i ≤ d − 1, and π[d] = π[d − 1]. π[d] is the number of all SPM-relevant suffixes. One creates a table S of size g: = π[d] to hold pairs of read numbers and read offsets. As in the counting phase, enumerate all suffixes of reads of length at least [ell]min passing the P- and the Q-filter. Suppose that s is such a suffix of read number p and with read offset q. Let u be the initial k-mer of s. Then we store (p, q, u) in a buffer B′ of fixed size b 2 . We choose this buffer size, as the elements in B′ require twice as much space as the elements in B. As in the counting phase, sort the buffer in lexicographic order of the k-mers it stores, and then process the buffer elements using the k-mer, say u, as a key to determine if u matches some element in K, say at index i. Then insert (p, q) at index π[i] − 1 in S and decrement π[i]. After all b* elements passed the buffer and have been processed, S holds all SPM-relevant suffixes (represented by read numbers and read offsets) in lexicographic order of their initial k-mers. Let u be the ith k-mer in K. Then all SPM-relevant suffixes with common prefix u are stored in S from index π[i] to π [ i + 1 ] − 1 . Thus S can uniquely be divided into buckets of SPM-relevant suffixes with the same initial k-mer. Each such bucket can be sorted independently from all other buckets. Moreover, each SPM-relevant suffix not occurring in the ith bucket, has an initial k-mer different from u and thus cannot have a common prefix of length ≥ [ell]min with the suffixes in the ith bucket. As a consequence, all suffix-prefix matches are derivable from pairs of SPM-relevant suffixes occurring in the same bucket. Thus, the suffix-prefix matches problem can be divided into d subproblems, each consisting of the computation of suffix-prefix matches from a bucket of SPM-relevant suffixes. This problem is considered later. To sort the ith bucket one extracts the remaining suffixes relevant for sorting the bucket and stores them in a table. This strategy minimizes the number of slow random accesses to the reads. Consider the ith bucket and let (p, q) be one of the suffixes in the bucket, referring to the suffix of read rp at read offset q. Then extract the suffix of rp starting at position q + k. As the maximum read length is considered to be constant, the total size of the remaining suffixes to be stored is O ( π [ i + 1 ] − π [ i ] ) . The remaining suffixes can be sorted using radixsort in O ( π [ i + 1 ] − π [ i ] ) time. An additional linear time scan over the sorted suffixes of the bucket delivers a table L of size π [ i + 1 ] − π [ i ] − 1 , such that Lj is the length of the longest common prefix of the suffixes S [ π [ i ] + j − 1 ] and S[π[i] + j] for all j, 1 ≤ j ≤ π [ i + 1 ] − π [ i ] − 1 . Sorting all remaining suffixes and computing the lcp-table L thus requires O(βmax) space and O ( ∑ d − 1 i = 0 ( π [ i + 1 ] − π [ i ] ) ) = O ( g ) time where βmax is the maximum size of a bucket and g is the total number of SPM-relevant suffixes. The bucket of sorted SPM-relevant suffixes and the corresponding table L are processed by Algorithm 2 described after the following implementation section and Algorithm 3 described in Additional file 1, Section 7. All in all, our algorithm runs in O(m + n + g) = O(n) time and O(m + 4k′ + 4k′′ + βmax + g + n) space. As we choose k′′ ≤ k′ [set membership] O(log2 n) and m, g, and βmax are all smaller than n, the space requirement is O(n). Thus the algorithm for identifying and sorting all ([ell]min, k)-SPM-relevant suffixes is optimal. Implementation We will now describe how to efficiently implement the algorithm described above. An essential technique used in our algorithm are integer codes for k-mers. These are widely used in sequence processing. As we have three different mer-sizes (k, k′, and k″) and dependencies between the corresponding integer codes, we shortly describe the technique here. In our problem, a k-mer always refers to a sequence of which it is a prefix. Therefore, we introduce integer codes for strings of length ≥ k: For all strings s of length at least k define the integer code ϕ ϕ k ( s ) = ∑ k i = 1 4 k − i , where ϕ is the mapping [A → 0, C → 1, G → 2, T → 3] uniquely assigning numbers from 0 to 3 to the bases in the alphabetical order of the bases. Note that only the first k symbols of s determine ϕk(s), which is an integer in the range [0…4k − 1]. For all strings s and s′ of length at least k, s [left triangle, eq] s′ implies ϕk(s) ≤ ϕk(s′), where [left triangle, eq] denotes the lexicographic order of strings and ≤ denotes the order of integers. Besides ϕk, we use the encodings ϕk′ and ϕ ϕ k k ' ' for some k′, k ″≤ k. ϕk′ encodes the prefix of s of length k′ and is defined in analogy to ϕk (replacing k by k′). ϕ ϕ k k ' ' ( s ) encodes the suffix s [ k − k ' ' + 1 … k ] of s[1…k] of length k″, i.e. ϕ ϕ k k ' ' ( s ) = ∑ k ' ' i = 1 4 k ' ' − i ϕ ( s [ k − k ' ' + i ] ) . ϕk′(s) and ϕ k k ' ' ( s ) can be computed from ϕk(s) according to the following equations: ϕ k ′ ( s ) = ϕ k ( s ) 4 k − k ' (1) ϕ k k ' ' ( s ) = ϕ k ( s ) mod 4 k ′′ (2) We implement k-mers by their integer codes. Each integer code can be computed in constant time by extracting the appropriate sequence of consecutive bit pairs from a 2bit per base encoding of the read set. In our implementation, we use the representation and the appropriate access functions from the GtEncseq software library [16]. As k ≤ ω 2 we can store each integer code in an integer of the machine’s word size. We sort m integer codes for the initial k-mers using quicksort, adapting the code from [17]. Our implementation works without recursion and uses an extra stack of size O(log2 m) to sort m integers. This small additional space requirement is the main reason to choose quicksort instead of radixsort, which is usually more than twice as fast, but requires O(m) extra working space, which we do not want to afford. The sets P and Q are implemented by bit vectors vP and vQ of 4k′ and 4k′′ bits, respectively. Bit vPq is set if and only if q = ϕk′(r) for some r ∈ R . Bit vQq is set if and only if q = ϕ k k ′′ ( r ) for some read r ∈ R . To obtain the bit index, one computes ϕk′(s) and ϕ k k ' ' ( s ) from ϕk(s) using Equations (1) and (2). Equation (1) can be implemented by a bitwise right shift of 2(k − k′) bits. Equation (2) can be implemented by a bitwise and operation with the integer 22k′′ − 1. Thus, given the integer code for s, both ϕk′(s) and ϕk″k(s) can be computed in constant time. Therefore, the sets P and Q can be constructed in O(m) time and each access takes constant time. When determining the k-mer codes in the counting phase and in the insertion phase, we sweep a window of width k over the sequence reads and compute the integer code for the sequence in the current window in constant time. We implement the counts by a byte array of size d and store counts larger than 255 in an additional hash table. Additional file 1, Section 1 gives the details. The partial sums in table π are bounded by g, the number of SPM-relevant suffixes. For large read sets, g can be larger than 232 − 1. However, as the partial sum are strictly increasing, one can implement π by a 32 bit integer table PS of size d + 1, such that PS[i] = π[i] mod 232 for any i, 0 ≤ i ≤ d and an additional integer table of size 2 m a x { 0 , ⌈ l o g 2 g ⌉ − 32 } marking the boundaries of carry bits. Details are given in Additional file 1, Section 2. For the insertion phase we need a representation of the read set (2n bits), table K (2kd bits), set P and Q (4k′ and 4k′′ bits, respectively), table π (32(d + 1) bits) and table S of size g. As S holds pairs of read numbers and read offsets, each entry in S is stored compactly in σ = ⌈ l o g 2 m ⌉ + ⌈ l o g 2 ( λ max − min ) ⌉ bits. This would give an integer array of size ⌈ g σ ω ⌉ if we would store S completely. But we do not, as we employ a partitioning strategy, explained next. Although the data structures representing tables S, K, P and π are of different sizes, their access follows the same scheme: Suppose that i is the smallest index, such that g 2 ≤ π [ i ] . Roughly half of the suffixes to be inserted in S are placed in buckets of lower order (with index ≤ i) and the other half are placed in buckets of higher order (with index > i). The buckets of lower order are associated with the k-mers in K up to index i. Hence, for these, one needs table K and PS only up to index i. Let s be some suffix of length ≤ [ell]min such that ϕk(s) ≤ K[i]. To apply the P-filter to s, one checks vP at index ϕ k ( s ) 4 k − k ′ ≤ K [ i ] 4 k − k ′ , which is in the first half of vector vP. This strategy, dividing tables S, K, P and π into q = 2 parts of roughly the same size, can be generalized to q > 2 parts. Each part is defined by a lower and an upper integer code and by corresponding lower and upper boundaries referring to sections of the four mentioned tables. Partitioning S means to only allocate the maximum space for holding all buckets belonging to a single part. The four tables that can be split over q parts require h(g, k, d, k′′, σ) = 2kd + 4k″ + 32(d + 1) + g σ bits. Hence, in the insertion phase, our method requires 2 n + 4 k '' + h ( g , k , d , k '' , σ ) q bits, where 2n + 4k′′ bits are for the representation of the reads and the set Q (which cannot be split). As gσ dominates all other terms, h(g, k, d, k′′, σ) is much larger than 2n + 4k′′ so that the space gain of our partitioning strategy is obvious. As the space required for the insertion phase for any number of parts can be precalculated, one can choose a memory limit and calculate the minimal number of parts such that the limit is not exceeded. In particular, choosing the space peak of the counting phase as a memory limit for the insertion phase allows for balancing the space requirement of both phases. More details on the partitioning technique are given in Additional file 1, Section 3. An obvious disadvantage of the partitioning strategy (with, say q, parts) is the requirement of q scans over the read set. However, the sequential scan over the read set is very fast in practice and only makes up for a small part of the running time of the insertion phase. The expected size of a bucket to be sorted after the insertion phase is smaller than the average read length. The maximum bucket size (determining the space requirement for this phase) is 1 to 2 orders of magnitude smaller than d. As we can store ω 2 bases in one integer of ω bits, the remaining suffixes (which form the sort keys) can be stored in β max ( λ max − k ω + 2 ) integers, where βmax is the maximum size of a bucket and λmax is the maximum length of a read. The additional constant 2 is for the length of the remaining suffix, for the read number and the read offset. The sort keys are thus sequences of integers of different length which have to be compared up to the longest prefix of the strings they encode. We use quicksort in which ω 2 bases are compared using a single integer comparison. As a side effect of the comparison of the suffixes, we obtain the longest common prefix of two compared suffixes in constant extra time, and store this in a table L of the size of the bucket. The suffixes in the bucket and the table L are passed to Algorithm 2, described next, and to Algorithm 3( Additional file 1, Section 7). An efficient algorithm for computing suffix-prefix matches from buckets of sorted SPM-relevant suffixes The input to the algorithm described next is a bucket of sorted SPM-relevant suffixes, with the corresponding table L, as computed by the algorithm of the previous subsection. Consider the ith bucket in S and let Hj = S[π[i] + j] be the jth suffix in this bucket for all j, 0 ≤ j ≤ β − 1 where β = π[i + 1] − π[i] is the size of the bucket. By construction, we have Hj-1 [left triangle, eq] Hj, Lj ≥ k, and Lj is the length of the longest common prefix of Hj−1 and Hj for j, 1 ≤ j ≤ β − 1. Note that the bucket-wise computation does not deliver the lcp-values of pairs of SPM-relevant suffixes on the boundary of the buckets. That is, for all i > 0, the length of the longest common prefix of S[π[i] − 1] and S[π[i]] is not computed, because S[π[i] − 1] is the last suffix of the (i − 1)th bucket and S[π[i]] is the first suffix of the ith bucket. However, as both suffixes belong to two different buckets, their longest common prefix is smaller than k (and thus smaller than [ell]min) and therefore not of interest for the suffix-prefix matching problem. The suffixes occurring in a bucket will be processed in nested intervals, called lcp-intervals, a notion introduced for enhanced suffix arrays by [18]. We generalize this notion to table H and L as follows: An interval e.f, 0 ≤ e < f ≤ β − 1, is an lcp-interval of lcp-value [ell] if the following holds: • e = 0 or L e < | , (3) • L q ≥ | for all q , e + 1 ≤ q ≤ f , (4) • L q = | for at least one q , e + 1 ≤ q ≤ f , (5) • f = β − 1 or L f + 1 < | . (6) We will also use the notation [ell] − [e.f] for an lcp-interval [e.f] of lcp-value [ell]. If [ell] − [e.f] is an lcp-interval such that w = He[1…[ell]] is the longest common prefix of the suffixes He, He+1, …, Hf, then [e.f] is called the w-interval. An lcp-interval [ell]′ − [e′.f′] is said to be embedded in an lcp-interval [ell] − [e.f] if it is a proper subinterval of [ell] − [e.f] (i.e., e ≤ e′ < f′ ≤ f) and [ell]′ > [ell]. The lcp-interval [ell] − [e.f] is then said to be enclosing [e′.f′]. If [e.f] encloses [e′.f′] and there is no interval embedded in [e.f] that also encloses [e′.f′], then [e′.f′] is called a child interval of [e.f] and [e.f] is the parent interval of [e′.f′]. We distinguish lcp-intervals from singleton intervals [e′] for any e′, 0 ≤ e′, ≤ β − 1. [e′] represents He′. The parent interval of [e′] is the smallest lcp-interval [e.f] with e ≤ e′ ≤ f. This parent–child relationship of lcp-intervals with other lcp-intervals and singleton intervals constitutes a virtual tree which we call the lcp-interval tree for H and L. The root of this tree is the lcp-interval 0 − [0.(β − 1)]. The implicit edges to lcp-intervals are called branch-edges. The implicit edges to singleton-intervals are called leaf-edges. Additional file 1, Section 10 gives a comprehensive example illustrating these notions. Abouelhoda et al. ([18], Algorithm 4.4) present a linear time algorithm to compute the implicit branch-edges of the lcp-interval tree in bottom-up order. When applied to a bucket of sorted suffixes, the algorithm performs a linear scan of tables H and L. In the eth iteration, 0 ≤ e ≤ β − 2, it accesses the value Le+1 and He. We have non-trivially extended the algorithm to additionally deliver leaf edges. The pseudocode, with some additions in the lines marked as new, is given in Algorithm 1 (Figure ​(Figure1).1). We use the following notation and operations: An external file that holds a picture, illustration, etc. Object name is 1471-2105-13-82-1.jpg Open in a separate window Figure 1 Algorithm 1. Bottom-up traversal algorithm for arrays of SPM-relevant suffixes. This is an extension of [18, Algorithm 4.4] with the additional lines marked as new. • A stack stores triples ([ell], e, f) representing an lcp-interval [ell] − [e.f]. To access the elements of such a triple, say sv, we use the notation sv.lcp (for the lcp-value [ell]), sv.lb (for the left boundary e) and sv.rb (for the right boundary f). • stack.push(e) pushes an element e onto the stack. • stack.pop pops an element from the stack and returns it. • stack.top returns a reference to the topmost element of the stack. • [perpendicular] stands for an undefined value. • process_leafedge(firstedge, itv, (p, q)) processes an edge from the lcp-interval itv to the singleton interval representing the suffix rp[q…|rp|]. firstedge is true if and only if the edge is the first processed edge outgoing from itv. • process_branchedge(firstedge, itv, itv’) processes an edge from the lcp-interval itv to the lcp-interval itv’. The value itv’.rb is defined and firstedge is true if and only if the edge is the first edge outgoing from itv. • process_lcpinterval(itv) processes the lcp-interval itv. The value itv.rb is defined. Depending on the application, we use different functions process_leafedge, process_branchedge, and process_lcpinterval. Additional file 1, Section 4, explains why Algorithm 1 also delivers the leaf edges of the lcp-interval tree in the correct bottom-up order. Consider a path in the lcp-interval tree from the root to a singleton interval [e′] representing He′ = rp[q…|rp|]. Let [ell] − [e.f] be an lcp-interval on this path, and consider the edge on this path outgoing from [ell] − [e.f]. If the edge goes to an lcp-interval of, say lcp-value [ell]′, then the edge is implicitly labeled by the non-empty sequence r p [ q + l … q + l ' − 1 ] . Suppose the edge goes to a singleton interval: Then the edge is implicitly labeled by the non-empty sequence rp[q + [ell]…|rp| − 1]$p. If q + [ell] = |rp|, then rp[q + [ell]…|rp| − 1] is the empty string, which implies that the edge to the singleton interval is labeled by the sentinel $p. Such an edge is a terminal edge for rp. If the read offset q is 0, we call [e′] a whole-read interval for rp, and the path in the lcp-interval tree from the root to [e′] a whole-read path for rp. Consider a suffix-prefix match [left angle bracket] rp, rj, [ell] [right angle bracket] , such that the suffix w of rp of length [ell] has a prefix u of length k. Recall that u is the common prefix of all suffixes in the ith bucket. Due to the implicit padding of reads at their end, the symbol following w as a suffix of rp is $p. By definition, w is also prefix of rj and the symbol in rj following this occurrence of w is different from $p. Thus, there is a w-interval [e.f] in the lcp-interval tree for H and L. [e.f] is on the path from the root-interval to the whole-read leaf interval for rj. Moreover, there is a terminal edge for rp outgoing from [e.f]. Vice versa, an lcp-interval of lcp-value [ell] on the path to the whole-read leaf interval for rj and with a terminal edge for rp identifies the suffix-prefix match [left angle bracket] rp, rj, [ell] [right angle bracket] . This observation about suffix-prefix matches is exploited in Algorithm 2 (Figure ​(Figure2)2) which performs a bottom-up traversal of the lcp-interval tree for H and L, collecting whole-read leaves and terminal edges for lcp-intervals of lcp-value at least [ell]min. More precisely, whenever a whole-read leaf for rp, 1 ≤ p ≤ m, is found (line 9), p is appended to the list W. With each lcp-interval itv on the stack used in the bottom-up traversal, an integer itv.firstinW is associated. The elements in W[itv.firstinW…|W|] are exactly the read numbers of whole-read leaves collected for itv. The value of itv.firstinW is set whenever the first edge outgoing from itv is detected: If the first edge outgoing from itv is a leaf-edge, no previous whole-read leaf for itv has been processed: Thus |W| + 1 is the first index in list W where the whole read leaf information (if any) for itv will be stored (see line 8). If the first edge is a branch-edge to lcp-interval itv′, then the corresponding subset of W for itv′ must be inherited to itv. Technically, this is achieved by inheriting the firstinW-attribute from itv′ to itv, see line 18 of Algorithm 2. An external file that holds a picture, illustration, etc. Object name is 1471-2105-13-82-2.jpg Open in a separate window Figure 2 Algorithm 2. Bottom-up traversal of lcp-interval tree enumerating suffix-prefix matches. Whenever a terminal edge for read rp, outgoing from an interval itv is processed (line 11), p is added to the list T. Suppose that this terminal edge is outgoing from the lcp-interval itv. The first symbol of the label of the terminal edge is $p. Suppose there is a branch-edge outgoing from itv to some lcp-interval itv′. Then the first symbol, say a, of the implicit label of this edge must occur more than once. Thus it cannot be a sentinel, as these are considered different in the lexicographic ordering of the suffixes. Hence the first symbol a is either A, C, G or T. As these symbols are, with respect to the lexicographic order, smaller than the sentinels, the branch-edge from itv to itv’ appears before the terminal edge from itv. Hence the terminal edges outgoing from itv′ have been processed in line 25, and so we only need a single list T for the entire algorithm. As soon as all edges outgoing from itv have been processed, we have collected the terminal edges in T and the whole-read leaves in W. If itv.lcp exceeds the minimum length, Algorithm 2 computes the cartesian product of T with the appropriate subset of W and processes the corresponding suffix-prefix matches of length itv.lcp in line 25. At this point suffix-prefix matches may be output or post-processed to check for additional constraints, such as transitivity. Once the cartesian product has been computed, the elements from T are no longer needed and T is emptied (line 26). Note that the algorithm empties W once an lcp-interval of lcp-value smaller than [ell]min is processed. After this event, there will only be terminal edges from v-intervals such that the longest common prefix of v and the reads in W is smaller than [ell]min. Therefore there will be no suffix-prefix match of the form [left angle bracket]_, w, [ell][right angle bracket] such that [ell] ≥ [ell]min and w is a read represented in W. So the list can safely be emptied. The lcp-interval tree for H and L contains β leaf-edges. As all lcp-intervals have at least two children, there are at most β − 1 branch-edges and β lcp-intervals. As each of the three functions specified in Algorithm 2 is called once for every corresponding item, the number of functions calls is at most 3β − 1. Recall that Algorithm 2 is applied to each bucket and the total size of all buckets is g. Hence there are at most 3g − d calls to the three functions. process_leafedge and process_branchedge run in constant time. The running time of process_lcpinterval is determined by the number of SPMs processed. Assuming that the processing takes constant time, the overall running time of Algorithm 2 for all buckets is O(g + z) where z is the number of processed SPMs. Handling reverse complements of reads Reads may originate from both strands of a DNA molecule. For this reason, suffix-prefix matches shall also be computed between reads and reverse complements of other reads. Handling the reverse complements of all reads is conceptually easy to integrate into our approach: One just has to process –– R instead of R . The three steps which involve scanning the reads are extended to process both strands of all reads. This does not require doubling the size of the read set representation, as all information for the reverse complemented reads can efficiently be extracted from the forward reads. Additional file 1, Section 5, shows how to compute the integer codes for the reversed reads from the integer codes of the forward reads in constant time. The scan of the reverse complemented reads has a negligible impact on the runtime. Of course, the size of the table S, K and PS roughly doubles when additionally considering reverse complements. When computing suffix-prefix matches some minor modifications are necessary: Applying Algorithm 2 to –– R finds all SPMs, including some redundant ones, which we want to omit. This is formalized as follows: an SPM ⟨ r , s , l ⟩ ∈ S P M ( –– R ) is non-redundant if and only if one of the following conditions is true: • r ∈ R , s ∈ R (7) • r ∈ R , s ∈ –– R , r ≺ – s • r ∈ –– R , s ∈ R , s ≺ – r . For any SPM, these conditions can easily be checked in constant time, see Algorithm 3 ( Additional file 1, Section 7). Recognition of transitive and irreducible suffix-prefix matches For the construction of the string graph, we do not need transitive SPMs. An SPM ⟨ r , t , l ' ' ⟩ is transitive if and only if there are two SPMs [left angle bracket]r, s, [ell][right angle bracket] and [left angle bracket]s, t, [ell]′[right angle bracket] such that [ell] + [ell]′ = |s|+ [ell]″. Figure ​Figure33 shows a concrete example of a transitive SPM. An SPM which is not transitive is irreducible. An external file that holds a picture, illustration, etc. Object name is 1471-2105-13-82-3.jpg Open in a separate window Figure 3 Example of a transitive suffix-prefix match. An example of a transitive SPM. A set of three reads with a transitive SPM [left angle bracket]r, t, 10[right angle bracket] derived from [left angle bracket]s, t, 16[right angle bracket]. The following theorem characterizes an SPM by a read and a single irreducible SPM satisfying a length constraint and a match constraint, see Figure ​Figure44 for an illustration. An external file that holds a picture, illustration, etc. Object name is 1471-2105-13-82-4.jpg Open in a separate window Figure 4 Illustration of transitivity of a suffix-prefix match. Schematic illustration of transitivity. ⟨ r , t , l ' ' ⟩ is a transitive SPM derived from the SPM [left angle bracket]s, t, [ell]’[right angle bracket]. Hence, the prefix v of s is a suffix of r [ 1… | r | − l ' ' ] . Theorem 1. Let [left angle bracket]r, t, [ell]″[right angle bracket] be an SPM. Then [left angle bracket]r, t, [ell]″[right angle bracket] is transitive if and only if there is an s ∈ R and an irreducible SPM [left angle bracket]s, t, [ell]′[right angle bracket] such that [ell]′ > [ell]″, |r| − [ell]″ ≥ |s| − [ell]′ and s[1…|s| − [ell]′] = r[|r| − [ell]″ − (|s| − [ell]′) + 1…|r| − [ell]″]. The proof of Theorem 1 can be found in Additional file 1, Section 6. If the SPM [left angle bracket]r, t, [ell]″[right angle bracket] is transitive and [left angle bracket]s, t, [ell]′[right angle bracket] is the SPM satisfying the conditions of Theorem 1, then we say that [left angle bracket]r, t, [ell]″[right angle bracket] is derived from [left angle bracket]s, t, [ell]′[right angle bracket]. Theorem 1 suggests a way to decide the transitivity of an SPM [left angle bracket]r, t, [ell][right angle bracket]: Check if there is an irreducible SPM [left angle bracket]s, t, [ell]′[right angle bracket] from which it is derived. The check involves comparison of up to |s| − [ell]′ symbols to verify if s[1…|s| − [ell]′] is a suffix of r [ 1 … | r | − l ' ' ] . As there may be several irreducible SPMs from which [left angle bracket]r, t, [ell]″[right angle bracket] may be derived, it is necessary to store the corresponding left contexts: For any sequence s and any [ell]′, 1 ≤ [ell]′ < |s|, the left context LC(s, [ell]′) of s is the non-empty string s[1…|s| − [ell]′]. Due to the bottom-up nature of the traversal in Algorithm 2, the SPMs involving the different prefixes of a given read are enumerated in order of match length, from the longest to the shortest one. Thus, Algorithm 2 first delivers the irreducible SPM [left angle bracket]s, t, [ell]′[right angle bracket] from which ⟨ r , t , l ' ' ⟩ is possibly derived, because l ' > l ' ' . From Theorem 1 one can conclude that the first SPM, say [left angle bracket]s, t, [ell]′[right angle bracket], found on a whole-read path for t is always irreducible. Hence, one stores LC(s, [ell]′). An SPM [left angle bracket]r, t, [ell]″[right angle bracket] detected later while traversing the same whole-read path for t is classified as transitive if and only if LC(s, [ell]′) is a suffix of LC(r, [ell]″) (see Figure ​Figure55 for an illustration). If [left angle bracket]r, t, [ell]″[right angle bracket] is transitive it is discarded. Otherwise, LC(r, [ell]″) must be stored as well to check the transitivity of the SPMs found later for the same whole-read path. So each SPM is either classified as transitive, or irreducible, in which case a left context is stored. To implement this method, we use a dictionary D of left contexts, with an operation LCsearch(D, s), which returns true if there is some t [set membership] D such that t is a suffix of s. Otherwise, it adds s to D and returns false. Such a dictionary can, for example, be implemented by a trie [19] storing the left contexts in reverse order. In our implementation we use a blind-trie [20]. In Additional file 1, Section 7 we present a modification of Algorithm 2 to output non-redundant irreducible SPMs only. An external file that holds a picture, illustration, etc. Object name is 1471-2105-13-82-5.jpg Open in a separate window Figure 5 Transitivity and left contexts. Transitivity and left contexts. Let the SPM [left angle bracket]t, r, |z|[right angle bracket] be derived from [left angle bracket]s, r, |zz’|[right angle bracket]. Hence the left context LC(s, |zz’|) = xy is a suffix of the left context LC(t, |z|) = wxy. Let [left angle bracket]u, r, |z|[right angle bracket] be an irreducible SPM. Then LC(u, |yz|) = vy for some non empty string v and LC(s, |zz’|) is not a suffix of vy. Recognition of internally contained reads At the beginning of the methods section we have shown how to detect reads which are prefixes or suffixes of other reads. When constructing the string graph we also have to discard internally contained reads, which are contained in other reads without being a suffix or a prefix. More precisely, r ∈ R is internally contained, if a read r ' ∈ R exists, such that r′ = urw for some non-empty strings u and v. In Additional file 1, Section 8, we show how to efficiently detect internally contained reads. Construction of the assembly string graph Consider a read set R which is suffix- and prefix-free. The assembly string graph [8] is a graph of the relationships between the reads, constructed from S P M nr ( –– R ) , the set of all non-redundant irreducible SPMs from S P M ( –– R ) restricted to reads which are not internally contained. For each r ∈ R the graph contains two vertices denoted by r.B and r.E, representing the two extremities of the read. B stands for begin, E stands for end. For each non-redundant irreducible SPM ⟨ r , s , l ⟩ ∈ S P M nr ( –– R ) satisfying [ell] ≥ [ell]min, the graph contains two directed edges, defined as follows: 1. if ⟨ r , s , l ⟩ ∈ S P M nr ( –– R ) there are two edges: • r.E → s.E with edge label s[[ell] + 1…|s|] • s.B → r.B with edge label ¯ r [ l + 1 … | r | ] 2. if ⟨ r , ¯ s , l ⟩ ∈ S P M nr ( ¯¯¯¯ R ) there are two edges: • r.E → s.B with edge label ¯ s [ l + 1 … | s | ] • s.E → r.B with edge label ¯ r [ l + 1 … | r | ] 3. if ⟨ – s , r , l ⟩ ∈ S P M nr ( ¯¯¯¯ R ) there are two edges: • r.B → s.E with edge label s[[ell] + 1…|s|] • s.B → r.E with edge label r[[ell] + 1…|r|] In our implementation of the string graph, vertices are represented by integers from 0 to 2m − 1. To construct the graph from the list of non-redundant irreducible SPMs, we first calculate the outdegree of each vertex. From the counts we calculate partial sums. In a second scan over the list of SPMs, we insert the edges in an array of size 2ρ, where ρ = ∣ ∣ S P M nr ( ¯¯¯¯ R ) ∣ ∣ . This strategy allows to allocate exactly the necessary space for the edges and to access the first edge outgoing from a vertex in constant time. The array of edges is stored compactly using 2 ρ ( ⌈ l o g 2 ( 2 m ) ⌉ + ⌈ l o g 2 ( λ max − l min ) ⌉ ) bits, where λmax is the maximum length of a read. ⌈ l o g 2 ( 2 m ) ⌉ bits are used for the destination of an edge (the source of the edge is clear from the array index where the edge is stored). ⌈ l o g 2 ( λ max − l min ) ⌉ bits are used for the length of the edge label. To output the contigs, we first write references (such as read numbers and edge lengths) to a temporary file. Once this is completed, the memory for the string graph is deallocated, and the read sequences are mapped into memory. Finally, the sequences of the contigs are derived from the references and the contigs are output. To verify the correctness of our string graph implementation and to allow comparison with other tools, we have implemented the graph cleaning algorithms described in [9] as an experimental feature. More sophisticated techniques, such as the network flow approach described in [8], are left for future work, as the main focus of this paper lies in the efficient computation of the irreducible SPMs and the construction of the string graph.",Assembly,"readjoiner  fast  memory efficient string graphbased sequence assembler
basic definitions let    string  length   symbols   alphabet    denote  ith symbol    …  substring    position     ≤   ≤  …   prefix   end  position   …   suffix   start  position   substring    proper    different    substring    internal    neither  prefix   suffix    read    string   alphabet {   }   assume   sort   alphabetical order           leave triangle  denote  lexicographic order   substrings   read induce   alphabetical order   let    length    reverse complement   denote       sequence      …          indicate  watsoncrick complement  base  compute suffix  prefixfree read set  first step   approach  assemble  collection  read   eliminate read   prefix  suffix   read   describe  method  recognize  read consider  order set       …      read possibly  variable length    read may occur       indeed  multiset  assume      ≤  ≤   ith read     virtually pad   sentinel symbol    right end    alphabetical order   extend                     define  binary relation precede      precede           precede reflect  order   read   input   prefixfree    read       ′   ∖ {  }      prefix  ′   suffixfree          read ′   ∖ {  }      suffix  ′  obtain  prefix  suffixfree set  read  lexicographically sort  read use  modify radixsort  string  describe     algorithm  string   sort  first insert  bucket accord   first character  bucket   sort recursively accord   next character   read   bucket  bucket always consist  read    common prefix   bucket  smaller   constant  remain suffix   read   bucket  sort  insertion sort    sort process  length   longest common prefix lcp  two lexicographically consecutive read  calculate   byproduct  two lexicographically consecutive read   ′   lcp  length ell     conclude     prefix  ′  ell  ′     proper prefix  ′   mark   ell  ′    ′  identical   mark  read   larger accord   binary relation precede   handle reverse complement   mark read   suffix   read one simply apply  method   multiset        …          …            ¯¯¯       ≤  ≤     include  reverse complement   read  method also mark read   suffix   read   due   observation   read    suffix  read ′      prefix    ′    final step   algorithm one eliminate  read      mark  remain unmarked read    process   algorithm  compute  suffix  prefixfree set  read run      max   time  λmax   maximum length   read     machine word size   consider λmax    constant    imply   read      length  algorithm run   time compute suffixprefix match suppose     suffix  prefixfree set   read let ellmin     minimum length parameter  set        suffixprefix match spms  short   smallest set  triple leave angle bracketr ′ ellright angle bracket      ' ∈   string    exist      ′     ell ≥ ellmin ell   length   suffixprefix match leave angle bracketr ′ ellright angle bracket   suffixprefix match problem   find  suffixprefix match   read  length smaller  ellmin cannot  definition contribute  spm   ignore   thus  assume    contain read  length  least ellmin  method  solve  suffixprefix match problem present  consist  two main algorithms  first algorithm identify  lexicographically sort  spmrelevant suffix   subset   suffix   read   one  compute  suffixprefix match  second algorithm enumerate  match give  sort list   spmrelevant suffix consider  suffixprefix match leave angle bracketr ′ ellright angle bracket   definition  suffix  length ell   exactly match  prefix  length ell  ′ obviously  suffix   involve   match start   position   ≤  ≤      min      imply   must   least  length ellmin    suffix cannot start   first position    otherwise  would   prefix    read contradict  assumption    prefixfree  enumerate  set   suffixprefix match  length  least ellmin  preprocess  read  determine  proper suffix   read  may  involve   suffixprefix match  precisely   read   determine  match candidates   proper suffix       length     least ellmin     read ′     ′   common prefix  length  least      userdefined parameter satisfy  ≤    {  min    }    two reason  impose  constraint   first  want  represent  string  length    alphabet  size   one machine word thus  ≤    second  suffix   read    take  prefix  length   minimum length ellmin thus  choose  ≤ ellmin  set   match candidates   read form  set   ellmin kspmrelevant suffix  simplicity sake  use  notion spmrelevant suffix  ellmin    clear   context   spms   construct   spmrelevant suffix   spmrelevant suffix lead   spm  efficient algorithm  identify  sort  spmrelevant suffix  first two phase   algorithm follow  strategy   borrow   count sort algorithm  like   algorithm   count phase   insertion phase however   problem  elements  spmrelevant suffix   sort   determine   algorithm moreover  number  key  initial kmers whose occurrences  count    order   number  elements   sort therefore   space efficient solution    trivial  access  counter give  key   develop  time  space efficient method  access  counter   key exploit  fact  count  insert  spmrelevant suffix       immediately instead  items   countedinserted  first buffer   sort  linear scan  perform  count  insert step  contrast  count sort  algorithm use  extra sort step  obtain  final order  elements presorted   insertion phase   assumption   maximum read length   constant    imply   read      length  algorithm run   time  space     total length   read   best   knowledge  method employ  similar strategy   yet  develop   suffixprefix match problem  first give  description   algorithm use string notation   separate section  explain   efficiently implement  algorithm   follow   consider  read   forward direction however    difficult  extend  method  also incorporate  reverse complement   read   comment   issue   end   methods section  initial kmer   sequence    prefix    length   determine  match candidates efficiently  first enumerate  initial kmers   read  store    table  size        time  notion table size always refer   number  entries   table  next step lexicographically sort  kmers   table  ascend order  string sort problem   transform   integer sort problem see implementation    solve  radixsort    time   extra work space   next step  linear scan   sort kmers remove duplicate   table  count  many time  kmer occur   table  scan require  time let  ≤    number  different kmers    store   table   size   count   elements   require another table   size   addition   duplicate removal  count  linear scan   sort kmers construct two set     size   depend  two user define parameters ′ ≤    ″≤     set   initial ′mers   read    set   kmers      ' '   …      ∈    assume  elements   add      constant time   membership   set   decide  constant time thus  linear scan construct      time     subset   set  size   ′     store    ′ bits  require   ' ' bits      initial kmers   read  consider result   sort table    nonredundant key  initial kmers  read  table   size   count kmers  two set     construction  count     least    sum   count    next task   enumerate   read   suffix     position   ≤  ≤      min        ellmin  suffix    suffix    construction   length ≥ ellmin one extract two string   …′         ' '   …        occur        prefix   read    thus     match candidate    discard     occur      ≠      ' '   …     read  ∈   thus     match candidate    discard thus    serve  filter  efficiently detect suffix    discard  read   suffix   correspond string      enumerate    ellmin time check membership      require constant time therefore  read   process    ellmin time thus  enumeration  check require   ∑  ∈  ∣ ∣  ∣ ∣    min  time altogether  next task   process  suffix say    pass  pfilter   qfilter     …′ set membership          ' '   …   ∈  hold one    check    … occur    verify     match candidate   latter  true  appropriate counter need   incremented hence    count phase   algorithm  simplest way  check  occurrence       perform  binary search take    key however  would require olog2d time   kmer pass  filter    slow use  hash table turn     slow  well  would require  much extra space     want  afford  propose  efficient method  work  follow store  kmer  pass     qfilter   buffer   fix size       constant       full   kmers   add   sort  elements    ascend lexicographic order  perform  binary search       first element   say     sort    smallest element  binary search     find  smallest element   greater   equal   use olog2  time    element occur   say  index   simultaneously scan  begin   first index   begin  index    element     equal   element   say  index    increment  counter      index  simultaneous linear scan     part   take    time  find  kmers   occur     scan   associate increments    buffer  empty   next round suppose     total *kmers   pass  thus   ⌈  ∗  ⌉ round fill  buffer  round  associate   sort step  binary search   linear scan sort require  time use radixsort  give  run time     ∗                      ∗            ∗             ∗           ∗     ∗ ≤    ∑  ∈  ∣ ∣  ∣ ∣   run time  linear   total length   read   read   process   initial kmer    read  follow hold     ith initial kmer       number  spmrelevant suffix      prefix  prepare   insertion phase compute  partial sum     array   size                             ≤  ≤             number   spmrelevant suffix one create  table   size     hold pair  read number  read offset    count phase enumerate  suffix  read  length  least ellmin pass     qfilter suppose      suffix  read number    read offset  let    initial kmer     store      buffer ′  fix size     choose  buffer size   elements  ′ require twice  much space   elements      count phase sort  buffer  lexicographic order   kmers  store   process  buffer elements use  kmer say    key  determine   match  element   say  index   insert    index       decrement    * elements pass  buffer    process  hold  spmrelevant suffix represent  read number  read offset  lexicographic order   initial kmers let    ith kmer     spmrelevant suffix  common prefix   store    index            thus   uniquely  divide  bucket  spmrelevant suffix    initial kmer   bucket   sort independently    bucket moreover  spmrelevant suffix  occur   ith bucket   initial kmer different    thus cannot   common prefix  length ≥ ellmin   suffix   ith bucket   consequence  suffixprefix match  derivable  pair  spmrelevant suffix occur    bucket thus  suffixprefix match problem   divide   subproblems  consist   computation  suffixprefix match   bucket  spmrelevant suffix  problem  consider later  sort  ith bucket one extract  remain suffix relevant  sort  bucket  store    table  strategy minimize  number  slow random access   read consider  ith bucket  let    one   suffix   bucket refer   suffix  read   read offset   extract  suffix   start  position      maximum read length  consider   constant  total size   remain suffix   store                  remain suffix   sort use radixsort                time  additional linear time scan   sort suffix   bucket deliver  table   size                    length   longest common prefix   suffix             sπi       ≤  ≤               sort  remain suffix  compute  lcptable  thus require oβmax space    ∑                          time  βmax   maximum size   bucket     total number  spmrelevant suffix  bucket  sort spmrelevant suffix   correspond table   process  algorithm  describe   follow implementation section  algorithm  describe  additional file  section      algorithm run         time    ′  ′′  βmax     space   choose ′′ ≤ ′ set membership olog2      βmax   smaller    space requirement   thus  algorithm  identify  sort  ellmin kspmrelevant suffix  optimal implementation    describe   efficiently implement  algorithm describe   essential technique use   algorithm  integer cod  kmers   widely use  sequence process    three different mersizes  ′  ″  dependencies   correspond integer cod  shortly describe  technique    problem  kmer always refer   sequence      prefix therefore  introduce integer cod  string  length ≥    string   length  least  define  integer code        ∑              map  →   →   →   →  uniquely assign number       base   alphabetical order   base note    first  symbols   determine ϕks    integer   range …     string   ′  length  least   leave triangle  ′ imply ϕks ≤ ϕks′  leave triangle  denote  lexicographic order  string  ≤ denote  order  integers besides   use  encode ′      ' '   ′  ″≤  ′ encode  prefix    length ′   define  analogy   replace   ′     ' '    encode  suffix      ' '   …    …  length ″      ' '     ∑  ' '      ' '          ' '      ′     ' '      compute  ϕks accord   follow equations   ′              '     ' '          mod   ′′   implement kmers   integer cod  integer code   compute  constant time  extract  appropriate sequence  consecutive bite pair   2bit per base encode   read set   implementation  use  representation   appropriate access function   gtencseq software library    ≤     store  integer code   integer   machine word size  sort  integer cod   initial kmers use quicksort adapt  code    implementation work without recursion  use  extra stack  size olog2   sort  integers  small additional space requirement   main reason  choose quicksort instead  radixsort   usually   twice  fast  require  extra work space     want  afford  set     implement  bite vectors     ′  ′′ bits respectively bite vpq  set       ′    ∈   bite vqq  set          ′′      read  ∈    obtain  bite index one compute ′     ' '     ϕks use equations    equation    implement   bitwise right shift    ′ bits equation    implement   bitwise  operation   integer ′′   thus give  integer code    ′  ″   compute  constant time therefore  set      construct   time   access take constant time  determine  kmer cod   count phase    insertion phase  sweep  window  width    sequence read  compute  integer code   sequence   current window  constant time  implement  count   byte array  size   store count larger     additional hash table additional file  section  give  detail  partial sum  table   bound    number  spmrelevant suffix  large read set    larger     however   partial sum  strictly increase one  implement     bite integer table   size      psi   mod      ≤  ≤    additional integer table  size     {   ⌈      ⌉   } mark  boundaries  carry bits detail  give  additional file  section    insertion phase  need  representation   read set  bits table  2kd bits set    ′  ′′ bits respectively table     bits  table   size    hold pair  read number  read offset  entry    store compactly    ⌈      ⌉  ⌈       max  min  ⌉ bits  would give  integer array  size ⌈    ⌉   would store  completely       employ  partition strategy explain next although  data structure represent table        different size  access follow   scheme suppose     smallest index     ≤      roughly half   suffix   insert    place  bucket  lower order  index ≤     half  place  bucket  higher order  index    bucket  lower order  associate   kmers     index  hence   one need table       index  let    suffix  length ≤ ellmin   ϕks ≤   apply  pfilter   one check   index          ′ ≤         ′      first half  vector   strategy divide table          part  roughly   size   generalize     part  part  define   lower   upper integer code   correspond lower  upper boundaries refer  section   four mention table partition  mean   allocate  maximum space  hold  bucket belong   single part  four table    split   part require    ′′   2kd  ″        bits hence   insertion phase  method require      ''           ''     bits    ′′ bits    representation   read   set   cannot  split   dominate   term    ′′   much larger    ′′    space gain   partition strategy  obvious   space require   insertion phase   number  part   precalculated one  choose  memory limit  calculate  minimal number  part    limit   exceed  particular choose  space peak   count phase   memory limit   insertion phase allow  balance  space requirement   phase  detail   partition technique  give  additional file  section   obvious disadvantage   partition strategy  say  part   requirement   scan   read set however  sequential scan   read set   fast  practice   make    small part   run time   insertion phase  expect size   bucket   sort   insertion phase  smaller   average read length  maximum bucket size determine  space requirement   phase     order  magnitude smaller      store   base  one integer   bits  remain suffix  form  sort key   store   max   max       integers  βmax   maximum size   bucket  λmax   maximum length   read  additional constant     length   remain suffix   read number   read offset  sort key  thus sequence  integers  different length     compare    longest prefix   string  encode  use quicksort     base  compare use  single integer comparison   side effect   comparison   suffix  obtain  longest common prefix  two compare suffix  constant extra time  store    table    size   bucket  suffix   bucket   table   pass  algorithm  describe next   algorithm  additional file  section   efficient algorithm  compute suffixprefix match  bucket  sort spmrelevant suffix  input   algorithm describe next   bucket  sort spmrelevant suffix   correspond table   compute   algorithm   previous subsection consider  ith bucket    let   sπi     jth suffix   bucket     ≤  ≤              size   bucket  construction    leave triangle    ≥      length   longest common prefix        ≤  ≤    note   bucketwise computation   deliver  lcpvalues  pair  spmrelevant suffix   boundary   bucket         length   longest common prefix  sπi    sπi   compute  sπi     last suffix      bucket  sπi   first suffix   ith bucket however   suffix belong  two different bucket  longest common prefix  smaller    thus smaller  ellmin  therefore   interest   suffixprefix match problem  suffix occur   bucket   process  nest intervals call lcpintervals  notion introduce  enhance suffix array    generalize  notion  table     follow  interval   ≤    ≤      lcpinterval  lcpvalue ell   follow hold •           •   ≥         ≤  ≤    •       least one      ≤  ≤    •                 also use  notation ell     lcpinterval   lcpvalue ell  ell     lcpinterval     …ell   longest common prefix   suffix   …     call  winterval  lcpinterval ell′  ′′  say   embed   lcpinterval ell       proper subinterval  ell     ≤ ′  ′ ≤   ell′  ell  lcpinterval ell     say   enclose ′′   enclose ′′     interval embed    also enclose ′′  ′′  call  child interval       parent interval  ′′  distinguish lcpintervals  singleton intervals ′   ′  ≤ ′ ≤    ′ represent ′  parent interval  ′   smallest lcpinterval    ≤ ′ ≤   parentchild relationship  lcpintervals   lcpintervals  singleton intervals constitute  virtual tree   call  lcpinterval tree      root   tree   lcpinterval       implicit edge  lcpintervals  call branchedges  implicit edge  singletonintervals  call leafedges additional file  section  give  comprehensive example illustrate  notions abouelhoda    algorithm  present  linear time algorithm  compute  implicit branchedges   lcpinterval tree  bottomup order  apply   bucket  sort suffix  algorithm perform  linear scan  table      eth iteration  ≤  ≤     access  value      nontrivially extend  algorithm  additionally deliver leaf edge  pseudocode   additions   line mark  new  give  algorithm  figure ​figure1  use  follow notation  operations  external file  hold  picture illustration etc object name  jpg open   separate window figure  algorithm  bottomup traversal algorithm  array  spmrelevant suffix    extension   algorithm    additional line mark  new •  stack store triple ell   represent  lcpinterval ell    access  elements    triple say   use  notation svlcp   lcpvalue ell svlb   leave boundary   svrb   right boundary  • stackpushe push  element  onto  stack • stackpop pop  element   stack  return  • stacktop return  reference   topmost element   stack • perpendicular stand   undefined value • process_leafedgefirstedge itv   process  edge   lcpinterval itv   singleton interval represent  suffix rpq… firstedge  true      edge   first process edge outgo  itv • process_branchedgefirstedge itv itv process  edge   lcpinterval itv   lcpinterval itv  value itvrb  define  firstedge  true      edge   first edge outgo  itv • process_lcpintervalitv process  lcpinterval itv  value itvrb  define depend   application  use different function process_leafedge process_branchedge  process_lcpinterval additional file  section  explain  algorithm  also deliver  leaf edge   lcpinterval tree   correct bottomup order consider  path   lcpinterval tree   root   singleton interval ′ represent ′  rpq… let ell     lcpinterval   path  consider  edge   path outgo  ell     edge go   lcpinterval  say lcpvalue ell′   edge  implicitly label   nonempty sequence       …    '     suppose  edge go   singleton interval   edge  implicitly label   nonempty sequence rpq  ell…      ell    rpq  ell…     empty string  imply   edge   singleton interval  label   sentinel    edge   terminal edge     read offset     call ′  wholeread interval     path   lcpinterval tree   root  ′  wholeread path   consider  suffixprefix match leave angle bracket   ell right angle bracket     suffix     length ell   prefix   length  recall     common prefix   suffix   ith bucket due   implicit pad  read   end  symbol follow    suffix      definition   also prefix     symbol   follow  occurrence    different   thus    winterval    lcpinterval tree         path   rootinterval   wholeread leaf interval   moreover    terminal edge   outgo   vice versa  lcpinterval  lcpvalue ell   path   wholeread leaf interval      terminal edge   identify  suffixprefix match leave angle bracket   ell right angle bracket   observation  suffixprefix match  exploit  algorithm  figure ​figure2  perform  bottomup traversal   lcpinterval tree     collect wholeread leave  terminal edge  lcpintervals  lcpvalue  least ellmin  precisely whenever  wholeread leaf    ≤  ≤   find line    append   list    lcpinterval itv   stack use   bottomup traversal  integer itvfirstinw  associate  elements  witvfirstinw…  exactly  read number  wholeread leave collect  itv  value  itvfirstinw  set whenever  first edge outgo  itv  detect   first edge outgo  itv   leafedge  previous wholeread leaf  itv   process thus      first index  list    whole read leaf information    itv   store see line    first edge   branchedge  lcpinterval itv′   correspond subset    itv′ must  inherit  itv technically   achieve  inherit  firstinwattribute  itv′  itv see line   algorithm   external file  hold  picture illustration etc object name  jpg open   separate window figure  algorithm  bottomup traversal  lcpinterval tree enumerate suffixprefix match whenever  terminal edge  read  outgo   interval itv  process line    add   list  suppose   terminal edge  outgo   lcpinterval itv  first symbol   label   terminal edge   suppose    branchedge outgo  itv   lcpinterval itv′   first symbol say    implicit label   edge must occur    thus  cannot   sentinel    consider different   lexicographic order   suffix hence  first symbol   either        symbols   respect   lexicographic order smaller   sentinels  branchedge  itv  itv appear   terminal edge  itv hence  terminal edge outgo  itv′   process  line      need  single list    entire algorithm  soon   edge outgo  itv   process   collect  terminal edge     wholeread leave    itvlcp exceed  minimum length algorithm  compute  cartesian product     appropriate subset    process  correspond suffixprefix match  length itvlcp  line    point suffixprefix match may  output  postprocessed  check  additional constraints   transitivity   cartesian product   compute  elements     longer need    empty line  note   algorithm empty    lcpinterval  lcpvalue smaller  ellmin  process   event     terminal edge  vintervals    longest common prefix     read    smaller  ellmin therefore     suffixprefix match   form leave angle bracket_  ellright angle bracket   ell ≥ ellmin     read represent     list  safely  empty  lcpinterval tree     contain  leafedges   lcpintervals   least two children        branchedges   lcpintervals     three function specify  algorithm   call   every correspond item  number  function call       recall  algorithm   apply   bucket   total size   bucket   hence        call   three function process_leafedge  process_branchedge run  constant time  run time  process_lcpinterval  determine   number  spms process assume   process take constant time  overall run time  algorithm    bucket         number  process spms handle reverse complement  read read may originate   strand   dna molecule   reason suffixprefix match shall also  compute  read  reverse complement   read handle  reverse complement   read  conceptually easy  integrate   approach one    process   instead     three step  involve scan  read  extend  process  strand   read    require double  size   read set representation   information   reverse complement read  efficiently  extract   forward read additional file  section  show   compute  integer cod   reverse read   integer cod   forward read  constant time  scan   reverse complement read   negligible impact   runtime  course  size   table     roughly double  additionally consider reverse complement  compute suffixprefix match  minor modifications  necessary apply algorithm     find  spms include  redundant ones   want  omit   formalize  follow  spm ⟨      ⟩ ∈         nonredundant     one   follow condition  true •  ∈    ∈   •  ∈    ∈     ≺   •  ∈     ∈    ≺      spm  condition  easily  check  constant time see algorithm   additional file  section  recognition  transitive  irreducible suffixprefix match   construction   string graph    need transitive spms  spm ⟨      ' ' ⟩  transitive       two spms leave angle bracketr  ellright angle bracket  leave angle bracket  ell′right angle bracket   ell  ell′   ell″ figure ​figure33 show  concrete example   transitive spm  spm    transitive  irreducible  external file  hold  picture illustration etc object name  jpg open   separate window figure  example   transitive suffixprefix match  example   transitive spm  set  three read   transitive spm leave angle bracketr  right angle bracket derive  leave angle bracket  right angle bracket  follow theorem characterize  spm   read   single irreducible spm satisfy  length constraint   match constraint see figure ​figure44   illustration  external file  hold  picture illustration etc object name  jpg open   separate window figure  illustration  transitivity   suffixprefix match schematic illustration  transitivity ⟨      ' ' ⟩   transitive spm derive   spm leave angle bracket  ellright angle bracket hence  prefix      suffix    …      ' '   theorem  let leave angle bracketr  ell″right angle bracket   spm  leave angle bracketr  ell″right angle bracket  transitive         ∈    irreducible spm leave angle bracket  ell′right angle bracket   ell′  ell″   ell″ ≥   ell′  …  ell′    ell″    ell′  …  ell″  proof  theorem    find  additional file  section    spm leave angle bracketr  ell″right angle bracket  transitive  leave angle bracket  ell′right angle bracket   spm satisfy  condition  theorem    say  leave angle bracketr  ell″right angle bracket  derive  leave angle bracket  ell′right angle bracket theorem  suggest  way  decide  transitivity   spm leave angle bracketr  ellright angle bracket check     irreducible spm leave angle bracket  ell′right angle bracket     derive  check involve comparison      ell′ symbols  verify  …  ell′   suffix     …      ' '     may  several irreducible spms   leave angle bracketr  ell″right angle bracket may  derive   necessary  store  correspond leave contexts   sequence    ell′  ≤ ell′    leave context lcs ell′     nonempty string …  ell′ due   bottomup nature   traversal  algorithm   spms involve  different prefix   give read  enumerate  order  match length   longest   shortest one thus algorithm  first deliver  irreducible spm leave angle bracket  ell′right angle bracket   ⟨      ' ' ⟩  possibly derive   '   ' '   theorem  one  conclude   first spm say leave angle bracket  ell′right angle bracket find   wholeread path    always irreducible hence one store lcs ell′  spm leave angle bracketr  ell″right angle bracket detect later  traverse   wholeread path    classify  transitive     lcs ell′   suffix  lcr ell″ see figure ​figure55   illustration  leave angle bracketr  ell″right angle bracket  transitive   discard otherwise lcr ell″ must  store  well  check  transitivity   spms find later    wholeread path   spm  either classify  transitive  irreducible   case  leave context  store  implement  method  use  dictionary   leave contexts   operation lcsearchd   return true      set membership       suffix   otherwise  add     return false   dictionary   example  implement   trie  store  leave contexts  reverse order   implementation  use  blindtrie   additional file  section   present  modification  algorithm   output nonredundant irreducible spms   external file  hold  picture illustration etc object name  jpg open   separate window figure  transitivity  leave contexts transitivity  leave contexts let  spm leave angle brackett  zright angle bracket  derive  leave angle bracket  zzright angle bracket hence  leave context lcs      suffix   leave context lct   wxy let leave angle bracketu  zright angle bracket   irreducible spm  lcu      non empty string   lcs     suffix   recognition  internally contain read   begin   methods section   show   detect read   prefix  suffix   read  construct  string graph  also   discard internally contain read   contain   read without   suffix   prefix  precisely  ∈   internally contain   read  ' ∈  exist   ′  urw   nonempty string     additional file  section   show   efficiently detect internally contain read construction   assembly string graph consider  read set    suffix  prefixfree  assembly string graph    graph   relationships   read construct            set   nonredundant irreducible spms         restrict  read    internally contain    ∈   graph contain two vertices denote     represent  two extremities   read  stand  begin  stand  end   nonredundant irreducible spm ⟨      ⟩ ∈         satisfy ell ≥ ellmin  graph contain two direct edge define  follow   ⟨      ⟩ ∈           two edge •  →   edge label sell  … •  →   edge label ¯      …       ⟨   ¯    ⟩ ∈      ¯¯¯¯     two edge •  →   edge label ¯      …     •  →   edge label ¯      …       ⟨       ⟩ ∈      ¯¯¯¯     two edge •  →   edge label sell  … •  →   edge label rell  …   implementation   string graph vertices  represent  integers        construct  graph   list  nonredundant irreducible spms  first calculate  outdegree   vertex   count  calculate partial sum   second scan   list  spms  insert  edge   array  size     ∣ ∣      ¯¯¯¯   ∣ ∣   strategy allow  allocate exactly  necessary space   edge   access  first edge outgo   vertex  constant time  array  edge  store compactly use    ⌈         ⌉  ⌈       max   min  ⌉  bits  λmax   maximum length   read ⌈         ⌉ bits  use   destination   edge  source   edge  clear   array index   edge  store ⌈       max   min  ⌉ bits  use   length   edge label  output  contigs  first write reference   read number  edge lengths   temporary file    complete  memory   string graph  deallocated   read sequence  map  memory finally  sequence   contigs  derive   reference   contigs  output  verify  correctness   string graph implementation   allow comparison   tool   implement  graph clean algorithms describe     experimental feature  sophisticate techniques    network flow approach describe    leave  future work   main focus   paper lie   efficient computation   irreducible spms   construction   string graph",5
142,Bracken,"Bracken: estimating species abundance in metagenomics data
Our new method, Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN), estimates species abundances in metagenomics samples by probabilistically re-distributing reads in the taxonomic tree. Reads assigned to nodes above the species level are distributed down to the species nodes, while reads assigned at the strain level are re-distributed upward to their parent species. For example, in Fig. 1 we would distribute reads assigned to the Mycobacteriaceae family and the Mycobacterium genus down to M. marinum and M. avium, and reads assigned to each M. marinum strain would be reassigned to the M. marinum species. As we show below, Bracken can easily reestimate abundances at other taxonomic levels (e.g., genus or phylum) using the same algorithm. In order to re-assign reads classified at higher-level nodes in the taxonomy, we need to compute a probabilistic estimate of the number of reads that should be distributed to the species below that node. To illustrate using the nodes in Fig. 1, we need to allocate all reads assigned to Mycobacterium (G1) to M. marinum (S1) and M. avium (S2) below it, and reads assigned to the Mycobacteriaceae would have to be allocated to M. marinum (S1), M. avium (S2), andHoyosella altamirensis (S3). Schematic showing a partial taxonomic tree for the Mycobacteriaceae family. Figure 1: Schematic showing a partial taxonomic tree for the Mycobacteriaceae family. Download full-size imageDOI: 10.7717/peerjcs.104/fig-1 Reallocating reads from a genus-level node in the taxonomy to each genome below it can be accomplished using Bayes’ theorem, if the appropriate probabilities can be computed. Let P(Si) be the probability that a read in the sample belongs to genome Si, P(Gj) be the probability that a read is classified by Kraken at the genus level Gj, and P(Gj|Si) be the probability that a read from genome Si is classified by Kraken as the parent genus Gj. Then the probability that a read classified at genus Gj belongs to the genome Si can be expressed as Eq. (1): (1)P(Si∣∣Gj)=P(Gj∣∣Si)P(Si)P(Gj). Note that because we began by assuming that a read was classified at node Gj, P(Gj) = 1. Next we consider how to compute P(Gj|Si), the probability that a read from genome Si will be classified by Kraken at the parent genus Gj. We estimate this probability for reads of length r by classifying the sequences (genomes) that we used to build the database using that same database, as follows. For each k-mer in the sequences, Kraken assigns it a taxonomy ID by a fast lookup in its database. To assign a taxonomy ID for a read of length r, Kraken examines all k-mer classifications in that read. For example, for k = 31 and r = 75, the read will contain 45 k-mers. Our procedure examines, for each genome in the database, a sliding window of length r across the entire genome. To find the taxonomy ID Kraken would assign to each window, we simply find the deepest taxonomy node in the set of k-mers in that window. Since each k-mer in a database sequence is assigned to a taxonomy ID somewhere along the path from the genome’s taxonomy ID to the root, the highest-weighted root-to-leaf path (and thus the Kraken classification) corresponds to the deepest node. For each genome Si of length Li we thus generate (Li − r + 1) mappings to taxonomical IDs. For node Gj, we then count the number of reads from Si that are assigned to it, NGj(Si). P(Gj|Si) is then the proportion of reads from Si that were assigned to the genus node Gj; i.e., P(Gj|Si) = NGj(Si)∕(Li − r + 1). We also calculate the proportion of reads from Si that were assigned to every node from genome Si to the root node of the taxonomy tree. The final term that we must calculate from Eq. (1) is P(Si), the probability that a read in the sample belongs to genome Si, which is computed in relation to other genomes from the same genus. For example, if the sample contains three genomes in the same genus, and if 30% of all reads from those three genomes belong to Si, then P(Si) = 0.3. We estimate this probability using the reads that are uniquely assigned by Kraken to genome Si, as follows. If we let USi be the proportion of genome Si that is unique, then (2)USi=NSiLi−r+1 where NSi is the number of k-mers of length r that are uniquely assigned to genome Si by Kraken, and Li is the genome length. For example, if Li = 1 Mbp and only 250,000 k-mers are unique to genome Si, then USi = 0.25. Then, using the number of reads KSi from a sample that Kraken actually assigns to Si, we can estimate the number of reads that likely derive from Si as: (3)KˆSi=KSiUSi. For example, if Kraken classifies 1,000 reads as genome Si and 25% of the reads from Si are unique, then we would estimate that 4,000 reads (1,000/0.25) from Si are contained in the sample. If genus Gj contains n genomes, we estimate the number of reads KˆS for each of the n genomes and then calculate P(Si) by: (4)P(Si)=KˆSi∑a=1nKˆSa. Using this result in Eq. (1) above allows us to compute P(Si|Gj) for each genome Si. Each probability P(Si|Gj) is then used to estimate the proportion of the reads assigned to genus Gj that belong to each of the genomes below it. These calculations are repeated for each taxonomic level above the genus level (family, class, etc.), with read distribution at each level going to all genomes classified within that taxonomic subtree. To compute species abundance, any genome-level (strain-level) reads are simply added together at the species level. In cases where only one genome from a given species is detected by Kraken in the dataset, we simply add the reads distributed downward from the genus level (and above) to the reads already assigned by Kraken to the species level. In cases where multiple genomes exist for a given species, the reads distributed to each genome are combined and added to the Kraken-assigned species level reads. The added reads give the final species-level abundance estimates. This method can also estimate abundance for other taxonomic levels. In such cases, only higher nodes within the taxonomy tree undergo read distribution. After distributing reads downward, we estimate abundance for a node at the level specified by combining the distributed reads across all genomes within that node’s subtree.",AbundanceEstimation,"bracken estimate species abundance  metagenomics data
 new method bracken bayesian reestimation  abundance  classification  kraken estimate species abundances  metagenomics sample  probabilistically redistribute read   taxonomic tree read assign  nod   species level  distribute    species nod  read assign   strain level  redistribute upward   parent species  example  fig   would distribute read assign   mycobacteriaceae family   mycobacterium genus    marinum   avium  read assign    marinum strain would  reassign    marinum species   show  bracken  easily reestimate abundances   taxonomic level  genus  phylum use   algorithm  order  reassign read classify  higherlevel nod   taxonomy  need  compute  probabilistic estimate   number  read    distribute   species   node  illustrate use  nod  fig   need  allocate  read assign  mycobacterium    marinum    avium     read assign   mycobacteriaceae would    allocate   marinum   avium  andhoyosella altamirensis  schematic show  partial taxonomic tree   mycobacteriaceae family figure  schematic show  partial taxonomic tree   mycobacteriaceae family download fullsize imagedoi peerjcsfig reallocate read   genuslevel node   taxonomy   genome     accomplish use bay theorem   appropriate probabilities   compute let psi   probability   read   sample belong  genome  pgj   probability   read  classify  kraken   genus level   pgjsi   probability   read  genome   classify  kraken   parent genus    probability   read classify  genus  belong   genome    express    psi∣∣gjpgj∣∣sipsipgj note    begin  assume   read  classify  node  pgj   next  consider   compute pgjsi  probability   read  genome    classify  kraken   parent genus   estimate  probability  read  length   classify  sequence genomes   use  build  database use   database  follow   kmer   sequence kraken assign   taxonomy    fast lookup   database  assign  taxonomy    read  length  kraken examine  kmer classifications   read  example          read  contain  kmers  procedure examine   genome   database  slide window  length  across  entire genome  find  taxonomy  kraken would assign   window  simply find  deepest taxonomy node   set  kmers   window since  kmer   database sequence  assign   taxonomy  somewhere along  path   genomes taxonomy    root  highestweighted roottoleaf path  thus  kraken classification correspond   deepest node   genome   length   thus generate      mappings  taxonomical ids  node    count  number  read     assign   ngjsi pgjsi    proportion  read     assign   genus node   pgjsi  ngjsi∕      also calculate  proportion  read     assign  every node  genome    root node   taxonomy tree  final term   must calculate     psi  probability   read   sample belong  genome    compute  relation   genomes    genus  example   sample contain three genomes    genus      read   three genomes belong    psi    estimate  probability use  read   uniquely assign  kraken  genome   follow   let usi   proportion  genome    unique  usinsilir  nsi   number  kmers  length    uniquely assign  genome   kraken     genome length  example     mbp    kmers  unique  genome   usi    use  number  read ksi   sample  kraken actually assign     estimate  number  read  likely derive    kˆsiksiusi  example  kraken classify  read  genome      read    unique   would estimate   read     contain   sample  genus  contain  genomes  estimate  number  read kˆs      genomes   calculate psi  psikˆsi∑a1nkˆsa use  result     allow   compute psigj   genome   probability psigj   use  estimate  proportion   read assign  genus   belong     genomes    calculations  repeat   taxonomic level   genus level family class etc  read distribution   level go   genomes classify within  taxonomic subtree  compute species abundance  genomelevel strainlevel read  simply add together   species level  case   one genome   give species  detect  kraken   dataset  simply add  read distribute downward   genus level     read already assign  kraken   species level  case  multiple genomes exist   give species  read distribute   genome  combine  add   krakenassigned species level read  add read give  final specieslevel abundance estimate  method  also estimate abundance   taxonomic level   case  higher nod within  taxonomy tree undergo read distribution  distribute read downward  estimate abundance   node   level specify  combine  distribute read across  genomes within  nod subtree",6
143,mOTUs2,"Microbial abundance, activity and population genomic profiling with mOTUs2
The mOTUs2 profiler The mOTU profiler version 2 (mOTUs2) is a stand-alone, open source, computational tool that estimates the relative abundance of known as well as genomically uncharacterized microbial community members at the species level using metagenomic shotgun sequencing data. The taxonomic profiling method is based on ten universally occurring, protein coding, single-copy phylogenetic marker genes (MGs), which were extracted from more than 25,000 reference genomes13 and more than 3100 metagenomic samples (Supplementary Data 1; in total ca. 367,000 non-redundant MG sequences). The MGs were grouped into >7700 MG-based operational taxonomic units (mOTUs) that represent microbial species, many of which (ca. 30%) still lack sequenced reference genomes. In addition to (i) taxonomic profiling, the tool allows for (ii) basal transcriptional activity profiling of community members using metatranscriptomic data as well as (iii) determining proxies for strain population genomic distances based on single-nucleotide variations (SNVs) within the phylogenetic marker genes that comprise mOTUs. Generation and annotation of the mOTUs2 database The mOTUs2 profiler relies on a custom-built database of MG sequences extracted from reference genomes (ref-MGs) and from metagenomic samples (meta-MGs). The reference genomes were grouped into species-level clusters (specI clusters) and MG sequences from these reference genomes were grouped based on their specI affiliation into reference marker gene clusters (ref-MGCs). These ref-MGCs were augmented by meta-MGs and the remaining meta-MGs were clustered into meta-MGCs. MGCs of different MGs were subsequently grouped based on their specI affiliation or binned based on co-abundance analysis into reference genome-based mOTUs (ref-mOTUs) and metagenomic mOTUs (meta-mOTUs), respectively. The resulting mOTUs were quality-controlled, compiled into a sequence database for short-read mapping and taxonomically annotated. Regular updates of the of the mOTU database will be made available at: http://motu-tool.org. Collection of MGs from reference genomes and metagenomes The 25,038 reference genomes used for the mOTU database were downloaded from the proGenomes database13. Metagenomic data were downloaded from the Genbank Sequence Read Archive (https://www.ncbi.nlm.nih.gov/sra) and the European Nucleotide Archive (https://www.ebi.ac.uk/ena) (accession numbers are listed in Supplementary Data 1). Most samples were obtained from human microbiome studies, including 1210 samples from different major human body sites (oral, skin, gut and vaginal14, 15 and 1693 further samples from various human gut microbiome studies16,17,18,19,20,21. In addition, we used 243 metagenomic samples from an ocean microbiome study22. All samples were processed for marker gene identification9. Briefly, quality-controlled raw sequencing reads were subjected to metagenomic assembly and genes predicted on contiguous sequences longer than 500 base pairs (bp). MGs were subsequently extracted using the fetchMGs tool (available at http://motu-tool.org/fetchMG.html). In short, fetchMGs identifies MGs using HMM models built with HMMER3 (http://hmmer.org) applying a set of optimized cutoffs4, 9, and extracts corresponding nucleotide sequences with the Seqtk tool. With this workflow we extracted a set of 40 MGs (COG0012, COG0016, COG0018, COG0048, COG0049, COG0052, COG0080, COG0081, COG0085, COG0087, COG0088, COG0090, COG0091, COG0092, COG0093, COG0094, COG0096, COG0097, COG0098, COG0099, COG0100, COG0102, COG0103, COG0124, COG0172, COG0184, COG0185, COG0186, COG0197, COG0200, COG0201, COG0202, COG0215, COG0256, COG0495, COG0522, COG0525, COG0533, COG0541, COG0552)32, 33 from all 25,038 reference genomes. Not all of these genes are currently suitable for metagenomic applications due to high rates of ambiguous mapping of short reads owing to highly conserved regions within MG sequences as well as lower assembly rates observed for some MGs4, 9. Hence, a selected subset of ten MGs (COG0012, COG0016, COG0018, COG0172, COG0215, COG0495, COG0525, COG0533, COG0541, COG0552) was extracted from genes that were predicted in metagenomes as described above. Grouping of MGs into ref-MGCs and meta-MGCs Reference genomes were processed and clustered into specI clusters to build ref-MGCs4. To this end, we calculated pairwise global nucleotide identities for all genome for each of the 40 MGs using vsearch (version v1.9.3)40. Genome-to-genome distances were calculated as the gene length-weighted arithmetic mean of the individual MG sequence distances. The resulting distance matrix was used as input for average linkage clustering using an optimized cutoff of 96.5% nucleotide identity4, resulting in 5306 specI clusters. To assess the quality of grouping genomes into specI clusters, we tested whether the taxonomic annotations of the individual genomes provided by the NCBI were congruent (Supplementary Figure 18). More specifically, all specI clusters were annotated taxonomically in accordance to their member genomes. SpecI clusters were either homogeneous (all members had the same species-level annotation), heterogeneous (different species annotations found in the same cluster) or undetermined (clusters only containing genomes with non-binomial species names such as: Synechocystis sp. PCC 6803). We further evaluated how many NCBI species names occurred multiple times (in different clusters). Subsequently, the ten MGs suited for metagenomics were extracted from the specI clusters resulting in over 51,000 ref-MGCs. To enable the profiling of species that are not yet represented by reference genomes, we extracted MG sequences from metagenomic assemblies using the fetchMGs tool. For clustering, we first calculated all pairwise distances between MGs from ref-MGs and meta-MGs using vsearch (version v1.9.3)40 and retained alignments of at least 20 aligned bases. Then, we used open-reference clustering (employing the average linkage hierarchical clustering algorithm) to augment the pre-existing ref-MGCs with meta-MGs. The remaining meta-MGs sequences were clustered into meta-MGCs containing only meta-MGs. Binning of MGCs into mOTUs As the clustering of meta-MGs into meta-MGCs was performed independently for each of the ten MGs, it resulted in unbinned meta-MGCs (as opposed to the ref-MGCs, which were grouped into mOTUs based on their specI cluster affiliation). In order to bin MGCs into mOTUs (i.e., to link MGCs originating from the same species), we utilized the property that genes (and therefore, MGCs) from the same species are expected to co-vary in abundance across metagenomic samples41. Accordingly, we calculated the correlation between pairwise MGC abundances across all samples for each biome. We optimized the correlation measure and prevalence filtering (as a means against the spurious correlation between low-prevalence MGCs, see9) for each biome separately based on the AU-ROC determined by cross-validating the grouping of ref-MGCs for which membership in the same specI clusters served as a ground truth. As a result, we defined the following biome-specific parameters: human gut - prevalence filter: five samples, Pearson correlation of log-transformed relative abundance; ocean - prevalence filter: five samples, Pearson correlation of relative abundance; human oral cavity - prevalence filter: 50 samples, Pearson correlation of relative abundance; human vagina - prevalence filter: five samples, Pearson correlation of log transform relative abundance; human skin - prevalence filter: ten samples, Spearman correlation of log-transformed relative abundance. In order to combine the biome-specific correlations we transformed each of these into an FDR-calibrated association measure in such a way that for a given FDR value, the same association value was assigned. To obtain a single measure of association for each pair of MGCs, we computed the maximum of the FDR-calibrated association values across biomes. For the actual binning, we used a slightly modified version of the greedy algorithm described in ref. 9. As an initialization step, the ref-MGCs were grouped according to their specI cluster affiliations. Then, meta-MGCs were progressively binned starting from the highest FDR-calibrated association values and decreasing until a cutoff value of 0.8 was reached. In this procedure, an MGC was added (binned) to an existing group (or another MGC to form a bin of size two) if this MG (among the ten possible ones) was not already present. Only groups with at least 6 MGCs were retained and defined as mOTUs, which resulted in 2494 meta-mOTUs (consisting only of meta-MGCs) and 5232 ref-mOTUs (containing at least one ref-MGC and possibly additional meta-MGCs). MGCs that remained unbinned were grouped into a single unbinned group. Note that although specI clusters and ref-mOTUs are conceptually similar, there are two major differences: first, ref-mOTUs are composed of MGCs of at least six out of the ten different MGs used for metagenomics, while specI clusters represent genomes that are grouped based on distances calculated from up to 40 MGs; second, ref-mOTUs can, as described above, contain MGs and MGCs that were assembled from metagenomic samples. To assess the expected taxonomic consistency of the binning strategy of meta-MGCs, a fraction of the ref-MGCs were treated in the same ways as meta-MGCs and their taxonomic affiliation (known from ref-mOTU membership) was only used afterwards to ascertain the error rate of the binning algorithm (Supplementary Figure 2a). Across all metagenomic samples used to construct the mOTUs, 1223 ref-mOTUs were detected and could be used for 100-fold resampled 5-fold cross-validation. We also assessed the agreement of the MGCs for each mOTU in terms of relative abundance and prevalence across metagenomic samples (Supplementary Figures 2b,c). Relative abundance and prevalence showed higher agreement for meta-mOTUs than for ref-mOTUs. This was expected since the binning algorithm is directly influenced by these two parameters. We additionally evaluated the homogeneity of GC content among the MG sequences within each mOTU (Supplementary Figure 2d). meta-MGCs showed very homogeneous GC content, as expected for genes that originate from the same genome, but not for erroneously binned MG sequences. Construction of the mOTUs2 mapping database We compiled a sequence database against which short metagenomic reads can be aligned to quantify the abundance of MGCs and mOTUs. To construct a non-redundant mOTUs mapping database, we removed identical MG sequences. MG sequences in the database were extended at the start and end of the gene by up to 100 nt, based on their genome or metagenomic assembly of origin, to reduce known mapping artifacts at gene boundaries. The resulting non-redundant database consists of the sequence files in FASTA format along with MGC and mOTU annotations, as well as the coordinates of the coding segments of the MG sequences. The sequence files were further indexed for searches with BWA42. For SNV calling, we constructed an additional database that only consists of the centroid (medoid) sequence of every MGC so that SNVs can be identified with respect to one reference sequence per MGC. Taxonomic annotation of meta-mOTUs To assign taxonomic affiliations to meta-mOTUs, we first annotated each MG using Uniprot’s UniRef90 (https://www.uniprot.org/uniref, release 2017_08) as a reference protein sequence database43, which was supplemented with a set of additional marine protein sequences as described in44. Similarities between translated MG sequences and reference database entries were computed using MMSEQS245 with the following parameters: search -a true -e 1E-5 --max-seqs 1000. Taxonomic affiliation was assigned using a weighted Lowest Common Ancestor (LCA) approach as follows: for each MG, all protein matches in the reference database with a value ≥90% of the highest bitscore were kept. Then, outlier taxa were excluded by using a bitscore-weighted LCA annotation that covered at least 75% of the sum of all bitscores of each MG. Next, we transferred the annotation of the best-scoring MG member to each MGC and used the MGC annotations to assign a taxonomy to meta-mOTUs as follows: for each meta-mOTU and for each taxonomy rank, we required at least three MGCs to be annotated to consider the meta-mOTUs as annotated at this rank. Annotated meta-mOTUs were considered consistent if at least half of the MGC taxonomy annotations were in agreement. Phylogenetic analysis of mOTUs To explore the phylogeny of mOTUs (ref-mOTUs and meta-mOTUs), a reference tree was reconstructed by combining the phylogenetic signal of the ten sets of marker genes selected (Supplementary Figure 4). For this, all marker genes were translated into amino acid sequences and analyzed using ETE Toolkit v3.1. 146. In particular, the program ete-build was used to run the following phylogenetic workflow: First, each set of marker proteins was independently aligned using ClustalOmega47. Next, alignment columns with less than three aligned residues were removed. Finally, the ten individual MG alignments were concatenated and used to infer a maximum likelihood phylogenetic tree using IQTree48 and the LG model. The mOTUs2 profiling workflow The mOTUs2 workflow for taxonomic profiling consists of three steps: alignment of metagenomic sequencing reads to MGs, estimation of read abundances for every marker gene cluster (MGC), and calculation of mOTU abundances. As input, mOTUs2 expects the user to provide quality controlled sequencing reads. These are aligned to the MGs of the mOTU database using BWA (mem algorithm, default parameters)42. The resulting alignments are filtered and only those with at least 97% nucleotide identity are retained. Further, alignments are filtered according to their lengths (default: 75 bp minimum alignment length; can be adjusted using the -l option). Next, we compute the best alignment(s) for every insert (read pair) to the MGCs using BWA alignment scores. Inserts with a single highest scoring alignment are flagged as “unique alignments”, whereas inserts with multiple highest scoring alignments are flagged as “multiple alignments”. Subsequently, abundances for each MGC are calculated by summing up the number of all inserts flagged as unique alignments resulting in a unique alignment profile. Inserts flagged as multiple alignments are distributed among their best-scoring MGCs in accord with their respective abundances estimated based on the unique alignment profile. Thus, the final abundances are calculated as the sum of the unique abundance profiles and the distributed contributions of the inserts flagged as multiple alignments. In addition to these MGC insert counts, MGC base coverages are calculated by first summing up the total number of bases aligning to each MGC and then dividing by the respective gene lengths. Finally, the abundances of the mOTUs are calculated as the median of their respective MGC abundances (insert counts and base coverages). In order to reduce false positive results, we require a certain number of MGCs to be detected, that is to have metagenomic reads mapped to them (default: 3 MGs, -g option in mOTUs2). Although mOTUs2 is able to profile many organisms not yet represented by reference genomes, there are still around 25% of the MGCs that could not be binned into mOTUs (see section 2.5). Reads mapping to those MGCs are assigned to a group labelled as “unbinned” (shown as “-1” in mOTU abundance profiles). The abundance of this group is calculated as the median of unbinned MGCs summed by COG. Description of taxonomic profiling outputs The mOTUs2 profiler returns multiple taxonomic profiles, since abundances based on read mappings can be calculated in different ways. One major distinction is the unit of counts. Either fragments such as inserts (or reads for single-pair sequencing) or mapped base-pairs can be counted. Counting the mapped base-pairs has the advantage that the mean base coverage can easily be computed by dividing the number of bases aligned to a certain gene by its corresponding length (mOTUs2 output -y option: “base.coverage”). Count based statistics are powerful for differential abundance testing (output -y option: “insert.raw_counts”). As the counts could in principle be non-integer numbers due to inserts mapping to multiple genes (see section 3.1), all counts are rounded to integers. For relative abundance-based estimates, gene-length normalizations are required to account for varying lengths of MG sequences and varying numbers of MGCs present in each mOTU. To this end, we previously introduced “scaled counts” that retains most of the characteristics of insert counts. In this approach, coverages are calculated as described above and are then normalized to sum up to the number of inserts that align to MGCs (output -y option: “insert.scaled_counts”). Single-nucleotide variant analysis with MGs The mOTUs2 profiler has new functionality to compute metagenomic SNV profiles using the MGs comprising mOTUs as reference sequences. The resulting SNV profiles are highly correlated to those obtained by whole genome SNV profiling (see main text, Fig. 5, Supplementary Figures 16, 17). The overall SNV calling pipeline starts by aligning metagenomic sequences to centroid sequences of MGCs (see above), before the resulting bam files are post-processed using metaSNV functions34. The mOTUs2 command map_snv maps the reads using BWA42 and performs read filtering in a similar fashion as described for taxonomic profiling. For the SNV analyses, only inserts flagged as unique alignments are kept and the resulting sam file is sorted and converted into a bam file. Using the snv_call command, the tool (i) computes base coverages, (ii) calls SNVs, (iii) generates filtered allele frequency tables, and (iv) calculates distances between strain populations. These four steps are directly built upon metaSNV capabilities34, although the procedure was adapted to mOTUs2 to facilitate its use with genes rather than genomes. Firstly, each bam file is processed to compute per sample coverages for every reference sequence/mOTU, both vertical (average number of reads per position) and horizontal (percentage of the sequence covered at least once). SNVs are subsequently called using samtools mpileup49, followed by two post-processing steps. This includes a filtering step, which was modified to include parallelized computing capabilities as well as the removal of padded regions in the allele frequency tables. The filtering parameters remain identical, with updated default values to account for the universal character of the genes considered: (-fb) minimal percentage of the sequence horizontally covered per sample and per mOTU (default = 80), (-fd) minimal average vertical coverage per sample and per mOTU (default = 5), (-fm) minimum number of samples meeting the listed criteria per mOTU (default = 2), (-fc) minimum vertical coverage per SNV position (default = 5), (-fp) minimum proportion of samples meeting the previous criterion at said position (default = 0.9). Finally, the filtered allele frequency tables are used to compute genetic distances between samples for each mOTU, Manhattan distances as well as major allele distances are used as the population genomic distance measure. For the latter, only allele frequency changes above 50% between the two samples are taken into account. The mOTU profiler uses parallelized computing capabilities for this step. The output directory (-o) includes three files: two with the coverage information for each mOTU, both horizontal (*.cov.tab file) and vertical (*.perc.tab file), and a log file. Additionally, there are two directories: (i) per mOTU filtered allele frequencies of identified SNVs across samples (filtered-* directory) and (ii) per mOTU genetic distances between samples (distances-* directory), both Manhattan (mann.dist files) and major allele (allele.dist files). Benchmarking mOTUs2 against other tools To evaluate its accuracy and robustness, we benchmarked mOTUs2 against two established tools for taxonomic profiling of metagenomic samples: MetaPhlAn27, which is based on clade-specific marker genes, and Kraken50, which is based on exact alignments of genomic k-mers. MetaPhlAn2 (version 2.6.0) was executed with default parameters. For Kraken-labelled analyses, we executed Kraken for read classification and calculated relative abundances with Bracken6. Kraken and Bracken were installed as version 1.0.0 using conda. The Minikraken database (version minikraken_20171101_8GB_dustmasked) was downloaded from https://ccb.jhu.edu/software/kraken/. The Minibracken database was downloaded from https://ccb.jhu.edu/software/bracken/ on 1 February 2018. We executed kraken using paired-end and single-end data using default parameters. Abundance estimation with Bracken was performed with the following parameters: -k minikraken_8GB_75mers_distrib.txt -l S -o result.abundance.bracken. Comparison of mOTUs with metagenome-assembled genomes We further validated the mOTUs using metagenome-assembled genomes (MAGs) reconstructed from different environments. For this purpose, we first extracted 4880 metagenomic sequencing runs from human gut samples available from the European Nucleotide Archive (accession numbers are listed in Supplementary Data 2). Raw reads from each run were assembled using metaSPAdes v3.10.051 and subsequently binned with MetaBAT2 (v2.12.1)52 with a minimum contig length threshold of 2000 bp. Sequencing coverage required for binning was inferred by mapping the raw reads back to the assemblies using BWA v0.7.1642 and then retrieving the percentage of mapped read bases with samtools v1.549 and the jgi_summarize_bam_contig_depths function from MetaBAT2. Quality scores (QS) of each metagenome-assembled genome (MAG) were estimated with CheckM v1.0.753, calculated as the level of completeness - 5 x contamination, as previously described23. Good-quality MAGs (QS > 50) were kept for subsequent downstream analyses. MAGs from marine samples (Ocean MAGs) were obtained as a subset of about 8000 MAGs, which are described in a recent publication23. In order to identify ocean-associated MAGs, we first searched for the keywords: ocean, marine, baltic sea and north sea to extract entries in Supplementary Table 1 of23 and found 400 samples matching these keywords. From these samples, we selected 1845 MAGs (from Supplementary Data 2) that were reconstructed from these metagenomes. Correspondence between MAGs and mOTUs was established using the following procedure: first, we extracted the ten MGs from the MAGs using fetchMGs (see above), obtaining a set of MG-MAGs. Second, we aligned the MG-MAGs to the MG database of the mOTUs using vsearch -usearch_global (parameters: --id 0.96 --minqt 0.7). Finally, we evaluated the congruency of the MG-MAGs to mOTU matches. For this, we first checked if at least three MG-MAGs could be assigned to a mOTU (by mapping to a MGC that is part of a mOTU). If this was not the case the MAG was annotated as “unassigned/-1”. Next, we removed all alignments to MGCs not assigned to mOTUs and assigned a MAG to a mOTU if >50% of the MG-MAGs were consistently matched to the same mOTU. Otherwise (if no majority mOTU was found) the MAG is annotated as “inconsistent”. Benchmarking mOTUs2 using simulated metagenomes To be able to assess taxonomic quantification accuracy, ten human gut metagenomic samples were simulated using 15,102 Human gut MAGs: a subset of the 19,302 MAGs described before, excluding the MAGs created from samples used to construct the mOTU database (Supplementary Figure 8). MAGs with an ANI > 96.5% were de-replicated to have one representative MAG per species (cut-off according to ref. 4). The ANI was calculated with the fastANI tool [https://github.com/ParBLiSS/FastANI]. The corresponding fastq files (as well as the simulated abundance data) are available at: http://motu-tool.org/download.html. Metagenomic read data were simulated using BEAR54: first, we generated 100 M inserts (2 × 100 M paired-end reads of 150 nt length) with 350 nt insert distance (standard deviation: 30) using generate_reads.py. Second, trim_reads.pl with default parameters was used to add the quality scores, introduce errors and shorten the reads. Every sample was simulated based on mOTUs2 profiled relative abundances from ten real samples. For each simulated sample, we randomly selected 50 MAGs with a representative reference genome sequence in the superset of the Kraken, MetaPhlAn2, or ref-mOTU databases and 50 additional MAGs sampled from those that lacked any reference database representation (which does not preclude these MAGs to map to meta-mOTUs). The benchmark was performed by evaluating precision-recall plots of the simulated metagenomes based on the number of true positives (TP) false positives (FP) representing species that are predicted but not present in the real sample, and false negatives (FN) representing species that are missed by the profiler. Precision is calculated as TP/(TP + FP) and recall as TP/(TP + FN). Next we evaluated the mean absolute error (MAE) defined as the average absolute difference between estimated relative abundances and relative abundances simulated as ground truth. Finally we evaluated the accuracy of alpha diversity estimates using the difference between predicted and actual Shannon index (abbreviated as H’). Benchmarking mOTUs2 using the CAMI framework We further evaluated mOTUs2 in the CAMI framework25, which includes eight simulated samples (one low complexity, two medium complexity and five high complexity) for which the ground truth is available. Within the first CAMI community challenge, ten metagenomic profiling tools including MetaPhlAn2 and mOTUs1 were already benchmarked on these data sets. To comparatively assess the performance of mOTUs2 in this context, we converted its output to CAMI/Bioboxes format (-C option in the mOTUs2 profiler) and used OPAL 0.2.926 (developed by the same authors as CAMI) for consistency of performance assessments. Using precision-recall plots we evaluated mOTUs2 employing five different parameter sets: high precision (-l 140 -g 6 -C precision), default (-l 100 -g 3 -C precision), recall (-l 75 -g 3 -C recall), high recall (-l 50 -g 2 -C recall) and maximum recall (-l 30 -g 1 -C recall). Hence mOTUs2 are represented by five red dots in the precision-recall plots, demonstrating that it can be tuned to obtain a range of precision-recall trade-offs. The evaluation of the mean absolute error (MAE), which it is called L1 norm in the CAMI paper, was also obtained with OPAL. By default, OPAL re-normalises the relative abundances of the gold standard and the profiling result to each sum to 1 before calculating the MAE, which apparently substantially deteriorates the quantification accuracy of mOTUs2 (see Supplementary Figure 11b). For this reason, we included both re-normalised and relative abundances without any post-processing in our evaluation for mOTUs2. This aims for maximum transparency in the comparison to the other tools, which could only be evaluated with the re-normalised version (but could theoretically also benefit from an evaluation of non-normalised relative abundances). Determining environmental specificity of mOTUs To determine the environmental specificity of the mOTUs, we used the set of >3100 metagenomes (Supplementary Data 1) to assess the environmental specificity of all meta-mOTUs and the subset of ref-mOTUs that are present in these samples (Supplementary Figure 5a). To this end, we generated mOTUs2 profiles of these samples with default settings and removed samples with less than 500 scaled insert counts. Based on the resulting profiles (https://motu-tool.org/data/All_2481_at_least_500.motu.nr.out.20180307.tsv), we classified a mOTU to be present in a specific environment if it was detected in more than three samples from that environment. Analysis of community structure We assessed correlations of the Shannon index calculated based on 16S rRNA gene-based analyses and three metagenomic profiling tools (mOTUs2, MetaPhlAn2 and Kraken). For this we used data from two different biomes: metagenomes generated from stool samples of a colorectal cancer (CRC) study21 and metagenomes from seawater samples of the Tara Oceans expedition22. For the CRC study, amplicon sequencing data of the V4 region of the 16S rRNA were downloaded from the European Nucleotide Archive (ENA) database (http://www.ebi.ac.uk/ena): accession number ERP005534. For the ocean water samples, 16S rRNA gene containing fragments were extracted from metagenomic sequencing reads (miTAGs55). To ensure comparability between the data sets, we extracted the first 100 bp from each miTAG sequence starting from the V4 primer sequence. Ribosomal RNA data were initially processed using USEARCH56 (version 9.2.64) as follows: paired-end reads were merged and quality-filtered using the fastq_mergepairs command with default settings. Merged reads were filtered using the fastq_filter command (-fastq_maxee 0.1). Sequences were de-replicated using the fastx_uniques command, singletons were excluded and the remaining unique sequences were clustered into operational taxonomic units (OTUs) at 97% with chimera removal using the cluster_otus command. Finally, OTU abundances for each sample were determined using the usearch_global command (-strand both; -id 0.97). The OTU abundance tables were downsampled to the minimum number of reads per sample (CRC: 40,805 reads, TARA: 1494 reads) to normalize for uneven sequencing depths using the R function rarefy within the vegan package57. The Shannon index of diversity was computed for each sample and all methods (16S rRNA gene-based and metagenomic method-based) using the R function diversity of the vegan package. In order to obtain a 95% confidence interval we used bootstrapping (n = 100,000) by resampling pairs of Shannon index values. The confidence intervals reflect the 2.5 and 97.5 percentile of the bootstrapped samples. Between sample distances were determined using human body site samples for which more than one time point was available for the same individual. More specifically, for each body site, we compared community compositional distances between samples from the same individual (intra-individual) to distances between this and other individuals (inter-individual). Canberra and Bray-Curtis distances were computed with the vegdist R function of the vegan package and the log-Euclidean distance was computed as the Euclidean distance of the log-transformed relative abundances after the addition of a pseudocount smaller than the smallest non-zero value. For each of the three distances and each sample, we identified the most similar sample (i.e. the one with the minimum distance value) and determined the proportion of cases in which both samples belonged to the same individual. Analysis of metatranscriptomes To demonstrate the use of mOTUs2 to assess basal transcriptional activity of microbial community members, we used a dataset from 36 samples for which metagenomic and metatranscriptomic sequencing data are available31. Each sample (36 metagenomes and 36 metatranscriptomes) was subjected to profiling using mOTUs2, Kraken/Bracken and MetaPhlAn2. All resulting profiles were transformed to relative abundances, and log-transformed after adding a small pseudocount. After that, Spearman correlations between corresponding metagenomic and metatranscriptomic profiles generated from the same sample were calculated and compared between profiling methods (Fig. 4a and Supplementary Figure 11). We moreover evaluated how well species abundance estimates correlated between metagenomic and metatranscriptomic profiles for the twelve most abundant taxa at the class level. Class level information for mOTUs and MetaPhlAn2 was available as part of the profiler output. Class level annotations for Kraken were obtained using NCBI taxonomy identifiers. Comparison of SNV profiles from MGs and whole genomes To assess the comparability of SNV profiles generated with mOTUs2 and whole genomes, we used samples from 2807 human microbiome samples14, 15 and 139 prokaryote-enriched metagenomes from the Tara Oceans project22. Metagenomic reads were mapped to the mOTUs centroid database using the mOTUs2 command map_snv and in addition to a set of 5306 reference genomes13. Genomic distances of strain populations between samples were estimated based on SNV profiles computed both on mOTUs and the whole genomes using the motus snv_call command. The filtering parameters used within the snv_call command were adapted to the specificity of datasets and references. The allele frequency tables were filtered using a horizontal coverage (-fb) equal to 40% for whole genome-mapped reads and 80% for mOTU-mapped reads, a vertical coverage (-fd) of 10, a per position coverage (-fc) of 5 and a position prevalence (-fp) of 0.90. The minimum number of samples per reference (-fm) was 20 for the human samples and 5 for the Ocean samples. Whole-genome-based distances were compared to those from mOTUs using Pearson’s correlation (Fig. 5a). We selected the ref-mOTUs/genomes that passed the filtering thresholds for both methods and correlated between sample distances between the two methods (n.b. there were no species from the vaginal supersite passing the filtering requirements for both methods). Individuality of microbial populations across body sites We tested for the individuality of microbial strain populations on the subset of the human microbiome samples described above (5.4.1), for which at least two time point data were available. For each body site, we compared SNV profile distances between samples from the same individual (intra-individual, intra-body-site distances) to distances between this and other individuals (inter-individual, intra-body-site distances). To determine whether intra-individual distances were smaller than inter-individual distances (see Supplementary Figure 17b)—indicating individuality of strain populations—we used ROC analysis. ROC curves (see Supplementary Figure 17a) ascertain how accurately small distances predict whether a pair of samples originated from the same individual (with similarly small inter-individual distances being considered false positives) when systematically varying the distance cutoff. ROC curves can be summarized by the area under the curve (AU-ROC) with higher values corresponding to clearer separation between intra- and inter-individual distances (Fig. 5b and Supplementary Figure 17a). Confidence intervals on the AU-ROC (Fig. 5b) were obtained by bootstrapping using the pROC package",AbundanceEstimation,"microbial abundance activity  population genomic profile  motus2
 motus2 profiler  motu profiler version  motus2   standalone open source computational tool  estimate  relative abundance  know  well  genomically uncharacterized microbial community members   species level use metagenomic shotgun sequence data  taxonomic profile method  base  ten universally occur protein cod singlecopy phylogenetic marker genes mgs   extract     reference genomes13     metagenomic sample supplementary data   total   nonredundant  sequence  mgs  group   mgbased operational taxonomic units motus  represent microbial species many     still lack sequence reference genomes  addition   taxonomic profile  tool allow   basal transcriptional activity profile  community members use metatranscriptomic data  well  iii determine proxies  strain population genomic distance base  singlenucleotide variations snvs within  phylogenetic marker genes  comprise motus generation  annotation   motus2 database  motus2 profiler rely   custombuilt database   sequence extract  reference genomes refmgs   metagenomic sample metamgs  reference genomes  group  specieslevel cluster speci cluster   sequence   reference genomes  group base   speci affiliation  reference marker gene cluster refmgcs  refmgcs  augment  metamgs   remain metamgs  cluster  metamgcs mgcs  different mgs  subsequently group base   speci affiliation  bin base  coabundance analysis  reference genomebased motus refmotus  metagenomic motus metamotus respectively  result motus  qualitycontrolled compile   sequence database  shortread map  taxonomically annotate regular update     motu database   make available   collection  mgs  reference genomes  metagenomes   reference genomes use   motu database  download   progenomes database13 metagenomic data  download   genbank sequence read archive    european nucleotide archive  accession number  list  supplementary data   sample  obtain  human microbiome study include  sample  different major human body sit oral skin gut  vaginal14     sample  various human gut microbiome studies16  addition  use  metagenomic sample   ocean microbiome study22  sample  process  marker gene identification9 briefly qualitycontrolled raw sequence read  subject  metagenomic assembly  genes predict  contiguous sequence longer   base pair  mgs  subsequently extract use  fetchmgs tool available    short fetchmgs identify mgs use hmm model build  hmmer3  apply  set  optimize cutoffs4   extract correspond nucleotide sequence   seqtk tool   workflow  extract  set   mgs cog0012 cog0016 cog0018 cog0048 cog0049 cog0052 cog0080 cog0081 cog0085 cog0087 cog0088 cog0090 cog0091 cog0092 cog0093 cog0094 cog0096 cog0097 cog0098 cog0099 cog0100 cog0102 cog0103 cog0124 cog0172 cog0184 cog0185 cog0186 cog0197 cog0200 cog0201 cog0202 cog0215 cog0256 cog0495 cog0522 cog0525 cog0533 cog0541 cog0552     reference genomes     genes  currently suitable  metagenomic applications due  high rat  ambiguous map  short read owe  highly conserve regions within  sequence  well  lower assembly rat observe   mgs4  hence  select subset  ten mgs cog0012 cog0016 cog0018 cog0172 cog0215 cog0495 cog0525 cog0533 cog0541 cog0552  extract  genes   predict  metagenomes  describe  group  mgs  refmgcs  metamgcs reference genomes  process  cluster  speci cluster  build refmgcs4   end  calculate pairwise global nucleotide identities   genome      mgs use vsearch version  genometogenome distance  calculate   gene lengthweighted arithmetic mean   individual  sequence distance  result distance matrix  use  input  average linkage cluster use  optimize cutoff   nucleotide identity4 result   speci cluster  assess  quality  group genomes  speci cluster  test whether  taxonomic annotations   individual genomes provide   ncbi  congruent supplementary figure   specifically  speci cluster  annotate taxonomically  accordance   member genomes speci cluster  either homogeneous  members    specieslevel annotation heterogeneous different species annotations find    cluster  undetermined cluster  contain genomes  nonbinomial species name   synechocystis  pcc    evaluate  many ncbi species name occur multiple time  different cluster subsequently  ten mgs suit  metagenomics  extract   speci cluster result    refmgcs  enable  profile  species    yet represent  reference genomes  extract  sequence  metagenomic assemblies use  fetchmgs tool  cluster  first calculate  pairwise distance  mgs  refmgs  metamgs use vsearch version   retain alignments   least  align base   use openreference cluster employ  average linkage hierarchical cluster algorithm  augment  preexist refmgcs  metamgs  remain metamgs sequence  cluster  metamgcs contain  metamgs bin  mgcs  motus   cluster  metamgs  metamgcs  perform independently     ten mgs  result  unbinned metamgcs  oppose   refmgcs   group  motus base   speci cluster affiliation  order  bin mgcs  motus   link mgcs originate    species  utilize  property  genes  therefore mgcs    species  expect  covary  abundance across metagenomic samples41 accordingly  calculate  correlation  pairwise mgc abundances across  sample   biome  optimize  correlation measure  prevalence filter   mean   spurious correlation  lowprevalence mgcs see9   biome separately base   auroc determine  crossvalidating  group  refmgcs   membership    speci cluster serve   grind truth   result  define  follow biomespecific parameters human gut  prevalence filter five sample pearson correlation  logtransformed relative abundance ocean  prevalence filter five sample pearson correlation  relative abundance human oral cavity  prevalence filter  sample pearson correlation  relative abundance human vagina  prevalence filter five sample pearson correlation  log transform relative abundance human skin  prevalence filter ten sample spearman correlation  logtransformed relative abundance  order  combine  biomespecific correlations  transform      fdrcalibrated association measure    way    give fdr value   association value  assign  obtain  single measure  association   pair  mgcs  compute  maximum   fdrcalibrated association value across biomes   actual bin  use  slightly modify version   greedy algorithm describe  ref    initialization step  refmgcs  group accord   speci cluster affiliations  metamgcs  progressively bin start   highest fdrcalibrated association value  decrease   cutoff value    reach   procedure  mgc  add bin   exist group  another mgc  form  bin  size two    among  ten possible ones   already present  group   least  mgcs  retain  define  motus  result   metamotus consist   metamgcs   refmotus contain  least one refmgc  possibly additional metamgcs mgcs  remain unbinned  group   single unbinned group note  although speci cluster  refmotus  conceptually similar   two major differences first refmotus  compose  mgcs   least six    ten different mgs use  metagenomics  speci cluster represent genomes   group base  distance calculate     mgs second refmotus   describe  contain mgs  mgcs   assemble  metagenomic sample  assess  expect taxonomic consistency   bin strategy  metamgcs  fraction   refmgcs  treat    ways  metamgcs   taxonomic affiliation know  refmotu membership   use afterwards  ascertain  error rate   bin algorithm supplementary figure  across  metagenomic sample use  construct  motus  refmotus  detect  could  use  fold resampled fold crossvalidation  also assess  agreement   mgcs   motu  term  relative abundance  prevalence across metagenomic sample supplementary figure 2bc relative abundance  prevalence show higher agreement  metamotus   refmotus   expect since  bin algorithm  directly influence   two parameters  additionally evaluate  homogeneity   content among   sequence within  motu supplementary figure  metamgcs show  homogeneous  content  expect  genes  originate    genome    erroneously bin  sequence construction   motus2 map database  compile  sequence database   short metagenomic read   align  quantify  abundance  mgcs  motus  construct  nonredundant motus map database  remove identical  sequence  sequence   database  extend   start  end   gene      base   genome  metagenomic assembly  origin  reduce know map artifacts  gene boundaries  result nonredundant database consist   sequence file  fasta format along  mgc  motu annotations  well   coordinate   cod segment    sequence  sequence file   index  search  bwa42  snv call  construct  additional database   consist   centroid medoid sequence  every mgc   snvs   identify  respect  one reference sequence per mgc taxonomic annotation  metamotus  assign taxonomic affiliations  metamotus  first annotate   use uniprots uniref90  release 2017_08   reference protein sequence database43   supplement   set  additional marine protein sequence  describe in44 similarities  translate  sequence  reference database entries  compute use mmseqs245   follow parameters search  true   maxseqs  taxonomic affiliation  assign use  weight lowest common ancestor lca approach  follow     protein match   reference database   value ≥   highest bitscore  keep  outlier taxa  exclude  use  bitscoreweighted lca annotation  cover  least    sum   bitscores    next  transfer  annotation   bestscoring  member   mgc  use  mgc annotations  assign  taxonomy  metamotus  follow   metamotu    taxonomy rank  require  least three mgcs   annotate  consider  metamotus  annotate   rank annotate metamotus  consider consistent   least half   mgc taxonomy annotations   agreement phylogenetic analysis  motus  explore  phylogeny  motus refmotus  metamotus  reference tree  reconstruct  combine  phylogenetic signal   ten set  marker genes select supplementary figure     marker genes  translate  amino acid sequence  analyze use ete toolkit    particular  program etebuild  use  run  follow phylogenetic workflow first  set  marker proteins  independently align use clustalomega47 next alignment columns  less  three align residues  remove finally  ten individual  alignments  concatenate  use  infer  maximum likelihood phylogenetic tree use iqtree48    model  motus2 profile workflow  motus2 workflow  taxonomic profile consist  three step alignment  metagenomic sequence read  mgs estimation  read abundances  every marker gene cluster mgc  calculation  motu abundances  input motus2 expect  user  provide quality control sequence read   align   mgs   motu database use bwa mem algorithm default parameters  result alignments  filter      least  nucleotide identity  retain  alignments  filter accord   lengths default   minimum alignment length   adjust use   option next  compute  best alignments  every insert read pair   mgcs use bwa alignment score insert   single highest score alignment  flag  “unique alignments” whereas insert  multiple highest score alignments  flag  “multiple alignments” subsequently abundances   mgc  calculate  sum   number   insert flag  unique alignments result   unique alignment profile insert flag  multiple alignments  distribute among  bestscoring mgcs  accord   respective abundances estimate base   unique alignment profile thus  final abundances  calculate   sum   unique abundance profile   distribute contributions   insert flag  multiple alignments  addition   mgc insert count mgc base coverages  calculate  first sum   total number  base align   mgc   divide   respective gene lengths finally  abundances   motus  calculate   median   respective mgc abundances insert count  base coverages  order  reduce false positive result  require  certain number  mgcs   detect     metagenomic read map   default  mgs  option  motus2 although motus2  able  profile many organisms  yet represent  reference genomes   still around    mgcs  could   bin  motus see section  read map   mgcs  assign   group label  “unbinned” show  “”  motu abundance profile  abundance   group  calculate   median  unbinned mgcs sum  cog description  taxonomic profile output  motus2 profiler return multiple taxonomic profile since abundances base  read mappings   calculate  different ways one major distinction   unit  count either fragment   insert  read  singlepair sequence  map basepairs   count count  map basepairs   advantage   mean base coverage  easily  compute  divide  number  base align   certain gene   correspond length motus2 output  option “basecoverage” count base statistics  powerful  differential abundance test output  option “insertraw_counts”   count could  principle  noninteger number due  insert map  multiple genes see section   count  round  integers  relative abundancebased estimate genelength normalizations  require  account  vary lengths   sequence  vary number  mgcs present   motu   end  previously introduce “scaled counts”  retain    characteristics  insert count   approach coverages  calculate  describe     normalize  sum    number  insert  align  mgcs output  option “insertscaled_counts” singlenucleotide variant analysis  mgs  motus2 profiler  new functionality  compute metagenomic snv profile use  mgs comprise motus  reference sequence  result snv profile  highly correlate   obtain  whole genome snv profile see main text fig  supplementary figure    overall snv call pipeline start  align metagenomic sequence  centroid sequence  mgcs see    result bam file  postprocessed use metasnv functions34  motus2 command map_snv map  read use bwa42  perform read filter   similar fashion  describe  taxonomic profile   snv analyse  insert flag  unique alignments  keep   result sam file  sort  convert   bam file use  snv_call command  tool  compute base coverages  call snvs iii generate filter allele frequency table   calculate distance  strain populations  four step  directly build upon metasnv capabilities34 although  procedure  adapt  motus2  facilitate  use  genes rather  genomes firstly  bam file  process  compute per sample coverages  every reference sequencemotu  vertical average number  read per position  horizontal percentage   sequence cover  least  snvs  subsequently call use samtools mpileup49 follow  two postprocessing step  include  filter step   modify  include parallelize compute capabilities  well   removal  pad regions   allele frequency table  filter parameters remain identical  update default value  account   universal character   genes consider  minimal percentage   sequence horizontally cover per sample  per motu default    minimal average vertical coverage per sample  per motu default    minimum number  sample meet  list criteria per motu default    minimum vertical coverage per snv position default    minimum proportion  sample meet  previous criterion  say position default   finally  filter allele frequency table  use  compute genetic distance  sample   motu manhattan distance  well  major allele distance  use   population genomic distance measure   latter  allele frequency change     two sample  take  account  motu profiler use parallelize compute capabilities   step  output directory  include three file two   coverage information   motu  horizontal *covtab file  vertical *perctab file   log file additionally   two directories  per motu filter allele frequencies  identify snvs across sample filtered* directory   per motu genetic distance  sample distances* directory  manhattan manndist file  major allele alleledist file benchmarking motus2   tool  evaluate  accuracy  robustness  benchmarked motus2  two establish tool  taxonomic profile  metagenomic sample metaphlan27   base  cladespecific marker genes  kraken50   base  exact alignments  genomic kmers metaphlan2 version   execute  default parameters  krakenlabelled analyse  execute kraken  read classification  calculate relative abundances  bracken6 kraken  bracken  instal  version  use conda  minikraken database version minikraken_20171101_8gb_dustmasked  download    minibracken database  download     february   execute kraken use pairedend  singleend data use default parameters abundance estimation  bracken  perform   follow parameters  minikraken_8gb_75mers_distribtxt    resultabundancebracken comparison  motus  metagenomeassembled genomes   validate  motus use metagenomeassembled genomes mags reconstruct  different environments   purpose  first extract  metagenomic sequence run  human gut sample available   european nucleotide archive accession number  list  supplementary data  raw read   run  assemble use metaspades   subsequently bin  metabat2    minimum contig length threshold    sequence coverage require  bin  infer  map  raw read back   assemblies use bwa    retrieve  percentage  map read base  samtools    jgi_summarize_bam_contig_depths function  metabat2 quality score    metagenomeassembled genome mag  estimate  checkm  calculate   level  completeness    contamination  previously described23 goodquality mags     keep  subsequent downstream analyse mags  marine sample ocean mags  obtain   subset    mags   describe   recent publication23  order  identify oceanassociated mags  first search   keywords ocean marine baltic sea  north sea  extract entries  supplementary table  of23  find  sample match  keywords   sample  select  mags  supplementary data    reconstruct   metagenomes correspondence  mags  motus  establish use  follow procedure first  extract  ten mgs   mags use fetchmgs see  obtain  set  mgmags second  align  mgmags    database   motus use vsearch usearch_global parameters   minqt  finally  evaluate  congruency   mgmags  motu match    first check   least three mgmags could  assign   motu  map   mgc   part   motu      case  mag  annotate  “unassigned” next  remove  alignments  mgcs  assign  motus  assign  mag   motu     mgmags  consistently match    motu otherwise   majority motu  find  mag  annotate  “inconsistent” benchmarking motus2 use simulate metagenomes   able  assess taxonomic quantification accuracy ten human gut metagenomic sample  simulate use  human gut mags  subset    mags describe  exclude  mags create  sample use  construct  motu database supplementary figure  mags   ani    dereplicated   one representative mag per species cutoff accord  ref   ani  calculate   fastani tool   correspond fastq file  well   simulate abundance data  available   metagenomic read data  simulate use bear54 first  generate   insert     pairedend read    length    insert distance standard deviation  use generate_readspy second trim_readspl  default parameters  use  add  quality score introduce errors  shorten  read every sample  simulate base  motus2 profile relative abundances  ten real sample   simulate sample  randomly select  mags   representative reference genome sequence   superset   kraken metaphlan2  refmotu databases   additional mags sample    lack  reference database representation    preclude  mags  map  metamotus  benchmark  perform  evaluate precisionrecall plot   simulate metagenomes base   number  true positives  false positives  represent species   predict   present   real sample  false negative  represent species   miss   profiler precision  calculate  tptp    recall  tptp   next  evaluate  mean absolute error mae define   average absolute difference  estimate relative abundances  relative abundances simulate  grind truth finally  evaluate  accuracy  alpha diversity estimate use  difference  predict  actual shannon index abbreviate   benchmarking motus2 use  cami framework   evaluate motus2   cami framework25  include eight simulate sample one low complexity two medium complexity  five high complexity    grind truth  available within  first cami community challenge ten metagenomic profile tool include metaphlan2  motus1  already benchmarked   data set  comparatively assess  performance  motus2   context  convert  output  camibioboxes format  option   motus2 profiler  use opal  develop    author  cami  consistency  performance assessments use precisionrecall plot  evaluate motus2 employ five different parameter set high precision      precision default      precision recall      recall high recall      recall  maximum recall      recall hence motus2  represent  five red dot   precisionrecall plot demonstrate     tune  obtain  range  precisionrecall tradeoffs  evaluation   mean absolute error mae    call  norm   cami paper  also obtain  opal  default opal renormalise  relative abundances   gold standard   profile result   sum    calculate  mae  apparently substantially deteriorate  quantification accuracy  motus2 see supplementary figure    reason  include  renormalise  relative abundances without  postprocessing   evaluation  motus2  aim  maximum transparency   comparison    tool  could   evaluate   renormalise version  could theoretically also benefit   evaluation  nonnormalised relative abundances determine environmental specificity  motus  determine  environmental specificity   motus  use  set   metagenomes supplementary data   assess  environmental specificity   metamotus   subset  refmotus   present   sample supplementary figure    end  generate motus2 profile   sample  default settings  remove sample  less   scale insert count base   result profile   classify  motu   present   specific environment    detect    three sample   environment analysis  community structure  assess correlations   shannon index calculate base   rrna genebased analyse  three metagenomic profile tool motus2 metaphlan2  kraken    use data  two different biomes metagenomes generate  stool sample   colorectal cancer crc study21  metagenomes  seawater sample   tara oceans expedition22   crc study amplicon sequence data    region    rrna  download   european nucleotide archive ena database  accession number erp005534   ocean water sample  rrna gene contain fragment  extract  metagenomic sequence read mitags55  ensure comparability   data set  extract  first     mitag sequence start    primer sequence ribosomal rna data  initially process use usearch56 version   follow pairedend read  merge  qualityfiltered use  fastq_mergepairs command  default settings merge read  filter use  fastq_filter command fastq_maxee  sequence  dereplicated use  fastx_uniques command singletons  exclude   remain unique sequence  cluster  operational taxonomic units otus    chimera removal use  cluster_otus command finally otu abundances   sample  determine use  usearch_global command strand     otu abundance table  downsampled   minimum number  read per sample crc  read tara  read  normalize  uneven sequence depths use   function rarefy within  vegan package57  shannon index  diversity  compute   sample   methods  rrna genebased  metagenomic methodbased use   function diversity   vegan package  order  obtain   confidence interval  use bootstrapping     resampling pair  shannon index value  confidence intervals reflect     percentile   bootstrapped sample  sample distance  determine use human body site sample     one time point  available    individual  specifically   body site  compare community compositional distance  sample    individual intraindividual  distance     individuals interindividual canberra  braycurtis distance  compute   vegdist  function   vegan package   logeuclidean distance  compute   euclidean distance   logtransformed relative abundances   addition   pseudocount smaller   smallest nonzero value     three distance   sample  identify   similar sample   one   minimum distance value  determine  proportion  case    sample belong    individual analysis  metatranscriptomes  demonstrate  use  motus2  assess basal transcriptional activity  microbial community members  use  dataset   sample   metagenomic  metatranscriptomic sequence data  available31  sample  metagenomes   metatranscriptomes  subject  profile use motus2 krakenbracken  metaphlan2  result profile  transform  relative abundances  logtransformed  add  small pseudocount   spearman correlations  correspond metagenomic  metatranscriptomic profile generate    sample  calculate  compare  profile methods fig   supplementary figure   moreover evaluate  well species abundance estimate correlate  metagenomic  metatranscriptomic profile   twelve  abundant taxa   class level class level information  motus  metaphlan2  available  part   profiler output class level annotations  kraken  obtain use ncbi taxonomy identifiers comparison  snv profile  mgs  whole genomes  assess  comparability  snv profile generate  motus2  whole genomes  use sample   human microbiome samples14    prokaryoteenriched metagenomes   tara oceans project22 metagenomic read  map   motus centroid database use  motus2 command map_snv   addition   set   reference genomes13 genomic distance  strain populations  sample  estimate base  snv profile compute   motus   whole genomes use  motus snv_call command  filter parameters use within  snv_call command  adapt   specificity  datasets  reference  allele frequency table  filter use  horizontal coverage  equal    whole genomemapped read    motumapped read  vertical coverage     per position coverage      position prevalence     minimum number  sample per reference      human sample     ocean sample wholegenomebased distance  compare    motus use pearsons correlation fig   select  refmotusgenomes  pass  filter thresholds   methods  correlate  sample distance   two methods     species   vaginal supersite pass  filter requirements   methods individuality  microbial populations across body sit  test   individuality  microbial strain populations   subset   human microbiome sample describe      least two time point data  available   body site  compare snv profile distance  sample    individual intraindividual intrabodysite distance  distance     individuals interindividual intrabodysite distance  determine whether intraindividual distance  smaller  interindividual distance see supplementary figure —indicating individuality  strain populations— use roc analysis roc curve see supplementary figure  ascertain  accurately small distance predict whether  pair  sample originate    individual  similarly small interindividual distance  consider false positives  systematically vary  distance cutoff roc curve   summarize   area   curve auroc  higher value correspond  clearer separation  intra  interindividual distance fig   supplementary figure  confidence intervals   auroc fig   obtain  bootstrapping use  proc package",6
144,DiTASiC,"Abundance estimation and differential testing on strain level in metagenomics data
DiTASiC is designed as a comprehensive approach for abundance estimation and differential abundance assessment of individual taxa. Thereby, the main focus is on distinguishing on the strain level with highly similar sequences and its corresponding challenges. The steps of the DiTASiC workflow are illustrated in Figure 1, it consists of three main parts: mapping, abundance estimation and differential abundance assessment. An external file that holds a picture, illustration, etc. Object name is btx237f1.jpg Open in a separate window Fig. 1 Workflow of DiTASiC. It consists of three main parts: (i) mapping, (ii) taxa abundance estimation and (iii) differential abundance assessment. (i) We rely on prior pre-filtering of species by external profiling tools such as Kraken or Mash. Reads are mapped to the given reference genome sequences and the number of matching reads per reference are counted (mapping abundance). A similarity matrix reflecting the genome similarities is constructed. (ii) Subsequently, a GLM is built for resolution of read count ambiguities, resulting in corrected abundance estimates along with standard errors. (iii) For the comparison of metagenomes, abundances are formulated as distributions and their divergence reflects differential events. A final list of tested taxa with fold change and adjusted P-values is reported In the first two parts we built on some of the core ideas of our previously published tool GASiC (Lindner and Renard, 2013), while strongly improving on abundance quantification and introducing new methodology to address the critical aspects of variance of abundance estimates and differential abundance. In a metagenomics sample measured by NGS technologies we face millions to billions of reads which are derived from diverse taxa. DiTASiC relies on a pre-filtering of species by fast profiling tools such as Kraken (Wood and Salzberg, 2014), CLARK (Ounit et al., 2015), Kaiju (Menzel et al., 2016), or by using Mash (Ondov et al., 2016), a genome distance calculator, to reduce the number of potential reference genomes and keep the main focus on species expected in the data. Here, we specifically aim at revealing the picture on the highest available strain levels. In the first mapping step, all reads are assigned to the given references as a first attempt to decipher their potential origin. The number of hits per reference genome is counted. We refer to it as mapping abundance of a taxon. In the next step of abundance estimation, a new generalized linear model (GLM) is introduced for the resolution of shared read counts, which are crucial on strain level. As a result, more accurate abundance estimates are obtained for the different strains along with standard errors for abundance uncertainty. In the last section, the focus is on the comparison of whole metagenomics samples and the assessment of differential abundance of taxa. Thereby, we concentrate on a method to integrate the variance of abundance estimates. Abundances are transformed into distributions, divergence of distributions is used to infer differential events and corresponding P-values are calculated. The details of the three DiTASiC parts are explained in the following sections. The following notation is applied: different metagenomics samples are denoted as D = {Dk, k = 1,…, K}, each containing N = {Nk, k= 1,…, K} total input reads. A set of taxa S = {Si, i= 1,…, M} with known reference sequences is considered. Thereby, Si is synonymously used for both the taxa itself as well as its exact reference genome. Mapping and abundance estimation are addressed for each data set separately, while the last step of differential abundance estimation is defined on a pair of samples from D. 2.1 Mapping To identify their origin, the assignment of reads is conducted by a competitive mapping approach, which means all selected reference genome sequences S are simultaneously offered to all reads of a sample D ϵ D for mapping. Particularly on strain level, reference sequences exhibit high sequence similarities, thus some reads are expected to match to different genome sequences equally well. These reads are defined as shared reads and we account for all their multiple hits. However, the exact matching position in a reference genome Si is not of importance and several position hits of one read on the same reference Si are counted as one. For the mapping itself, a pseudo-alignment approach provided as part of the kallisto implementation (Bray et al., 2016) is applied. As no exact alignments are required for our purpose, a pseudo-aligner is sufficient and proves to be much faster and accurate using a fast kmer-based approach. Here, we gain significant improvements over our previously published tool GASiC, which relied on individual reference alignments by Bowtie 2 (Langmead and Salzberg, 2012). Altogether, we extract and count the number of read hits each reference genome receives and refer to it as mapping abundance ci of taxon Si. In case the data set D consists of mainly dissimilar references and is dominated by clearly unique mappings, the observed mapping abundances ci may already closely reflect the underlying true abundances of the taxa. However, if many similar references are present, which is a common scenario on strain level, a large bias is present due to multiple hits of shared reads. The sum of the mapping abundances of all taxa then drastically exceeds the number of input reads. 2.2 Abundance estimation Following the idea introduced in GASiC, we rely on a simulation-based representation of reference genome similarities to resolve the effect of shared reads. A similarity matrix is constructed, which encodes the proportion of reads which are expected to be shared among all pairwise combinations of reference sequences considered. Reads are simulated using Mason (Holtgrewe, 2010) based on each reference sequence, and are subsequently mapped to all references following the same competitive mapping setup as applied to the reads of D in the step before. The key element is to imitate sequencing, read, and mapping characteristics as good as possible to reproduce the source of ambiguities. Parameters such as read length and mismatch probability are crucial for the simulation of reads, and are inferred from the raw reads of D. The square matrix A = (aij), i,j = 1,…,M, is computed column-wise for each reference, with aij referring to the count of reads simulated from reference j which map to reference i. Next, the matrix is normalized column-wise by the read count ajj, the number of simulated reads which are assigned back to their reference of origin. Thus, the matrix A =  (aij/ajj), i,j = 1,…,M, holds values between zero and one. Replacing the classic linear model of GASiC, we formulate a new GLM with the vector of absolute mapping abundances c and similarity matrix A to correct for the shared read biases. Aiming to recover the true, but unknown, abundances r of the taxa: c = A · r + ε with A = (aij), i,j= 1,…,M, c = (c1, c2, …, cM)T, r = (r1, r2, …, rM)T with non-negativity constraint r ≥ 0, and error term epsilon. The observed mapping count ci of taxon i corresponds to a summed mixture of the underlying true abundance ri of taxon i and a proportion of shared read counts rj due to the other references: c i = r i + M ∑ i ≠ j a i j ⋅ r j + ε i , with taxon i and taxa j = ⎧ ⎨ ⎩ 1 … M ⎫ ⎬ ⎭ ≠ i The GLM is defined by an identity link function as a linear relation of components holds to explain the observed mapping counts. However, in this setting of discrete counts the error epsilon is defined to follow a Poisson distribution. We expect and observed no overdispersion in the abundance estimates within a sample after ambiguity correction by the model (Supplementary Material). This is in contrast to measurements of replicate samples, which may display overdispersion and motivate a negative-binomial assumption (Anders and Huber, 2010). The GLM is internally solved by an ‘iteratively reweighted least squares’ to find the maximum likelihood estimates referring to the ‘true’ abundance estimates ri for each taxon i. Along with the abundance estimates, standard errors are computed which report the range of accuracy and reliability of the abundance estimates. Further, P-values are given for each taxa estimate as a measure of significance. In case of high uncertainty about the presence of a crucial amount of taxa within the selected set of references, the application of an implemented filtering is possible. Thereby, P-values above a set threshold, commonly a value of 0.05, and estimates below a minimum number of assigned reads are used as indicators for false-positive estimates. The filtering step helps to numerically stabilize the equation system in case of many absent taxa and a re-optimization step is subsequently conducted. 2.3 Differential abundance In this section, the focus is on comparing metagenomics samples. The objective is to identify which taxa significantly change their abundance from one metagenome sample to another as well as which hold a constant abundance. For the differential abundance assessment of similar strains the integration of the variance of their abundance estimates is crucial. Hence, in place of directly comparing abundance point estimates of taxa between samples, we make use of the estimates as well as their standard errors. First, the comparison of different samples requires accounting for potentially different numbers of total input reads N. The number of input reads has a significant impact on the computed abundances r and standard error estimates. A linear dependence is clearly noticeable (see Supplementary Fig. S1) and is in agreement with theoretical derivations of the GLM framework. The abundance count estimate r scales linear with the number of reads whereas the standard error scales quadratic. This means the accuracy of abundance estimates improves with increased number of input reads as expected. Altogether, a normalization factor is required and a factor of Nx/Ny is correspondingly applied to samples Dx and Dy to achieve a comparable base between samples. In the next step, we integrate abundance estimates and corresponding standard errors to infer an abundance distribution for each taxon in each sample. Here, it is assumed that the unknown true abundance count of a taxon underlies a Poisson distribution. The potential bias due to falsely assigned reads to taxa, after correction for read ambiguities by the GLM model, is not expected to exceed the variance of a Poisson distribution. But, an analytical approach is not feasible here, as the exact distribution is described in practice by a mixture of Poisson distributions. However, an empirical approach can be pursued, which is realized by a two-step sampling process: In the first step, we define intervals with abundance estimates ri ± their standard errors as boundaries for each listed taxon. We use a scale unit of one standard error, as this reflects the uncertainty interval which is expected to contain the abundance estimate. Subsequently, potential abundance point estimates are uniformly sampled from this interval. Concurrently each of these sampled values refers to a λ value of a Poisson distribution. In the second step, for each taxon and each potential λ of it, 500 values in a default setup are drawn from the corresponding defined Poisson distribution with parameter λ. This creates one empirical distribution based on a specific λ for the taxon. Pooling all empirical distributions, created by all the different λ which are assigned to the taxon, results in an overall empirical distribution comprising 50 000 Poisson draws by default setup. We refer to it as empirical abundance distribution of a taxon. In order to assess whether taxa show differential abundance between two samples, their abundance distributions need to be compared. As we rely on empirical distributions here, no analytical form of standard differential testing is applicable. Yet, we can transfer the assessment of differential abundance to the question to which extent the corresponding abundance distributions overlap. Clearly separated distributions refer to a significant abundance change, while an increasing overlap points to smaller or no significant difference. Measuring the separation of the distributions is implemented by randomly drawing pairs of values from either distribution. The difference within each pair is computed and yields an overall distribution of differences as a result. Thereby, the location of the zero value related to the distribution of differences is meaningful. A zero value moving towards the center of the distribution reflects a higher previous overlap and corresponds to a less significant abundance change. An empirical P-value is correspondingly inferred by determining the quantile of the zero value within the distribution. In case a taxon is only detected within one sample, while absent in the other, the single abundance distribution of the taxon is tested against a user-defined threshold corresponding to a minimum read count. The latter test yields the significance of taxa presence in this one sample. Generally, P-values are calculated individually for all taxa considered in the samples of comparison, either to assess differential abundance of taxa present in both samples or to infer new appearance of taxa in only one sample. Thus, P-values need to be adjusted for multiplicity, which is performed by the method of Benjamini-Hochberg (Benjamini and Hochberg, 1995). A final report is provided listing all taxa tested for differential abundance along with normalized abundance estimates for each sample, log2 fold changes, and adjusted P-values. 2.4 Implementation DiTASiC is implemented in Python3 and R (version ≥ 3.3.1), and is available from https://rki_bioinformatics.gitlab.io/ditasic. Further, a linked webpage and user manual provides easy guidance through the three main commands. DiTASiC is based on a flexible design and allows the integration of mapping algorithms and read simulators of choice. Our implementation uses the current state of the art pseudo-alignment algorithm provided within the kallisto framework (Bray et al., 2016), which can be individually called by the command kallisto pseudo. As a prerequisite, an overall index is built on selected reference sequences. Using the generated tsv and ec file formats, we extract the mapping counts of the contigs and merge them according to genomes. This allows circumventing the use of large SAM files. Further, read simulators need to be optimally adapted to capture the read characteristics. Here, the Mason simulator (Holtgrewe, 2010) serves as default.",AbundanceEstimation,"abundance estimation  differential test  strain level  metagenomics data
ditasic  design   comprehensive approach  abundance estimation  differential abundance assessment  individual taxa thereby  main focus   distinguish   strain level  highly similar sequence   correspond challenge  step   ditasic workflow  illustrate  figure   consist  three main part map abundance estimation  differential abundance assessment  external file  hold  picture illustration etc object name  btx237f1jpg open   separate window fig  workflow  ditasic  consist  three main part  map  taxa abundance estimation  iii differential abundance assessment   rely  prior prefiltering  species  external profile tool   kraken  mash read  map   give reference genome sequence   number  match read per reference  count map abundance  similarity matrix reflect  genome similarities  construct  subsequently  glm  build  resolution  read count ambiguities result  correct abundance estimate along  standard errors iii   comparison  metagenomes abundances  formulate  distributions   divergence reflect differential events  final list  test taxa  fold change  adjust pvalues  report   first two part  build     core ideas   previously publish tool gasic lindner  renard   strongly improve  abundance quantification  introduce new methodology  address  critical aspects  variance  abundance estimate  differential abundance   metagenomics sample measure  ngs technologies  face millions  billions  read   derive  diverse taxa ditasic rely   prefiltering  species  fast profile tool   kraken wood  salzberg  clark ounit    kaiju menzel      use mash ondov     genome distance calculator  reduce  number  potential reference genomes  keep  main focus  species expect   data   specifically aim  reveal  picture   highest available strain level   first map step  read  assign   give reference   first attempt  decipher  potential origin  number  hit per reference genome  count  refer    map abundance   taxon   next step  abundance estimation  new generalize linear model glm  introduce   resolution  share read count   crucial  strain level   result  accurate abundance estimate  obtain   different strain along  standard errors  abundance uncertainty   last section  focus    comparison  whole metagenomics sample   assessment  differential abundance  taxa thereby  concentrate   method  integrate  variance  abundance estimate abundances  transform  distributions divergence  distributions  use  infer differential events  correspond pvalues  calculate  detail   three ditasic part  explain   follow section  follow notation  apply different metagenomics sample  denote    {   … }  contain   {  … } total input read  set  taxa   {  … }  know reference sequence  consider thereby   synonymously use    taxa   well   exact reference genome map  abundance estimation  address   data set separately   last step  differential abundance estimation  define   pair  sample    map  identify  origin  assignment  read  conduct   competitive map approach  mean  select reference genome sequence   simultaneously offer   read   sample     map particularly  strain level reference sequence exhibit high sequence similarities thus  read  expect  match  different genome sequence equally well  read  define  share read   account    multiple hit however  exact match position   reference genome     importance  several position hit  one read    reference   count  one   map   pseudoalignment approach provide  part   kallisto implementation bray     apply   exact alignments  require   purpose  pseudoaligner  sufficient  prove   much faster  accurate use  fast kmerbased approach   gain significant improvements   previously publish tool gasic  rely  individual reference alignments  bowtie  langmead  salzberg  altogether  extract  count  number  read hit  reference genome receive  refer    map abundance   taxon   case  data set  consist  mainly dissimilar reference   dominate  clearly unique mappings  observe map abundances  may already closely reflect  underlie true abundances   taxa however  many similar reference  present    common scenario  strain level  large bias  present due  multiple hit  share read  sum   map abundances   taxa  drastically exceed  number  input read  abundance estimation follow  idea introduce  gasic  rely   simulationbased representation  reference genome similarities  resolve  effect  share read  similarity matrix  construct  encode  proportion  read   expect   share among  pairwise combinations  reference sequence consider read  simulate use mason holtgrewe  base   reference sequence   subsequently map   reference follow   competitive map setup  apply   read     step   key element   imitate sequence read  map characteristics  good  possible  reproduce  source  ambiguities parameters   read length  mismatch probability  crucial   simulation  read   infer   raw read    square matrix   aij   …  compute columnwise   reference  aij refer   count  read simulate  reference   map  reference  next  matrix  normalize columnwise   read count ajj  number  simulate read   assign back   reference  origin thus  matrix    aijajj   … hold value  zero  one replace  classic linear model  gasic  formulate  new glm   vector  absolute map abundances   similarity matrix   correct   share read bias aim  recover  true  unknown abundances    taxa           aij  …     … cmt     … rmt  nonnegativity constraint  ≥   error term epsilon  observe map count   taxon  correspond   sum mixture   underlie true abundance   taxon    proportion  share read count  due    reference        ∑  ≠     ⋅        taxon   taxa   ⎧ ⎨ ⎩  …  ⎫ ⎬ ⎭ ≠   glm  define   identity link function   linear relation  components hold  explain  observe map count however   set  discrete count  error epsilon  define  follow  poisson distribution  expect  observe  overdispersion   abundance estimate within  sample  ambiguity correction   model supplementary material    contrast  measurements  replicate sample  may display overdispersion  motivate  negativebinomial assumption anders  huber   glm  internally solve   iteratively reweighted least square  find  maximum likelihood estimate refer   true abundance estimate    taxon  along   abundance estimate standard errors  compute  report  range  accuracy  reliability   abundance estimate  pvalues  give   taxa estimate   measure  significance  case  high uncertainty   presence   crucial amount  taxa within  select set  reference  application   implement filter  possible thereby pvalues   set threshold commonly  value    estimate   minimum number  assign read  use  indicators  falsepositive estimate  filter step help  numerically stabilize  equation system  case  many absent taxa   reoptimization step  subsequently conduct  differential abundance   section  focus   compare metagenomics sample  objective   identify  taxa significantly change  abundance  one metagenome sample  another  well   hold  constant abundance   differential abundance assessment  similar strain  integration   variance   abundance estimate  crucial hence  place  directly compare abundance point estimate  taxa  sample  make use   estimate  well   standard errors first  comparison  different sample require account  potentially different number  total input read   number  input read   significant impact   compute abundances   standard error estimate  linear dependence  clearly noticeable see supplementary fig     agreement  theoretical derivations   glm framework  abundance count estimate  scale linear   number  read whereas  standard error scale quadratic  mean  accuracy  abundance estimate improve  increase number  input read  expect altogether  normalization factor  require   factor  nxny  correspondingly apply  sample     achieve  comparable base  sample   next step  integrate abundance estimate  correspond standard errors  infer  abundance distribution   taxon   sample    assume   unknown true abundance count   taxon underlie  poisson distribution  potential bias due  falsely assign read  taxa  correction  read ambiguities   glm model   expect  exceed  variance   poisson distribution   analytical approach   feasible    exact distribution  describe  practice   mixture  poisson distributions however  empirical approach   pursue   realize   twostep sample process   first step  define intervals  abundance estimate  ±  standard errors  boundaries   list taxon  use  scale unit  one standard error   reflect  uncertainty interval   expect  contain  abundance estimate subsequently potential abundance point estimate  uniformly sample   interval concurrently    sample value refer    value   poisson distribution   second step   taxon   potential     value   default setup  draw   correspond define poisson distribution  parameter   create one empirical distribution base   specific    taxon pool  empirical distributions create    different    assign   taxon result   overall empirical distribution comprise   poisson draw  default setup  refer    empirical abundance distribution   taxon  order  assess whether taxa show differential abundance  two sample  abundance distributions need   compare   rely  empirical distributions   analytical form  standard differential test  applicable yet   transfer  assessment  differential abundance   question   extent  correspond abundance distributions overlap clearly separate distributions refer   significant abundance change   increase overlap point  smaller   significant difference measure  separation   distributions  implement  randomly draw pair  value  either distribution  difference within  pair  compute  yield  overall distribution  differences   result thereby  location   zero value relate   distribution  differences  meaningful  zero value move towards  center   distribution reflect  higher previous overlap  correspond   less significant abundance change  empirical pvalue  correspondingly infer  determine  quantile   zero value within  distribution  case  taxon   detect within one sample  absent     single abundance distribution   taxon  test   userdefined threshold correspond   minimum read count  latter test yield  significance  taxa presence   one sample generally pvalues  calculate individually   taxa consider   sample  comparison either  assess differential abundance  taxa present   sample   infer new appearance  taxa   one sample thus pvalues need   adjust  multiplicity   perform   method  benjaminihochberg benjamini  hochberg   final report  provide list  taxa test  differential abundance along  normalize abundance estimate   sample log2 fold change  adjust pvalues  implementation ditasic  implement  python3   version ≥    available     link webpage  user manual provide easy guidance   three main command ditasic  base   flexible design  allow  integration  map algorithms  read simulators  choice  implementation use  current state   art pseudoalignment algorithm provide within  kallisto framework bray       individually call   command kallisto pseudo   prerequisite  overall index  build  select reference sequence use  generate tsv   file format  extract  map count   contigs  merge  accord  genomes  allow circumvent  use  large sam file  read simulators need   optimally adapt  capture  read characteristics   mason simulator holtgrewe  serve  default",6
145,FastViromeExplorer,"FastViromeExplorer: a pipeline for virus and phage identification and abundance profiling in metagenomics data
FastViromeExplorer, written in Java, has two main steps: (1) the read mapping step where all reads are mapped to a reference database, and (2) the filtering step where the mapping results are subjected to three major filters (detailed later) for output of the final results on virus types and abundances. The input of the read alignment step is raw reads (single-end or paired-end) in fastq format. FastViromeExplorer uses the reference database downloaded from NCBI containing 8,957 RefSeq viral genomes as default but can also use any updated or customized databases as reference. FastViromeExplorer incorporates the reference database as an input parameter, so that user can use any database of his choice as input. A precomputed kallisto index file, generated for the 8,957 genomes is distributed here: http://bench.cs.vt.edu/FastViromeExplorer/. First, FastViromeExplorer calls kallisto (Bray et al., 2016) as a subprocess to map the input reads against the reference database. Kallisto was developed to map RNA-seq data to a reference transcriptome (all the transcripts for a genome) leveraging the pseudoalignment process and estimate the abundance of the transcripts using the Expectation-Maximization (EM) algorithm (Dempster, Laird & Rubin, 1977). As there is no actual sequence alignment of the entire read over the reference sequences, the pseudoalignment process enables read mapping to be both lightweight and superfast. Essentially, kallisto searches for exact matches for a short k-mer (default size 31 bp) between the metagenomic reads and the sequences in the virus/phage database. For example, kallisto was able to map and quantify 30 million paired-end RNA-seq reads from a human transcriptome sample in less than 10 min on a small laptop computer with a 1.3-GHz processor (Bray et al., 2016). In addition to the ultrafast speed, kallisto also gives accurate estimation of abundance of each transcript or reference sequence (Schaeffer et al., 2017; Soneson et al., 2016). Consequently, kallisto could provide an ideal tool for detection and quantification of viruses in metagenomic samples that commonly have tens of millions of reads, mapping of which using commonly used programs such as BLAST can be time-consuming and often infeasible without computer clusters. Therefore, FastViromeExplorer deploys kallisto for the purpose of read mapping and abundance estimation of the viruses. Since kallisto searches for exact matches for a short k-mer (default size 31 bp) between the metagenomic reads and the sequences in the virus/phage database, if a 31 bp match is found then the virus is detected. If multiple hits occur, then kallisto uses an EM algorithm to help resolve the redundancy and quantify the abundances of the detected viruses. The k-mer size in kallisto can be altered depending on user’s need. For example, if the sample is expected to contain viral sequences that are divergent from those in the reference database the k-mer size can be reduced to improve detection sensitivity. After the first alignment step, FastViromeExplorer takes the output of kallisto that includes information of the aligned reads together with estimated abundances or estimated read counts of all the identified viruses for the processing of the second step. The second step filters the output of the first step using three criteria, introduced to ensure the quality of virus detection and especially to reduce the number of false positive viruses from the result. In detail, the first criterion, hereafter referred to as “R”, is based on the ratio of the observed extent of genome coverage with the expected extent of genome coverage, computed as equation M1        (1) Co is the observed extent of genome coverage by the mapped reads, computed as equation M2        (2) where Ls is the actual length of the genome that is supported or covered by the mapped reads and Lg is the length of the genome. Ce is the expected extent of genome coverage, assuming a Poisson distribution for the mapped reads along the genome, and therefore, equation M3        (3) where N is the number of mapped reads to the genome, Lr is the read length, and Lg is the length of the genome. If a virus has R < 0.3, FastViromeExplorer discards the virus. This criterion is motivated by the observation that some viruses detected by our tool only have reads mapped to the repeat regions of their genomes. For example, while analyzing the fecal samples from Lee et al. (2017), we found that for the BeAn 58058 virus (NC_032111.1), all the reads were mapped to one particular region of its genome, from 8,200 bp to 8,700 bp (see Fig. S1). Analyzing this region using RepeatMasker (Smit, Hubley & Green, 1996) revealed that it is a simple repeat region and falls into the class of Alu elements. If the virus is truly present in the sample, we expect reads to be mapped to not only the repeat region but also other regions of the genome. Therefore, finding this virus is likely an artifact caused by the prevalence of repeat regions instead of real biological signals. If the reads are all mapped to a repeat region, the observed coverage of the virus genome Co is expected to be much lower than Ce, as a result, R is low and by imposing a cutoff of 0.3 (determined based on our empirical analyses), viruses that have reads mapped to only repeat regions get filtered out. The second criterion requires Co ≥ 0.1; that is, a virus that has Co < 0.1 is discarded. This criterion requires that the mapped reads should cover at least 10% of the viral genome. Manual inspection of the results of our tool reveals that very large viruses may have several repeat regions in their genomes and as a result, though all the reads are mapped to the repeat regions, they are mapped to different repeat regions. In these cases, the difference between Co and Ce may be small and therefore R can be high enough to pass the first filter. However, it is very likely that the result is simply an artifact of repetitive sequences. For example, while analyzing the fecal samples (Lee et al., 2017), we found that Pandoravirus dulcis (NC_021858.1), a very large virus with 1,908,524 bp, has several repeat regions, and all the reads were mapped only to the repeat regions (see Fig. S2). Hence, to alleviate this artifact, Co ≥ 0.1 is used as the second filter. As repeat regions of a virus usually cover less than 10% of the genome (Philippe et al., 2013), if any virus is covered by more than 10% by the reads, it is reasonable to assume that the reads are not merely from repeat regions and thus the virus should be considered in the result. The third criterion is based on the number of mapped reads N. Extensive empirical analysis and inspection of the results of our tool show that for very small viruses, only a few reads are enough to cover a good portion of the viral genome, resulting in high R and Co that pass criteria 1 and 2. For example, in the fecal samples (Lee et al., 2017) that we analyzed, four reads were mapped to Rose rosette virus RNA3 (NC_015300.1). As the viral sequence has only 1,544 bp, four reads of length 150 bp were enough to pass criteria 1 and 2. But as only a handful of reads are mapped, it is likely that the virus is false positive. To be more stringent, FastViromeExplorer applies the third filter requiring the number of mapped reads to be greater than 10, and therefore discards the ones with N < 10. After applying all the filters, FastViromeExplorer outputs the final result that contains a list of identified viruses in the given sample along with the estimated read count or abundance and taxonomy of the viruses. The output list is sorted by the abundance with the most abundant viruses on the top of the list. It is worth noting that the three criteria are introduced to improve the virus detection specificity by alleviating artifacts caused by factors such as repeat sequences and low genome coverage. The actual cutoff values for R, Co, and N are based on our empirical experience and literature observation. However, depending on the specific studies and the need of users, the cutoff values used here might not be suitable. To allow flexibility and customization, FastViromeExplorer incorporates these three filters as parameters so that users can easily adjust the values to adapt to their own studies. For example, users can deploy more stringent criteria by setting higher values for R, Co, and N than the default, to get a “high confidence” set of viruses or can lower these values to increase sensitivity to detect divergent viruses or viral reads in metagenomic data where coverage may be expected to not be uniform (Solonenko et al., 2013). FastViromeExplorer was run on both simulated and real data to examine its running time and accuracy. FastViromeExplorer used kallisto (version 0.43.1) with default settings and generated pseudoalignment results in sam format and filtered abundance results in a tab-delimited file. The abundance results contain identified virus names, NCBI accession numbers, NCBI taxonomic path, and estimated read counts. FastViromeExplorer was run on two different reference databases, the default database distributed together with FastViromeExplorer, that is, the NCBI RefSeq database containing 8,957 genomes of eukaryotic viruses and phages, and the set of sequences collected from the JGI “earth virome” study (Paez-Espino et al., 2016) containing 125,842 metagenomic viral contigs (mVCs). The taxonomic annotation and host information for these mVCs were collected from the IMG/VR database (Paez-Espino et al., 2017). In addition to the challenge of mapping 10s or 100s of millions of metagenomic reads, tools for the accurate identification and quantification of viral genomes must also be capable of handling ever-growing reference databases of viral sequences. In order to measure how the indexing step of kallisto scales with reference databases of different sizes, kallisto was applied to index five different databases. Three databases were generated from NCBI RefSeq viral database, one containing only phages (2,187 phage genomes), one containing only eukaryotic viruses (6,770 eukaryotic virus genomes), and one containing both phages and eukaryotic viruses (8,957 viral genomes). The other two databases were created from sequences collected from Paez-Espino et al. (2016), one containing all the 125,842 mVCs and the other containing half of the mVCs. The time analysis of kallisto’s indexing step was produced on a Linux based cluster with 64 CPUs and 128 GB RAM. The indexing step was run using default k-mer size 31 and default number of threads 1. The precomputed kallisto index file for the full 125,842 mVCs from JGI is available here: http://bench.cs.vt.edu/FastViromeExplorer/. To evaluate the performance of FastViromeExplorer, we compared speed and accuracy with ViromeScan, a recently developed virus annotation pipeline that calls Bowtie2 as a subprocess for read mapping, that was shown to be 1,000 times faster than previous tools (Rampelli et al., 2016). ViromeScan was run with default settings and with the eukaryotic DNA/RNA virus database containing 4,370 genome sequences, the largest reference database provided by ViromeScan, and with a custom database consisting of the 125,842 mVCs from JGI. ViromeScan generated alignment results and abundances of viruses at family, genus, and species level. We also ran blastn (version ncbi-blast-2.6.0 +) using both the NCBI RefSeq viral database and the large JGI database. Blastn only generated the alignment result in text format. All the time analyses were calculated using elapsed real time from Unix’s time command. To examine the virus detection and quantification accuracy of FastViromeExplorer, simulated metagenomic data were used. A randomly selected collection of genomes containing 4,000 virus genomes and 2,000 bacteria genomes were obtained from NCBI RefSeq database. Four paired-end read datasets, each containing one million reads of length 100 bp, were generated from these genomes using the read simulator WGSIM (https://github.com/lh3/wgsim). For all the datasets, 49% reads were from viruses and 51% from bacteria. The four datasets were generated using 1% sequencing error rate and 3%, 5%, 7%, or 10% mutation frequencies respectively. ViromeScan and blastn were also applied to these four datasets. As ViromeScan uses eukaryotic viruses as the reference database, for comparison, both FastViromeExplorer and blastn were run on a reference database containing only NCBI RefSeq eukaryotic viruses. ViromeScan was run with the eukaryotic virus database provided by ViromeScan. Under the default setting, ViromeScan removed all the mapped reads during its quality filtering and trimming step (trimBWAstyle.pl script) and did not produce any results. Therefore, it was run without ViromeScan’s quality filtering and trimming step. With the ground truth for the alignment of the reads, recall, precision, and F1 score were calculated using the following formula: equation M4        (4) equation M5        (5) equation M6        (6) To examine the running time and performance of FastViromeExplorer in detecting viruses on real data, the fecal metagenomics datasets described in Lee et al. (2017) were downloaded from NCBI under the accession number SRP093449 and annotated with both FastViromeExplorer and ViromeScan. The study tracked bacteria colonization in a fecal microbiota transplantation (FMT) experiment through the analysis of metagenomic data. To examine how the viruses/bacteriophages were affected by the transplantation, we reanalyzed the four fecal metagenomic samples collected from a healthy donor and three samples from a recipient patient suffering mild/moderate ulcerative colitis. The three samples for the recipient were collected prior to FMT, four weeks after FMT, and eight weeks after FMT, respectively. All the reads were Illumina paired-end reads with 150 bp read length. Seven data sets of different sizes (1, 3, 5, 10, 20, 30, and 40 million reads) were also generated from the samples and annotated by FastViromeExplorer and ViromeScan to compare their running time on large datasets. To examine the effect of the reference database on results, FastViromeExplorer was applied to the samples using two different reference databases, FastViromeExplorer’s default reference database and the set of 125,842 mVCs collected from the study Paez-Espino et al. (2016). While using the NCBI RefSeq database as reference, a Linux based laptop with Intel core i5-3230M CPU @ 2.60 GHz * 4 processors and 12 GB RAM was used to produce the results, and while using the 125,842 mVCs as reference, a Linux based cluster with 64 CPUs and 128 GB RAM was used to produce the results. While using the cluster, only one thread was used to run the tools. To examine the applicability of FastViromeExplorer on environmental samples, an ocean water metagenome file described in Aylward et al. (2017) was downloaded from NCBI SRA under the accession number SRX2912986 and analyzed with FastViromeExplorer. The metagenome sequencing file had around 18 million paired-end reads and the 125,842 mVCs collected from the study Paez-Espino et al. (2016) was used as reference database. As the original study focused on ocean virome, a viral contig set collected from Global Ocean Virome (GOV) study (Roux et al., 2016) was also used as reference database. The GOV contig set contains 298,383 epipelagic and mesopelagic viral contigs and a precomputed kallisto index file for this viral contig set is available here: http://bench.cs.vt.edu/FastViromeExplorer/.",AbundanceEstimation,"fastviromeexplorer  pipeline  virus  phage identification  abundance profile  metagenomics data
fastviromeexplorer write  java  two main step   read map step   read  map   reference database    filter step   map result  subject  three major filter detail later  output   final result  virus type  abundances  input   read alignment step  raw read singleend  pairedend  fastq format fastviromeexplorer use  reference database download  ncbi contain  refseq viral genomes  default   also use  update  customize databases  reference fastviromeexplorer incorporate  reference database   input parameter   user  use  database   choice  input  precomputed kallisto index file generate    genomes  distribute   first fastviromeexplorer call kallisto bray      subprocess  map  input read   reference database kallisto  develop  map rnaseq data   reference transcriptome   transcripts   genome leverage  pseudoalignment process  estimate  abundance   transcripts use  expectationmaximization  algorithm dempster laird  rubin      actual sequence alignment   entire read   reference sequence  pseudoalignment process enable read map    lightweight  superfast essentially kallisto search  exact match   short kmer default size     metagenomic read   sequence   virusphage database  example kallisto  able  map  quantify  million pairedend rnaseq read   human transcriptome sample  less   min   small laptop computer   ghz processor bray     addition   ultrafast speed kallisto also give accurate estimation  abundance   transcript  reference sequence schaeffer    soneson    consequently kallisto could provide  ideal tool  detection  quantification  viruses  metagenomic sample  commonly  tens  millions  read map   use commonly use program   blast   timeconsuming  often infeasible without computer cluster therefore fastviromeexplorer deploy kallisto   purpose  read map  abundance estimation   viruses since kallisto search  exact match   short kmer default size     metagenomic read   sequence   virusphage database     match  find   virus  detect  multiple hit occur  kallisto use   algorithm  help resolve  redundancy  quantify  abundances   detect viruses  kmer size  kallisto   alter depend  users need  example   sample  expect  contain viral sequence   divergent     reference database  kmer size   reduce  improve detection sensitivity   first alignment step fastviromeexplorer take  output  kallisto  include information   align read together  estimate abundances  estimate read count    identify viruses   process   second step  second step filter  output   first step use three criteria introduce  ensure  quality  virus detection  especially  reduce  number  false positive viruses   result  detail  first criterion hereafter refer   “”  base   ratio   observe extent  genome coverage   expect extent  genome coverage compute  equation             observe extent  genome coverage   map read compute  equation              actual length   genome   support  cover   map read     length   genome    expect extent  genome coverage assume  poisson distribution   map read along  genome  therefore equation              number  map read   genome    read length     length   genome   virus     fastviromeexplorer discard  virus  criterion  motivate   observation   viruses detect   tool   read map   repeat regions   genomes  example  analyze  fecal sample  lee     find    bean  virus nc_032111   read  map  one particular region   genome       see fig  analyze  region use repeatmasker smite hubley  green  reveal     simple repeat region  fall   class  alu elements   virus  truly present   sample  expect read   map     repeat region  also  regions   genome therefore find  virus  likely  artifact cause   prevalence  repeat regions instead  real biological signal   read   map   repeat region  observe coverage   virus genome   expect   much lower     result   low   impose  cutoff   determine base   empirical analyse viruses   read map   repeat regions get filter   second criterion require  ≥     virus       discard  criterion require   map read  cover  least    viral genome manual inspection   result   tool reveal   large viruses may  several repeat regions   genomes    result though   read  map   repeat regions   map  different repeat regions   case  difference     may  small  therefore    high enough  pass  first filter however    likely   result  simply  artifact  repetitive sequence  example  analyze  fecal sample lee     find  pandoravirus dulcis nc_021858   large virus     several repeat regions    read  map    repeat regions see fig  hence  alleviate  artifact  ≥   use   second filter  repeat regions   virus usually cover less     genome philippe      virus  cover       read   reasonable  assume   read   merely  repeat regions  thus  virus   consider   result  third criterion  base   number  map read  extensive empirical analysis  inspection   result   tool show    small viruses    read  enough  cover  good portion   viral genome result  high     pass criteria     example   fecal sample lee      analyze four read  map  rise rosette virus rna3 nc_015300   viral sequence     four read  length    enough  pass criteria        handful  read  map   likely   virus  false positive    stringent fastviromeexplorer apply  third filter require  number  map read   greater    therefore discard  ones      apply   filter fastviromeexplorer output  final result  contain  list  identify viruses   give sample along   estimate read count  abundance  taxonomy   viruses  output list  sort   abundance    abundant viruses   top   list   worth note   three criteria  introduce  improve  virus detection specificity  alleviate artifacts cause  factor   repeat sequence  low genome coverage  actual cutoff value       base   empirical experience  literature observation however depend   specific study   need  users  cutoff value use  might   suitable  allow flexibility  customization fastviromeexplorer incorporate  three filter  parameters   users  easily adjust  value  adapt    study  example users  deploy  stringent criteria  set higher value        default  get  “high confidence” set  viruses   lower  value  increase sensitivity  detect divergent viruses  viral read  metagenomic data  coverage may  expect    uniform solonenko    fastviromeexplorer  run   simulate  real data  examine  run time  accuracy fastviromeexplorer use kallisto version   default settings  generate pseudoalignment result  sam format  filter abundance result   tabdelimited file  abundance result contain identify virus name ncbi accession number ncbi taxonomic path  estimate read count fastviromeexplorer  run  two different reference databases  default database distribute together  fastviromeexplorer    ncbi refseq database contain  genomes  eukaryotic viruses  phages   set  sequence collect   jgi “earth virome” study paezespino    contain  metagenomic viral contigs mvcs  taxonomic annotation  host information   mvcs  collect   imgvr database paezespino     addition   challenge  map     millions  metagenomic read tool   accurate identification  quantification  viral genomes must also  capable  handle evergrowing reference databases  viral sequence  order  measure   index step  kallisto scale  reference databases  different size kallisto  apply  index five different databases three databases  generate  ncbi refseq viral database one contain  phages  phage genomes one contain  eukaryotic viruses  eukaryotic virus genomes  one contain  phages  eukaryotic viruses  viral genomes   two databases  create  sequence collect  paezespino    one contain    mvcs    contain half   mvcs  time analysis  kallistos index step  produce   linux base cluster   cpus    ram  index step  run use default kmer size   default number  thread   precomputed kallisto index file   full  mvcs  jgi  available    evaluate  performance  fastviromeexplorer  compare speed  accuracy  viromescan  recently develop virus annotation pipeline  call bowtie2   subprocess  read map   show    time faster  previous tool rampelli    viromescan  run  default settings    eukaryotic dnarna virus database contain  genome sequence  largest reference database provide  viromescan    custom database consist    mvcs  jgi viromescan generate alignment result  abundances  viruses  family genus  species level  also run blastn version ncbiblast  use   ncbi refseq viral database   large jgi database blastn  generate  alignment result  text format   time analyse  calculate use elapse real time  unixs time command  examine  virus detection  quantification accuracy  fastviromeexplorer simulate metagenomic data  use  randomly select collection  genomes contain  virus genomes   bacteria genomes  obtain  ncbi refseq database four pairedend read datasets  contain one million read  length    generate   genomes use  read simulator wgsim     datasets  read   viruses    bacteria  four datasets  generate use  sequence error rate       mutation frequencies respectively viromescan  blastn  also apply   four datasets  viromescan use eukaryotic viruses   reference database  comparison  fastviromeexplorer  blastn  run   reference database contain  ncbi refseq eukaryotic viruses viromescan  run   eukaryotic virus database provide  viromescan   default set viromescan remove   map read   quality filter  trim step trimbwastylepl script    produce  result therefore   run without viromescans quality filter  trim step   grind truth   alignment   read recall precision   score  calculate use  follow formula equation          equation          equation           examine  run time  performance  fastviromeexplorer  detect viruses  real data  fecal metagenomics datasets describe  lee     download  ncbi   accession number srp093449  annotate   fastviromeexplorer  viromescan  study track bacteria colonization   fecal microbiota transplantation fmt experiment   analysis  metagenomic data  examine   virusesbacteriophages  affect   transplantation  reanalyzed  four fecal metagenomic sample collect   healthy donor  three sample   recipient patient suffer mildmoderate ulcerative colitis  three sample   recipient  collect prior  fmt four weeks  fmt  eight weeks  fmt respectively   read  illumina pairedend read    read length seven data set  different size         million read  also generate   sample  annotate  fastviromeexplorer  viromescan  compare  run time  large datasets  examine  effect   reference database  result fastviromeexplorer  apply   sample use two different reference databases fastviromeexplorers default reference database   set   mvcs collect   study paezespino     use  ncbi refseq database  reference  linux base laptop  intel core i53230m cpu @  ghz *  processors    ram  use  produce  result   use   mvcs  reference  linux base cluster   cpus    ram  use  produce  result  use  cluster  one thread  use  run  tool  examine  applicability  fastviromeexplorer  environmental sample  ocean water metagenome file describe  aylward     download  ncbi sra   accession number srx2912986  analyze  fastviromeexplorer  metagenome sequence file  around  million pairedend read    mvcs collect   study paezespino     use  reference database   original study focus  ocean virome  viral contig set collect  global ocean virome gov study roux     also use  reference database  gov contig set contain  epipelagic  mesopelagic viral contigs   precomputed kallisto index file   viral contig set  available  ",6
146,Kallisto,"Near-optimal probabilistic RNA-seq quantification
Index construction. The construction of the index starts with the formation of a colored de Bruijn graph15 from the transcriptome, where the colors correspond to transcripts. In the colored transcriptome de Bruijn graph, each node corresponds to a k-mer and every k-mer receives a color for each transcript it occurs in. Contigs are defined to be linear stretches of the de Bruijn graph that have identical colorings. This ensures that all k-mers in a contig are associated with the same equivalence class (the converse is not true: two different contigs can be associated with the same equivalence class). Once the graph and contigs have been constructed, kallisto stores a hash table mapping each k-mer to the contig it is contained in, along with the position within the contig. This structure is called the kallisto index. For error-free reads, there can be a difference between the equivalence class of a read and the intersection of its k-compatibility classes. But for a read of length l this can only happen if there are two transcripts that have the same l – k + 1 k-mers occurring in different order. This is unlikely to happen for large k because it would imply that the T-DBG has a directed cycle shorter than l – k + 1. This fact also provides a criterion that can be tested. Pseudoalignment. Reads are pseudoaligned by looking up the k-compatibility class for each k-mer in the read in the kallisto index and then intersecting the identified k-compatibility classes. In the case of paired-end reads, the k-compatibility class lookup is done for both ends of the fragment and all the resulting classes are intersected. Since the T-DBG identifies each k-mer with its reverse complement, the k-mer hashing in kallisto is strand-agnostic; however, the implementation could also be adapted to require specific strandedness of reads from strand-specific protocols. To further speed up the processing, kallisto uses the structural information stored in the index: because all k-mers in a contig of the T-DBG have the same k-compatibility class, it would be redundant to include more than one k-mer from a contig in the intersection of k-compatibility classes. This observation is leveraged in kallisto by finding the distances to the junctions at the end of its contig each time a k-mer is looked up using the hash. If the read does arise from a transcript in the T-DBG, the k-mers up to those distances can be skipped without affecting the result of the intersection, resulting in fewer hash lookups. To help ensure that the read is consistent with the T-DBG, kallisto checks the last k-mer that is skipped to ensure its k-compatibility class is equal as expected. In rare case when there is a mismatch, kallisto defaults to examining each k-mer of the read. For the majority of reads, kallisto ends up performing a hash lookup for only two k-mers (Supplementary Fig. 11). While pseudoalignment does not require or make use of the locations of k-mers in transcripts, it is possible to extract such data from the T-DBG, and a “pseudobam output” option of kallisto takes advantage of this to produce an alignment file containing positions of reads within transcripts. With pseudobam it is possible to examine the location of reads within transcripts and genes of interest for quality control and analysis purposes. Quantification. In order to rapidly quantify transcript abundances from pseudoalignments, kallisto makes use of the following form of the likelihood function for RNA-seq: In equation (1), F is the set of fragments, T is the set of transcripts, lt is the (effective) length3 of transcript t and yf,t is a compatibility matrix defined as 1 if fragment f is compatible with t and 0 otherwise. The parameters are the αt, the probabilities of selecting fragments from transcripts. The likelihood can be rewritten as a product over equivalence classes, in which similar summation terms have been factored together. In the factorization the numbers ce are the number of counts observed from equivalence class e. When equation (1) is written in terms of the equivalence classes, the equivalence class counts are sufficient statistics and thus, in the computations, are based on a much smaller set of data (usually hundreds of thousands of equivalence classes instead of tens of millions of reads). The likelihood function is iteratively optimized with the EM algorithm, with iterations terminating when, for every transcript t, αtN > 0.01 (N is the total number of fragments) changes less than 1% from iteration to iteration. The transcript abundances are output by kallisto in transcripts per million9 (TPM) units. Bias correction. There are many sources of bias in RNA-seq, but previous work has identified sequence-specific bias12 as particularly problematic. Sequence-specific bias arises as a result of nonrandom priming of fragments, where the nucleotide sequences at the 3′ and 5′ ends affect the probability of sampling. The kallisto correction is similar to that of Roberts et al.12; however, it uses 6-mers of the transcript sequence overlapping the 5′ fragment, starting 2 bp upstream of the fragment. First kallisto measures the empirical frequency of 6-mers as estimated from the first 1 million pseudoalignable reads. To apply the bias correction, it uses an initial estimate for the abundance, using 50 rounds of the EM algorithm. The bias of 6-mers is used to adjust the effective length of each transcript by adding the bias of each 6-mer on both strands. To account for edge effects, kallisto only add the 6-mers from the start up to the length of the transcript minus the average fragment length. This process is repeated once more with an updated expression estimate after 550 rounds of the EM algorithm. Bootstrap. The bootstrap is highly efficient in kallisto both because the EM algorithm is very fast and because the sufficient statistics of the model are the equivalence class counts. This latter fact means that bootstrap samples can be very rapidly generated once pseudoalignment of the fragments is completed. With the N original fragments having been categorized by equivalence class, generating a new bootstrap sample consists of sampling N counts from a multinomial distribution over the equivalence classes, with the probability of each class being proportional to its count in the original data. The transcript abundances for these new samples are then recomputed using the EM algorithm. In kallisto the number of bootstraps to be performed is an option passed to the program, and because a large amount of data can be produced, the output is compressed in HDF5. The HDF5 files can be read into another program for processing (for example, R) or can be converted to plain text using kallisto.",AbundanceEstimation,"nearoptimal probabilistic rnaseq quantification
index construction  construction   index start   formation   color  bruijn graph15   transcriptome   color correspond  transcripts   color transcriptome  bruijn graph  node correspond   kmer  every kmer receive  color   transcript  occur  contigs  define   linear stretch    bruijn graph   identical color  ensure   kmers   contig  associate    equivalence class  converse   true two different contigs   associate    equivalence class   graph  contigs   construct kallisto store  hash table map  kmer   contig   contain  along   position within  contig  structure  call  kallisto index  errorfree read     difference   equivalence class   read   intersection   kcompatibility class    read  length     happen    two transcripts          kmers occur  different order   unlikely  happen  large    would imply   tdbg   direct cycle shorter        fact also provide  criterion    test pseudoalignment read  pseudoaligned  look   kcompatibility class   kmer   read   kallisto index   intersect  identify kcompatibility class   case  pairedend read  kcompatibility class lookup     end   fragment    result class  intersect since  tdbg identify  kmer   reverse complement  kmer hash  kallisto  strandagnostic however  implementation could also  adapt  require specific strandedness  read  strandspecific protocols   speed   process kallisto use  structural information store   index   kmers   contig   tdbg    kcompatibility class  would  redundant  include   one kmer   contig   intersection  kcompatibility class  observation  leverage  kallisto  find  distance   junctions   end   contig  time  kmer  look  use  hash   read  arise   transcript   tdbg  kmers    distance   skip without affect  result   intersection result  fewer hash lookups  help ensure   read  consistent   tdbg kallisto check  last kmer   skip  ensure  kcompatibility class  equal  expect  rare case     mismatch kallisto default  examine  kmer   read   majority  read kallisto end  perform  hash lookup   two kmers supplementary fig   pseudoalignment   require  make use   locations  kmers  transcripts   possible  extract  data   tdbg   “pseudobam output” option  kallisto take advantage    produce  alignment file contain position  read within transcripts  pseudobam   possible  examine  location  read within transcripts  genes  interest  quality control  analysis purpose quantification  order  rapidly quantify transcript abundances  pseudoalignments kallisto make use   follow form   likelihood function  rnaseq  equation     set  fragment    set  transcripts    effective length3  transcript   yft   compatibility matrix define    fragment   compatible     otherwise  parameters     probabilities  select fragment  transcripts  likelihood   rewrite   product  equivalence class   similar summation term   factor together   factorization  number    number  count observe  equivalence class   equation   write  term   equivalence class  equivalence class count  sufficient statistics  thus   computations  base   much smaller set  data usually hundreds  thousands  equivalence class instead  tens  millions  read  likelihood function  iteratively optimize    algorithm  iterations terminate   every transcript  αtn      total number  fragment change less    iteration  iteration  transcript abundances  output  kallisto  transcripts per million9 tpm units bias correction   many source  bias  rnaseq  previous work  identify sequencespecific bias12  particularly problematic sequencespecific bias arise   result  nonrandom prim  fragment   nucleotide sequence   ′  ′ end affect  probability  sample  kallisto correction  similar    roberts   however  use mers   transcript sequence overlap  ′ fragment start   upstream   fragment first kallisto measure  empirical frequency  mers  estimate   first  million pseudoalignable read  apply  bias correction  use  initial estimate   abundance use  round    algorithm  bias  mers  use  adjust  effective length   transcript  add  bias   mer   strand  account  edge effect kallisto  add  mers   start    length   transcript minus  average fragment length  process  repeat     update expression estimate   round    algorithm bootstrap  bootstrap  highly efficient  kallisto     algorithm   fast    sufficient statistics   model   equivalence class count  latter fact mean  bootstrap sample    rapidly generate  pseudoalignment   fragment  complete    original fragment   categorize  equivalence class generate  new bootstrap sample consist  sample  count   multinomial distribution   equivalence class   probability   class  proportional   count   original data  transcript abundances   new sample   recomputed use   algorithm  kallisto  number  bootstrap   perform   option pass   program    large amount  data   produce  output  compress  hdf5  hdf5 file   read  another program  process  example     convert  plain text use kallisto",6
147,GASiC,"Metagenomic abundance estimation and diagnostic testing on species level
The GASiC workflow is depicted in Figure 1. As in most reference-based methods, the reads are first aligned to every genome in a set of references and the number of reads matching to each genome is counted. We call these counts the ‘observed abundances’, as opposed to the ‘abundance estimates’ which we want to obtain in the end. In the next step, GASiC constructs a similarity matrix encoding the alignment similarities between the reference sequences. The similarity matrix and the observed abundances are then used together in a linear system of equations, where GASiC solves for the corrected abundances using a constrained optimization routine to obtain the estimates. The whole procedure can be iterated using bootstrap (9) samples from the original dataset. This yields more stable abundance estimates and provides an intuitive non-parametric statistical test for the presence of a species. Figure 1. GASiC workflow. Metagenomic reads are first aligned to the reference genomes and matching reads are counted for each genome (observed abundances). GASiC then uses the reference genomes to construct a similarity matrix encoding the genome similarities while considering influences of the applied sequencing technology. The similarity matrix and the observed abundances are used in a linear system of equations to model the influence of reference genome similarities on read alignment. GASiC solves the system of equations using a constrained optimization routine to calculate the estimated true abundances of the reference genomes in the dataset. Bootstrapping from the reads delivers stable abundance estimates and allows GASiC to test for the presence of each species in the dataset. Open in new tabDownload slide GASiC workflow. Metagenomic reads are first aligned to the reference genomes and matching reads are counted for each genome (observed abundances). GASiC then uses the reference genomes to construct a similarity matrix encoding the genome similarities while considering influences of the applied sequencing technology. The similarity matrix and the observed abundances are used in a linear system of equations to model the influence of reference genome similarities on read alignment. GASiC solves the system of equations using a constrained optimization routine to calculate the estimated true abundances of the reference genomes in the dataset. Bootstrapping from the reads delivers stable abundance estimates and allows GASiC to test for the presence of each species in the dataset. We first introduce some notation that will be used in the following. Starting from the experiment side, the sequencing dataset is denoted as D, containing N reads in total. The reads may originate from a set of M Species forumla with known reference sequences or possibly from other sources (noise, contaminants) with no relation to any species in S. forumla is synonymously used for both the species itself as well as its reference sequence. For quantification of species we use the term ‘abundance’, which is the number of reads belonging to the species divided by the total number of reads N. Due to amplification biases, this abundance may not represent the true absolute abundance of the species in the data, but may be valuable when comparing abundances of multiple (in particular similar) species. Alignment The reads in D are aligned to all species S with an alignment method suitable for the characteristics of D. Then, we count the number of reads forumla from D that were successfully aligned to forumla irrespective of the number of matching positions in forumla or matches to other species. In particular, we neither restrict ourselves to unique matches only, nor assume any phylogenetic structure within the forumla⁠, as is done for example in MEGAN. If the dataset only contains very dissimilar species, the read counts forumla may already be suitable estimates for the true abundances. Otherwise, the forumla are in general highly disturbed and dominated by shared matches, such that the forumla cannot directly be used as abundance estimates. Similarity estimation A proper similarity estimation of the reference sequences is required to achieve accurate similarity correction of the forumla⁠. The similarities between sequences are encoded in a similarity matrix forumla⁠, where forumla denotes the probability that a read drawn from forumla can be aligned to forumla⁠. In practice, we simulate a set of reads from every reference forumla with a read simulator which is able to imitate the sequencing technology and error characteristics of D. For example, Mason (10) and Grinder (11) simulate Illumina, 454 and Sanger reads; and dwgsim (sourceforge.net/projects/dnaa/) simulates Illumina, ABI SOLiD and IonTorrent reads. Then, we align the simulated reads of forumla to forumla using the very same settings as for aligning the reads in dataset D and count the number of matching reads forumla The matrix entries are then estimated as forumla The key element of similarity estimation is a proper read simulation since we use the simulated reads to estimate the reference genome similarities, the source of ambiguous alignments. Thus, the simulated reads should have the read characteristics and the error characteristics of the instrument (read length, paired/single end, etc.) and should cover the reference genome at least once. For very complex metagenomic communities with a high number of species M, the calculation of the complete similarity matrix may become infeasible because of its computational complexity forumla We recommend to first estimate similarities using, for example, fast k-mer-based methods (12) and refine the estimates via the simulation approach only for genomes with sufficiently high (e.g. forumla⁠) similarity. Similarity correction We introduce a linear model to correct the forumla for the genome similarity using the similarity matrix A. Let forumla denote the true, but unknown, abundance of species forumla⁠. We then assume that the observed abundance forumla is a mixture of the true abundances forumla of all species forumla weighted with the estimated probability forumla that a read from j can be aligned to i: formula To simplify notation, we use a matrix representation of the true and the observed abundances, i.e. forumla and forumla In matrix notation, this can be written as formula Since direct inversion of the matrix A may result in instable abundance estimates, we formulate the solution for c as a non-negative LASSO (13,14) problem: formula formula The constraints enforce the result to be meaningful, i.e. each estimated relative abundance forumla must be equal to or greater than zero and the sum of all relative abundances must be less than or equal to one. The first conditions also ensure that the correction produces abundances lower than or equal to the measured abundances. The last condition allows the presence of reads from a totally unrelated species, since the abundances are allowed to sum up to less than or equal to one. It also enforces the sparsity of results such that only meaningful contributions have abundances larger than 0. We solve the constraint optimization problem with the COBYLA method implemented in SciPy (www.scipy.org/). Error estimation and testing We apply a bootstrapping procedure on the steps described before, first, to estimate how errors in the input data propagate through the correction algorithm and, second, to calculate P-values to test for the presence of a species in the sample. To this end, we generate B bootstrap samples from the dataset D and perform similarity correction for each sample separately, yielding a distribution forumla of abundances for each species i. We calculate the average abundance forumla and estimate the standard error forumla To test whether a species is present in the sample, we count how many bootstrap samples yielded a higher abundance than an a priori defined detection threshold t: formula Quality check As the composition of the reference genome set is critical for the complete method, GASiC offers an additional quality check after the alignment to reference genomes. The quality check step analyses the outputted SAM files of the read alignment tool and provides helpful statistics to the user to judge the appropriateness of the results. Besides reporting statistical measures, such as the number of mapped reads or the average genome coverage, GASiC generates a coverage histogram which often allows the user to exclude certain genomes from the reference set or to detect possibly important missing reference genomes. For example, a high number of uncovered bases in combination with a typical Poisson distribution at higher coverage may indicate that the considered species is not contained in the dataset, but a closely related species. In addition to the statistics and the histogram, GASiC produces warning messages in critical setups, e.g. when the dataset may be too small for abundance estimation or large parts of the genome are not covered although there is evidence for the genome in the dataset. Technical details We implemented GASiC in the Python programming language (www.python.org), making extensive use of the high performance scientific computing libraries SciPy and NumPy (www.scipy.org). Since GASiC is independent from the choice of the alignment algorithm and read simulator, we already integrated interfaces to a set of tools. The user can add custom interfaces easily, a brief manual is provided within the code. We set value on comprehensible and well-documented code, such that GASiC can easily be adapted to the users needs without deeper knowledge of Python. GASiC requires the widespread SAM alignment format (15) as output from the alignment tool to analyse the results, since most alignment tools either directly support SAM output or alignment results can be readily converted into SAM files.",AbundanceEstimation,"metagenomic abundance estimation  diagnostic test  species level
 gasic workflow  depict  figure     referencebased methods  read  first align  every genome   set  reference   number  read match   genome  count  call  count  observe abundances  oppose   abundance estimate   want  obtain   end   next step gasic construct  similarity matrix encode  alignment similarities   reference sequence  similarity matrix   observe abundances   use together   linear system  equations  gasic solve   correct abundances use  constrain optimization routine  obtain  estimate  whole procedure   iterate use bootstrap  sample   original dataset  yield  stable abundance estimate  provide  intuitive nonparametric statistical test   presence   species figure  gasic workflow metagenomic read  first align   reference genomes  match read  count   genome observe abundances gasic  use  reference genomes  construct  similarity matrix encode  genome similarities  consider influence   apply sequence technology  similarity matrix   observe abundances  use   linear system  equations  model  influence  reference genome similarities  read alignment gasic solve  system  equations use  constrain optimization routine  calculate  estimate true abundances   reference genomes   dataset bootstrapping   read deliver stable abundance estimate  allow gasic  test   presence   species   dataset open  new tabdownload slide gasic workflow metagenomic read  first align   reference genomes  match read  count   genome observe abundances gasic  use  reference genomes  construct  similarity matrix encode  genome similarities  consider influence   apply sequence technology  similarity matrix   observe abundances  use   linear system  equations  model  influence  reference genome similarities  read alignment gasic solve  system  equations use  constrain optimization routine  calculate  estimate true abundances   reference genomes   dataset bootstrapping   read deliver stable abundance estimate  allow gasic  test   presence   species   dataset  first introduce  notation    use   follow start   experiment side  sequence dataset  denote   contain  read  total  read may originate   set   species forumla  know reference sequence  possibly   source noise contaminants   relation   species   forumla  synonymously use    species   well   reference sequence  quantification  species  use  term abundance    number  read belong   species divide   total number  read  due  amplification bias  abundance may  represent  true absolute abundance   species   data  may  valuable  compare abundances  multiple  particular similar species alignment  read    align   species    alignment method suitable   characteristics     count  number  read forumla     successfully align  forumla irrespective   number  match position  forumla  match   species  particular  neither restrict   unique match   assume  phylogenetic structure within  forumla⁠     example  megan   dataset  contain  dissimilar species  read count forumla may already  suitable estimate   true abundances otherwise  forumla   general highly disturb  dominate  share match    forumla cannot directly  use  abundance estimate similarity estimation  proper similarity estimation   reference sequence  require  achieve accurate similarity correction   forumla⁠  similarities  sequence  encode   similarity matrix forumla⁠  forumla denote  probability   read draw  forumla   align  forumla⁠  practice  simulate  set  read  every reference forumla   read simulator   able  imitate  sequence technology  error characteristics    example mason   grinder  simulate illumina   sanger read  dwgsim sourceforgenetprojectsdnaa simulate illumina abi solid  iontorrent read   align  simulate read  forumla  forumla use    settings   align  read  dataset   count  number  match read forumla  matrix entries   estimate  forumla  key element  similarity estimation   proper read simulation since  use  simulate read  estimate  reference genome similarities  source  ambiguous alignments thus  simulate read    read characteristics   error characteristics   instrument read length pairedsingle end etc   cover  reference genome  least    complex metagenomic communities   high number  species   calculation   complete similarity matrix may become infeasible    computational complexity forumla  recommend  first estimate similarities use  example fast kmerbased methods   refine  estimate via  simulation approach   genomes  sufficiently high  forumla⁠ similarity similarity correction  introduce  linear model  correct  forumla   genome similarity use  similarity matrix  let forumla denote  true  unknown abundance  species forumla⁠   assume   observe abundance forumla   mixture   true abundances forumla   species forumla weight   estimate probability forumla   read     align   formula  simplify notation  use  matrix representation   true   observe abundances  forumla  forumla  matrix notation    write  formula since direct inversion   matrix  may result  instable abundance estimate  formulate  solution     nonnegative lasso  problem formula formula  constraints enforce  result   meaningful   estimate relative abundance forumla must  equal   greater  zero   sum   relative abundances must  less   equal  one  first condition also ensure   correction produce abundances lower   equal   measure abundances  last condition allow  presence  read   totally unrelated species since  abundances  allow  sum   less   equal  one  also enforce  sparsity  result    meaningful contributions  abundances larger    solve  constraint optimization problem   cobyla method implement  scipy wwwscipyorg error estimation  test  apply  bootstrapping procedure   step describe  first  estimate  errors   input data propagate   correction algorithm  second  calculate pvalues  test   presence   species   sample   end  generate  bootstrap sample   dataset   perform similarity correction   sample separately yield  distribution forumla  abundances   species   calculate  average abundance forumla  estimate  standard error forumla  test whether  species  present   sample  count  many bootstrap sample yield  higher abundance    priori define detection threshold  formula quality check   composition   reference genome set  critical   complete method gasic offer  additional quality check   alignment  reference genomes  quality check step analyse  output sam file   read alignment tool  provide helpful statistics   user  judge  appropriateness   result besides report statistical measure    number  map read   average genome coverage gasic generate  coverage histogram  often allow  user  exclude certain genomes   reference set   detect possibly important miss reference genomes  example  high number  uncover base  combination   typical poisson distribution  higher coverage may indicate   consider species   contain   dataset   closely relate species  addition   statistics   histogram gasic produce warn message  critical setups    dataset may   small  abundance estimation  large part   genome   cover although   evidence   genome   dataset technical detail  implement gasic   python program language wwwpythonorg make extensive use   high performance scientific compute libraries scipy  numpy wwwscipyorg since gasic  independent   choice   alignment algorithm  read simulator  already integrate interfaces   set  tool  user  add custom interfaces easily  brief manual  provide within  code  set value  comprehensible  welldocumented code   gasic  easily  adapt   users need without deeper knowledge  python gasic require  widespread sam alignment format   output   alignment tool  analyse  result since  alignment tool either directly support sam output  alignment result   readily convert  sam file",6
148,Salmon,"Salmon: Accurate, Versatile and Ultrafast Quantification from RNA-seq Data using Lightweight-Alignment
Salmon consists of three components: a lightweight-alignment model, an online phase that estimates initial expression levels and model parameters and constructs equivalence classes over the input fragments, and an offline phase that refines the expression estimates. The online and offline phases together optimize the estimates of the latent parameters α, and each method can compute η directly from these parameters. The online phase uses a variant of stochastic, collapsed variational Bayesian inference [7]. The offline phase applies the variational Bayesian EM algorithm [12] over a reduced representation of the data represented by the equivalence classes until a data-dependent convergence criterion is satisfied. An overview of our method is given in Figure 1, and we describe each component in more detail below. Figure 1: Download figureOpen in new tab Figure 1: Overview of Salmon’s method and components. Salmon excepts either raw or aligned reads as input, performs an online inference when processing fragments or alignments, builds equivalence classes over these fragments and subsequently refines abundance estimates using an offline inference algorithm on a reduced representation of the data. 4.1 Lightweight-alignment A key computational challenge in inferring relative transcript abundances is to determine the potential loci-of-origin for a sequenced fragment. To make the optimization tractable, all positions cannot be considered. However, if the sequence of a fragment is substantially different from the sequence of a given transcript at a particular position, it is very unlikely that the fragment originated from this transcript and position — these positions will have their probability truncated to 0 and will be omitted from the optimization. Determining a set of potential loci-of-origin for a sequenced fragment is typically done by aligning the reads to the genome or transcriptome using tools like Bowtie2 [13], STAR [9], or HISAT [10]. While Salmon can process the alignments generated by such tools (when they are given with respect to the transcriptome), it provides another method to determine the potential loci-of-origin of the fragments directly, using a procedure that we call lightweight-alignment. The main motivation behind lightweight-alignment is that achieving accurate quantification of transcript abundance from RNA-seq data does not require knowing the optimal alignment between the sequenced fragment and the transcript for every potential locus of origin. Rather, simply knowing which transcripts (and positions within these transcripts) match the fragments reasonably well is sufficient. Formally, we define lightweight-alignment as a procedure that, given the transcripts Ƭ and a fragment fi, returns a set of 3-tuples A (Τ, fi) = {(ti1, pi1, si1), …,}. Each tuple consists of 3 elements: a transcript ti′, a position pi′ within this transcript, and a score si′ that summarizes the quality of the match between fi and ti′ at position pi′. We describe, here, the lightweight-alignment approach for a single read (it extends naturally to paired-end reads by looking for lightweight-alignments for read pairs that are appropriately positioned on the same transcript). Salmon attempts to find a chain of super-maximal exact matches (SMEMs) and maximal exact matches (MEMs) that cover a read. Recall, a maximal exact match is a substring that is shared by the query (read) and reference (transcript) that cannot be extended in either direction without introducing a mismatch. A super-maximal exact match [14] is a MEM that is not contained within any other MEM on the query. Salmon attempts to cover the read using SMEMs. Differences — whether due to read errors or true variation of the sample being sequenced from the reference — will often prevent SMEMs from spanning an entire read. However, one will often be able to find approximately-consistent, co-linear chains of SMEMs that are shared between the read and target transcripts. A chain of SMEMs is a collection of 3-tuples c = {(q1, t1, ℓ1), … } where each qi is a position on the query (read), ti is a position on the reference (transcript), and ℓi is the length of the SMEM. If ∑i |(qi+1 − qi) − (ti+1 − ti)| = 0, then we say that the chains are consistent — the space between the location of SMEMs on the query and the reference are the same. If, instead, we require that ∑i |(qi+1 − qi) − (ti+1 − ti)| = δ, then we say that the chain is approximately consistent, or δ-consistent. Consistent chains can deal only with substitution errors and mutations, while δ-consistent chains can also account for indels. Figure 2 shows an example. Figure 2: Download figureOpen in new tab Figure 2: A δ-consistent chain of matches covering the read. Here, the coverage (score) of the chain is Embedded Image, and Embedded Image. While the discussion above is in terms of SMEMs, the chains constructed by Salmon typically consist of a mix of SMEMs and MEMs. This is because, like BWA-mem [14], Salmon breaks SMEMs that are too large (by default, greater than 1.5 times the minimum required MEM length), to prevent them from masking potentially high-scoring MEM chains. In order for Salmon to consider a read to match a transcript locus sufficiently well, there must be a δ-consistent chain between the read and the transcript sequence, beginning at the locus, that covers a user-specified fraction of the read (65% by default). Using this procedure, Salmon implements lightweight-alignment by finding, for a fragment fi, all transcript position pairs (ti′, pi′) that share a δ-consistent chain with fi covering at least fraction c of the fragment. The score, si′, of this lightweight-alignment is simply the fraction of the fragment covered by the chain. Salmon searches for SMEMs using the FMD-index [15]. Specifically, Salmon uses a slightly-modified version of the BWA [15] index, replacing the default sparse sampling with a dense sampling to improve speed. When Salmon is run in lightweight-alignment mode, one must have first prepared an index for the target transcriptome against which lightweight-alignment is to be performed. The Salmon index is built using the index command of Salmon. Unlike k-mer-based indices (e.g. as used in Sailfish [5] or Kallisto [11]), the parameters for lightweight-alignment (e.g. the fraction of the read required to be covered, or the minimum length MEMs considered in chains) can be modified without re-building the index. This allows one to easily modify the sensitivity and specificity of the lightweight-alignment procedure without the need to re-create the index (which often takes longer than quantification). 4.2 Online phase The online phase of Salmon attempts to solve the variational Bayesian inference problem described in section 3, and optimizes a collapsed variational objective function [3] using a variant of stochastic collapsed Variational Bayesian inference [7]. The inference procedure is a streaming algorithm that updates estimated read counts α after every small group B t (called a mini-batch) of observations. The pseudo-code for the algorithm is given in Algorithm 1. Algorithm 1 Laissez-faire SCVB0 1: while B t ← pop(work-queue) do 2: Embedded Image 3: for read r ∈ Bt do 4: x ← 0 5: for alignment a of r do 6: y ← the transcript involved in alignment a 7: xy ← xy + αy · Pr {a | y} ▹ Add a’s contribution to the local weight for transcript y 8: end for ▹ Normalize the contributions for all alignments of r 9: for alignment a of r do 10: y ← the transcript involved in alignment a 11: Embedded Image 12: end for 13: Sample a ∈ r and update auxiliary models using a 14: end for 15: Embedded Image ▹ Update the global weights with local observations from Bt 16: end while The observation weight for mini-batch t, vt, in line 15 of Algorithm 1 is an increasing sequence sequence in t, and is set, as in [4], to adhere to the Robbins-Monroe conditions. Here, the α represent the (weighted) estimated counts of fragments originating from each transcript. Using this method, the expected value of η can be computed directly from α using equation 16. We employ a weak Dirichlet conjugate-prior with Embedded Image for all ti ∈ Ƭ As outlined in [7], the SCVB0 inference algorithm is similar to variants of the online-EM [16] algorithm with a modified prior. The procedure in algorithm 1 is run independently by as many worker threads as the user has specified. The threads share a single work-queue upon which a parsing thread places mini-batches of alignment groups. An alignment group is simply the collection of all alignments (i.e. all multi-mapping locations) for a particular read. The mini-batch itself consists of a collection of some small, fixed number of alignment groups (1, 000 by default). Each worker thread processes one alignment group at a time, using the current weights of each transcript and the current auxiliary parameters to estimate the probability that a read came from each potential transcript of origin. The processing of mini-batches occurs in parallel, so that very little synchronization is required, only an atomic compare-and-swap loop to update the global transcript weights at the end of processing of each mini-batch — hence the moniker laissez-faire. This lack of synchronization means that when estimating xy, we can not be certain that the most up-to-date values of α are being used. However, due to the stochastic and additive nature of the updates, this has little to no detrimental effect [17]. The inference procedure itself is generic over the type of alignments being processed; they may be either regular alignments (e.g. coming from a bam file), or lightweight-alignments generated as described in section 4.1 above. After the entire mini-batch has been processed, the global weights for each transcript a are updated. These updates are sparse; i.e. only transcripts which appeared in some alignment in mini-batch Bt will have their global weight updated after B t has been processed. This ensures, as in [4], that updates to the parameters α can be performed efficiently. 4.3 Streaming determination of equivalence classes during the online phase During its online phase, in addition to performing streaming inference of transcript abundances, Salmon also constructs a highly-reduced representation of the sequencing experiment. Specifically, Salmon constructs “rich” equivalence classes over all of the sequenced fragments. We define an equivalence relation ∼ over fragments. Let M (fx) = {ti | (ti, pi, si) ∈ A (Τ, fi)} be the set of transcripts to which fx maps (this can also be analogously computed using traditional alignments). We say fx ∼ fy if and only if M (fx) = M (fy). Related, but distinct notions of alignment-based equivlance classes have been introduced previously (e.g. [18]), and shown to greatly reduce the time required to perform iterative optimization such as that described in Section 4.4. Fragments which are equivalent can be grouped together for the purpose of inference. Salmon builds up a set of fragment-level equivalence classes by maintaining an efficient concurrent cuckoo hash map [19]. To construct this map, we associate each fragment fx with t x = M (fx), which we will call the label of the fragment. Then, we query the hash map for t x. If this key is not in the map, we create a new equivalence class with this label, and set its count to 1. Otherwise, we increment the count of the equivalence class with this label that we find in the map. The efficient, concurrent nature of the data structure means that many threads can simultaneously query and write to the map while encountering very little contention. Each key in the hash map is associated with a value that we call a “rich” equivalence class. For each equivalence class 𝒞j, we retain a count d j, which is the total number of fragments contained within this class. We also maintain, for each class, a weight vector w j. The entries of this vector are in one-to-one correspondence with transcripts i in the label of this equivalence class such that Embedded Image That is, Embedded Image is the average conditional probability of observing a fragment from 𝒞j given ti over all fragments in this equivalence class. Since the fragments in 𝒞j are all exchangeable, the pairing between the conditional probability for a particular fragment and a particular transcript need not be maintained, as the following series of equalities holds: Embedded Image Thus, the aggregate weights stored in the “rich” equivalence classes gives us the power of considering the conditional probabilities specified in the full model, without having to continuously reconsider each of the fragments in ℱ. 4.4 Offline phase In its offline phase, Salmon uses the “rich” equivalence classes learned during the online phase to refine the inference. Given the set 𝒞 of rich equivalence classes of fragments, we can use an expectation maximization (EM) algorithm to optimize the likelihood of the parameters given the data. The abundances η can be computed directly from α, and we compute maximum likelihood estimates of these parameters which represent the estimated counts (i.e. number of fragments) deriving from each transcript, where: Embedded Image and Embedded Image. If we write this same likelihood in terms of the equivalence classes 𝒞, we have: Embedded Image EM update rule This likelihood, and hence that represented in eq. (9), can then be optimized by applying the following update equation iteratively Embedded Image We apply this update equation until the maximum relative difference in the α parameters satisfies: Embedded Image for all Embedded Image. Let α′ be the estimates after having achieved convergence. We can then approximate ηi by Embedded Image, where: Embedded Image 1 Variational Bayes optimization Instead of the standard EM updates of eq. (11), we can, optionally, perform Variational Bayesian optimization by applying VBEM updates as in [2], but adapted to be with respect to the equivalence classes: Embedded Image where Embedded Image Here, Ψ (·) is the digamma function, and, upon convergence of the parameters, we can obtain an estimate of the expected value of the posterior nucleotide fractions as: Embedded Image where Embedded Image. Variational Bayesian optimization in the offline-phase of Salmon is selected by passing the --useVBOpt flag to the Salmon quant command. 4.5 Sampling from the posterior After the convergence of the parameter estimates has been achieved in the offline phase, it is possible to draw samples from the posterior distribution using collapsed, blockwise Gibbs sampling over the equivalence classes. Samples can be drawn by iterating over the equivalence classes, and re-sampling assignments for some fraction of fragments in each class according to the multinomial distribution defined by holding the assignments for all other fragments fixed. Many samples can be drawn quickly, since many Gibbs chains can be run in parallel. Further, due to the accuracy of the preceding inference, the chains begin sampling from a good position in the latent variable space almost immediately. These posterior samples can be used to obtain estimates for quantities of interest about the posterior distribution, such as its variance, or to produce confidence intervals. When Salmon is passed the --useGSOpt parameter, it will draw a number of posterior samples that can be specified with the --numGibbsSamples parameter.",AbundanceEstimation,"salmon accurate versatile  ultrafast quantification  rnaseq data use lightweightalignment
salmon consist  three components  lightweightalignment model  online phase  estimate initial expression level  model parameters  construct equivalence class   input fragment   offline phase  refine  expression estimate  online  offline phase together optimize  estimate   latent parameters    method  compute  directly   parameters  online phase use  variant  stochastic collapse variational bayesian inference   offline phase apply  variational bayesian  algorithm    reduce representation   data represent   equivalence class   datadependent convergence criterion  satisfy  overview   method  give  figure    describe  component   detail  figure  download figureopen  new tab figure  overview  salmons method  components salmon except either raw  align read  input perform  online inference  process fragment  alignments build equivalence class   fragment  subsequently refine abundance estimate use  offline inference algorithm   reduce representation   data  lightweightalignment  key computational challenge  infer relative transcript abundances   determine  potential locioforigin   sequence fragment  make  optimization tractable  position cannot  consider however   sequence   fragment  substantially different   sequence   give transcript   particular position    unlikely   fragment originate   transcript  position —  position    probability truncate      omit   optimization determine  set  potential locioforigin   sequence fragment  typically   align  read   genome  transcriptome use tool like bowtie2  star   hisat   salmon  process  alignments generate   tool    give  respect   transcriptome  provide another method  determine  potential locioforigin   fragment directly use  procedure   call lightweightalignment  main motivation behind lightweightalignment   achieve accurate quantification  transcript abundance  rnaseq data   require know  optimal alignment   sequence fragment   transcript  every potential locus  origin rather simply know  transcripts  position within  transcripts match  fragment reasonably well  sufficient formally  define lightweightalignment   procedure  give  transcripts    fragment  return  set  tuples     {ti1 pi1 si1 …}  tuple consist   elements  transcript ′  position ′ within  transcript   score ′  summarize  quality   match    ′  position ′  describe   lightweightalignment approach   single read  extend naturally  pairedend read  look  lightweightalignments  read pair   appropriately position    transcript salmon attempt  find  chain  supermaximal exact match smems  maximal exact match mems  cover  read recall  maximal exact match   substring   share   query read  reference transcript  cannot  extend  either direction without introduce  mismatch  supermaximal exact match    mem    contain within   mem   query salmon attempt  cover  read use smems differences — whether due  read errors  true variation   sample  sequence   reference —  often prevent smems  span  entire read however one  often  able  find approximatelyconsistent colinear chain  smems   share   read  target transcripts  chain  smems   collection  tuples   {   … }      position   query read    position   reference transcript     length   smem  ∑            say   chain  consistent —  space   location  smems   query   reference     instead  require  ∑            say   chain  approximately consistent  δconsistent consistent chain  deal   substitution errors  mutations  δconsistent chain  also account  indels figure  show  example figure  download figureopen  new tab figure   δconsistent chain  match cover  read   coverage score   chain  embed image  embed image   discussion    term  smems  chain construct  salmon typically consist   mix  smems  mems    like bwamem  salmon break smems    large  default greater   time  minimum require mem length  prevent   mask potentially highscoring mem chain  order  salmon  consider  read  match  transcript locus sufficiently well  must   δconsistent chain   read   transcript sequence begin   locus  cover  userspecified fraction   read   default use  procedure salmon implement lightweightalignment  find   fragment   transcript position pair ′ ′  share  δconsistent chain   cover  least fraction    fragment  score ′   lightweightalignment  simply  fraction   fragment cover   chain salmon search  smems use  fmdindex  specifically salmon use  slightlymodified version   bwa  index replace  default sparse sample   dense sample  improve speed  salmon  run  lightweightalignment mode one must  first prepare  index   target transcriptome   lightweightalignment    perform  salmon index  build use  index command  salmon unlike kmerbased indices   use  sailfish   kallisto   parameters  lightweightalignment   fraction   read require   cover   minimum length mems consider  chain   modify without rebuild  index  allow one  easily modify  sensitivity  specificity   lightweightalignment procedure without  need  recreate  index  often take longer  quantification  online phase  online phase  salmon attempt  solve  variational bayesian inference problem describe  section   optimize  collapse variational objective function  use  variant  stochastic collapse variational bayesian inference   inference procedure   stream algorithm  update estimate read count   every small group   call  minibatch  observations  pseudocode   algorithm  give  algorithm  algorithm  laissezfaire scvb0      popworkqueue   embed image   read  ∈         alignment         transcript involve  alignment          {  } ▹ add  contribution   local weight  transcript   end  ▹ normalize  contributions   alignments     alignment         transcript involve  alignment   embed image  end   sample  ∈   update auxiliary model use   end   embed image ▹ update  global weight  local observations    end   observation weight  minibatch    line   algorithm    increase sequence sequence     set     adhere   robbinsmonroe condition    represent  weight estimate count  fragment originate   transcript use  method  expect value     compute directly   use equation   employ  weak dirichlet conjugateprior  embed image    ∈   outline    scvb0 inference algorithm  similar  variants   onlineem  algorithm   modify prior  procedure  algorithm   run independently   many worker thread   user  specify  thread share  single workqueue upon   parse thread place minibatches  alignment group  alignment group  simply  collection   alignments   multimapping locations   particular read  minibatch  consist   collection   small fix number  alignment group    default  worker thread process one alignment group   time use  current weight   transcript   current auxiliary parameters  estimate  probability   read come   potential transcript  origin  process  minibatches occur  parallel    little synchronization  require   atomic compareandswap loop  update  global transcript weight   end  process   minibatch — hence  moniker laissezfaire  lack  synchronization mean   estimate      certain    uptodate value     use however due   stochastic  additive nature   update   little   detrimental effect   inference procedure   generic   type  alignments  process  may  either regular alignments  come   bam file  lightweightalignments generate  describe  section     entire minibatch   process  global weight   transcript   update  update  sparse   transcripts  appear   alignment  minibatch     global weight update      process  ensure     update   parameters    perform efficiently  stream determination  equivalence class   online phase   online phase  addition  perform stream inference  transcript abundances salmon also construct  highlyreduced representation   sequence experiment specifically salmon construct “rich” equivalence class     sequence fragment  define  equivalence relation   fragment let    {     ∈   }   set  transcripts    map   also  analogously compute use traditional alignments  say             relate  distinct notions  alignmentbased equivlance class   introduce previously    show  greatly reduce  time require  perform iterative optimization    describe  section  fragment   equivalent   group together   purpose  inference salmon build   set  fragmentlevel equivalence class  maintain  efficient concurrent cuckoo hash map   construct  map  associate  fragment           call  label   fragment   query  hash map      key     map  create  new equivalence class   label  set  count   otherwise  increment  count   equivalence class   label   find   map  efficient concurrent nature   data structure mean  many thread  simultaneously query  write   map  encounter  little contention  key   hash map  associate   value   call  “rich” equivalence class   equivalence class   retain  count      total number  fragment contain within  class  also maintain   class  weight vector    entries   vector   onetoone correspondence  transcripts    label   equivalence class   embed image   embed image   average conditional probability  observe  fragment   give    fragment   equivalence class since  fragment     exchangeable  pair   conditional probability   particular fragment   particular transcript need   maintain   follow series  equalities hold embed image thus  aggregate weight store   “rich” equivalence class give   power  consider  conditional probabilities specify   full model without   continuously reconsider    fragment    offline phase   offline phase salmon use  “rich” equivalence class learn   online phase  refine  inference give  set   rich equivalence class  fragment   use  expectation maximization  algorithm  optimize  likelihood   parameters give  data  abundances    compute directly     compute maximum likelihood estimate   parameters  represent  estimate count  number  fragment derive   transcript  embed image  embed image   write   likelihood  term   equivalence class    embed image  update rule  likelihood  hence  represent       optimize  apply  follow update equation iteratively embed image  apply  update equation   maximum relative difference    parameters satisfy embed image   embed image let ′   estimate   achieve convergence    approximate   embed image  embed image  variational bay optimization instead   standard  update      optionally perform variational bayesian optimization  apply vbem update     adapt    respect   equivalence class embed image  embed image      digamma function  upon convergence   parameters   obtain  estimate   expect value   posterior nucleotide fraction  embed image  embed image variational bayesian optimization   offlinephase  salmon  select  pass  usevbopt flag   salmon quant command  sample   posterior   convergence   parameter estimate   achieve   offline phase   possible  draw sample   posterior distribution use collapse blockwise gibbs sample   equivalence class sample   draw  iterate   equivalence class  resampling assignments   fraction  fragment   class accord   multinomial distribution define  hold  assignments    fragment fix many sample   draw quickly since many gibbs chain   run  parallel  due   accuracy   precede inference  chain begin sample   good position   latent variable space almost immediately  posterior sample   use  obtain estimate  quantities  interest   posterior distribution    variance   produce confidence intervals  salmon  pass  usegsopt parameter   draw  number  posterior sample    specify   numgibbssamples parameter",6
149,Sailfish,"Sailfish enables alignment-free isoform quantification from RNA-seq reads using lightweight algorithms
Indexing. The first step in the Sailfish pipeline is building an index from the set of reference transcripts T. Given a k-mer length k, we compute an index Ik(T) containing four components. The first component is a minimum perfect hash function h on the set of k-mers kmers(T) contained in T. A minimum perfect hash function is a bijection between kmers(T) and the set of integers {0,1,..., |kmers(T)| – 1}. Sailfish uses the BDZ minimum perfect hash function9. The second component of the index is an array C containing a count C(si) for every si∈ kmers(T). Finally, the index contains a lookup table mapping each transcript to the multiset of k-mers that it contains, and a reverse lookup table mapping each k-mer to the set of transcripts in which it appears. The index is a product only of the reference transcripts and the choice of k, and thus needs only to be recomputed when either of these change. Quantification. The second step in the Sailfish pipeline is the quantification of relative transcript abundance; this requires the Sailfish index Ik(T) for the reference transcripts T as well as a set of RNA-seq reads . First, we count the number of occurrences of each si ∈ kmers(T) ∩ kmers(). Because we know exactly the set of k-mers that need to be counted and already have a perfect hash function h for this set, we can perform this counting in a particularly efficient manner, even faster than efficient hash-based approaches. For example, performing concurrent 20-mer lookups using 8 threads, Jellyfish10 requires an average of 0.35 μs/key while the minimal perfect hash requires an average of 0.1 μs/key. We maintain an array  of the appropriate size |kmers(T)|, where  contains the number of times we have thus far observed si in . In an unstranded protocol, sequencing reads, and hence the k-mers they contain, may originate from transcripts in either the forward or reverse direction. To account for both possibilities, we check both the forward and reverse-complement k-mers from each read and use a majority-rule heuristic to determine which of the k-mers to increment in the final array of counts . If the number of k-mers appearing in h from the forward direction of the read is greater than the number of reverse-complement k-mers, then we only increment the counts for k-mers appearing in this read in the forward direction. Otherwise, only counts for k-mers appearing in the reverse-complement of this read are incremented in the array of counts. Ties are broken in favor of the forward-directed reads. In a stranded RNA-seq protocol, the reads have a known orientation. Reads provided to Sailfish can be specified as originating from unstranded, forward-strand or reverse-strand reads, and the appropriate k-mers are counted in each case. By taking advantage of atomic integers and the compare-and-swap (CAS) operation provided by modern processors, which allows many hardware threads to efficiently update the value of a memory location without the need for explicit locking, we can stream through and update the counts in  in parallel while sustaining very little resource contention. We then apply an expectation-maximization (EM) algorithm to obtain estimates of the relative abundance of each transcript. We define a k-mer equivalence class as the set of all k-mers that appear in the same set of transcripts with the same frequency. In other words, let ×(s) be a vector such that entry t of ×(s) gives how many times k-mer s appears in transcript t∈ T. Then the equivalence class of a k-mer si is given by [si] = {sj∈ kmers(T) : ×(sj) = ×(si)}. When performing the EM procedure, we will allocate counts to transcripts according to the set of equivalence classes rather than the full set of k-mers. We will let denote the total count of k-mers in  that originate from equivalence class [si]. We say that transcript t contains equivalence class [s] if [s] is a subset of the multiset of k-mers of t and denote this by [s] ⊆ t. Estimating abundances via an EM algorithm. The EM algorithm (Algo. 1) alternates between estimating the fraction of counts of each observed k-mer that originates from each transcript (E-step) and estimating the relative abundances of all transcripts given this allocation (M-step). We initially allocate k-mers to transcripts proportional to their occurrences in the transcript (i.e., if a transcript is the only potential origin for a particular k-mer, then all observations of that k-mer are attributed to this transcript, whereas for a k-mer that appears once in each of n different transcripts and occurs m times in the set of reads, m/n observations are attributed to each potential transcript of origin). The E-step of the EM algorithm computes the fraction of each k-mer equivalence class's total count that is allocated to each transcript. For equivalence class [sj] and transcript ti, this value is computed by where μ′i is the currently estimated relative abundance of transcript i. These allocations are then used in the M-step of the algorithm to compute the relative abundance of each transcript. The relative abundance of transcript i is estimated by where μi is The variable li′ denotes the adjusted length of transcript i and is simply li′ = li − k + 1, where li is the length of transcript i in nucleotides. However, rather than perform the standard EM update steps, we perform updates according to the SQUAREM procedure11 described in Algo. 2, where μ′ = μ′0, ...,μ′|T| is a vector of relative abundance maximum-likelihood estimates, and EM(•) is a standard iteration of the expectation-maximization procedure as outlined in Algo. 1. For a detailed explanation of the SQUAREM procedure and its proof of convergence, see ref. 11. Intuitively, the SQUAREM procedure builds an approximation of the Jacobian of μ′ from three successive steps along the EM solution path, and uses the magnitude of the differences between these solutions to determine a step size by which to update the estimates according to the update rule (line 7). The procedure is then capable of making relatively large updates to the μ′ parameters, which substantially improves the speed of convergence. In Sailfish, the iterative SQUAREM procedure is repeated until a specified convergence criterion is met (by default, the procedure terminates when no transcript with a relative abundance greater than 10−7 has a relative change greater than half of a percent between consecutive iterations). Additionally, a user may specify either a fixed number of iterations to perform or a required minimum relative change, ɛ, in transcript abundance estimates between SQUAREM iterations; if no relative change in transcript abundance exceeds ɛ, then the procedure is considered to have converged and the estimation procedure terminates. Bias correction. The bias correction procedure implemented in Sailfish is based on the model introduced by Zheng et al.14. Briefly, it performs a regression analysis on a set of potential bias factors where the response variables are the estimated transcript abundances (KPKMs). Sailfish automatically considers transcript length, GC content and dinucleotide frequencies as potential bias factors, as this specific set of features was suggested by Zheng et al.14 For each transcript, the prediction of the regression model represents the contribution of the bias factors to this transcript's estimated abundance. Hence, these regression estimates (which may be positive or negative) are subtracted from the original estimates to obtain bias-corrected KPKMs. For further details on this bias correction procedure, see ref. 14. The original method used a generalized additive model for regression; Sailfish implements the approach using random forest regression to leverage high-performance implementations of this technique. The key idea here is to do the bias correction after abundance estimation rather than earlier in the pipeline. The bias correction of Sailfish can be disabled with the --no-bias-correction command line option. Finally, we note that it is possible to include other potential features, like normalized coverage plots that can encode positional bias, into the bias correction phase. However, in the current version of Sailfish, we have not implemented or tested bias correction for these features. Computing KPKM, RPKM and TPM. Sailfish outputs K-mers Per Kilobase per Million mapped k-mers (KPKM), Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million (TPM) as quantities predicting the relative abundance of different isoforms. The RPKM estimate is the most commonly used and is ideally 109 times the rate at which reads are observed at a given position, but the TPM estimate has also become somewhat common6. For Sailfish, the KPKM measure is more natural than RPKM, as the k-mer is the basic unit of transcript coverage; however, the two measures are proportional. Given the relative transcript abundances estimated by the EM procedure described above, the TPM for transcript i is given by Let Ci be the number of k-mers mapped to transcript i. Then, the KPKM is given by where N is the total count of mapped (i.e., hashable) k-mers and the final equality is approximate only because we replace li with li′. The KPKM is proportional to the RPKM, which can be estimated by replacing Ci with the estimated number of reads mapped to transcript i, and N with the estimated number of mapped reads; these can be calculated from the mapped k-mer counts and the average read length. The Fragments Per Kilobase per Million mapped fragments (FPKM) measure is also proportional to the K/RPKM, but is meant to denote that fragments rather than reads are mapped—for single end data, the reads and fragments coincide, for paired-end data, each concordant read pair is considered a single fragment. Accuracy metrics. For all experiments, the Pearson correlation metric was computed between estimates of abundance that were log transformed. As in previous work6, this was done to prevent the most abundant transcripts from dominating the correlations, but it necessitates discarding samples that have either a true or estimated abundance of 0. Spearman correlations were computed on estimates that were not log transformed. For the synthetic data, we additionally compute the Root Mean Squared Error (RMSE) and median Percentage Error. Given the vectors x = x1, x2, ..., xn of ground-truth abundances and y = y1, y2, ..., yn of estimated abundances, the RMSE is defined as The percentage error (PE) between a true abundance xi and the corresponding estimate yi is defined as Unlike the definition presented in other work12, we do not define division by 0 when computing the percentage error. We then define the medPE as the median of the percentage error for all transcripts that have a true abundance greater than 0 (i.e., the median of the values PE(xi ,yi) for all 1 ≤ i ≤ n where xi ≠ 0). When comparing against the qPCR-based abundances, we used the KPKM estimates produced by Sailfish and FPKM estimates produced by RSEM, Cufflinks and eXpress directly. The FluxSimulator records the number of reads that were actually generated by each transcript during the simulated sequencing experiment. These read counts and the transcript lengths were used to compute the ground truth FPKMs directly using equation (6). However, we found that the FPKM estimates computed by RSEM, eXpress and Cufflinks appeared, for a number of transcripts, to be systematically inflated with respect to the ground truth FPKMs (this inflation persisted even when estimates were compared directly to ground truth relative transcript fractions). This appears to be due to the effective-length normalization procedures employed by these methods. Indeed, when Cufflinks was run with --no-effective-length-correction, the RMSE and medPE decreased significantly, while the correlations with ground truth FPKMs increased marginally as well. Thus, for the synthetic tests, all Cufflinks results were computed with the --no-effective-length-correction flag enabled. Though RSEM and eXpress do not expose such a flag directly, we computed FPKM estimates without effective-length corrections for these methods by using the estimated read counts (output directly by both methods) and the true transcript lengths using equation (6). Using FPKMs computed in this manner reduced the RMSE from 2,083.04 to 8.83 and medPE from 19.03 to 14.06 for eXpress and reduced the RMSE from 18.77 to 8.90 and the medPE from 17.90 to 12.48 for RSEM. Sailfish computes the estimated k-mer counts and KPKMs in a consistent manner directly from the underlying transcript abundances, so there is no difference between the KPKMs provided by Sailfish and those that would be computed using the estimated k-mer counts directly. Simulated data. The simulated Homo sapiens RNA-seq data were generated by the Flux Simulator17 v1.2.1 with the parameters listed in Supplementary Note 3. This resulted in a dataset of 75 M 76 bp × 2 paired-end reads. The reads produced by the Flux Simulator are grouped by their origin (i.e., reads originating from the same genomic locus appear together in the read file). However, this synthetic artifact violates the assumption made by eXpress that reads are produced in a random order. Thus, we post-process the reads generated by the Flux Simulator and shuffle them (while keeping mates appropriately paired) into a random order. We also randomize the orientation of these reads to simulate an unstranded protocol. The shuffled, mate-pair reads are then split into separate files. To ensure a sufficient mapping rate under the parameters used to produce alignments for eXpress and Cufflinks (−v 3 for Bowtie and −N 3 for TopHat22), the synthetic reads were quality trimmed using SICKLE (https://github.com/najoshi/sickle) with the default parameters. Untrimmed reads are more appropriate for RSEM's default parameters, and so RSEM was provided with untrimmed reads. While mapping rates increased significantly for the trimmed reads, the overall accuracy of the methods depended little on whether trimmed or untrimmed reads were used. RSEM, eXpress and Cufflinks were all given paired-end alignments since they make special use of these data. TopHat22 was provided with the option --mate-inner-dist 200, to adjust the expected mate-pair inner-distance to the simulated average, and Bowtie was given the option −X 800 to ensure that paired-end reads originating from all valid fragments would be considered. The untrimmed synthetic read files were provided directly to Sailfish without any extra information since the same quantification procedure is used whether single or paired-end reads are provided. Software comparisons. For eXpress, all reads were aligned with Bowtie3 v0.12.9 using the parameters −a and −v 3, as suggested in the eXpress manuscript, which allows up to three mismatches per read and reports all alignments. All alignments for RSEM were computed with default parameters using the rsem-calculate-expression command, which used Bowtie v0.12.9 as the underlying aligner. To prepare alignments for Cufflinks, TopHat was run using Bowtie v0.12.9 (--bowtie1) and with options -N3 and --read-edit-dist 3 to allow up to three mismatches per read. For RSEM, eXpress and Cufflinks, the reported times are the sum of the times required for alignment and the times required for quantification. The time required for each method is further decomposed into the times for the alignment and quantification steps in Figure 2. Choice of software options and effect on runtime. Most expression estimation software, including RSEM, eXpress and Cufflinks, provides a myriad of program options to the user, which allow for trade-offs between various desiderata. For example, the total time required by TopHat and Cufflinks is lower when Cufflinks is run without bias correction (e.g., 1.92 h as opposed to 2.27 h with bias correction on the SRX016366 data). However, without bias correction, Cufflinks yields slightly lower accuracy (Pearson σ = 0.82, Spearman ρ = 0.81) than the other methods, while still taking 16 times longer to run than Sailfish. Similarly, although aligned reads can be streamed directly into eXpress via Bowtie, we empirically observed lower overall runtimes when aligning reads and quantifying expressions separately (and in sequence), so these times were reported. Finally, when we provided RSEM with the alignments used by eXpress (as was done in previous work7), we observed substantially increased runtime on the qPCR datasets (46.5 h with the alignments generated from SRX016366 and 35.6 h with the alignments generated from SRX016367), since RSEM does not appear to efficiently handle the large number of multiple alignments that can be generated by short reads when allowing many mismatches; instead, we report timings for RSEM when the alignment is performed using its own default parameters. In general, we attempted to run each piece of software with options that would be common in a standard usage scenario. However, despite the inherent difficulty of comparing a set of tools parameterized on an array of potential options, the core thesis that Sailfish can provide accurate expression estimates much faster than any existing tool remains true, as the fastest performing alternatives, even when sacrificing accuracy for speed, were over an order of magnitude slower than Sailfish. Sailfish version 0.6.3 was used for all experiments, and all analyses were done with a k-mer size of k = 20 and using 16 concurrent threads (−p 16). In all experiments involving the real but not simulated data, Sailfish was run with the --polya option, which discards k-mers consisting of k consecutive As or Ts, and bias correction was enabled. The KPKM values reported by Sailfish were used as transcript abundance estimates. RSEM6 version 1.2.3 was run with the --no-bam-output flag, and with −p 16. In the experiments involving the real data, rsem-prepare-reference was used to prepare a reference index that appended poly-A tails to all transcripts, and the quantification was performed with the --estimate-rspd flag. For the synthetic data, RSEM was given the --paired-end option to produce paired-end alignments from the reads. Apart from these options, RSEM was run with the default parameters for all experiments. eXpress7 version 1.3.1 was used for all experiments. It was run with default parameters on the MACQ data, and without bias correction (--no-bias-correct) on the synthetic data. For the synthetic test, Bowtie was given the appropriate −1 and −2 options to compute paired-end alignments. Cufflinks5 version 2.1.1 was used for experiments and was run with bias correction (−b) and multi-read recovery (−u) on the MACQ data, and with multi-read recovery (−u) and no effective-length correction (--no-effective-length-correction) on the synthetic data. For all tests, both Cufflinks and TopHat were run with the −p 16 option to take advantage of up to 16 concurrent threads of execution. All experiments were run on a computer with 8 AMD Opteron 6220 processors (four cores each) and 256 Gb of RAM. For all experiments, the wall time was measured using the built-in bash time command. Implementation of Sailfish. Sailfish has two basic sub-commands, index and quant. The index command initially builds a hash of all k-mers in the set of reference transcripts using the Jellyfish10 software. This hash is then used to build the minimum perfect hash, count array and look-up tables described above. The index command takes as input a k-mer size via the −k option and a set of reference transcripts in FASTA format via the −t parameter. It produces the Sailfish index described above, and it can optionally take advantage of multiple threads with the target number of threads being provided via a −p option. The quant sub-command estimates the relative abundance of transcripts given a set of reads. The quant command takes as input a Sailfish index (computed via the index command described above and provided via the −i parameter). Additionally, it requires the set of reads, provided as a list of FASTA or FASTQ files. Finally, just as can the index command, the quant command can take advantage of multiple processors, the target number of which is provided via the − p option. Sailfish is implemented in C++11 and takes advantage of several C++11 language and library features. In particular, Sailfish makes heavy use of built-in atomic data types. Parallelization across multiple threads in Sailfish is accomplished via a combination of the standard library's thread facilities and the Intel Threading Building Blocks (TBB) library23. Sailfish is available as an open-source program under the GPLv3 license, and has been developed and tested on Linux and Macintosh OS X. Algorithm 1. An EM iteration. One iteration of the expectation-maximization procedure that updates the estimated k-mer allocations α(·,·) and computes new estimates of relative transcript abundance μ′′ based on the current estimates of relative transcript abundance μ′. Algorithm 2. A SQUAREM iteration. One iteration updates the relative abundance estimates according to an accelerated EM procedure whose update direction and magnitude are dynamically computed11.",AbundanceEstimation,"sailfish enable alignmentfree isoform quantification  rnaseq read use lightweight algorithms
indexing  first step   sailfish pipeline  build  index   set  reference transcripts  give  kmer length   compute  index ikt contain four components  first component   minimum perfect hash function    set  kmers kmerst contain    minimum perfect hash function   bijection  kmerst   set  integers { kmerst  } sailfish use  bdz minimum perfect hash function9  second component   index   array  contain  count csi  every ∈ kmerst finally  index contain  lookup table map  transcript   multiset  kmers   contain   reverse lookup table map  kmer   set  transcripts    appear  index   product    reference transcripts   choice    thus need    recomputed  either   change quantification  second step   sailfish pipeline   quantification  relative transcript abundance  require  sailfish index ikt   reference transcripts   well   set  rnaseq read  first  count  number  occurrences    ∈ kmerst ∩ kmers   know exactly  set  kmers  need   count  already   perfect hash function    set   perform  count   particularly efficient manner even faster  efficient hashbased approach  example perform concurrent mer lookups use  thread jellyfish10 require  average   μskey   minimal perfect hash require  average   μskey  maintain  array    appropriate size kmerst   contain  number  time   thus far observe      unstranded protocol sequence read  hence  kmers  contain may originate  transcripts  either  forward  reverse direction  account   possibilities  check   forward  reversecomplement kmers   read  use  majorityrule heuristic  determine    kmers  increment   final array  count    number  kmers appear     forward direction   read  greater   number  reversecomplement kmers    increment  count  kmers appear   read   forward direction otherwise  count  kmers appear   reversecomplement   read  incremented   array  count tie  break  favor   forwarddirected read   strand rnaseq protocol  read   know orientation read provide  sailfish   specify  originate  unstranded forwardstrand  reversestrand read   appropriate kmers  count   case  take advantage  atomic integers   compareandswap cas operation provide  modern processors  allow many hardware thread  efficiently update  value   memory location without  need  explicit lock   stream   update  count    parallel  sustain  little resource contention   apply  expectationmaximization  algorithm  obtain estimate   relative abundance   transcript  define  kmer equivalence class   set   kmers  appear    set  transcripts    frequency   word let    vector   entry    give  many time kmer  appear  transcript ∈    equivalence class   kmer   give    {∈ kmerst    }  perform   procedure   allocate count  transcripts accord   set  equivalence class rather   full set  kmers   let denote  total count  kmers    originate  equivalence class   say  transcript  contain equivalence class      subset   multiset  kmers    denote    ⊆  estimate abundances via   algorithm   algorithm algo  alternate  estimate  fraction  count   observe kmer  originate   transcript estep  estimate  relative abundances   transcripts give  allocation mstep  initially allocate kmers  transcripts proportional   occurrences   transcript    transcript    potential origin   particular kmer   observations   kmer  attribute   transcript whereas   kmer  appear      different transcripts  occur  time   set  read  observations  attribute   potential transcript  origin  estep    algorithm compute  fraction   kmer equivalence class' total count   allocate   transcript  equivalence class   transcript   value  compute   ′   currently estimate relative abundance  transcript   allocations   use   mstep   algorithm  compute  relative abundance   transcript  relative abundance  transcript   estimate      variable ′ denote  adjust length  transcript    simply ′           length  transcript   nucleotides however rather  perform  standard  update step  perform update accord   squarem procedure11 describe  algo   ′  ′ ′   vector  relative abundance maximumlikelihood estimate  •   standard iteration   expectationmaximization procedure  outline  algo    detail explanation   squarem procedure   proof  convergence see ref  intuitively  squarem procedure build  approximation   jacobian  ′  three successive step along   solution path  use  magnitude   differences   solutions  determine  step size    update  estimate accord   update rule line   procedure   capable  make relatively large update   ′ parameters  substantially improve  speed  convergence  sailfish  iterative squarem procedure  repeat   specify convergence criterion  meet  default  procedure terminate   transcript   relative abundance greater     relative change greater  half   percent  consecutive iterations additionally  user may specify either  fix number  iterations  perform   require minimum relative change   transcript abundance estimate  squarem iterations   relative change  transcript abundance exceed    procedure  consider   converge   estimation procedure terminate bias correction  bias correction procedure implement  sailfish  base   model introduce  zheng   briefly  perform  regression analysis   set  potential bias factor   response variables   estimate transcript abundances kpkms sailfish automatically consider transcript length  content  dinucleotide frequencies  potential bias factor   specific set  feature  suggest  zheng     transcript  prediction   regression model represent  contribution   bias factor   transcript' estimate abundance hence  regression estimate  may  positive  negative  subtract   original estimate  obtain biascorrected kpkms   detail   bias correction procedure see ref   original method use  generalize additive model  regression sailfish implement  approach use random forest regression  leverage highperformance implementations   technique  key idea      bias correction  abundance estimation rather  earlier   pipeline  bias correction  sailfish   disable   nobiascorrection command line option finally  note    possible  include  potential feature like normalize coverage plot   encode positional bias   bias correction phase however   current version  sailfish    implement  test bias correction   feature compute kpkm rpkm  tpm sailfish output kmers per kilobase per million map kmers kpkm read per kilobase per million map read rpkm  transcripts per million tpm  quantities predict  relative abundance  different isoforms  rpkm estimate    commonly use   ideally  time  rate   read  observe   give position   tpm estimate  also become somewhat common6  sailfish  kpkm measure   natural  rpkm   kmer   basic unit  transcript coverage however  two measure  proportional give  relative transcript abundances estimate    procedure describe   tpm  transcript   give  let    number  kmers map  transcript    kpkm  give      total count  map  hashable kmers   final equality  approximate    replace   ′  kpkm  proportional   rpkm    estimate  replace    estimate number  read map  transcript      estimate number  map read    calculate   map kmer count   average read length  fragment per kilobase per million map fragment fpkm measure  also proportional   krpkm   mean  denote  fragment rather  read  mapped— single end data  read  fragment coincide  pairedend data  concordant read pair  consider  single fragment accuracy metrics   experiment  pearson correlation metric  compute  estimate  abundance   log transform   previous work6     prevent   abundant transcripts  dominate  correlations   necessitate discard sample   either  true  estimate abundance   spearman correlations  compute  estimate    log transform   synthetic data  additionally compute  root mean square error rmse  median percentage error give  vectors        groundtruth abundances         estimate abundances  rmse  define   percentage error    true abundance    correspond estimate   define  unlike  definition present   work12    define division    compute  percentage error   define  medpe   median   percentage error   transcripts    true abundance greater     median   value pexi     ≤  ≤    ≠   compare   qpcrbased abundances  use  kpkm estimate produce  sailfish  fpkm estimate produce  rsem cufflinks  express directly  fluxsimulator record  number  read   actually generate   transcript   simulate sequence experiment  read count   transcript lengths  use  compute  grind truth fpkms directly use equation  however  find   fpkm estimate compute  rsem express  cufflinks appear   number  transcripts   systematically inflate  respect   grind truth fpkms  inflation persist even  estimate  compare directly  grind truth relative transcript fraction  appear   due   effectivelength normalization procedures employ   methods indeed  cufflinks  run  noeffectivelengthcorrection  rmse  medpe decrease significantly   correlations  grind truth fpkms increase marginally  well thus   synthetic test  cufflinks result  compute   noeffectivelengthcorrection flag enable though rsem  express   expose   flag directly  compute fpkm estimate without effectivelength corrections   methods  use  estimate read count output directly   methods   true transcript lengths use equation  use fpkms compute   manner reduce  rmse      medpe      express  reduce  rmse       medpe      rsem sailfish compute  estimate kmer count  kpkms   consistent manner directly   underlie transcript abundances     difference   kpkms provide  sailfish    would  compute use  estimate kmer count directly simulate data  simulate homo sapiens rnaseq data  generate   flux simulator17    parameters list  supplementary note   result   dataset        pairedend read  read produce   flux simulator  group   origin  read originate    genomic locus appear together   read file however  synthetic artifact violate  assumption make  express  read  produce   random order thus  postprocess  read generate   flux simulator  shuffle   keep mat appropriately pair   random order  also randomize  orientation   read  simulate  unstranded protocol  shuffle matepair read   split  separate file  ensure  sufficient map rate   parameters use  produce alignments  express  cufflinks    bowtie     tophat22  synthetic read  quality trim use sickle    default parameters untrimmed read   appropriate  rsem' default parameters   rsem  provide  untrimmed read  map rat increase significantly   trim read  overall accuracy   methods depend little  whether trim  untrimmed read  use rsem express  cufflinks   give pairedend alignments since  make special use   data tophat22  provide   option mateinnerdist   adjust  expect matepair innerdistance   simulate average  bowtie  give  option    ensure  pairedend read originate   valid fragment would  consider  untrimmed synthetic read file  provide directly  sailfish without  extra information since   quantification procedure  use whether single  pairedend read  provide software comparisons  express  read  align  bowtie3  use  parameters      suggest   express manuscript  allow   three mismatch per read  report  alignments  alignments  rsem  compute  default parameters use  rsemcalculateexpression command  use bowtie    underlie aligner  prepare alignments  cufflinks tophat  run use bowtie  bowtie1   options   readeditdist   allow   three mismatch per read  rsem express  cufflinks  report time   sum   time require  alignment   time require  quantification  time require   method   decompose   time   alignment  quantification step  figure  choice  software options  effect  runtime  expression estimation software include rsem express  cufflinks provide  myriad  program options   user  allow  tradeoffs  various desiderata  example  total time require  tophat  cufflinks  lower  cufflinks  run without bias correction     oppose     bias correction   srx016366 data however without bias correction cufflinks yield slightly lower accuracy pearson    spearman       methods  still take  time longer  run  sailfish similarly although align read   stream directly  express via bowtie  empirically observe lower overall runtimes  align read  quantify expressions separately   sequence   time  report finally   provide rsem   alignments use  express     previous work7  observe substantially increase runtime   qpcr datasets     alignments generate  srx016366      alignments generate  srx016367 since rsem   appear  efficiently handle  large number  multiple alignments    generate  short read  allow many mismatch instead  report time  rsem   alignment  perform use   default parameters  general  attempt  run  piece  software  options  would  common   standard usage scenario however despite  inherent difficulty  compare  set  tool parameterized   array  potential options  core thesis  sailfish  provide accurate expression estimate much faster   exist tool remain true   fastest perform alternatives even  sacrifice accuracy  speed    order  magnitude slower  sailfish sailfish version   use   experiment   analyse     kmer size      use  concurrent thread     experiment involve  real   simulate data sailfish  run   polya option  discard kmers consist   consecutive     bias correction  enable  kpkm value report  sailfish  use  transcript abundance estimate rsem6 version   run   nobamoutput flag       experiment involve  real data rsempreparereference  use  prepare  reference index  append polya tail   transcripts   quantification  perform   estimaterspd flag   synthetic data rsem  give  pairedend option  produce pairedend alignments   read apart   options rsem  run   default parameters   experiment express7 version   use   experiment   run  default parameters   macq data  without bias correction nobiascorrect   synthetic data   synthetic test bowtie  give  appropriate    options  compute pairedend alignments cufflinks5 version   use  experiment   run  bias correction   multiread recovery    macq data   multiread recovery    effectivelength correction noeffectivelengthcorrection   synthetic data   test  cufflinks  tophat  run     option  take advantage     concurrent thread  execution  experiment  run   computer   amd opteron  processors four core      ram   experiment  wall time  measure use  builtin bash time command implementation  sailfish sailfish  two basic subcommands index  quant  index command initially build  hash   kmers   set  reference transcripts use  jellyfish10 software  hash   use  build  minimum perfect hash count array  lookup table describe   index command take  input  kmer size via   option   set  reference transcripts  fasta format via   parameter  produce  sailfish index describe     optionally take advantage  multiple thread   target number  thread  provide via   option  quant subcommand estimate  relative abundance  transcripts give  set  read  quant command take  input  sailfish index compute via  index command describe   provide via   parameter additionally  require  set  read provide   list  fasta  fastq file finally     index command  quant command  take advantage  multiple processors  target number    provide via    option sailfish  implement    take advantage  several  language  library feature  particular sailfish make heavy use  builtin atomic data type parallelization across multiple thread  sailfish  accomplish via  combination   standard library' thread facilities   intel thread build block tbb library23 sailfish  available   opensource program   gplv3 license    develop  test  linux  macintosh   algorithm    iteration one iteration   expectationmaximization procedure  update  estimate kmer allocations   compute new estimate  relative transcript abundance ′′ base   current estimate  relative transcript abundance ′ algorithm   squarem iteration one iteration update  relative abundance estimate accord   accelerate  procedure whose update direction  magnitude  dynamically computed11",6
150,ROCker,"ROCker: accurate detection and quantification of target genes in short-read metagenomic data sets by modeling sliding-window bitscores
Implementation ROCker is implemented in the Ruby programming language and its workflow consists of five tasks. (i) Build: Reads a user-provided list of UniProt (Universal Protein Resource) protein identifiers and downloads the corresponding whole genome sequences encoding these proteins for generating data sets that simulate shotgun, short-read, Illumina metagenomes using GRINDER (5). A second list of known negative references, i.e. closely related proteins that should not be considered as true matches can also be given at this step in order to increase the performance of ROCker (see amoA example below). The training reference sequences are downloaded and annotated using the European Bioinformatics Institute REST API (6) and aligned using ClustalΩ (7). Subsequently, ROCker queries the reference protein sequences provided against the simulated shotgun data sets using BLASTx (8) or DIAMOND (9). (ii)Compile: Translates search results to alignment columns, and identifies the most discriminant bitscore per alignment in a 20 amino acid window (or another, user-defined length) in a set of sequences using pROC (10). The latter algorithm calculates sensitivity and specificity using the number of true and false positive matches in each window. The bitscore thresholds are calculated as the value in the ROC curve that maximizes the distance to the identity line (i.e. the non-discriminatory diagonal line in the ROC curve) according to the Youden method. Windows are iteratively refined to reduce low-accuracy regions (<95% estimated accuracy), for all windows with sufficient data (≥5 amino acid positions and ≥3 true positives available). Thresholds in regions with insufficient data are inferred by linear interpolation of surrounding windows. (iii)Filter: Uses the calculated set of bitscore thresholds (as estimated by the compile task) to filter the result of a preexisting search. (iv)Search: Executes a search of metagenomic sequences against target protein sequences (i.e. single protein function) using BLASTx or DIAMOND, and filters the output according to the most-discriminating bitscores calculated in the Compile step. (v)Plot: Generates a graphical representation of the alignment, the thresholds and the matches obtained, together with summary statistics (See Supplementary Data). Target gene sequences Protein sequences for nitrogen cycle reference genes were obtained from the National Center for Biotechnology Information (NCBI) (downloaded in March 2014) and Uniprot (downloaded in June 2015). In order to avoid mis-annotated references, all protein sequences were aligned and visually inspected for the presence of characteristic amino acids or protein motifs and their phylogenetic relationships. Having a list of well-curated reference sequences is key for accurate ROCker results. All reference protein sequences used in the analysis for NirK (n = 147), NosZ (n = 173), PmoA (n = 9), archaeal AmoA (n = 5), bacterial AmoA (n = 7) and RpoB (n = 757) are available through http://enve-omics.gatech.edu. Simulated data sets and benchmark analyses Generation of simulated shotgun data sets Simulated data sets were constructed using the ‘Build’ function in ROCker based on an input list of UniProt identifiers for each protein sequence (-P option). GRINDER's parameters differed from their default options as follows: sequencing depth of 3 (for NosZ and NirK, 10 for bacterial and archaea AmoA simulated data sets), remove ‘-∼*NnKkMmRrYySsWwBbVvHhDdXx’ characters, sequencing error ‘uniform 0.1’, mutation ratio ‘95 5’ and read length distribution ‘L uniform 5’, where L is the average read length of the simulated data set. Simulated data sets ranged from 1 to 43 million reads in size (Supplementary Data). The CPU time (cput) in hours required for generating simulated data sets can be approximated by using a power law regression as follows: cput = 3.0672*D1.096 (r2 = 0.948), where D is the number of protein reference sequences used. Calculated ROCker profiles can be re-used in following similarity searches. The processing of a similarity search output (i.e. ROCker-based filtering) typically takes from a few seconds to a couple of minutes on a personal computer, depending on the number of matching sequences. Similarity search analysis The simulated shotgun data sets were used as query sequences for BLASTx (BLAST+2.2.8) and DIAMOND (v0.7.9.58) searches against the reference protein sequences that corresponded to the input UniProt IDs. Default settings were used for BLASTx except that e-value was set to 0.01. For DIAMOND, the settings used were ‘min score’ of 20 and ‘sensitive’. These settings were used to make DIAMOND comparable to BLASTx in terms of sensitivity, albeit at the expense of speed; users that want faster DIAMOND searches should opt for the default settings instead. In all cases, only best matches were considered by using the script BlastTab.best_hit_sorted.pl from the enveomics collection (11). The BLASTx searches were used for generating ROCker profiles for NosZ, NirK and RpoB protein references (profiles available through http://enve-omics.ce.gatech.edu/rocker). Hidden Markov models for each set of proteins were built using full-length alignments with HMMer (12). For hidden Markov model (HMM)-based searches, the read sequences were first translated to amino acids using FragGeneScan (13), and subsequently used as query sequences in the hmmsearch algorithm implemented in HMMer (12) (Supplementary Data). Ten-fold cross-validation calculations Both NosZ and NirK ROCker profiles were further evaluated by performing a tenfold cross-validation test. To ensure that multi-copy references encoded in the same genome were grouped together in cross-validation sets, we randomly separated the genomes into ten subsets (rather than using protein UniProt identifiers). For each subset, a simulated data set was generated as a query (Test) to challenge a ROCker profile built with the remaining nine subsets (Model). Similarity searches were performed using BLASTx with the parameters described above. FNR and FDR were calculated for each subset and for 100, 150, 200, 250 and 300 bp read length simulated data sets. All generated data sets are available through http://enve-omics.ce.gatech.edu/data/rocker. Shotgun metagenomes Publicly available shotgun metagenomes were downloaded from the Sequence Read Archive, Metagenomics RAST or other web resources (see Supplementary Data for details). The data sets included two representative Midwest USA agricultural sites (Havana and Urbana, Illinois, USA) (4), two prairie soils that underwent infrared heating for 10 years (warming and control; Oklahoma, USA) (14), tropical (Misiones, Argentina) and boreal forests (Alaska, USA) (15), Alaskan permafrost active layer (Alaska, USA) (16), two beach sands (17) and a deep marine sediment (18) related to the Deepwater Horizon oil spill (Florida, USA), human stool (19) and a waste water enrichment sample (20). Sequence processing of shot-gun metagenomes SolexaQA (21) was used for quality trimming of raw Illumina metagenomic reads to extract the longest continuous segment with a Phred score ≥ 20. All paired-end or single reads (when only one read was available) longer than 50 bp were used for further analysis. Fraction of genomes encoding nitrogen cycle genes RpoB (RNA polymerase beta subunit) sequences were obtained from reviewed proteins in UniProt/Swiss-Prot. A total of 757 sequences were visually inspected for conservation of functional domains and complete alignment and were used to construct a simulated data set and ROCker profile (similar options as above for nitrogen cycle genes but using the ‘–per-genus’ option in the building step in order to reduce redundancy caused by sampling individual species with many representative sequences). Short-reads from soil metagenomes were used as query sequences for independent BLASTx searches (same settings as above) against the NosZ, NirK, AmoA or RpoB protein references. The ROCker-filtered or e-value-filtered counts were normalized by the median length of the sequences of each protein reference. The fraction of microbial genomes encoding either nosZ, nirK or amoA (i.e. genome equivalent) was calculated as the ratio of nirK, nosZ or amoA read counts to rpoB read counts using ROCker profiles or e-values. Phylogenetic placement of amoA and nosZ reads Protein reference sequences for NosZ or Amoa/PmoA were aligned using ClustalΩ (7) with default parameters. The alignment was used to build a phylogenetic tree in RAxML (22) v8.0.19 (LG model). nosZ- or amoA-reads were extracted from soil metagenomes using ROCker (BLASTx option), and their protein-coding sequences were predicted using FragGeneScan. The latter sequences were added to the NosZ or Amoa/PmoA protein alignment using MAFFT (‘addfragments’) (23) and were placed in the corresponding phylogenetic tree using RAxML EPA (24) (-f v option). An in house script (‘JPlace.to_iToL.rb’ available through http://enve-omics.gatech.edu) was used to prepare the visualization of the generated jplace file (25) in iTOL (26). Availability and dependencies of ROCker The ROCker package, documentation and pre-computed profiles are available through http://enve-omics.ce.gatech.edu/rocker. ROCker is distributed both as a packaged Ruby gem (https://rubygems.org/gems/bio-rocker) and source code (https://github.com/lmrodriguezr/rocker) under the terms of the Artistic License 2.0. Complete ROCker execution requires the rest-client and json Ruby gems, as well as R (including the pROC package), NCBI-BLAST+ or DIAMOND, GRINDER and ClustalΩ or MUSCLE (27). In addition, ROCker models can be built online through http://enve-omics.ce.gatech.edu/rocker-build/. Go to:",AbundanceEstimation,"rocker accurate detection  quantification  target genes  shortread metagenomic data set  model slidingwindow bitscores
implementation rocker  implement   ruby program language   workflow consist  five task  build read  userprovided list  uniprot universal protein resource protein identifiers  download  correspond whole genome sequence encode  proteins  generate data set  simulate shotgun shortread illumina metagenomes use grinder   second list  know negative reference  closely relate proteins     consider  true match  also  give   step  order  increase  performance  rocker see amoa example   train reference sequence  download  annotate use  european bioinformatics institute rest api   align use clustalω  subsequently rocker query  reference protein sequence provide   simulate shotgun data set use blastx   diamond  iicompile translate search result  alignment columns  identify   discriminant bitscore per alignment    amino acid window  another userdefined length   set  sequence use proc   latter algorithm calculate sensitivity  specificity use  number  true  false positive match   window  bitscore thresholds  calculate   value   roc curve  maximize  distance   identity line   nondiscriminatory diagonal line   roc curve accord   youden method windows  iteratively refine  reduce lowaccuracy regions  estimate accuracy   windows  sufficient data ≥ amino acid position  ≥ true positives available thresholds  regions  insufficient data  infer  linear interpolation  surround windows iiifilter use  calculate set  bitscore thresholds  estimate   compile task  filter  result   preexist search ivsearch execute  search  metagenomic sequence  target protein sequence  single protein function use blastx  diamond  filter  output accord   mostdiscriminating bitscores calculate   compile step vplot generate  graphical representation   alignment  thresholds   match obtain together  summary statistics see supplementary data target gene sequence protein sequence  nitrogen cycle reference genes  obtain   national center  biotechnology information ncbi download  march   uniprot download  june   order  avoid misannotated reference  protein sequence  align  visually inspect   presence  characteristic amino acids  protein motifs   phylogenetic relationships   list  wellcurated reference sequence  key  accurate rocker result  reference protein sequence use   analysis  nirk    nosz    pmoa    archaeal amoa    bacterial amoa     rpob     available   simulate data set  benchmark analyse generation  simulate shotgun data set simulate data set  construct use  build function  rocker base   input list  uniprot identifiers   protein sequence  option grinder' parameters differ   default options  follow sequence depth    nosz  nirk   bacterial  archaea amoa simulate data set remove *nnkkmmrryysswwbbvvhhddxx character sequence error uniform  mutation ratio    read length distribution  uniform      average read length   simulate data set simulate data set range     million read  size supplementary data  cpu time cput  hours require  generate simulate data set   approximate  use  power law regression  follow cput  *        number  protein reference sequence use calculate rocker profile   reuse  follow similarity search  process   similarity search output  rockerbased filter typically take    second   couple  minutes   personal computer depend   number  match sequence similarity search analysis  simulate shotgun data set  use  query sequence  blastx blast  diamond  search   reference protein sequence  correspond   input uniprot ids default settings  use  blastx except  evalue  set    diamond  settings use  min score    sensitive  settings  use  make diamond comparable  blastx  term  sensitivity albeit   expense  speed users  want faster diamond search  opt   default settings instead   case  best match  consider  use  script blasttabbest_hit_sortedpl   enveomics collection   blastx search  use  generate rocker profile  nosz nirk  rpob protein reference profile available   hide markov model   set  proteins  build use fulllength alignments  hmmer   hide markov model hmmbased search  read sequence  first translate  amino acids use fraggenescan   subsequently use  query sequence   hmmsearch algorithm implement  hmmer  supplementary data tenfold crossvalidation calculations  nosz  nirk rocker profile   evaluate  perform  tenfold crossvalidation test  ensure  multicopy reference encode    genome  group together  crossvalidation set  randomly separate  genomes  ten subsets rather  use protein uniprot identifiers   subset  simulate data set  generate   query test  challenge  rocker profile build   remain nine subsets model similarity search  perform use blastx   parameters describe  fnr  fdr  calculate   subset          read length simulate data set  generate data set  available   shotgun metagenomes publicly available shotgun metagenomes  download   sequence read archive metagenomics rast   web resources see supplementary data  detail  data set include two representative midwest usa agricultural sit havana  urbana illinois usa  two prairie soil  undergo infrared heat   years warm  control oklahoma usa  tropical misiones argentina  boreal forest alaska usa  alaskan permafrost active layer alaska usa  two beach sand    deep marine sediment  relate   deepwater horizon oil spill florida usa human stool    waste water enrichment sample  sequence process  shotgun metagenomes solexaqa   use  quality trim  raw illumina metagenomic read  extract  longest continuous segment   phred score ≥   pairedend  single read   one read  available longer     use   analysis fraction  genomes encode nitrogen cycle genes rpob rna polymerase beta subunit sequence  obtain  review proteins  uniprotswissprot  total   sequence  visually inspect  conservation  functional domains  complete alignment   use  construct  simulate data set  rocker profile similar options    nitrogen cycle genes  use  pergenus option   build step  order  reduce redundancy cause  sample individual species  many representative sequence shortreads  soil metagenomes  use  query sequence  independent blastx search  settings     nosz nirk amoa  rpob protein reference  rockerfiltered  evaluefiltered count  normalize   median length   sequence   protein reference  fraction  microbial genomes encode either nosz nirk  amoa  genome equivalent  calculate   ratio  nirk nosz  amoa read count  rpob read count use rocker profile  evalues phylogenetic placement  amoa  nosz read protein reference sequence  nosz  amoapmoa  align use clustalω   default parameters  alignment  use  build  phylogenetic tree  raxml    model nosz  amoareads  extract  soil metagenomes use rocker blastx option   proteincoding sequence  predict use fraggenescan  latter sequence  add   nosz  amoapmoa protein alignment use mafft addfragments    place   correspond phylogenetic tree use raxml epa    option   house script jplaceto_itolrb available    use  prepare  visualization   generate jplace file   itol  availability  dependencies  rocker  rocker package documentation  precomputed profile  available   rocker  distribute    package ruby gem   source code    term   artistic license  complete rocker execution require  restclient  json ruby gems  well   include  proc package ncbiblast  diamond grinder  clustalω  muscle   addition rocker model   build online    ",6
151,GAAS,"The GAAS metagenomic tool and its estimations of viral and microbial average genome size in four major biomes
GAAS: Genome relative Abundance and Average Size in random shotgun libraries GAAS software package. GAAS was implemented as a standalone software package in Perl and is freely available at http://sourceforge.net/projects/gaas/. It accepts and produces files in standard formats (FASTA sequences, Newick trees, tabular BLAST results, SVG graphics). The GAAS methodology is described in detail below and is outlined in Figure 6. thumbnail        Download: PPTPowerPoint slide PNGlarger image TIFForiginal image Figure 6. Flowchart of GAAS to calculate relative abundance and average genome size. GAAS runs BLAST and uses various corrections to obtain accurate estimations. https://doi.org/10.1371/journal.pcbi.1000593.g006 Similarity filtering. BLAST analyses (NCBI BLAST 2.2.1) were conducted through GAAS in order to determine significant similarities between metagenomic sequences and completely sequenced genomes. Similarities were filtered based on a combination of maximum E-value, minimum similarity percentage and minimum relative alignment length. E-value filtering removed non-significant similarities, and the alignment similarity percentage and relative length were used to select for strong similarities likely to reflect the taxonomy of the metagenomic sequences. E-values depend on the size of the database and the absolute length of alignments between query and target sequences, and thus may not be comparable between analyses [32],[33]. Relative alignment length, also called alignment coverage [34], is the ratio of the length of the alignment to the length of the query sequence (Figure S7). It is independent of the database size and sequence length, and provides an intuitive and consistent threshold to select significant similarities. Since the ends of sequenced reads can be of lower quality, similarities were kept only if the length of the alignment represented the majority of the length of the query sequence. Sequences with no similarity satisfying the filtering criteria were ignored in the rest of the analysis. Similarity weighting. In order to avoid the loss of relevant similarities by reliance upon smallest E-values alone [5], all significant similarities for each query sequence (as defined by our criteria above) were kept and assigned weights as follows. Based on the Karlin-Altschul equation, the expect value Eij between a metagenomic query sequence i and a target genome sequence j is given by:  where m'i is the effective query sequence length, n' is the effective database size (in number of residues) and S'ij is the high-scoring pair (HSP) bitscore [32]. Using the effective length corrects for the “edge effect” of local alignment and is significant for sequences smaller than 200 bp such as sequences produced by the high throughput Roche-454 GS20 platform. Assuming that a query sequence is more likely to have local similarities to longer target genomes, each of the E-values can be reformulated into an expect value Fij of a similarity in a given target genome by:  where t'j is the effective length [35] of the target genome j. Using the length of the target genome in the F-value produces an expect value relative to the target genome, not to the totality of the genome database (as is the case of the E-value). From Fij, a weight wij can be calculated as  with zi being a constant such that for a given metagenomic query sequence i, . This weight carries the statistical meaning of the expect value of the similarity relative to the given genome in such a way that the larger the expect value, the lower the weight. Therefore, for a given query sequence i, the weight was calculated as . Genome relative abundance using genome length normalization. The relative abundance of sequences in a random shotgun library is proportional not only to the relative abundance of the genomes in the library but also to their length. Similarly to the normalization used in proteomics [36]–[38], normalization by genome length is needed to obtain correct relative abundance of the species in a metagenome. For each target genome j, the weights wij to that genome were added to obtain Wj. The weighted similarities Wj to each genome were then normalized by the actual length tj of the genome (including chromosomes, organelles, plasmids and other replicons) to obtain accurate relative abundance estimates:  where x is a constant such that . Average genome length calculation. GAAS relies on the relatively stable genome size found within taxa [39] to calculate average genome length. The average genome length was calculated as a weighted average of individual genome lengths. The length of the genome for each individual organism identified in the metagenome was weighted by the relative abundance of that organism as calculated by GAAS. Thus, the mean genome length L was calculated as:  where rj was the relative abundance of organism k, and lj its individual genome length. Confidence intervals for relative abundance and average genome length estimates. A bootstrap procedure was implemented in GAAS to provide empirical confidence intervals for relative abundance and average genome length estimates. The estimation of community composition and average genome length was repeated many times using a random subsample of 10,000 sequences for each repetition. Confidence intervals were determined based on the percentiles of the observed estimates, e.g. 5th and 95th percentiles for a 90% confidence interval. Reference databases for viral, microbial and eukaryotic metagenomes NCBI RefSeq (ftp://ftp.ncbi.nih.gov/refseq/release) (Release 32, August 31, 2008) was used as the target database for the estimation of taxonomic composition and average genome size. Three databases containing exclusively complete genomic sequences were created from the viral, microbial, and eukaryotic RefSeq files. All incomplete sequences were identified as having descriptions containing words such as “shotgun”, “contig”, “partial”, “end” and “part”, and were removed from the database. A taxonomy file containing only the taxonomic ID of the sequences in these three databases was produced using the NCBI Taxonomy classification. Sequences with a description matching the following words were excluded from that file unless the chromosomal sequences were also available for the same organism: “plasmid”, “transposon”, “chloroplast”, “plastid”, “mitochondrion”, “apicoplast”, “macronuclear”, “cyanelle” and “kinetoplast”. The complete viral, microbial, and eukaryal sequence files with accompanying taxonomic IDs are available at http://biome.sdsu.edu/gaas/data/. Mapping to phylogenetic trees Similarly to the Interactive Tree Of Life (ITOL) [40] and MetaMapper (http://scums.sdsu.edu/Mapper), GAAS is able to graph the relative abundance of viral, microbial or eukaryotic species on phylogenetic trees such as the Viral Proteomic Tree (VPT) or Tree Of Life (http://itol.embl.de). The Viral Proteomic Tree was constructed using the approach introduced in the Phage Proteomic Tree and extending it to the >3,000 viral sequences present in the NCBI RefSeq viral collection (Edwards, R. A.; unpublished data, 2009). Benchmark using simulated viral metagenomes Simulated metagenomes were created to test the validity and accuracy of the GAAS approach using the free software program Grinder (http://sourceforge.net/projects/biogrinder), which was developed in conjunction with GAAS. Grinder creates metagenomes from genomes present in a user-supplied FASTA file. Users can simulate realistic metagenomes by setting Grinder options such as community structure, read length and sequencing error rate. Over 9,500 simulated metagenomes based on the NCBI RefSeq virus collection were generated using Grinder. The viral database was chosen since its large amount of mosaicism and horizontal gene transfer represents a worst-case scenario. Therefore, benchmark results using the viral database are expected to be valid for higher-order organisms such as Bacteria, Archaea and eukaryotes. The parameters used were a coverage of 0.5 fold, and a sequencing error rate of 1% (0.9% substitutions, 0.1% indels). Half of the simulated metagenomes had a uniform rank-abundance distribution, while the other half followed a power law with model parameter 1.2. Sequence length in the artificial metagenomes was varied from 50 to 800 bp for the analysis of read length effects on GAAS estimates. For each simulated viral metagenome, GAAS was run repeatedly with different parameter sets (relative alignment length and percentage of identity). The maximum E-value was fixed to 0.001 in order to remove similarities due to chance alone. Each set of variable parameters was tested on a minimum of 1,200 different Grinder-generated metagenomes. All computations were run on an 8-node Intel dual-core Linux cluster. Due to the limited number of whole genome sequences available, a great majority of the sampled organisms in a metagenome cannot be assigned to a taxonomy. To evaluate the effect of sequences from novel organisms on GAAS estimates, the taxonomy of 80% randomly chosen organisms in the database was made inaccessible to GAAS rendering them “unknown”. A control simulation with 100% known organisms was run for comparison (Figure S2). The accuracy of GAAS estimates was evaluated by comparing GAAS results to actual community composition and average genome size of the simulated metagenomes. The relative error for average genome size was calculated as , where x and xe are the true and estimated values respectively. For the composition, the cumulative error was calculated as , where ri is the relative error on the relative abundance of the target genome i and n is the total number of sequences in the database. Because the benchmark results were not normal, non-parametric statistical tests were used for all pairwise (Mann-Whitney U test) and multi-factor comparisons (Friedman test) of average errors. Non-parametric correlations were calculated using Kendall's tau. Benchmark using simulated microbial metagenomes GAAS was also tested on the three simulated metagenomes available at IMG/m (http://fames.jgi-psf.org). Parameter setting and data processing were conducted as in viral benchmark experiments. Points on the IMG/m microbial benchmark graphs represent the average of 58 repetitions. Microbial strains typically have a largely identical genome, with a fraction coding for additional genes and accounting for differences in genome length. An additional simulation was performed to investigate how the presence of closely related genomes influences the accuracy of the GAAS estimates. The 15 Escherichia coli strains present in the NCBI RefSeq database, ranging from 4.64 to 5.57 Mb in genome size, were used to produce ∼4,500 shotgun libraries with Grinder. The parameters used were the same as for the simulated viral metagenomes, but with a coverage of 0.0014 fold (>1,000 sequences). Half of the simulated metagenomes were treated as in the viral benchmark, using the GAAS approach and assuming no unknown species. The other half were treated similarly but taking only the top similarity. Points on the graph of the microbial strain benchmark represent the average of >2,200 repetitions. Meta-analysis of 169 metagenomes The composition and average genome size for 169 metagenomes were calculated using GAAS. Most of these metagenomes were publicly available from the CAMERA [41], NCBI [42], or MG-RAST [43] (Table S2), and a few dozens were viromes and microbiomes newly collected from solar saltern ponds, chicken guts, different soils and an oceanic oxygen minimum zone (Protocol S1). The metagenomes used here therefore represent viral, bacterial, archaeal, and protist communities sampled from a diverse array of biomes and were categorized as one of the following: “aquatic”, “terrestrial”, “sediment”, “host-associated”, and “manipulated / perturbed”. The large number of aquatic metagenomes was further subdivided into: “ocean”, “hypersaline”, “freshwater”, “hot spring” and “microbialites”. Sampling, filtering, processing and sequencing methods differed among compiled metagenomes. Table 1 provides a summary of the number of metagenomes from each biome (a list of the complete dataset is presented in detail in Table S2). thumbnail        Download: PPTPowerPoint slide PNGlarger image TIFForiginal image Table 1. Summary of metagenomes by type used in the meta-analysis. https://doi.org/10.1371/journal.pcbi.1000593.t001 For all metagenomes, GAAS was run using a threshold E-value of 0.001, and an alignment relative length of 60%. In addition, for bacterial, archaeal and eukaryotic metagenomes, similarities were calculated using BLASTN with an alignment similarity of 80%. Due to the low number of similarities in viral metagenomes using BLASTN, TBLASTX was used for viruses, with a threshold alignment similarity of 75%. All average genome length estimates produced from less than 100 similarities were discarded to keep results as accurate as possible. Manipulated metagenomes were ultimately not used in the meta-analysis because they do not accurately represent environmental conditions. Statistical pairwise differences between average genome lengths across biomes were assessed using Mann-Whitney U rank-sum tests. The average genome length and relative abundance results obtained for all metagenomes with our GAAS method were compared to the “standard” analytical approach where: 1) only the top similarity for each metagenomic sequence is kept, 2) there is no filtering by alignment similarity or relative length, and 3) no normalization by genome length is carried out. The virome from the Sargasso Sea was chosen to illustrate in detail the difference between the results obtained with the two methods (Figure 3). Correlation between viral and microbial average genome length Average genome lengths were calculated for 25 pairs of microbial and viral metagenomes sampled from the same location at the same time. The statistical relationship between viral and microbial average genome length in paired metagenomes was evaluated using Kendall's tau, since lengths were not normally distributed. Regression analysis was performed with Generalized Linear Models (GLM). Interactions between genome lengths and biome classifications were not significant and were not included in final models. Statistical analyses All statistical analyses of the GAAS benchmark results, environmental genome length and genome length correlations described above were performed using the free statistical software package R",AbundanceEstimation," gaas metagenomic tool   estimations  viral  microbial average genome size  four major biomes
gaas genome relative abundance  average size  random shotgun libraries gaas software package gaas  implement   standalone software package  perl   freely available    accept  produce file  standard format fasta sequence newick tree tabular blast result svg graphics  gaas methodology  describe  detail    outline  figure  thumbnail        download pptpowerpoint slide pnglarger image tifforiginal image figure  flowchart  gaas  calculate relative abundance  average genome size gaas run blast  use various corrections  obtain accurate estimations  similarity filter blast analyse ncbi blast   conduct  gaas  order  determine significant similarities  metagenomic sequence  completely sequence genomes similarities  filter base   combination  maximum evalue minimum similarity percentage  minimum relative alignment length evalue filter remove nonsignificant similarities   alignment similarity percentage  relative length  use  select  strong similarities likely  reflect  taxonomy   metagenomic sequence evalues depend   size   database   absolute length  alignments  query  target sequence  thus may   comparable  analyse  relative alignment length also call alignment coverage    ratio   length   alignment   length   query sequence figure    independent   database size  sequence length  provide  intuitive  consistent threshold  select significant similarities since  end  sequence read    lower quality similarities  keep    length   alignment represent  majority   length   query sequence sequence   similarity satisfy  filter criteria  ignore   rest   analysis similarity weight  order  avoid  loss  relevant similarities  reliance upon smallest evalues alone   significant similarities   query sequence  define   criteria   keep  assign weight  follow base   karlinaltschul equation  expect value eij   metagenomic query sequence    target genome sequence   give    '   effective query sequence length '   effective database size  number  residues  '   highscoring pair hsp bitscore  use  effective length correct   “edge effect”  local alignment   significant  sequence smaller      sequence produce   high throughput roche gs20 platform assume   query sequence   likely   local similarities  longer target genomes    evalues   reformulate   expect value fij   similarity   give target genome    '   effective length    target genome  use  length   target genome   fvalue produce  expect value relative   target genome    totality   genome database    case   evalue  fij  weight wij   calculate       constant     give metagenomic query sequence    weight carry  statistical mean   expect value   similarity relative   give genome    way   larger  expect value  lower  weight therefore   give query sequence   weight  calculate   genome relative abundance use genome length normalization  relative abundance  sequence   random shotgun library  proportional     relative abundance   genomes   library  also   length similarly   normalization use  proteomics  normalization  genome length  need  obtain correct relative abundance   species   metagenome   target genome   weight wij   genome  add  obtain   weight similarities    genome   normalize   actual length    genome include chromosomes organelles plasmids   replicons  obtain accurate relative abundance estimate      constant    average genome length calculation gaas rely   relatively stable genome size find within taxa   calculate average genome length  average genome length  calculate   weight average  individual genome lengths  length   genome   individual organism identify   metagenome  weight   relative abundance   organism  calculate  gaas thus  mean genome length   calculate       relative abundance  organism     individual genome length confidence intervals  relative abundance  average genome length estimate  bootstrap procedure  implement  gaas  provide empirical confidence intervals  relative abundance  average genome length estimate  estimation  community composition  average genome length  repeat many time use  random subsample   sequence   repetition confidence intervals  determine base   percentiles   observe estimate  5th  95th percentiles    confidence interval reference databases  viral microbial  eukaryotic metagenomes ncbi refseq ftpftpncbinihgovrefseqrelease release  august    use   target database   estimation  taxonomic composition  average genome size three databases contain exclusively complete genomic sequence  create   viral microbial  eukaryotic refseq file  incomplete sequence  identify   descriptions contain word   “shotgun” “contig” “partial” “end”  “part”   remove   database  taxonomy file contain   taxonomic    sequence   three databases  produce use  ncbi taxonomy classification sequence   description match  follow word  exclude   file unless  chromosomal sequence  also available    organism “plasmid” “transposon” “chloroplast” “plastid” “mitochondrion” “apicoplast” “macronuclear” “cyanelle”  “kinetoplast”  complete viral microbial  eukaryal sequence file  accompany taxonomic ids  available   map  phylogenetic tree similarly   interactive tree  life itol   metamapper  gaas  able  graph  relative abundance  viral microbial  eukaryotic species  phylogenetic tree    viral proteomic tree vpt  tree  life   viral proteomic tree  construct use  approach introduce   phage proteomic tree  extend     viral sequence present   ncbi refseq viral collection edwards   unpublished data  benchmark use simulate viral metagenomes simulate metagenomes  create  test  validity  accuracy   gaas approach use  free software program grinder    develop  conjunction  gaas grinder create metagenomes  genomes present   usersupplied fasta file users  simulate realistic metagenomes  set grinder options   community structure read length  sequence error rate   simulate metagenomes base   ncbi refseq virus collection  generate use grinder  viral database  choose since  large amount  mosaicism  horizontal gene transfer represent  worstcase scenario therefore benchmark result use  viral database  expect   valid  higherorder organisms   bacteria archaea  eukaryotes  parameters use   coverage   fold   sequence error rate    substitutions  indels half   simulate metagenomes   uniform rankabundance distribution    half follow  power law  model parameter  sequence length   artificial metagenomes  vary        analysis  read length effect  gaas estimate   simulate viral metagenome gaas  run repeatedly  different parameter set relative alignment length  percentage  identity  maximum evalue  fix    order  remove similarities due  chance alone  set  variable parameters  test   minimum   different grindergenerated metagenomes  computations  run   node intel dualcore linux cluster due   limit number  whole genome sequence available  great majority   sample organisms   metagenome cannot  assign   taxonomy  evaluate  effect  sequence  novel organisms  gaas estimate  taxonomy   randomly choose organisms   database  make inaccessible  gaas render  “unknown”  control simulation   know organisms  run  comparison figure   accuracy  gaas estimate  evaluate  compare gaas result  actual community composition  average genome size   simulate metagenomes  relative error  average genome size  calculate         true  estimate value respectively   composition  cumulative error  calculate       relative error   relative abundance   target genome      total number  sequence   database   benchmark result   normal nonparametric statistical test  use   pairwise mannwhitney  test  multifactor comparisons friedman test  average errors nonparametric correlations  calculate use kendall' tau benchmark use simulate microbial metagenomes gaas  also test   three simulate metagenomes available  imgm  parameter set  data process  conduct   viral benchmark experiment point   imgm microbial benchmark graph represent  average   repetitions microbial strain typically   largely identical genome   fraction cod  additional genes  account  differences  genome length  additional simulation  perform  investigate   presence  closely relate genomes influence  accuracy   gaas estimate   escherichia coli strain present   ncbi refseq database range       genome size  use  produce  shotgun libraries  grinder  parameters use       simulate viral metagenomes    coverage   fold  sequence half   simulate metagenomes  treat    viral benchmark use  gaas approach  assume  unknown species   half  treat similarly  take   top similarity point   graph   microbial strain benchmark represent  average   repetitions metaanalysis   metagenomes  composition  average genome size   metagenomes  calculate use gaas    metagenomes  publicly available   camera  ncbi   mgrast  table     dozens  viromes  microbiomes newly collect  solar saltern ponds chicken gut different soil   oceanic oxygen minimum zone protocol   metagenomes use  therefore represent viral bacterial archaeal  protist communities sample   diverse array  biomes   categorize  one   follow “aquatic” “terrestrial” “sediment” “hostassociated”  “manipulated  perturbed”  large number  aquatic metagenomes   subdivide  “ocean” “hypersaline” “freshwater” “hot spring”  “microbialites” sample filter process  sequence methods differ among compile metagenomes table  provide  summary   number  metagenomes   biome  list   complete dataset  present  detail  table  thumbnail        download pptpowerpoint slide pnglarger image tifforiginal image table  summary  metagenomes  type use   metaanalysis    metagenomes gaas  run use  threshold evalue     alignment relative length    addition  bacterial archaeal  eukaryotic metagenomes similarities  calculate use blastn   alignment similarity   due   low number  similarities  viral metagenomes use blastn tblastx  use  viruses   threshold alignment similarity    average genome length estimate produce  less   similarities  discard  keep result  accurate  possible manipulate metagenomes  ultimately  use   metaanalysis     accurately represent environmental condition statistical pairwise differences  average genome lengths across biomes  assess use mannwhitney  ranksum test  average genome length  relative abundance result obtain   metagenomes   gaas method  compare   “standard” analytical approach     top similarity   metagenomic sequence  keep     filter  alignment similarity  relative length    normalization  genome length  carry   virome   sargasso sea  choose  illustrate  detail  difference   result obtain   two methods figure  correlation  viral  microbial average genome length average genome lengths  calculate   pair  microbial  viral metagenomes sample    location    time  statistical relationship  viral  microbial average genome length  pair metagenomes  evaluate use kendall' tau since lengths   normally distribute regression analysis  perform  generalize linear model glm interactions  genome lengths  biome classifications   significant    include  final model statistical analyse  statistical analyse   gaas benchmark result environmental genome length  genome length correlations describe   perform use  free statistical software package ",6
152,GRAMMy,"Accurate genome relative abundance estimation based on shotgun metagenomic reads
A finite mixture model We developed a finite mixture model for the GRAMMy framework. Following Angly et al. we used genome relative abundance (GRA) as the relative abundance measure of mostly unicellular microbial organisms [15]. We describe the sampling and sequencing procedure as follows: First, randomly choose a genome  with probability  proportional to , where  is the abundance and  is the genome length. Second, randomly generate a read  from it. Without loss of generality, we further assume that for the given genome  we can reasonably approximate the generation of shotgun reads by some component distribution  such that the probability of generating a read  from  is . With a reasonable assumption of independence between the two sampling steps, the whole procedure is probabilistically equivalent to sampling from a mixture distribution , with the mixing parameters denoted by ,  and the component distributions denoted by , where m is the number of genomes. Subsequently, each read set, denoted by , can be regarded as a realized independent, identically distributed (iid) sample of size  from the mixture . The relative abundance of known genomes is exactly a transformation of the mixing parameters , which can be estimated based on the read set . A schematic view of the finite mixture model is shown in Figure 1. With the component distributions properly set up, we can find the maximum likelihood estimate (MLE) of the mixing parameters. In many studies, our knowledge of the genomes present in the community is limited. Under these circumstances, we can define the mixture with the first  components for known genomes and the last -th component for the collective of unknown genomes. Note that for the  known components, we suppose that their genome sequences  and genome sizes  are known. Therefore, the GRA for known genomes  is the normalized abundance, where the relative abundance for the -th known genome is , where . In the biological setting, we want to estimate vector , which is a measure of organism relative abundance. In the transformed mixture problem,  is related to the mixing parameters  by: (1) or the inverse: (2) for . The number of sampled reads is both proportional to the genome relative abundance and the length. Because the two factors are confounded, the missing knowledge of the genome length  prohibits the estimation of  from the data. Since the effective genome length  for the unknown genomes is not available, we cannot estimate the relative abundance of the unknown component. However, the relative abundance of known genomes can still be estimated using our procedures. Estimation of GRA using Expectation Maximization (EM) algorithm To estimate the mixing parameters, we adopted the EM algorithm to calculate the maximum likelihood estimate (MLE). In the EM framework, we assume a ‘missing’ data matrix , in which each entry  is a random variable indicating whether  is from  or not. Then we can solve for the parameters by iteratively estimating  and  using Algorithm 1 (see supporting Methods). Note that a variable with superscript  stands for its value at the -th iteration, e.g.,  is the estimate of  at the -th step. The EM at the -th iteration is: E-step Assuming  known,  can be updated by the corresponding posterior probabilities: (3) M-step Assuming  known, the new mixing parameters  are updated by: (4) When the MLE of  is found, using Equation (1), the MLE of  can be calculated, thereby solving the original problem. The space complexity of the EM algorithm is  and the time complexity of the EM algorithm is , where  the average number of associated genomes for one read and  is the time cost related to the convergence criteria for EM. Since  and  are both constants not related to n, the algorithm is linear in space and time complexity with the read number . Further, the concavity of the log-likelihood function can be shown and the EM algorithm is guaranteed to converge to global maximum (see Text S1). Read probability approximations The probability  is assessed based on . Ideally, it is the probability that  is generated when read being uniformly sampled from genome . Let  be the number of copies of read  in . Then the probability is approximated by: (5) However, due to sequencing errors and natural genetic variations, the 's are not readily observable. When the mapping or alignment results from BLAST, BLAT, or other mapping tools are available, the number of high quality hits of  on  can effectively be used as 's. To keep only these reliable and statistically significant hits, raw hits are filtered by E-value, alignment length and identity rate. We refer to the finite mixture model with the read probability from mapping and alignment results as ‘map’ in the remainder of the paper. An alternative way to assess the read probabilities is by using k-mer composition. For the j-th genome, we calculate the fraction of a k-word w by , the normalized frequency of the word  in genome . For a read , we define pseudo-likelihood for  by: (6) where  is the set of words formed by sliding windows of size k along . This probabilistic assignment captures the overall similarity between reads and genomes, an idea adopted in other composition-based studies such as in Sandberg et al. [35]. It is especially useful when a large number of reads do not have reliable hits with reference genomes. We will refer to the finite mixture model with the read probability from the multinomial k-mer composition as ‘k-mer’ in the remainder of the paper. Standard errors for GRA estimates We also derived the asymptotic covariance matrix for the mixing parameters  using the asymptotic theory for MLE estimates. Because there are  independent parameters in , we can choose them as  and denote by . Further, let  and  be the MLE estimates for  and its corresponding GRA, respectively. Then, the asymptotic standard error for  is approximately: (7) for , where  is the observed information matrix. If only a small number (as compared to number of parameters) of reads are mapped, the conditions for the asymptotic to hold cannot be satisfied. We can alternatively use the bootstrap covariance estimator for the standard error of MLE: (8) for , where  is the bootstrap mean estimator. Numerical error measures We use the following measures to evaluate the accuracy of the GRA estimate. Let the true GRA be  and its estimate . The first measure is the commonly used root mean square version of relative error  [36]. We also included three other error measures:  (the average relative error),  (the maximum relative error), and  (the Total Variation Distance [37]), which are all commonly used to evaluate the accuracy of an estimate. Real read sets and reference genome sets In preparing the real read sets, we downloaded the FAMeS data from JGI (http://FAMeS.jgi-psf.org), the ‘hg’ data from TraceDB (ftp://ftp.ncbi.nih.gov/pub/TraceDB/, NCBI project id: 16729), the ‘uhg’ data from Sequence Read Archive (http://www.ncbi.nlm.nih.gov/Traces/sra/, NCBI project id: 32089), the ‘jhg’ data from BGI (http://gutmeta.genomics.org.cn/) [30] and the ‘amd’ data from TraceDB (NCBI project id: 13696). In preparing the reference genome sets, we downloaded currently available complete and draft bacteria genomes from the NCBI Refseq (http://ftp.ncbi.nih.gov/refseq), MetaHit (http://www.metahit.eu/), HMCJ (http://metagenome.jp), WUSTL Gordon Lab (http://genome.wustl.edu/) and JGI (http://genome.jgi-psf.org/). We manually curated genomes to remove redundancy and organized them into a NCBI Taxonomy (http://www.ncbi.nlm.nih.gov/Taxonomy) database. We used the genome information available from IMG/M (http://img.jgi.doe.gov), IMG/HMP (http://www.hmpdacc-resources.org/cgi-bin/img_hmp) and GOLD (http://www.genomesonline.org) to group them by habitats [38], [39]. Finally, we obtained 388 human gastrointestinal tract genomes for a human gut reference genome set (‘HGS’). Read filtering and assignment procedures In the ‘map’ read probability backend, we used BLAT to map reads to reference genomes. We prefer BLAT to BLAST, as BLAT is tens of times faster in handling low-sensitivity similarity search for massive number of sequences than BLAST. Since we only kept alignment results with identity rate greater than 90%, the BLAT result should not differ much from what if BLAST was used. For the human gut and simulated data, we used similar filtering methods as by Turnbaugh et al. [17], [40] (E-value ≤0.0001, aligned length more than 75% of its RL and identity ≥90%). In the ‘k-mer’ read-probability backend, we used k-mer length k = 6. For GAAS and MEGAN, we used the same mapping results from BLAT, as a common starting point. We used GAAS's default filtering options (E-value≤0.0001, aligned length more than 80% of its RL, and identity ≥80%), as well as MEGAN's default options (min-score = 35 for RL equal to 100 bp and min-score = 50 for RL equal to 400 bp; top percent = 5%, min support = 2), for comparisons. In evaluating the ribotype and protein marker based method, we used the E.coli 16S rRNA rrsE and ribosome protein rpoB genes to retrieve homolog sequences from the simulated reads, which were then filtered by options (E-value0.0001, aligned length more than 75% of its RL and identity 90%), according to [28]. Our validations have shown that variations of these parameters within a reasonable range had little effect on the results. Higher level taxonomic statistics Many downstream analyses can be carried out based on GRAMMy's estimates. For example, the average genome length  is readily obtainable: (9) Subsequently, we can test the statistical significance of the median average genome length difference between two sample groups by Wilcoxon test (wilcox.test in R). Since genome size bias has already been corrected, we can use GRAMMy estimates to calculate the relative abundance of a higher-level taxon by simple addition. For this purpose, we used the NCBI Taxonomy, which has the taxonomic assignments for all reference genomes we used here. To illustrate, for a specific taxonomic level , the relative abundance of a -th specific taxon  is: (10) and (11) where  can be any one of the seven hierarchical levels in the taxonomy, from species to kingdom. Hierarchical biclustering It is possible to use GRAMMy estimates for clustering analysis and statistical hypothesis testing. We clustered the samples based on the pairwise similarities (correlations) of their relative abundance distribution. Because of the long-tailed shape of the distribution, the signal-to-noise ratio is low for these less abundant genomes. Therefore, using the thresholds .05% for the minimum abundance and 50% for the minimum occurrence [18], we selected the estimates for these more abundant genomes (which are more reliable for clustering). We used rank transformation, which normalizes GRAs by taking their ranks and applying score transformation and R function heatmap for hierarchical clustering.",AbundanceEstimation,"accurate genome relative abundance estimation base  shotgun metagenomic reads
 finite mixture model  develop  finite mixture model   grammy framework follow angly    use genome relative abundance gra   relative abundance measure  mostly unicellular microbial organisms   describe  sample  sequence procedure  follow first randomly choose  genome   probability  proportional       abundance     genome length second randomly generate  read    without loss  generality   assume    give genome    reasonably approximate  generation  shotgun read   component distribution     probability  generate  read        reasonable assumption  independence   two sample step  whole procedure  probabilistically equivalent  sample   mixture distribution    mix parameters denote      component distributions denote       number  genomes subsequently  read set denote     regard   realize independent identically distribute iid sample  size    mixture   relative abundance  know genomes  exactly  transformation   mix parameters     estimate base   read set   schematic view   finite mixture model  show  figure    component distributions properly set    find  maximum likelihood estimate mle   mix parameters  many study  knowledge   genomes present   community  limit   circumstances   define  mixture   first  components  know genomes   last  component   collective  unknown genomes note     know components  suppose   genome sequence   genome size   know therefore  gra  know genomes    normalize abundance   relative abundance    know genome       biological set  want  estimate vector     measure  organism relative abundance   transform mixture problem   relate   mix parameters      inverse     number  sample read   proportional   genome relative abundance   length   two factor  confound  miss knowledge   genome length  prohibit  estimation     data since  effective genome length    unknown genomes   available  cannot estimate  relative abundance   unknown component however  relative abundance  know genomes  still  estimate use  procedures estimation  gra use expectation maximization  algorithm  estimate  mix parameters  adopt   algorithm  calculate  maximum likelihood estimate mle    framework  assume  miss data matrix     entry    random variable indicate whether          solve   parameters  iteratively estimate    use algorithm  see support methods note   variable  superscript  stand   value    iteration     estimate      step      iteration  estep assume  know    update   correspond posterior probabilities  mstep assume  know  new mix parameters   update     mle    find use equation   mle     calculate thereby solve  original problem  space complexity    algorithm     time complexity    algorithm      average number  associate genomes  one read     time cost relate   convergence criteria   since      constants  relate    algorithm  linear  space  time complexity   read number    concavity   loglikelihood function   show    algorithm  guarantee  converge  global maximum see text  read probability approximations  probability   assess base   ideally    probability    generate  read  uniformly sample  genome  let    number  copy  read      probability  approximate   however due  sequence errors  natural genetic variations  '   readily observable   map  alignment result  blast blat   map tool  available  number  high quality hit      effectively  use  '  keep   reliable  statistically significant hit raw hit  filter  evalue alignment length  identity rate  refer   finite mixture model   read probability  map  alignment result  map   remainder   paper  alternative way  assess  read probabilities   use kmer composition   jth genome  calculate  fraction   kword     normalize frequency   word   genome    read   define pseudolikelihood         set  word form  slide windows  size  along   probabilistic assignment capture  overall similarity  read  genomes  idea adopt   compositionbased study    sandberg      especially useful   large number  read    reliable hit  reference genomes   refer   finite mixture model   read probability   multinomial kmer composition  kmer   remainder   paper standard errors  gra estimate  also derive  asymptotic covariance matrix   mix parameters  use  asymptotic theory  mle estimate     independent parameters     choose     denote    let      mle estimate     correspond gra respectively   asymptotic standard error    approximately        observe information matrix    small number  compare  number  parameters  read  map  condition   asymptotic  hold cannot  satisfy   alternatively use  bootstrap covariance estimator   standard error  mle        bootstrap mean estimator numerical error measure  use  follow measure  evaluate  accuracy   gra estimate let  true gra     estimate   first measure   commonly use root mean square version  relative error    also include three  error measure   average relative error   maximum relative error    total variation distance     commonly use  evaluate  accuracy   estimate real read set  reference genome set  prepare  real read set  download  fames data  jgi    data  tracedb ftpftpncbinihgovpubtracedb ncbi project    uhg data  sequence read archive  ncbi project    jhg data  bgi     amd data  tracedb ncbi project    prepare  reference genome set  download currently available complete  draft bacteria genomes   ncbi refseq  metahit  hmcj  wustl gordon lab   jgi   manually curated genomes  remove redundancy  organize    ncbi taxonomy  database  use  genome information available  imgm  imghmp   gold   group   habitats   finally  obtain  human gastrointestinal tract genomes   human gut reference genome set hgs read filter  assignment procedures   map read probability backend  use blat  map read  reference genomes  prefer blat  blast  blat  tens  time faster  handle lowsensitivity similarity search  massive number  sequence  blast since   keep alignment result  identity rate greater    blat result   differ much    blast  use   human gut  simulate data  use similar filter methods   turnbaugh     evalue ≤ align length        identity ≥   kmer readprobability backend  use kmer length     gaas  megan  use   map result  blat   common start point  use gaas' default filter options evalue≤ align length        identity ≥  well  megan' default options minscore     equal     minscore     equal    top percent   min support    comparisons  evaluate  ribotype  protein marker base method  use  ecoli  rrna rrse  ribosome protein rpob genes  retrieve homolog sequence   simulate read    filter  options evalue0 align length        identity  accord    validations  show  variations   parameters within  reasonable range  little effect   result higher level taxonomic statistics many downstream analyse   carry  base  grammy' estimate  example  average genome length   readily obtainable  subsequently   test  statistical significance   median average genome length difference  two sample group  wilcoxon test wilcoxtest   since genome size bias  already  correct   use grammy estimate  calculate  relative abundance   higherlevel taxon  simple addition   purpose  use  ncbi taxonomy    taxonomic assignments   reference genomes  use   illustrate   specific taxonomic level   relative abundance    specific taxon           one   seven hierarchical level   taxonomy  species  kingdom hierarchical biclustering   possible  use grammy estimate  cluster analysis  statistical hypothesis test  cluster  sample base   pairwise similarities correlations   relative abundance distribution    longtailed shape   distribution  signaltonoise ratio  low   less abundant genomes therefore use  thresholds    minimum abundance     minimum occurrence   select  estimate    abundant genomes    reliable  cluster  use rank transformation  normalize gras  take  rank  apply score transformation   function heatmap  hierarchical cluster",6
153,MetaID,"MetaID: a novel method for identification and quantification of metagenomic samples
Datasets The input genome dataset consists of a catalogue of 2,031 completely sequenced genomes retrieved from NCBI (ftp://ftp.ncbi.nih.gov/genomes/Bacteria/) in July 2012. The nucleotide sequences from the 2,031 bacterial genomes spans across 292 genera, 537 species and 1,246 strains. After downloading the entire dataset for the bacterial genomes all the plasmid sequences for the respective bacterial genomes were removed. Each of the 2,031 genomes was tagged using the first three letters of their genus and species names. In addition, the entire strain name was retained for clarity purpose. For example, the genome Chlamydia trachomatis D/UW-3/CX was tagged as CHL_TRA_ D/UW-3/CX. Table S10 in Additional File 1 lists the entire set of 2,031 genomes and their associated statistics such as the length of the genome, the number of n-grams (n = 12) in the genome, the number of unique and common n-grams in the genome and the repeat ratio. The n-gram model for nucleotide representation An n-gram is any subsequence of a nucleotide sequence of fixed length n. In literature, these nucleotide subsequences have been called alternatively as n-mers, oligonucleotide, oligopeptide, etc. For the purpose of obtaining common and unique n-grams across all the 2,031 bacterial genomes, all possible n-grams were extracted from each of the genomes in the dataset. Given a dataset of genome sequences D, let d i be the complete nucleotide sequence for an organism O i in D where 𝑑𝑖=(𝑠1𝑠2…𝑠𝑘), where 𝑠𝑖∈Σ where Σ represent the set of four nucleotide A, G, C and T, then a set of (𝑘−𝑛+1)n-grams can be obtained from 𝑑𝑖 as 𝑔1=(𝑠1…𝑠𝑛), 𝑔2=(𝑠2…𝑠𝑛+1),...,𝑔𝑘−𝑛+1=(𝑠𝑘−𝑛+1…𝑠𝑘). Using this n-gram model, the following property of n-grams can be observed. There are countable numbers of n-grams across all the genomes that are highly abundant. This phenomenon is related to Zipf's law [15]. Here it is important to note that few of the bacterial genomes contain additional letters namely N, R, Y, W, M, S, or K to account for two ambiguous bases in any given position. For example the letter N at any given position indicates unknown base, the letter R at any given position indicates either A or G, the letter Y at any given position indicates either C or T and so on. In addition to that letters B, D, H and V represents 3-base ambiguities. Therefore, ∑ = {A, C, T, G, N, R, Y, W, M, S, K, B, D, H, V}. Unique and common n-gram profile From the entire 2,031 genomes, all possible non-repeating n-grams (n = 12) were obtained. The n-grams from each genome were compared against the n-grams in the other genomes to finally arrive at a set of unique (present in a single genome) and common (present in multiple genomes) n-grams. The unique n-gram set includes two columns - the n-gram and the genome in which it is present. On the other hand the common n-gram set includes four columns - the n-gram, frequency of its occurrence in the entire dataset, its weight assigned by the scoring function and the genomes in which it is present. Scoring function The scoring function obtains a set of common and unique n-grams based on the n-gram model discussed above. The scoring function is parameterized with the length of the n-gram and the target dataset to begin with. The scoring function reads in the nucleotide sequences of each genome, and generates all possible n-grams without any repeats. If a nucleotide sequence is of length 𝑘, then the total number of n-grams is given by (𝑘−𝑛+1). Once all the n-grams are generated, the scoring function compares all the n-grams from a genome against those from all the other genomes in the dataset. After successful comparison the scoring function determines a profile of all the common and unique n-grams in the dataset. All the unique n-grams are assigned a weight of unity, i.e. 1, and the common n-grams are assigned weights using a dampening factor that accounts for how popular the n-gram is with respect to the genomes present in the dataset. For any n-gram 𝑥, the dampening factor is given by the expression 𝗅𝗈𝗀𝑒∣∣𝑐∣∣∣∣𝑐:𝑥∈𝑐∣∣/𝗅𝗈𝗀𝑒∣∣𝑐∣∣, where ∣∣𝐶∣∣ denotes the total number of genomes in the dataset and ∣∣{𝑐:𝑥∈𝑐}∣∣ denotes the total number of genomes in which the n-gram 𝑥 is present. This factor is similar to the term 'weighting' as discussed in our previous study [14]. The damping factor adjusts the weights of the n-grams in such a way that popular n-grams receive a low weightage and vice-versa. Table S3 in Additional File 2 shows the weights assigned to few hypothetical n-grams based upon their frequency of occurrence in the dataset. If the n-gram is present only in a single genome then its weight is unity, i.e. 1, and if it is present in all the genomes then its weight is zero, i.e. 0. Model building The model-building step involves indexing the entire set of common and unique n-grams and assigning appropriate weight to each n-gram based on its frequency profile across the reference genome set. For model building our tool considers either the entire set (100%) or a partial (75%, 50% and 25%) set of non-repeatable n-grams from each genome. For model building using a partial genome set, non-repeating n-grams are randomly selected from the genome. The number of n-grams selected from each genome is proportionate to their size. Model building is a very crucial step in MetaID and it is also a time consuming process. In case of adding new genomes to the dataset or adding a completely different community including viral, fungus, archaeal, etc., the model-building step has to be carried out again. Therefore, this update process can be scheduled at periodic intervals. Moreover, model-building step in our tool is an offline process. Repeat ratio While harvesting the n-grams (n = 12) from the reference genomes we observed that there are a large number of n-grams that have the tendency to re-appear. Therefore, we came up with a parameter ""repeat ratio"" to account for the abundances of repeated n-grams in each genome. Repeat ratio is determined by computing the fraction of the repeated n-grams to the total number of n-grams in the genome. Here repeat ratios are represented as percentages. The Table S11 and Figure S1 (in Additional File 2) presents a histogram of the repeat ratio distribution across the 2,031 bacterial genomes. Across 2,031 bacterial genomes the repeat ratio distribution ranged widely between 0.85% to 71.53%. Only small fractions of the genome, i.e., 3.3% have repeat ratios within 10%. Almost about 69.2% of the genomes have a repeat ratio between 25% and 60%. In total nearly 99.6% of the genomes have their repeat ratios ranging from 10% to 70%. The mean and the standard deviation of the repeat ratios across 2,031 bacterial genomes were observed to be 27.57 and 12.52 respectively. Testing and identification (classification) Though the objectives behind our testing and identification (classification) steps are the same, there is a subtle difference between them. For testing we consider 1%, 3%, 5%, and 7% of the non-repeated n-grams randomly chosen from each genome and try to identify their origin. In contrast, for identification we consider the entire set of metagenomic reads to harvest all possible n-grams (n = 12) and try to determine the constituent organisms in a given community. Let us consider 𝑅={𝑔1,𝑔2,𝑔3…,𝑔𝑛} as a set of n-grams obtained from the reads or randomly selected from the genome and 𝐺={𝐺1,𝐺2,𝐺3…,𝐺𝑚} as the set of genomes present in the database. We define a mapping from R to G as 𝑅→𝐺 where all the elements in domain R maps to a single element in co-domain G i.e. 𝑔1,𝑔2,𝑔3…,𝑔𝑛→𝐺𝑥 where 𝐺𝑥 is the only single range in co-domain 𝐺 and 𝐺𝑥∈𝐺. To obtain a mapping from R to G we construct a 𝑛*𝑚 matrix of the form ⎛⎝⎜⎜⎜𝑦0,0⋮𝑦𝑛,0⋯⋱⋯𝑦0,𝑚⋮𝑦𝑛,𝑚⎞⎠⎟⎟⎟ where we define 𝑇={𝑐1,𝑐2,𝑐3…,𝑐𝑚} where 𝑐0={𝑦0,0,𝑦1,0,𝑦2,0…,𝑦𝑛,0}, 𝑐1={𝑦0,1,𝑦1,1,𝑦2,1…,𝑦𝑛,1}, 𝑐𝑚={𝑦0,𝑚,𝑦1,𝑚,𝑦2,𝑚…,𝑦𝑛,𝑚} are the columns in the 𝑛*𝑚 matrix and 𝑦𝑒,𝑓 represent the weight assigned to an n-gram e that is present in genome f or 0 if the n-gram e is not present in the genome f. In the above-mentioned 𝑛*𝑚 matrix we define ∑𝑐𝑧=∑(𝑦0,𝑧+𝑦1,𝑧+𝑦2,𝑧+…+𝑦𝑛,𝑧) as the sum of all the elements in the column z. After computing the sum of each column in the 𝑛*𝑚 matrix we arrange all the column sums in a descending order. We then associate 𝑔1,𝑔2,𝑔3…,𝑔𝑛→𝐺𝑥 provided that ∑𝑐𝑥>∑𝑐𝑥−1>∑𝑐𝑥−2>…>∑𝑐1 and ∑𝑐𝑥>∑𝑐𝑥+1>∑𝑐𝑥+2>…>∑𝑐𝑚. In summary, after obtaining the n-grams from the reads or from the genomes we construct a matrix with the rows representing the n-grams and the columns representing the entire set of genomes in the dataset. We then replace each matrix entry with the weight of the n-gram corresponding to that particular genome. If an n-gram is not part of the genome then we replace that entry with a zero i.e., 0. After filling the matrix entries, we determine the column sum against each genome; identify the highest column sum and associate (map) the entire set of n-grams to that particular genome. It is important to note that in the identification step we try to map a set of reads to a genome instead of mapping each single read to a genome. This is because it is hard to classify each single read to a genome due to the intense computation involvement and lack of discriminatory signals in them. Again, in order to ensure a successful classification we compared our classification results against the classifications performed by MetaSim. MetaSim reads Metagenomic reads for our mock-staggered communities were obtained using the MetaSim simulation tool. On parameterizing MetaSim with the genomes, their abundance profile, the empirical error model (Table S12 in Additional File 2) and the total number of reads to be generated; MetaSim generates a set of reads against each genome. For our mock-staggered community, MetaSim generated about 3 million 100 bp pair-end reads. Table S13 in Additional File 2 shows the parameter settings used in MetaSim for constructing the mock staggered community and the details of the simulation output. Mock communities Two different mock communities were used in this study. The first one is the mock-even community that is constructed from two datasets namely HC1 and HC2 obtained from MetaPhlAn website (http://www.huttenhower.org/metaphlan). The original datasets consisted of 100 genomes each with an equal abundance of 1%. From these datasets, we constructed a mock-even community of 167 microbial genomes (72 from HC1 + 95 from HC2) that are also present in our 2,031 set of reference genomes. The entire set of reads pertaining to these 167 genomes was included in our community to ensure that their abundances are equal i.e. 1%. We eliminated the rest 33 genomes either due to their absence in our dataset or because there was no appropriate mapping found between the KEGG ID's in HC1 and HC2 to our NCBI names in the database. Secondly, we constructed a mock-staggered community by randomly choosing 100 microbial genomes out of the 2,031 genomes in our dataset. The final mock-staggered community included genomes with genome sizes varying between 641,770 to 9,033,684 and with their repeat ratios ranging from 7 to 63. For this community, we randomly assigned an abundance value for each genome between 0.1% and 10% totaling up to 100% (Table S9 in Additional File 1). Abundance estimation Considering a set of reads from a genome, we harvested all possible non-repeated n-grams (n = 12) and mapped them against their reference genome. Upon mapping, we counted the total number of n-grams that is in common (intersection) between the reads and the reference genome. We determined the relative ""Observed Abundance"" for a genome as the ratio of its number of non-repeated n-gram counts to the total sum of the non-repeating n-grams of the genomes present in the community multiplied by the total number of genomes in the sample. After determining the observed abundances we noticed that genomes with extreme repeat ratios i.e. above 50 or below 15 had a tendency to be estimated higher or lower respectively. Therefore to correct the observed abundances we either subtract or add the first standard deviation of the repeat ratios of 2031 genomes to the mean of the repeat ratios of 2031 genomes. On the other hand, if the repeat ratio of a genome lies between 15 and 50 then the mean of the repeat ratios of 2031 genomes is used as such. The corrected abundance for a genome is reported based on their repeat ratio using the following expressions: 𝐶𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒−𝗅𝗈𝗀10(𝑅𝑒𝑝𝑒𝑎𝑡𝑅𝑎𝑡𝑖𝑜𝜇𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠). 15<𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜<50. 𝐶𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒−𝗅𝗈𝗀10(𝑅𝑒𝑝𝑒𝑎𝑡𝑅𝑎𝑡𝑖𝑜𝜇𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠±𝜎𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠). 𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜>50𝑎𝑛𝑑𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜<15. Where 𝜇 is the mean and 𝜎 is the standard deviation of the repeat ratios for the 2,031 genomes present in our dataset (Table S10 in Additional File 1). Note here that the mean and standard deviation for the repeat ratios will change with the addition or elimination of genomes in the dataset. From Figure S1 (Additional File 2), we noticed that most of the genomes have their repeat ratio ranging between 15% and 50%. Therefore when correcting the abundances (Corrected Abundance) we subtract one standard deviation from the mean for those genomes whose repeat ratio is above 50% and add one standard deviation to the mean for those genomes whose repeat ratio is below 15%. For the genomes with repeat ratios between 15% and 50%, the mean of the repeat ratio is considered as such. Here we report the abundance estimates for any given community in percentages i.e. 100% for the entire community or equal to the number of microbial species in the community. Therefore, if the corrected abundance does not add up to 100% or equal to the number of species we report the ""Estimated Abundance"" which is normalized to either 100% or equal to the number of species in the community. Performance metrics We report standard performance measure in terms of accuracy as percentages. Accuracy is defined as the ratio of number of entries (genomes) that have been correctly identified to the number of entries under consideration. In some cases, we have reported balanced accuracies wherever we have information about specificity and sensitivity",AbundanceEstimation,"metaid  novel method  identification  quantification  metagenomic samples
datasets  input genome dataset consist   catalogue   completely sequence genomes retrieve  ncbi ftpftpncbinihgovgenomesbacteria  july   nucleotide sequence    bacterial genomes span across  genera  species   strain  download  entire dataset   bacterial genomes   plasmid sequence   respective bacterial genomes  remove     genomes  tag use  first three letter   genus  species name  addition  entire strain name  retain  clarity purpose  example  genome chlamydia trachomatis duwcx  tag  chl_tra_ duwcx table s10  additional file  list  entire set   genomes   associate statistics    length   genome  number  ngrams      genome  number  unique  common ngrams   genome   repeat ratio  ngram model  nucleotide representation  ngram   subsequence   nucleotide sequence  fix length   literature  nucleotide subsequences   call alternatively  nmers oligonucleotide oligopeptide etc   purpose  obtain common  unique ngrams across    bacterial genomes  possible ngrams  extract     genomes   dataset give  dataset  genome sequence  let     complete nucleotide sequence   organism      𝑑𝑖𝑠1𝑠2…  ∈   represent  set  four nucleotide        set  𝑘𝑛ngrams   obtain    𝑔1𝑠1… 𝑔2𝑠2…𝑠𝑛𝑔𝑘𝑛𝑠𝑘𝑛… use  ngram model  follow property  ngrams   observe   countable number  ngrams across   genomes   highly abundant  phenomenon  relate  zipf' law     important  note     bacterial genomes contain additional letter namely          account  two ambiguous base   give position  example  letter    give position indicate unknown base  letter    give position indicate either     letter    give position indicate either        addition   letter      represent base ambiguities therefore ∑  {              } unique  common ngram profile   entire  genomes  possible nonrepeating ngrams     obtain  ngrams   genome  compare   ngrams    genomes  finally arrive   set  unique present   single genome  common present  multiple genomes ngrams  unique ngram set include two columns   ngram   genome     present    hand  common ngram set include four columns   ngram frequency   occurrence   entire dataset  weight assign   score function   genomes     present score function  score function obtain  set  common  unique ngrams base   ngram model discuss   score function  parameterized   length   ngram   target dataset  begin   score function read   nucleotide sequence   genome  generate  possible ngrams without  repeat   nucleotide sequence   length    total number  ngrams  give      ngrams  generate  score function compare   ngrams   genome       genomes   dataset  successful comparison  score function determine  profile    common  unique ngrams   dataset   unique ngrams  assign  weight  unity     common ngrams  assign weight use  dampen factor  account   popular  ngram   respect   genomes present   dataset   ngram   dampen factor  give   expression 𝗅𝗈𝗀𝑒∣∣∣∣∣∣∈∣∣𝗅𝗈𝗀𝑒∣∣∣∣  ∣∣∣∣ denote  total number  genomes   dataset  ∣∣{∈}∣∣ denote  total number  genomes    ngram   present  factor  similar   term 'weighting'  discuss   previous study   damp factor adjust  weight   ngrams    way  popular ngrams receive  low weightage  viceversa table   additional file  show  weight assign   hypothetical ngrams base upon  frequency  occurrence   dataset   ngram  present    single genome   weight  unity       present    genomes   weight  zero   model build  modelbuilding step involve index  entire set  common  unique ngrams  assign appropriate weight   ngram base   frequency profile across  reference genome set  model build  tool consider either  entire set    partial     set  nonrepeatable ngrams   genome  model build use  partial genome set nonrepeating ngrams  randomly select   genome  number  ngrams select   genome  proportionate   size model build    crucial step  metaid    also  time consume process  case  add new genomes   dataset  add  completely different community include viral fungus archaeal etc  modelbuilding step    carry   therefore  update process   schedule  periodic intervals moreover modelbuilding step   tool   offline process repeat ratio  harvest  ngrams      reference genomes  observe     large number  ngrams    tendency  reappear therefore  come    parameter ""repeat ratio""  account   abundances  repeat ngrams   genome repeat ratio  determine  compute  fraction   repeat ngrams   total number  ngrams   genome  repeat ratios  represent  percentages  table s11  figure   additional file  present  histogram   repeat ratio distribution across   bacterial genomes across  bacterial genomes  repeat ratio distribution range widely      small fraction   genome    repeat ratios within  almost     genomes   repeat ratio      total nearly    genomes   repeat ratios range      mean   standard deviation   repeat ratios across  bacterial genomes  observe      respectively test  identification classification though  objectives behind  test  identification classification step       subtle difference    test  consider        nonrepeated ngrams randomly choose   genome  try  identify  origin  contrast  identification  consider  entire set  metagenomic read  harvest  possible ngrams     try  determine  constituent organisms   give community let  consider {𝑔1𝑔2𝑔3…}   set  ngrams obtain   read  randomly select   genome  {𝐺1𝐺2𝐺3…}   set  genomes present   database  define  map      →    elements  domain  map   single element  codomain   𝑔1𝑔2𝑔3…→      single range  codomain   ∈  obtain  map      construct  * matrix   form ⎛⎝⎜⎜⎜⋮⋯⋱⋯𝑦0𝑚⋮𝑦𝑛𝑚⎞⎠⎟⎟⎟   define {𝑐1𝑐2𝑐3…}  {𝑦0𝑦1𝑦2…} {𝑦0𝑦1𝑦2…} {𝑦0𝑚𝑦1𝑚𝑦2𝑚…𝑦𝑛𝑚}   columns   * matrix  𝑦𝑒𝑓 represent  weight assign   ngram    present  genome      ngram    present   genome    abovementioned * matrix  define ∑∑𝑦0𝑧𝑦1𝑧𝑦2𝑧…𝑦𝑛𝑧   sum    elements   column   compute  sum   column   * matrix  arrange   column sum   descend order   associate 𝑔1𝑔2𝑔3…→ provide  ∑∑∑…∑  ∑∑∑…∑  summary  obtain  ngrams   read    genomes  construct  matrix   row represent  ngrams   columns represent  entire set  genomes   dataset   replace  matrix entry   weight   ngram correspond   particular genome   ngram   part   genome   replace  entry   zero    fill  matrix entries  determine  column sum   genome identify  highest column sum  associate map  entire set  ngrams   particular genome   important  note    identification step  try  map  set  read   genome instead  map  single read   genome      hard  classify  single read   genome due   intense computation involvement  lack  discriminatory signal     order  ensure  successful classification  compare  classification result   classifications perform  metasim metasim read metagenomic read   mockstaggered communities  obtain use  metasim simulation tool  parameterizing metasim   genomes  abundance profile  empirical error model table s12  additional file    total number  read   generate metasim generate  set  read   genome   mockstaggered community metasim generate   million   pairend read table s13  additional file  show  parameter settings use  metasim  construct  mock stagger community   detail   simulation output mock communities two different mock communities  use   study  first one   mockeven community   construct  two datasets namely hc1  hc2 obtain  metaphlan website   original datasets consist   genomes    equal abundance     datasets  construct  mockeven community   microbial genomes   hc1    hc2   also present    set  reference genomes  entire set  read pertain    genomes  include   community  ensure   abundances  equal    eliminate  rest  genomes either due   absence   dataset      appropriate map find   kegg '  hc1  hc2   ncbi name   database secondly  construct  mockstaggered community  randomly choose  microbial genomes     genomes   dataset  final mockstaggered community include genomes  genome size vary        repeat ratios range       community  randomly assign  abundance value   genome     total    table   additional file  abundance estimation consider  set  read   genome  harvest  possible nonrepeated ngrams     map    reference genome upon map  count  total number  ngrams    common intersection   read   reference genome  determine  relative ""observed abundance""   genome   ratio   number  nonrepeated ngram count   total sum   nonrepeating ngrams   genomes present   community multiply   total number  genomes   sample  determine  observe abundances  notice  genomes  extreme repeat ratios         tendency   estimate higher  lower respectively therefore  correct  observe abundances  either subtract  add  first standard deviation   repeat ratios   genomes   mean   repeat ratios   genomes    hand   repeat ratio   genome lie       mean   repeat ratios   genomes  use    correct abundance   genome  report base   repeat ratio use  follow expressions 𝐶𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒𝗅𝗈𝗀10𝑅𝑒𝑝𝑒𝑎𝑡𝑅𝑎𝑡𝑖𝑜𝜇𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠 𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜 𝐶𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑𝐴𝑏𝑢𝑛𝑑𝑎𝑛𝑐𝑒𝗅𝗈𝗀10𝑅𝑒𝑝𝑒𝑎𝑡𝑅𝑎𝑡𝑖𝑜𝜇𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠±𝜎𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜𝑜𝑓2031𝑔𝑒𝑛𝑜𝑚𝑒𝑠 𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜50𝑎𝑛𝑑𝑅𝑒𝑝𝑒𝑎𝑡𝑟𝑎𝑡𝑖𝑜     mean     standard deviation   repeat ratios    genomes present   dataset table s10  additional file  note    mean  standard deviation   repeat ratios  change   addition  elimination  genomes   dataset  figure  additional file   notice     genomes   repeat ratio range     therefore  correct  abundances correct abundance  subtract one standard deviation   mean   genomes whose repeat ratio     add one standard deviation   mean   genomes whose repeat ratio      genomes  repeat ratios      mean   repeat ratio  consider     report  abundance estimate   give community  percentages     entire community  equal   number  microbial species   community therefore   correct abundance   add     equal   number  species  report  ""estimated abundance""   normalize  either   equal   number  species   community performance metrics  report standard performance measure  term  accuracy  percentages accuracy  define   ratio  number  entries genomes    correctly identify   number  entries  consideration   case   report balance accuracies wherever   information  specificity  sensitivity",6
154,STAMP,"STAMP: statistical analysis of taxonomic and functional profiles
The original release of STAMP ( Parks and Beiko, 2010 ) was limited to comparing a single pair of taxonomic or functional profiles. This release adds statistical tests and plots for assessing differences between two or more treatment groups along with increased compatibility with popular bioinformatic software: Input data : STAMP can process functional and taxonomic profiles produced by QIIME ( Caporaso et al. , 2010 ), PICRUSt ( Langille et al. , 2013 ), MG-RAST ( Meyer et al. , 2008 ), IMG/M ( Markowitz et al. , 2008 ) and RITA ( MacDonald et al. , 2012 ). Custom profiles can also be specified as a tab-separated values file. STAMP can process input files containing hundreds of samples spanning thousands of features with a standard desktop computer. Statistical hypothesis tests : Welch’s t -test and White’s non-parametric t -test ( White et al. , 2009 ) are provided for comparing profiles organized into two groups. STAMP implements the ANOVA and Kruskal–Wallis H -test for comparing three or more groups of profiles. Statistically significant features can be further examined with post hoc tests (e.g. Tukey–Kramer) to determine which groups of profiles differ from each other. Effect size and confidence intervals : Widely used effect size measures are provided for all statistical tests to aid in determining features with biologically relevant differences between groups. Two-group tests use the difference in mean proportion effect size measure along with Welch’s confidence intervals. The eta-squared effect size measure is used when considering multiple groups. Filtering of features : A feature can be filtered based on its P -value, effect size or prevalence within a group of profiles, to create plots focused on features likely to be biologically relevant. Specific subsets of features can also be manually filtered. Plots : Numerous publication-quality plots can be produced using STAMP. Principal component analysis (PCA; e.g. Fig. 1 a) plots, bar plots (e.g. Supplementary Fig. S1 ), box-and-whisker plots (e.g. Fig. 1 b), scatter plots and heat maps permit an initial exploratory analysis of profiles. Extended error bar plots (e.g. Fig. 1 c) provide a single figure indicating statistically significant features along with the P -values, effect sizes and confidence intervals. Fig. 1. Example outputs from STAMP. ( a ) PCA plot comparing class-level taxonomic profiles of 44 CBM communities sampled from shallow core cuttings or other (drilled cores, deep core cuttings, produced waters) niches within the coalbed environment. ( b ) Box-and-whisker plot illustrating Rhodocyclaceae taxa are only present in appreciable numbers within communities sampled from shallow core cuttings. ( c ) COG categories differing significantly between Melainabacteria and Oxyphotobacteria genomes with an effect size ≥0.75% Open in new tabDownload slide Example outputs from STAMP. ( a ) PCA plot comparing class-level taxonomic profiles of 44 CBM communities sampled from shallow core cuttings or other (drilled cores, deep core cuttings, produced waters) niches within the coalbed environment. ( b ) Box-and-whisker plot illustrating Rhodocyclaceae taxa are only present in appreciable numbers within communities sampled from shallow core cuttings. ( c ) COG categories differing significantly between Melainabacteria and Oxyphotobacteria genomes with an effect size ≥0.75%",AbundanceEstimation,"stamp statistical analysis  taxonomic  functional profiles
 original release  stamp  park  beiko    limit  compare  single pair  taxonomic  functional profile  release add statistical test  plot  assess differences  two   treatment group along  increase compatibility  popular bioinformatic software input data  stamp  process functional  taxonomic profile produce  qiime  caporaso      picrust  langille      mgrast  meyer      imgm  markowitz       rita  macdonald      custom profile  also  specify   tabseparated value file stamp  process input file contain hundreds  sample span thousands  feature   standard desktop computer statistical hypothesis test  welch  test  white nonparametric  test  white       provide  compare profile organize  two group stamp implement  anova  kruskalwallis  test  compare three   group  profile statistically significant feature    examine  post hoc test  tukeykramer  determine  group  profile differ    effect size  confidence intervals  widely use effect size measure  provide   statistical test  aid  determine feature  biologically relevant differences  group twogroup test use  difference  mean proportion effect size measure along  welch confidence intervals  etasquared effect size measure  use  consider multiple group filter  feature   feature   filter base    value effect size  prevalence within  group  profile  create plot focus  feature likely   biologically relevant specific subsets  feature  also  manually filter plot  numerous publicationquality plot   produce use stamp principal component analysis pca  fig   plot bar plot  supplementary fig   boxandwhisker plot  fig   scatter plot  heat map permit  initial exploratory analysis  profile extend error bar plot  fig   provide  single figure indicate statistically significant feature along    value effect size  confidence intervals fig  example output  stamp    pca plot compare classlevel taxonomic profile   cbm communities sample  shallow core cuttings   drill core deep core cuttings produce water niches within  coalbed environment    boxandwhisker plot illustrate rhodocyclaceae taxa   present  appreciable number within communities sample  shallow core cuttings    cog categories differ significantly  melainabacteria  oxyphotobacteria genomes   effect size ≥ open  new tabdownload slide example output  stamp    pca plot compare classlevel taxonomic profile   cbm communities sample  shallow core cuttings   drill core deep core cuttings produce water niches within  coalbed environment    boxandwhisker plot illustrate rhodocyclaceae taxa   present  appreciable number within communities sample  shallow core cuttings    cog categories differ significantly  melainabacteria  oxyphotobacteria genomes   effect size ≥",6
155,imGLAD,"imGLAD: accurate detection and quantification of target organisms in metagenomes
Overview of the imGLAD pipeline imGLAD assumes that reads of a metagenomic dataset originate at random from all regions of the genome. Thus, the fraction of the genome that is recovered in the dataset (sequencing breadth) as well as the number of times each region is sequenced (sequencing depth), both depend on the abundance of the organism in the community. Highly conserved regions (e.g., rRNA and tRNA genes), as well as regions resulting from recent horizontal gene transfer (e.g., transposase and integrase genes), can recruit reads from other non-target genomes and misleadingly increase the value of sequencing depth (and hence, estimated relative abundance) in some datasets depending on the gene composition of the organisms present. To address this problem, we developed a framework to identify which fraction of a target genome corresponds to reads that belong to the target and what fraction is the result of spurious matches. This framework has two steps: initial training and subsequent prediction (Fig. 1). Training set selection can be automatic or user defined. The automatic training generates reads from a randomly selected number of genomes (default is 200 genomes) from RefSeq (Pruitt, Tatusova & Maglott, 2007), and builds in-silico-generated datasets of about 1 million reads each. Simulated reads from the target genome(s) are then generated in a similar way and added to the former datasets in order to create the positive datasets with decreasing target abundances. Reads from the target genome(s) are omitted for the construction of negative datasets. All other genomes used to create the datasets are sampled in equal proportions (i.e., same relative abundances). The user can also choose the genomes to use to generate the training set (e.g., genomes previously known to co-occur in the same environment). In this case, the construction of the training set will be performed based on these genomes rather than the default genome collection from RefSeq. Simulated Illumina-like reads are generated using ART-MountRainier (Huang et al., 2012) with default settings. Simulation of reads from additional sequencing platforms is provided as an option, using also ART-MountRainier. Reads from both positive and negative samples are then recruited against the target genome sequence (reference) using BLAT (Kent, 2002). Alternatively, BLAST can be used to improve sensitivity at the expense of computational time (Altschul et al., 1997). By default, reads with identity higher than 95% and at least 90% of the read length aligned are selected to calculate sequencing breadth and sequencing depth, after normalizing for the size of the dataset. This level of identity has been shown to capture well the genome-aggregate Average Nucleotide Identity (ANI) typically seen between most currently named bacterial species, i.e., >95% ANI within vs. <95% ANI between species (Konstantinidis & Tiedje, 2005; Rodriguez et al., 2018) and the sequence-discrete populations recovered frequently in metagenomes of natural habitats (Caro-Quintero & Konstantinidis, 2012), although different user-defined cut-offs can be used as well. Members of such sequence-discrete populations show high gene-content and nucleotide sequence similarity among themselves, often -but not always- >95% ANI, and/or lower relatedness (e.g., <90% ANI) to close relatives (reviewed in Caro-Quintero & Konstantinidis, 2012). Sequencing depth (SD) is calculated as the number of reads mapping to the genome (N) multiplied by the read length (L) divided by the total length of the genome (G), and sequencing breadth (SB) is calculated as the number of bases covered (B) divided by the total length of the genome, using Eqs. (1) and (2) below, respectively. If the genome consists of more than one contig (e.g., draft genomes), the length is assumed to be the sum of the lengths of all contigs. (1)SD=L∗N∕G (2)SB=B∕G. A logistic function is fitted to the resulting recruitment data (i.e., SB and SD values or SB values alone; see also below) that attempts to separate the positive from the negative training datasets in terms of sequencing depth and sequencing breadth (the latter two are the variables of the function). In particular, this approach calculates the parameters of the logistic function by computing the error in the training set, i.e., what SB and SD values are observed for the 100 positive vs. the 100 negative training datasets, and modifying the parameters accordingly to reduce the error until convergence is reached. Error is assessed by a log-likelihood maximization via gradient approach, which modifies the parameter values until the error is minimized. Regression coefficients of the logistic equation are calculated for the SD and SB variables as well as for an intercept term and thus, the model estimates three parameters, i.e., SD, SB, and intercept. Final parameters of the model are estimated by default only based on SB (sequencing breadth), as this variable was found to be the most discriminating parameter for positive vs. negative samples (see also below). However, an estimation including SD is also provided as an option in order to produce, in addition to the probability of presence/absence, an accurate estimation of the abundance of the target genome. Estimation of the probability of detection and limit of detection Once the parameters of the logistic function have been determined (above), SB and SD can be used to reliably predict the probability of presence of the target genome in any number of query metagenomes after the reads of the query have been recruited against the target genome and (observed) SB is estimated as described above for training datasets. The probability of presence is estimated according to: (3)p=1−11∓e−z where z is a linear function of the form βTt, β represents the regression parameters and t is either a vector composed of the SD (Eq. (1)) and SB (Eq. (2)) or, by default, a one-dimensional variable corresponding to SB. Based on the model parameters (Eq. (3)), it is possible to establish a detection limit for the target genome in each metagenomic dataset analyzed. This limit is defined as the minimum fraction (SB) that needs to be sampled in order to estimate a probability of presence at 0.95. The result is displayed as a black solid line in a 2D plot of SB and SD (e.g., Fig. 2). The SD value observed based on the read recruitment, when corresponding to a probability value equal or higher to 0.95, is then used to estimate the relative abundance of the organism in the sample. The SD corresponding to 0.95 probability then provides the limit of detection in terms of relative abundance. Identification of target genomes in metagenomic datasets with imGLAD. Figure 2: Identification of target genomes in metagenomic datasets with imGLAD. Positive datasets (crosses) are separated from negative datasets (dots) through a logistic function (solid line) based on in-silico training datasets. (A) Datasets with reads of E. coli are separated from negative datasets. (B) Datasets with reads of B. anthracis are separated from negative datasets. Red asterisks denote the position of the experimental metagenomes (remaining dots represent in-silico generated datasets). Note the differences in scale on the x-axes between positive and negative datasets. Download full-size imageDOI: 10.7717/peerj.5882/fig-2 Filtering conserved regions To avoid spurious results from reads mapping on regions of the (target) genome with insufficient diversity (high sequence conservation such as rRNA genes) or frequently undergoing horizontal gene transfer such as mobile elements, the user can create a filter for these regions using MyTaxa (Luo, Rodriguez & Konstantinidis, 2014a). This filter is created by predicting genes in the target genome and determining their classification weight using MyTaxa. If the MyTaxa classification score is at the bottom 5% or the gene is not scored (e.g., some hypothetical proteins) the gene is removed from the genome and further analysis. The filtered version of the genome is subsequently used for the model training and probability estimation steps. Bioinformatic tool comparisons and tool parameters used MetaPhlAn V2 (Truong et al., 2015) was run with the default settings using Bowtie version 2.2.8 (Langmead & Salzberg, 2012) for read mapping. MetaMLST (Zolfo et al., 2017) was used with default settings. PathoScope 2.0 (Hong et al., 2014) was run with default settings, using the same set of reference genomes that were used to build the training datasets for imGLAD. Four tests were performed to assess specificity and sensitivity. In all cases, sensitivity was calculated as the proportion of properly classified positive datasets among the total number of positive datasets. Specificity was defined instead as the fraction of correctly identified negative datasets among all negative datasets examined. For the first test, metagenomic datasets were created with similar parameters to the training dataset of E. coli (i.e., 100 datasets from RefSeq genomes). These datasets were spiked with seven different concentrations of the E. coli genome in order to provide 1% to 7% coverage of the genome (i.e., sequencing breadth). In the second test, Human Microbiome Project (HMP) metagenomes were spiked with reads from the E. coli genome in order to provide 1% to 7% sequencing breadth as above. 571 HMP datasets were used for each E. coli concentration. In the third test, the datasets constructed in test 1 were spiked with reads from close relatives of E. coli, i.e., Klebsiella (81% ANI), Salmonella (82% ANI), and Escherichia fergusonii (92% ANI), at random concentrations for each genome in addition to the E. coli reads. Finally, a test using close relatives, i.e., >95% ANI representing strains of the same species, was performed in the HMP datasets in a similar way as described above for test #3. Leaf inoculation experiments to test imGLAD and sample sequencing Fifty grams of field-grown spinach leaves were inoculated (spiked in) with cells of Escherichia coli O157:H7 strain RM6067, a strain linked to the 2006 spinach-associated outbreak in the USA (Carter et al., 2011). Three serial dilutions were performed resulting in three inoculation concentrations: 80, 8 ×103 and 8 ×105 cells per pellet, plus a control sample with no inoculated cells. Cells for inoculation were obtained from single colonies that were grown overnight, and cell concentrations were determined by enumeration of colony-forming units (CFUs) on LB agar plates. Leaves were subsequently washed, the leaf wash was filtered to remove plant debris, and leaf-associated microorganisms were pelleted by centrifugation at 10,000 g for 10 min at 4 °C. DNA extraction was performed using MoBio UltraClean Microbial DNA isolation kit according to manufacturer’s instruction (MoBio). DNA sequencing libraries were prepared using the Illumina Nextera XT DNA library prep kit according to manufacturer’s recommendations, except that the protocol was terminated after isolation of cleaned amplified double stranded libraries. Library concentrations were determined by fluorescent quantification using a Qubit HS DNA kit and Qubit 2.0 fluorometer (Thermo Fisher Scientific, formerly Life Technologies, Waltham, MA, USA) according to manufacturer’s recommendations and libraries were run on a High-Sensitivity DNA chip using the Bioanalyzer 2100 instrument (Agilent, Santa Clara, CA, USA) to determine average library insert sizes. An equimolar mixture of the libraries (final loading concentration of 11 pM) was sequenced using a MiSeq reagent v3 kit for 600 cycles (2 × 300 bp paired end run) on an in-house Illumina MiSeq instrument (Georgia Institute of Technology), running the MiSeq control software v2.4.0.4 (MCS). Adapter trimming and demultiplexing of sequenced samples was carried out by the MCS. Additionally, we used metagenomic datasets inoculated with Bacillus anthracis DNA, which were made available previously (Be et al., 2013). McFadden’s pseudo-R2 metric to assess the robustness of the logistic model/function with close relatives The ability of the logistic model to distinguish between positive and negative training datasets when close relatives of increasing relatedness to the target genome were used in the training step was assessed using the McFadden’s pseudo-R2 metric. Specifically, the model determined (fitted) by imGLAD for a certain training dataset was compared to a standard, null logistic model which only contained an intercept variable. Effectively, this null model represented the standard logistic curve centered on the same point as the fitted imGLAD model but without any adjustment to the shape of that curve. Specifically, the metric was defined as: (4)R2McFadden=1−log(LC)log(Lnull) where LC is the maximized likelihood value for the fitted model and Lnull is the maximized likelihood value for the null model (intercept only, no covariates). Therefore, if the comparison shows perfect congruence between the two models (pseudo-R2 close to 0 value) this means that the fitted model is not robust but similar to a randomly drawn model. In contrast, when pseudo-R2 approaches 1, this denotes a robust fitted model. Note that pseudo-R2 may not equal 1, even for robust models, because the null model may approximate the fitted model estimated by imGLAD by chance alone in some iterations since it is drawn using the same intercept value. For this evaluation, the genome of one close relative at a time was added to the (negative and positive) training datasets at similar relative abundance (i.e., 10×, to ensured complete genome coverage) as the target genome (E. coli strain O157-H7) was added in the positive datasets. The genomes of relatives were sorted into the following groups corresponding to their ANI values to the target genome (%): 90, 95, 96, 97, 98.0, 98.2, 98.4, 98.6, 98.8, 99.0, 99.2, 99.4, 99.6, and 100. No genome was found with ANI value between 99.8% and 99.9% ANI. In addition to these related genomes, a uniform background dataset, which included 200 genomes showing <80% ANI to E. coli strain O157-H7, was included to provide positive and negative training datasets of adequate complexity. Successive iterations of imGLAD with the resulting training datasets that each contained one close relative of varied ANI value to the target genome were performed, and models were evaluated using the Eq. (4) above as implemented in the scipy module of Python.",AbundanceEstimation,"imglad accurate detection  quantification  target organisms  metagenomes
overview   imglad pipeline imglad assume  read   metagenomic dataset originate  random   regions   genome thus  fraction   genome   recover   dataset sequence breadth  well   number  time  region  sequence sequence depth  depend   abundance   organism   community highly conserve regions  rrna  trna genes  well  regions result  recent horizontal gene transfer  transposase  integrase genes  recruit read   nontarget genomes  misleadingly increase  value  sequence depth  hence estimate relative abundance   datasets depend   gene composition   organisms present  address  problem  develop  framework  identify  fraction   target genome correspond  read  belong   target   fraction   result  spurious match  framework  two step initial train  subsequent prediction fig  train set selection   automatic  user define  automatic train generate read   randomly select number  genomes default   genomes  refseq pruitt tatusova  maglott   build insilicogenerated datasets    million read  simulate read   target genomes   generate   similar way  add   former datasets  order  create  positive datasets  decrease target abundances read   target genomes  omit   construction  negative datasets   genomes use  create  datasets  sample  equal proportion   relative abundances  user  also choose  genomes  use  generate  train set  genomes previously know  cooccur    environment   case  construction   train set   perform base   genomes rather   default genome collection  refseq simulate illuminalike read  generate use artmountrainier huang     default settings simulation  read  additional sequence platforms  provide   option use also artmountrainier read   positive  negative sample   recruit   target genome sequence reference use blat kent  alternatively blast   use  improve sensitivity   expense  computational time altschul     default read  identity higher     least    read length align  select  calculate sequence breadth  sequence depth  normalize   size   dataset  level  identity   show  capture well  genomeaggregate average nucleotide identity ani typically see   currently name bacterial species   ani within   ani  species konstantinidis  tiedje  rodriguez      sequencediscrete populations recover frequently  metagenomes  natural habitats caroquintero  konstantinidis  although different userdefined cutoffs   use  well members   sequencediscrete populations show high genecontent  nucleotide sequence similarity among  often   always  ani andor lower relatedness   ani  close relatives review  caroquintero  konstantinidis  sequence depth   calculate   number  read map   genome  multiply   read length  divide   total length   genome   sequence breadth   calculate   number  base cover  divide   total length   genome use eqs     respectively   genome consist    one contig  draft genomes  length  assume    sum   lengths   contigs sdl∗∕ sbb∕  logistic function  fit   result recruitment data     value   value alone see also   attempt  separate  positive   negative train datasets  term  sequence depth  sequence breadth  latter two   variables   function  particular  approach calculate  parameters   logistic function  compute  error   train set      value  observe    positive    negative train datasets  modify  parameters accordingly  reduce  error  convergence  reach error  assess   loglikelihood maximization via gradient approach  modify  parameter value   error  minimize regression coefficients   logistic equation  calculate      variables  well    intercept term  thus  model estimate three parameters     intercept final parameters   model  estimate  default  base   sequence breadth   variable  find     discriminate parameter  positive  negative sample see also  however  estimation include   also provide   option  order  produce  addition   probability  presenceabsence  accurate estimation   abundance   target genome estimation   probability  detection  limit  detection   parameters   logistic function   determine       use  reliably predict  probability  presence   target genome   number  query metagenomes   read   query   recruit   target genome  observe   estimate  describe   train datasets  probability  presence  estimate accord  ∓     linear function   form βtt  represent  regression parameters    either  vector compose            default  onedimensional variable correspond   base   model parameters     possible  establish  detection limit   target genome   metagenomic dataset analyze  limit  define   minimum fraction   need   sample  order  estimate  probability  presence    result  display   black solid line    plot      fig    value observe base   read recruitment  correspond   probability value equal  higher     use  estimate  relative abundance   organism   sample   correspond   probability  provide  limit  detection  term  relative abundance identification  target genomes  metagenomic datasets  imglad figure  identification  target genomes  metagenomic datasets  imglad positive datasets cross  separate  negative datasets dot   logistic function solid line base  insilico train datasets  datasets  read   coli  separate  negative datasets  datasets  read   anthracis  separate  negative datasets red asterisk denote  position   experimental metagenomes remain dot represent insilico generate datasets note  differences  scale   xaxes  positive  negative datasets download fullsize imagedoi peerjfig filter conserve regions  avoid spurious result  read map  regions   target genome  insufficient diversity high sequence conservation   rrna genes  frequently undergo horizontal gene transfer   mobile elements  user  create  filter   regions use mytaxa luo rodriguez  konstantinidis   filter  create  predict genes   target genome  determine  classification weight use mytaxa   mytaxa classification score    bottom    gene   score   hypothetical proteins  gene  remove   genome   analysis  filter version   genome  subsequently use   model train  probability estimation step bioinformatic tool comparisons  tool parameters use metaphlan  truong     run   default settings use bowtie version  langmead  salzberg   read map metamlst zolfo     use  default settings pathoscope  hong     run  default settings use   set  reference genomes   use  build  train datasets  imglad four test  perform  assess specificity  sensitivity   case sensitivity  calculate   proportion  properly classify positive datasets among  total number  positive datasets specificity  define instead   fraction  correctly identify negative datasets among  negative datasets examine   first test metagenomic datasets  create  similar parameters   train dataset   coli   datasets  refseq genomes  datasets  spike  seven different concentrations    coli genome  order  provide    coverage   genome  sequence breadth   second test human microbiome project hmp metagenomes  spike  read    coli genome  order  provide    sequence breadth    hmp datasets  use    coli concentration   third test  datasets construct  test   spike  read  close relatives   coli  klebsiella  ani salmonella  ani  escherichia fergusonii  ani  random concentrations   genome  addition    coli read finally  test use close relatives   ani represent strain    species  perform   hmp datasets   similar way  describe   test # leaf inoculation experiment  test imglad  sample sequence fifty grams  fieldgrown spinach leave  inoculate spike   cells  escherichia coli o157h7 strain rm6067  strain link    spinachassociated outbreak   usa carter    three serial dilutions  perform result  three inoculation concentrations       cells per pellet plus  control sample   inoculate cells cells  inoculation  obtain  single colonies   grow overnight  cell concentrations  determine  enumeration  colonyforming units cfus   agar plat leave  subsequently wash  leaf wash  filter  remove plant debris  leafassociated microorganisms  pelleted  centrifugation      min   ° dna extraction  perform use mobio ultraclean microbial dna isolation kit accord  manufacturers instruction mobio dna sequence libraries  prepare use  illumina nextera  dna library prep kit accord  manufacturers recommendations except   protocol  terminate  isolation  clean amplify double strand libraries library concentrations  determine  fluorescent quantification use  qubit  dna kit  qubit  fluorometer thermo fisher scientific formerly life technologies waltham  usa accord  manufacturers recommendations  libraries  run   highsensitivity dna chip use  bioanalyzer  instrument agilent santa clara  usa  determine average library insert size  equimolar mixture   libraries final load concentration     sequence use  miseq reagent  kit   cycle     pair end run   inhouse illumina miseq instrument georgia institute  technology run  miseq control software  mcs adapter trim  demultiplexing  sequence sample  carry    mcs additionally  use metagenomic datasets inoculate  bacillus anthracis dna   make available previously     mcfaddens pseudor2 metric  assess  robustness   logistic modelfunction  close relatives  ability   logistic model  distinguish  positive  negative train datasets  close relatives  increase relatedness   target genome  use   train step  assess use  mcfaddens pseudor2 metric specifically  model determine fit  imglad   certain train dataset  compare   standard null logistic model   contain  intercept variable effectively  null model represent  standard logistic curve center    point   fit imglad model  without  adjustment   shape   curve specifically  metric  define  r2mcfaddenloglcloglnull     maximize likelihood value   fit model  lnull   maximize likelihood value   null model intercept   covariates therefore   comparison show perfect congruence   two model pseudor2 close   value  mean   fit model   robust  similar   randomly draw model  contrast  pseudor2 approach   denote  robust fit model note  pseudor2 may  equal  even  robust model   null model may approximate  fit model estimate  imglad  chance alone   iterations since   draw use   intercept value   evaluation  genome  one close relative   time  add   negative  positive train datasets  similar relative abundance    ensure complete genome coverage   target genome  coli strain o157h7  add   positive datasets  genomes  relatives  sort   follow group correspond   ani value   target genome                  genome  find  ani value     ani  addition   relate genomes  uniform background dataset  include  genomes show  ani   coli strain o157h7  include  provide positive  negative train datasets  adequate complexity successive iterations  imglad   result train datasets   contain one close relative  vary ani value   target genome  perform  model  evaluate use      implement   scipy module  python",6
156,MetaPhlAn2,"MetaPhlAn2 for enhanced metagenomic taxonomic profiling
To the Editor: MetaPhlAn (metagenomic phylogenetic analysis)1 is a method for characterizing the taxonomic profiles of whole-metagenome shotgun (WMS) samples that has been used successfully in large-scale microbial community studies2,3. This work complements the original species-level profiling method with a system for eukaryotic and viral quantitation, strain-level identification and strain tracking. These and other extensions make the MetaPhlAn2 computational package (http://segatalab. cibio.unitn.it/tools/metaphlan2/ and Supplementary Software) an efficient tool for mining WMS samples. Our method infers the presence and read coverage of cladespecific markers to unequivocally detect the taxonomic clades present in a microbiome sample and estimate their relative abundance1. MetaPhlAn2 includes an expanded set of ~1 million markers (184 ± 45 for each bacterial species) from >7,500 species (Supplementary Tables 1–3), based on the approximately tenfold increase in the number of sequenced genomes in the past 2 years. Subspecies markers enable strain-level analyses, and quasi-markers improve accuracy and allow the detection of viruses and eukaryotic microbes (a full list of additions is provided in Supplementary Notes 1–3 and Supplementary Fig. 1). We validated MetaPhlAn2 using 24 synthetic metagenomes comprising 656 million reads and 1,295 species (Supplementary Note 4 and Supplementary Table 4). MetaPhlAn2 proved more accurate (average correlation: 0.95 ± 0.05) than mOTU4 and Kraken5 (0.80 ± 0.21 and 0.75 ± 0.22, respectively) (Fig. 1a, Supplementary Figs. 2–9 and Supplementary Tables 5–11), with fewer false positives (an average of 10, compared with 22 and 23 for mOTU and Kraken, respectively) and false negatives (an average of 12, compared with 27 for the other two methods), even when including genomes that were absent from the reference database (Supplementary Note 4). With the adoption of the BowTie2 fast mapper and support for parallelism, MetaPhlAn2 is more than ten times faster than MetaPhlAn, and its speed is comparable to that of other tested approaches (Supplementary Fig. 10). We applied MetaPhlAn2 to four elbow-skin samples that we sequenced from three subjects (Fig. 1b, Supplementary Note 5 and Supplementary Table 12). Our data showed that Propionibacterium acnes and Staphylococcus epidermidis dominated these sites, in agreement with expected genus-level results6, while providing species-level resolution. Together with these core species, we found Malassezia globosa in 93.65% of samples and confirmed it by coverage analysis (Supplementary Fig. 11). Although M. globosa is a known colonizer of the skin, its metagenomic characterization highlights the ability of MetaPhlAn2 to identify non-prokaryotic species. Phages (e.g., for Propionibacterium) and double-stranded DNA viruses of the Polyomavirus genus were also consistently detected. We subsequently profiled the whole set of 982 samples from other body sites from the Human Microbiome Project (HMP), including 219 samples sequenced after the initial publication (Supplementary Note 6 and Supplementary Fig. 12). Microbes have been tracked across samples extensively with culture-dependent approaches, and MetaPhlAn2 now offers this possibility in a culture-independent setting by fingerprinting the microbiome at the strain level. This is illustrated by the multipletime-point (n = 3) HMP data set, in which we found that speciesspecific strain fingerprints were subject specific and conserved longitudinally (Supplementary Note 7 and Supplementary Figs. 13–21). This confirms both strong subject-specific strain retention in the gut microbiome and the ability of MetaPhlAn2 to perform strain fingerprinting and tracking, as these retention patterns are unlikely to occur in longitudinal samples by chance. Additionally, strain identification is possible when a sample contains a previously sequenced genome (Supplementary Note 7, Supplementary Table 13 and Supplementary Fig. 22). MetaPhlAn2’s enhanced taxonomic profiling (including associated post-analysis, conversion and visualization tools) and its characterization of the HMP data set should serve as convenient tools and extended references for future analysis of the human microbiome",AbundanceEstimation,"metaphlan2  enhance metagenomic taxonomic profiling
  editor metaphlan metagenomic phylogenetic analysis   method  characterize  taxonomic profile  wholemetagenome shotgun wms sample    use successfully  largescale microbial community studies2  work complement  original specieslevel profile method   system  eukaryotic  viral quantitation strainlevel identification  strain track    extensions make  metaphlan2 computational package  cibiounitnittoolsmetaphlan2  supplementary software  efficient tool  mine wms sample  method infer  presence  read coverage  cladespecific markers  unequivocally detect  taxonomic clades present   microbiome sample  estimate  relative abundance1 metaphlan2 include  expand set  ~ million markers  ±    bacterial species   species supplementary table  base   approximately tenfold increase   number  sequence genomes   past  years subspecies markers enable strainlevel analyse  quasimarkers improve accuracy  allow  detection  viruses  eukaryotic microbes  full list  additions  provide  supplementary note   supplementary fig   validate metaphlan2 use  synthetic metagenomes comprise  million read   species supplementary note   supplementary table  metaphlan2 prove  accurate average correlation  ±   motu4  kraken5  ±    ±  respectively fig  supplementary figs   supplementary table   fewer false positives  average   compare      motu  kraken respectively  false negative  average   compare      two methods even  include genomes   absent   reference database supplementary note    adoption   bowtie2 fast mapper  support  parallelism metaphlan2    ten time faster  metaphlan   speed  comparable     test approach supplementary fig   apply metaphlan2  four elbowskin sample   sequence  three subject fig  supplementary note   supplementary table   data show  propionibacterium acnes  staphylococcus epidermidis dominate  sit  agreement  expect genuslevel results6  provide specieslevel resolution together   core species  find malassezia globosa    sample  confirm   coverage analysis supplementary fig  although  globosa   know colonizer   skin  metagenomic characterization highlight  ability  metaphlan2  identify nonprokaryotic species phages   propionibacterium  doublestranded dna viruses   polyomavirus genus  also consistently detect  subsequently profile  whole set   sample   body sit   human microbiome project hmp include  sample sequence   initial publication supplementary note   supplementary fig  microbes   track across sample extensively  culturedependent approach  metaphlan2  offer  possibility   cultureindependent set  fingerprint  microbiome   strain level   illustrate   multipletimepoint    hmp data set    find  speciesspecific strain fingerprint  subject specific  conserve longitudinally supplementary note   supplementary figs   confirm  strong subjectspecific strain retention   gut microbiome   ability  metaphlan2  perform strain fingerprint  track   retention pattern  unlikely  occur  longitudinal sample  chance additionally strain identification  possible   sample contain  previously sequence genome supplementary note  supplementary table   supplementary fig  metaphlan2s enhance taxonomic profile include associate postanalysis conversion  visualization tool   characterization   hmp data set  serve  convenient tool  extend reference  future analysis   human microbiome",6
157,MetaPhyler,"MetaPhyler: Taxonomic profiling for metagenomic sequences
Building a reliable phylogenetic marker genes database To use metagenomic sequences for taxonomic profiling, we analyze 31 protein coding marker genes previously shown to provide sufficient information for phylogenetic analysis [18]. These phylogenetic marker genes are universal, present only once in most genomes, and are rarely subject to horizontal gene transfer. Hence, they provide a more accurate estimation of the microbial composition than methods relying on 16S rRNA alone. In order to create an accurate and comprehensive reference dataset, we used the manually curated marker genes from AMPHORA as seed, and extended them by including marker genes from all complete genomes, the NCBI nr protein database and 60 draft genomes. In addition, we have also included phylogenetic marker genes from Archaea, whose information is not available in the seed dataset from AMPHORA. As a result, our final marker genes dataset covers 581 genera, 214 families, 99 orders, 46 classes and 27 phyla. B. Building MetaPhyler classifiers Many previous metagenomic studies employ homologybased classification methods, and apply a universal threshold for all genes. The taxonomic label of the best similarity hit is then transferred to the query sequence. An improved variant of this approach involves combining the top hits instead of only using the best one [7]. We propose that better classification results can be obtained by tuning the taxonomic classifier to each BLAST HSP length, reference gene, and taxonomic rank. In detail, by learning parameters from the reference database we build a taxonomic classifier for a particular reference gene G as follows (Figure 1): 1) Simulate 60bp metagenomic reads from all reference marker genes that were curated as described in the previous section and, as a negative set, from genomic sequences that do not contain marker genes. 2) Map these simulated reads against reference gene G using BLASTX. 3) To build a classifier for gene G at a specific taxonomic level, say order, in vector Border we store BLASTX bit scores between gene G and the simulated reads that are from the same order; in vector Belse we store bit scores for aligning all other reads against G. Then, we find the bit score cutoff bcut that minimizes the following error function:  bi∈Border I(bi < bcut) +  bj∈Belse I(bj > bcut) (1) where I is an indicator function, which equals 1 when the condition is met, and 0 otherwise. Note that the taxonomic tree is downloaded from the NCBI taxonomy database. 4) Repeat the previous three steps to find bit score cutoffs for simulated reads of lengths 120bp, 180bp and up to the length of gene G in 60bp increments. 5) To find cutoffs for sequences of arbitrary matching lengths based on the sequence lengths we have sampled, we build a linear regression: bL cut = a + bL (see below for why we choose linear regression), where L is the sequence length, bL cut is the bit score cutoff for length L, and a and b are parameters. 6) Repeat steps (3), (4) and (5) to build bit score cutoff regressions for other taxonomic levels (genus, family, class and phylum) for gene G. We, then, repeat the above procedures to build classifiers for all reference marker genes in our database. In step (3), we assume that bit scores from close phylogenetic neighbors are higher than distant neighbors. This is generally true because marker genes, which are more closely related phylogenetically, tend to have more similar sequences. But the phylogenetic relationships of the marker genes are not fully consistent with the corresponding taxonomic tree, which is downloaded from the NCBI taxonomy database. Ideally we would expect to see the cutoff bcut to be lower than all the scores in Border, but higher than scores in Belse. The error metric (Equation 1) we used is a count of the number of misclassified points, which is similar to the 2-norm distance used by SVM classifiers. Next, we show that in step (5) linear regression is a reasonable approximation of bit scores based on the matching HSP length. As described in [1], the bit score Sbit = (λS − ln K)/ ln 2 (2) where S is the raw score of the BLAST alignment, and λ and K are parameters depending on the database. In addition, the raw score S equals the sum of the scores of matching amino acids [1] S = Sij = log(qij/pipj ) (3) which is the log-odds ratio of the observed and expected frequencies. For gene G of length L, we can rewrite S = LSij/L. For metagenomic read G of length L (L ≤ L), 96 (1) Simulate 60bp  metagenomic reads bp eads (2) Map simulated reads against reference gene G using BLASTX. (3) Minimize error function bcut (4) Repeat (1), (2) and (3) to find cutoffs for longer simulated reads. (5) Build linear regression at each taxonomic level. Figure 1. Building MetaPhyler classifier. which only contains a subsequence of the full-length gene G, the raw score S = L Smn/L . Further, if we assume that the evolutionary mutations and amino acid compositions are randomly distributed across gene G, then Smn/L ≈ Sij/L = Sij (4) which indicates that S = L Sij . Hence, we can rewrite equation 2 for a gene fragment as S bit = (λL Sij − ln K)/ ln 2 (5) where Sij is a constant for a particular gene G. As a result, the bit score (S bit) of a subsequence of gene G is linearly correlated with the HSP length (L ), and we can estimate this relationship with a linear regression as in step (5). C. Classifying metagenomic sequences The query metagenomic sequences are initially mapped to the reference marker genes using BLASTX. MetaPhyler classifies each sequence individually based on its best reference hit. For example, assume that a query sequence Q has gene G as its best hit, the BLAST bit score is b and the HSP length is L. First we try to classify Q at the genus level by calculating the bit score cutoff bcut of gene G using the pre-computed linear regression function. If the bit score is higher than the cutoff (b ≥ bcut), then we transfer the genus label of reference G to query Q. Otherwise, we try to classify Q at higher taxonomic levels (family, order, class and phylum) using level-specific classifiers built for gene G, until either the classification is successful at one of the taxonomic levels or the query can not be classified. A side-effect of this algorithm, specifically the stringent classification strategy that can avoid assigning an organism to a lower-level taxonomic group if the evidence does not support this assignment, is the ability to identify novel organisms or taxa. The presence of novel organisms leads to a detectable discrepancy between the number of sequences assigned to a lower taxonomic level, and the number of sequences assigned to a higher (less specific) taxonomic level. For example, if a set of query sequences are classified into a particular order, but cannot be classified into any existing families under this order, then this indicates that these reads come from novel family-level clades (Figure 2). These sequences can be further analyzed using a de novo approach, e.g., using Minimus [12], which will potentially recover the full-length gene and, thus, characterize the novel bacterium (see the Results section for an example). Figure 2. Detecting novel organisms with MetaPhyler. In this example, the novel organism will be classified at the order level but not family level, indicating the presence of novel family-level clades.",AbundanceEstimation,"metaphyler taxonomic profile  metagenomic sequences
building  reliable phylogenetic marker genes database  use metagenomic sequence  taxonomic profile  analyze  protein cod marker genes previously show  provide sufficient information  phylogenetic analysis   phylogenetic marker genes  universal present     genomes   rarely subject  horizontal gene transfer hence  provide   accurate estimation   microbial composition  methods rely   rrna alone  order  create  accurate  comprehensive reference dataset  use  manually curated marker genes  amphora  seed  extend   include marker genes   complete genomes  ncbi  protein database   draft genomes  addition   also include phylogenetic marker genes  archaea whose information   available   seed dataset  amphora   result  final marker genes dataset cover  genera  families  order  class   phyla  build metaphyler classifiers many previous metagenomic study employ homologybased classification methods  apply  universal threshold   genes  taxonomic label   best similarity hit   transfer   query sequence  improve variant   approach involve combine  top hit instead   use  best one   propose  better classification result   obtain  tune  taxonomic classifier   blast hsp length reference gene  taxonomic rank  detail  learn parameters   reference database  build  taxonomic classifier   particular reference gene   follow figure   simulate 60bp metagenomic read   reference marker genes   curated  describe   previous section    negative set  genomic sequence    contain marker genes  map  simulate read  reference gene  use blastx   build  classifier  gene    specific taxonomic level say order  vector border  store blastx bite score  gene    simulate read      order  vector belse  store bite score  align   read     find  bite score cutoff bcut  minimize  follow error function  ∈border ibi  bcut   ∈belse ibj  bcut      indicator function  equal    condition  meet   otherwise note   taxonomic tree  download   ncbi taxonomy database  repeat  previous three step  find bite score cutoffs  simulate read  lengths 120bp 180bp     length  gene   60bp increments   find cutoffs  sequence  arbitrary match lengths base   sequence lengths   sample  build  linear regression  cut     see     choose linear regression     sequence length  cut   bite score cutoff  length       parameters  repeat step      build bite score cutoff regressions   taxonomic level genus family class  phylum  gene    repeat   procedures  build classifiers   reference marker genes   database  step   assume  bite score  close phylogenetic neighbor  higher  distant neighbor   generally true  marker genes    closely relate phylogenetically tend    similar sequence   phylogenetic relationships   marker genes   fully consistent   correspond taxonomic tree   download   ncbi taxonomy database ideally  would expect  see  cutoff bcut   lower    score  border  higher  score  belse  error metric equation   use   count   number  misclassified point   similar   norm distance use  svm classifiers next  show   step  linear regression   reasonable approximation  bite score base   match hsp length  describe    bite score sbit             raw score   blast alignment      parameters depend   database  addition  raw score  equal  sum   score  match amino acids    sij  logqijpipj      logodds ratio   observe  expect frequencies  gene   length    rewrite   lsijl  metagenomic read   length   ≤    simulate 60bp  metagenomic read  eads  map simulate read  reference gene  use blastx  minimize error function bcut  repeat      find cutoffs  longer simulate read  build linear regression   taxonomic level figure  build metaphyler classifier   contain  subsequence   fulllength gene   raw score    smnl     assume   evolutionary mutations  amino acid compositions  randomly distribute across gene   smnl ≈ sijl  sij   indicate     sij  hence   rewrite equation    gene fragment   bite   sij        sij   constant   particular gene    result  bite score  bite   subsequence  gene   linearly correlate   hsp length      estimate  relationship   linear regression   step   classify metagenomic sequence  query metagenomic sequence  initially map   reference marker genes use blastx metaphyler classify  sequence individually base   best reference hit  example assume   query sequence   gene    best hit  blast bite score     hsp length   first  try  classify    genus level  calculate  bite score cutoff bcut  gene  use  precomputed linear regression function   bite score  higher   cutoff  ≥ bcut   transfer  genus label  reference   query  otherwise  try  classify   higher taxonomic level family order class  phylum use levelspecific classifiers build  gene   either  classification  successful  one   taxonomic level   query    classify  sideeffect   algorithm specifically  stringent classification strategy   avoid assign  organism   lowerlevel taxonomic group   evidence   support  assignment   ability  identify novel organisms  taxa  presence  novel organisms lead   detectable discrepancy   number  sequence assign   lower taxonomic level   number  sequence assign   higher less specific taxonomic level  example   set  query sequence  classify   particular order  cannot  classify   exist families   order   indicate   read come  novel familylevel clades figure   sequence    analyze use   novo approach  use minimus    potentially recover  fulllength gene  thus characterize  novel bacterium see  result section   example figure  detect novel organisms  metaphyler   example  novel organism   classify   order level   family level indicate  presence  novel familylevel clades",6
158,TAxyPro,"Protein signature-based estimation of metagenomic abundances including all domains of life and viruses
Our novel taxonomic profiling approach ‘Taxy-Pro’ is based on mixture modeling of the protein domain frequencies in a metagenome as described in the following. We argue that the use of functional reference profiles as mixture model components provides a powerful framework to cope with the inherent underrepresentation of certain biological entities in current genome databases. This problem is especially evident in the case of viruses, and, in fact, it seems not possible to accurately estimate the fraction of viral DNA in metagenomes merely based on genomic reference data. To overcome this limitation, we explored the possibility of including metagenomic reference data to improve the profiling accuracy. While the inclusion in Taxy-Pro is straightforward, a classical BLAST-based pipeline suffers from the computational burden of such an extension. We therefore realized a speed-optimized ‘Combi-BLAST’ method to provide a classical ‘baseline’ approach for comparison with Taxy-Pro (see Supplementary Material). 2.1 Protein-based mixture modeling Our new method for taxonomic profiling combines the detection of protein domains with a mixture model reconstruction of the resulting domain frequencies by means of taxonomically labeled protein reference signatures. Thus, our Taxy-Pro approach comprises two steps: first, we estimate the overall protein domain distribution of a metagenome by relative frequencies of Pfam hits using the CoMet domain detection engine. The resulting profiles comprise the relative frequencies of 12 621 protein domain families according to release 24 of the Pfam database. Then we approximate the Pfam profile vector y of a metagenome by linear combination of the precalculated protein reference signatures An external file that holds a picture, illustration, etc. Object name is btt077i1.jpg with mixing weights wi according to equation image        (1) For estimation of the mixture weights, we use the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), whereby An external file that holds a picture, illustration, etc. Object name is btt077i2.jpg and An external file that holds a picture, illustration, etc. Object name is btt077i3.jpg. Summing up the mixture weights of all viral reference signatures, we finally obtain an estimate of the virus fraction in a given metagenome. The fraction of bacterial and archaeal DNA as well as the composition on more specific phylogenetic levels was estimated in the same way by summing up the weights of the corresponding reference signatures. The approximation error of the above mixture model, i.e. the divergence between the original Pfam profile vector y and its approximation An external file that holds a picture, illustration, etc. Object name is btt077i4.jpg, we refer to as the fraction of domain hits unexplained (FDU). The FDU was calculated according to equation image        (2) which is half the Manhattan distance between the two profile vectors and therefore ranges between 0 and 1. In terms of the used protein signatures, the FDU measures the fraction of Pfam domain hits that could not be reconstructed by the model, i.e. the fraction that is missing in the reconstructed profile vector An external file that holds a picture, illustration, etc. Object name is btt077i5.jpg of predicted domain frequencies. Owing to symmetry, there is an equally sized fraction of domain hits that are overpredicted by the model, i.e. that are not present in the actual metagenome profile y. Together, these two fractions make up the complete Manhattan distance according to the overall L1 approximation error. This error is an important indicator of the reliability of the abundance estimates: the better the model fits the actual distribution of Pfam hits, i.e. the lower the FDU value, the more we can trust the taxonomic profile that directly results from the estimated mixture weights. Note that half the Manhattan distance can also be used to measure the divergence between two taxonomic profiles, whereby the y-values represent different relative abundances of phyla. In this context, the distance measure corresponds to the Bray-Curtis dissimilarity, which is widely used in ecology for comparison of two assemblages (Bray and Curtis, 1957). 2.2 Reference signatures from viral metagenomes For the reference signature vector profiles An external file that holds a picture, illustration, etc. Object name is btt077i6.jpg, we not only used genomic Pfam profiles of several prokaryotes, eukaryotes and viruses, but we also used a collection of metagenomes for computation of reference vectors. Because the number of available phage genomes, which have a sufficient length for estimation of Pfam profiles, is rather small and variation of viral signatures is large, we use additional reference signatures obtained from a collection of viral metagenomes. In total, Taxy-Pro includes 92 phage genomes and 102 viral metagenomes in addition to 1730 bacterial, 122 archaeal and 50 eukaryotic genomes to obtain 2096 functional reference signatures. Because the estimation of Pfam profiles requires a sufficient amount of DNA sequence, we only use phage genomes with a length >100 kbp for computation of a reference signature. Depending on the particular treatment, the viral metagenome data in current databases contain a varying amount of microbial contamination. For reasons of accuracy the mixture approach should not be used with viral signatures that arise from sequence data with only a small virus fraction. Therefore, we implemented a selection criterion that requires the reference signatures from viral metagenomes to be well distinguishable from microbial metagenome signatures. Using the Pfam profiles of 260 metagenomes from the CAMERA website (http://camera.calit2.net/, as of April 2010), we trained linear classifiers to discriminate between viral and microbial metagenomes. A step-wise elimination was applied to reduce the signatures: regularized least squares classifiers on Pfam domain frequencies (Lingner et al., 2010) were optimized by a 5-fold cross-validation to minimize the classification error. The optimization was performed 10 times, each time with a different random partition for the cross-validation. Therefore, each signature was used 10 times as a test example and the one with the highest misclassification rate over the 10 runs was eliminated. The whole procedure was repeated for the reduced set of signatures until no test error occurred during the 10 cross-validation runs. In that way the original number of 151 viral metagenomes was reduced to 102 viral reference signatures that were finally included in the mixture model (see Supplementary Table S3). In contrast to the oligonucleotide signatures in the original Taxy approach, the Pfam signatures in Taxy-Pro depend on sequence length. To make genomic and metagenomic Pfam profiles compatible, we fragmented the genomic sequences before performing the CoMet domain detection. For that purpose, we used half overlapping fragments with a 400 bp length.",AbundanceEstimation,"protein signaturebased estimation  metagenomic abundances include  domains  life  viruses
 novel taxonomic profile approach taxypro  base  mixture model   protein domain frequencies   metagenome  describe   follow  argue   use  functional reference profile  mixture model components provide  powerful framework  cope   inherent underrepresentation  certain biological entities  current genome databases  problem  especially evident   case  viruses   fact  seem  possible  accurately estimate  fraction  viral dna  metagenomes merely base  genomic reference data  overcome  limitation  explore  possibility  include metagenomic reference data  improve  profile accuracy   inclusion  taxypro  straightforward  classical blastbased pipeline suffer   computational burden    extension  therefore realize  speedoptimized combiblast method  provide  classical baseline approach  comparison  taxypro see supplementary material  proteinbased mixture model  new method  taxonomic profile combine  detection  protein domains   mixture model reconstruction   result domain frequencies  mean  taxonomically label protein reference signatures thus  taxypro approach comprise two step first  estimate  overall protein domain distribution   metagenome  relative frequencies  pfam hit use  comet domain detection engine  result profile comprise  relative frequencies    protein domain families accord  release    pfam database   approximate  pfam profile vector    metagenome  linear combination   precalculated protein reference signatures  external file  hold  picture illustration etc object name  btt077i1jpg  mix weight  accord  equation image          estimation   mixture weight  use  expectationmaximization  algorithm dempster    whereby  external file  hold  picture illustration etc object name  btt077i2jpg   external file  hold  picture illustration etc object name  btt077i3jpg sum   mixture weight   viral reference signatures  finally obtain  estimate   virus fraction   give metagenome  fraction  bacterial  archaeal dna  well   composition   specific phylogenetic level  estimate    way  sum   weight   correspond reference signatures  approximation error    mixture model   divergence   original pfam profile vector    approximation  external file  hold  picture illustration etc object name  btt077i4jpg  refer    fraction  domain hit unexplained fdu  fdu  calculate accord  equation image           half  manhattan distance   two profile vectors  therefore range      term   use protein signatures  fdu measure  fraction  pfam domain hit  could   reconstruct   model   fraction   miss   reconstruct profile vector  external file  hold  picture illustration etc object name  btt077i5jpg  predict domain frequencies owe  symmetry    equally size fraction  domain hit   overpredicted   model     present   actual metagenome profile  together  two fraction make   complete manhattan distance accord   overall  approximation error  error   important indicator   reliability   abundance estimate  better  model fit  actual distribution  pfam hit   lower  fdu value     trust  taxonomic profile  directly result   estimate mixture weight note  half  manhattan distance  also  use  measure  divergence  two taxonomic profile whereby  yvalues represent different relative abundances  phyla   context  distance measure correspond   braycurtis dissimilarity   widely use  ecology  comparison  two assemblages bray  curtis   reference signatures  viral metagenomes   reference signature vector profile  external file  hold  picture illustration etc object name  btt077i6jpg    use genomic pfam profile  several prokaryotes eukaryotes  viruses   also use  collection  metagenomes  computation  reference vectors   number  available phage genomes    sufficient length  estimation  pfam profile  rather small  variation  viral signatures  large  use additional reference signatures obtain   collection  viral metagenomes  total taxypro include  phage genomes   viral metagenomes  addition   bacterial  archaeal   eukaryotic genomes  obtain  functional reference signatures   estimation  pfam profile require  sufficient amount  dna sequence   use phage genomes   length  kbp  computation   reference signature depend   particular treatment  viral metagenome data  current databases contain  vary amount  microbial contamination  reason  accuracy  mixture approach    use  viral signatures  arise  sequence data    small virus fraction therefore  implement  selection criterion  require  reference signatures  viral metagenomes   well distinguishable  microbial metagenome signatures use  pfam profile   metagenomes   camera website    april   train linear classifiers  discriminate  viral  microbial metagenomes  stepwise elimination  apply  reduce  signatures regularize least square classifiers  pfam domain frequencies lingner     optimize   fold crossvalidation  minimize  classification error  optimization  perform  time  time   different random partition   crossvalidation therefore  signature  use  time   test example   one   highest misclassification rate    run  eliminate  whole procedure  repeat   reduce set  signatures   test error occur    crossvalidation run   way  original number   viral metagenomes  reduce   viral reference signatures   finally include   mixture model see supplementary table   contrast   oligonucleotide signatures   original taxy approach  pfam signatures  taxypro depend  sequence length  make genomic  metagenomic pfam profile compatible  fragment  genomic sequence  perform  comet domain detection   purpose  use half overlap fragment     length",6
159,Trimmomatic,"Trimmomatic: a flexible trimmer for Illumina sequence data
Trimmomatic includes a variety of processing steps for read trimming and filtering, but the main algorithmic innovations are related to identification of adapter sequences and quality filtering, and are described in detail below. A list of the other processing steps is presented in the Supplementary Materials. 2.1 Removal of technical sequences Trimmomatic uses two approaches to detect technical sequences within the reads. The first, referred to as ‘simple mode’, works by finding an approximate match between the read and the user-supplied technical sequence. This mode has the advantage of working for all technical sequences, including adapters and polymerase chain reaction (PCR) primers, or fragments thereof. Such sequences can be detected in any location or orientation within the reads but requires a substantial minimum overlap between the read and technical sequence to prevent false-positive findings. However, short partial adapter sequences, which often occur at the ends of reads, are inherently unable to meet this minimum overlap requirement and therefore are not detectable. The second mode, referred to as ‘palindrome mode’, is specifically aimed at detecting this common ‘adapter read-through’ scenario, whereby the sequenced DNA fragment is shorter than the read length, and results in adapter contamination on the end of the reads. This is especially the case for longer read length as supported by the Miseq. Although such short fragments should normally be removed during library preparation, in practice this process is not perfectly efficient, and thus many libraries suffer from this problem to some extent. ‘Palindrome mode’ can only be used with paired-end data, but has considerable advantages in sensitivity and specificity over ‘simple’ mode. Note that the current technical sequence identification approaches in Trimmomatic are not designed to filter or categorize data on the basis of ‘barcodes’. 2.1.1 Simple mode In simple mode, each read is scanned from the 5′ end to the 3′ end to determine if any of the user-provided adapters are present. The standard ‘seed and extend’ approach (Li and Homer, 2010) is used to find initial matches between the technical sequences and the reads. The seed is not required to match perfectly, and a user-defined number of mismatches are tolerated. Based on this seed match, a local alignment is performed. If the alignment score exceeds the user-defined threshold, the aligned region plus the remainder after the alignment are removed. Figure 1 illustrates the alignments tested for each technical sequence. The process begins with a partial overlap of the 3′ end of the technical sequence with the 5′ end of the read, as shown in (A). Testing proceeds by moving the putative contaminant toward the 3′ end of the read. In both the partial overlap (A) and complete overlap at the 5′ end (B) scenarios, the entire read will be clipped. If the contaminant is found within the read (C), the bases from the 5′ end of the read to the beginning of the alignment are retained. The testing process continues until only a partial alignment on the 3′ end of the read remains (D). An external file that holds a picture, illustration, etc. Object name is btu170f1p.jpg Fig. 1. Putative sequence alignments as tested in simple mode. The alignment process begins with a partial overlap at the 5′ end of the read (A), increasing to a full-length 5′ overlap (B), followed by full overlaps at all positions (C) and finishes with a partial overlap at the 3′ end of the read (D). Note that the upstream ‘adapter’ sequence is for illustration only and is not part of the read or the aligned region Simple mode has the advantage that it can detect any technical sequence at any location in the read, provided that the alignment is sufficiently long and the read is sufficiently accurate. However, when only a short partial match is possible, such as in scenarios (A) and (D), the contaminant may not be reliably detectable. 2.1.2 Palindrome mode As noted above, ‘palindrome mode’ is specifically optimized for the detection of ‘adapter read-through’. When ‘read-through’ occurs, both reads in a pair will consist of an equal number of valid bases, followed by contaminating sequence from the ‘opposite’ adapters. Furthermore, the valid sequence within the two reads will be reverse complements. By detecting all three of these symptoms at once, adapter read-through can be identified with high sensitivity and specificity. For performance reasons, the actual algorithm combines these three tests. The adapter sequences are prepended to their respective reads, and then the combined read-with-adapter sequences from the pair are aligned against each other. A high-scoring alignment indicates that the first parts of each read are reverse complements, while the remaining parts of the reads match the respective adapters. The alignment is implemented using a ‘seed and extend’ approach, similar to that in simple mode. Global alignment scoring is used to ensure an end-to-end match across the entire overlap. Figure 2 illustrates the alignments tested in palindrome mode. The process begins with an overlap between the adapters and the start of the opposite reads, as shown in (A). This alignment would detect a read pair containing no useful sequence information, which could be caused by the direct ligation of the adapters. Detection of this scenario would result in the dropping of both reads. Testing then proceeds by moving the relative positioning of the reads ‘backwards’, testing for increasingly longer valid DNA fragments, illustrated in (B). This scenario would result in the trimming of both reads as illustrated. Even when only a small fragment of the adapter is overlapping, as shown in (C), the overall alignment is easily sufficient to ensure reliable detection. The process is complete when the overlapping region no longer reaches into the adapters (D). An external file that holds a picture, illustration, etc. Object name is btu170f2p.jpg Fig. 2. Putative sequence alignments as tested in palindrome mode. The alignment process begins with the adapters completely overlapping the reads (A) testing for immediate ‘read-through’, then proceeds by checking for later overlap (B), including partial adapter read-through (C), finishing when the overlap indicates no read-through into the adapters (D) The main advantage of palindrome mode is the longer alignment length, which ensures that the adapters can be reliably detected, even in the presence of read errors or where only a small number of adapter bases are present. If required, palindrome mode can be used to remove even a single adapter base, while retaining a low false-positive rate. We are aware of one other tool, AdapterRemoval (Lindgreen, 2012), which independently developed a similar approach. Note, however, because palindrome is limited to the detection of adapter read-through, a comprehensive strategy requires the combination of both simple and palindrome modes. 2.1.3 Alignment detection and scoring The algorithmic approach used for technical sequence alignments is somewhat unusual, avoiding the precalculated indexes often used in NGS alignments (Li and Homer, 2010). Initial sequence comparisons are done using a 16-base fragment from each sequence. The 16 bases are converted to the 64-bit integer, known as the seed, using a 4-bit code for each base: A = 0001, T = 0010, C = 0100 and T = 1000. These seeds are then compared using a bitwise-XOR, which determines which bits differ between the two seeds. This will result in a 0000 code for each matching base, and a code with two 1 s for each mismatch, e.g. 0011 for an A-T mismatch, as XOR(0001,0010) = 0011. The ‘1’s within this result are then counted using the ‘popcount’ operation, and this count will be exactly twice the number of differing bases for the 16-base fragments. If the seeds are within the user-specified distance, the full alignment scoring algorithm is used. Matching bases are scored as An external file that holds a picture, illustration, etc. Object name is btu170i1.jpg, which is ∼0.602, while mismatches are penalized depending on their quality score, by An external file that holds a picture, illustration, etc. Object name is btu170i2.jpg, which can thus vary from 0 to 4. This results in a higher penalty for bases that are believed to be highly accurate. ‘Simple’ mode aligns each read against each technical sequence, using local alignment. This is implemented by finding the highest scoring region within the alignment, and thus may omit divergent regions on the ends. ‘Palindrome’ mode aligns the forward and reverse reads, combined with their adapter sequences. It uses global alignment, which is the total alignment score of the overlapping region. 2.2 Quality filtering Trimmomatic offers two main quality filtering alternatives. Both approaches exploit the Illumina quality score of each base position to determine where the read should be cut, resulting in the retention of the 5′ portion, while the sequence on the 3′ of the cut point is discarded. This fits well with typical Illumina data, which generally have poorer quality toward the 3′ end. These two approaches are described in the following sections. 2.2.1 Sliding Window quality filtering The Sliding Window uses a relatively standard approach. This works by scanning from the 5′ end of the read, and removes the 3′ end of the read when the average quality of a group of bases drops below a specified threshold. This prevents a single weak base causing the removal of subsequent high-quality data, while still ensuring that a consecutive series of poor-quality bases will trigger trimming. 2.2.2 Maximum Information quality filtering A novel alternative approach was motivated by the realization that, for many applications, the incremental value of retaining additional bases in a read is related to the read length. Intuitively, it is clear that short reads are almost worthless because they occur multiple times within the target sequence and thus they give only ambiguous information. Even at the risk of introducing errors, it is worthwhile to retain additional low-quality bases early in a read, so that the trimmed read is sufficiently long to be informative. However, beyond a certain read length, retaining additional bases is less beneficial, and may even be detrimental. Reads of moderate length are likely to be already informative and, depending on the task at hand, can be almost as valuable as full-length reads. Therefore, the smaller potential benefit of retaining additional bases must be balanced against the increasing risk of retaining errors, which could cause the existing read value to be lost. As such, it is worthwhile for the trimming process to become increasingly strict as it progresses through the read, rather than to apply a fixed quality threshold. To the best of our knowledge, this approach has not been applied in any existing tools. The ‘Maximum Information’ quality filtering approach implements this adaptive approach. It uses a combination of three factors to determine how much of each read should be retained. The first factor models the ‘length threshold’ concept, whereby a read must be of at least a minimal length to be useful for the downstream application. As described above, very short reads have little value, as they are too ambiguous to be informative. On the other hand, most long reads can be mapped to few locations in the target sequence. If they cannot be uniquely mapped, because of them originating in a repetitive region, it is unlikely that a small number of additional bases will resolve this. For reads between these extremes, the marginal benefit of a small number of additional bases is considerable, as these extra bases may make the difference between an ambiguous and an informative read. A logistic curve was chosen to implement this scoring behavior, as it gives a relatively flat score for extreme values, while providing a steep transition around the user-specified threshold point. Given a target length t, the putative trimming to length l would give a length threshold score: equation image The second factor models ‘coverage’, and provides a linear score based on retained sequence length: equation image This reflects that, given reasonably high-accuracy bases, a longer read contains more information that is useful for most applications. The final factor models the ‘error rate’, and uses the error probabilities from the read quality scores to determine the accumulated likelihood of errors over the read. To calculate this score, we simply take the product of the probabilities that each base is correct, giving: equation image The correctness probabilities Pcorr of each base are calculated from the sequence quality scores. The error score typically begins as a high score at the start of the read, and depending on the read quality, typically drops rapidly at some point during the read. The Maximum Information algorithm determines the combined score of the three factors for each possible trimming position, and the best combined score determines how much of the read to trim. A user-specified strictness setting s, which can be set between 0 and 1, controls the balance between the ‘coverage’ factor (maximal for s = 0) and the ‘error rate’ factor (maximal for s = 1). This gives the following overall formula: equation image Figure 3 illustrates how the three factors are combined into a single score. The peak score is then used to determine the point where the read is trimmed. An external file that holds a picture, illustration, etc. Object name is btu170f3p.jpg Fig. 3. How Maximum Information mode combines uniqueness, coverage and error rate to determine the optimal trimming point",Trimming,"trimmomatic  flexible trimmer  illumina sequence data
trimmomatic include  variety  process step  read trim  filter   main algorithmic innovations  relate  identification  adapter sequence  quality filter   describe  detail   list    process step  present   supplementary materials  removal  technical sequence trimmomatic use two approach  detect technical sequence within  read  first refer   simple mode work  find  approximate match   read   usersupplied technical sequence  mode   advantage  work   technical sequence include adapters  polymerase chain reaction pcr primers  fragment thereof  sequence   detect   location  orientation within  read  require  substantial minimum overlap   read  technical sequence  prevent falsepositive find however short partial adapter sequence  often occur   end  read  inherently unable  meet  minimum overlap requirement  therefore   detectable  second mode refer   palindrome mode  specifically aim  detect  common adapter readthrough scenario whereby  sequence dna fragment  shorter   read length  result  adapter contamination   end   read   especially  case  longer read length  support   miseq although  short fragment  normally  remove  library preparation  practice  process   perfectly efficient  thus many libraries suffer   problem   extent palindrome mode    use  pairedend data   considerable advantage  sensitivity  specificity  simple mode note   current technical sequence identification approach  trimmomatic   design  filter  categorize data   basis  barcodes  simple mode  simple mode  read  scan   ′ end   ′ end  determine     userprovided adapters  present  standard seed  extend approach   homer   use  find initial match   technical sequence   read  seed   require  match perfectly   userdefined number  mismatch  tolerate base   seed match  local alignment  perform   alignment score exceed  userdefined threshold  align region plus  remainder   alignment  remove figure  illustrate  alignments test   technical sequence  process begin   partial overlap   ′ end   technical sequence   ′ end   read  show   test proceed  move  putative contaminant toward  ′ end   read    partial overlap   complete overlap   ′ end  scenarios  entire read   clip   contaminant  find within  read   base   ′ end   read   begin   alignment  retain  test process continue    partial alignment   ′ end   read remain   external file  hold  picture illustration etc object name  btu170f1pjpg fig  putative sequence alignments  test  simple mode  alignment process begin   partial overlap   ′ end   read  increase   fulllength ′ overlap  follow  full overlap   position   finish   partial overlap   ′ end   read  note   upstream adapter sequence   illustration     part   read   align region simple mode   advantage    detect  technical sequence   location   read provide   alignment  sufficiently long   read  sufficiently accurate however    short partial match  possible    scenarios     contaminant may   reliably detectable  palindrome mode  note  palindrome mode  specifically optimize   detection  adapter readthrough  readthrough occur  read   pair  consist   equal number  valid base follow  contaminate sequence   opposite adapters furthermore  valid sequence within  two read   reverse complement  detect  three   symptoms   adapter readthrough   identify  high sensitivity  specificity  performance reason  actual algorithm combine  three test  adapter sequence  prepended   respective read    combine readwithadapter sequence   pair  align     highscoring alignment indicate   first part   read  reverse complement   remain part   read match  respective adapters  alignment  implement use  seed  extend approach similar    simple mode global alignment score  use  ensure  endtoend match across  entire overlap figure  illustrate  alignments test  palindrome mode  process begin   overlap   adapters   start   opposite read  show    alignment would detect  read pair contain  useful sequence information  could  cause   direct ligation   adapters detection   scenario would result   drop   read test  proceed  move  relative position   read backwards test  increasingly longer valid dna fragment illustrate    scenario would result   trim   read  illustrate even    small fragment   adapter  overlap  show    overall alignment  easily sufficient  ensure reliable detection  process  complete   overlap region  longer reach   adapters   external file  hold  picture illustration etc object name  btu170f2pjpg fig  putative sequence alignments  test  palindrome mode  alignment process begin   adapters completely overlap  read  test  immediate readthrough  proceed  check  later overlap  include partial adapter readthrough  finish   overlap indicate  readthrough   adapters   main advantage  palindrome mode   longer alignment length  ensure   adapters   reliably detect even   presence  read errors     small number  adapter base  present  require palindrome mode   use  remove even  single adapter base  retain  low falsepositive rate   aware  one  tool adapterremoval lindgreen   independently develop  similar approach note however  palindrome  limit   detection  adapter readthrough  comprehensive strategy require  combination   simple  palindrome modes  alignment detection  score  algorithmic approach use  technical sequence alignments  somewhat unusual avoid  precalculated index often use  ngs alignments   homer  initial sequence comparisons   use  base fragment   sequence   base  convert   bite integer know   seed use  bite code   base               seed   compare use  bitwisexor  determine  bits differ   two seed   result    code   match base   code  two     mismatch      mismatch  xor     within  result   count use  popcount operation   count   exactly twice  number  differ base   base fragment   seed  within  userspecified distance  full alignment score algorithm  use match base  score   external file  hold  picture illustration etc object name  btu170i1jpg     mismatch  penalize depend   quality score   external file  hold  picture illustration etc object name  btu170i2jpg   thus vary      result   higher penalty  base   believe   highly accurate simple mode align  read   technical sequence use local alignment   implement  find  highest score region within  alignment  thus may omit divergent regions   end palindrome mode align  forward  reverse read combine   adapter sequence  use global alignment    total alignment score   overlap region  quality filter trimmomatic offer two main quality filter alternatives  approach exploit  illumina quality score   base position  determine   read   cut result   retention   ′ portion   sequence   ′   cut point  discard  fit well  typical illumina data  generally  poorer quality toward  ′ end  two approach  describe   follow section  slide window quality filter  slide window use  relatively standard approach  work  scan   ′ end   read  remove  ′ end   read   average quality   group  base drop   specify threshold  prevent  single weak base cause  removal  subsequent highquality data  still ensure   consecutive series  poorquality base  trigger trim  maximum information quality filter  novel alternative approach  motivate   realization   many applications  incremental value  retain additional base   read  relate   read length intuitively   clear  short read  almost worthless   occur multiple time within  target sequence  thus  give  ambiguous information even   risk  introduce errors   worthwhile  retain additional lowquality base early   read    trim read  sufficiently long   informative however beyond  certain read length retain additional base  less beneficial  may even  detrimental read  moderate length  likely   already informative  depend   task  hand   almost  valuable  fulllength read therefore  smaller potential benefit  retain additional base must  balance   increase risk  retain errors  could cause  exist read value   lose     worthwhile   trim process  become increasingly strict   progress   read rather   apply  fix quality threshold   best   knowledge  approach    apply   exist tool  maximum information quality filter approach implement  adaptive approach  use  combination  three factor  determine  much   read   retain  first factor model  length threshold concept whereby  read must    least  minimal length   useful   downstream application  describe   short read  little value     ambiguous   informative    hand  long read   map   locations   target sequence   cannot  uniquely map    originate   repetitive region   unlikely   small number  additional base  resolve   read   extremes  marginal benefit   small number  additional base  considerable   extra base may make  difference   ambiguous   informative read  logistic curve  choose  implement  score behavior   give  relatively flat score  extreme value  provide  steep transition around  userspecified threshold point give  target length   putative trim  length  would give  length threshold score equation image  second factor model coverage  provide  linear score base  retain sequence length equation image  reflect  give reasonably highaccuracy base  longer read contain  information   useful   applications  final factor model  error rate  use  error probabilities   read quality score  determine  accumulate likelihood  errors   read  calculate  score  simply take  product   probabilities   base  correct give equation image  correctness probabilities pcorr   base  calculate   sequence quality score  error score typically begin   high score   start   read  depend   read quality typically drop rapidly   point   read  maximum information algorithm determine  combine score   three factor   possible trim position   best combine score determine  much   read  trim  userspecified strictness set     set     control  balance   coverage factor maximal       error rate factor maximal      give  follow overall formula equation image figure  illustrate   three factor  combine   single score  peak score   use  determine  point   read  trim  external file  hold  picture illustration etc object name  btu170f3pjpg fig   maximum information mode combine uniqueness coverage  error rate  determine  optimal trim point",7
160,SeqPurge,"SeqPurge: highly-sensitive adapter trimming for paired-end NGS data
Before going into the algorithm details, we will briefly define the problem more formally: Given forward/reverse adapter sequences and the forward/reverse read produced from one DNA fragment, remove all bases from the reads that stem from a read-through into the adapters, if present. Those bases that must not be trimmed because they stem from the actual sequence of interest are called insert in the following text. Calculation of non-random match probability As mentioned in the introduction, our algorithm is not based on sequence alignment. It uses a rather simple probabilistic approach. Given two DNA/RNA sequences of length n, we count the number of matching bases k between the sequences. Given the probability p of a single base match, we calculate the probability P to observe k or more matching bases in sequences of length n using the binomial distribution: 𝑃=∑𝑖=𝑘𝑛𝑛!𝑖!(𝑛−𝑖)! 𝑝𝑖(1−𝑝)𝑛−𝑖 We call two sequences a (non-random) match, if P is less than a given threshold. This threshold is the main parameter that balances sensitivity/specificity of our algorithm. Using this simple approach is possible because modelling indels is not necessary - an insertion/deletion in the insert is present in both read directions. We use a fixed match probability p of 0.25 for all bases, i.e. we assume that all four bases occur at the same rate. Based on our performance evaluations, this simplification causes no major problems. Algorithm description The primary design goal of SeqPurge was to achieve a very high sensitivity while maintaining a state-of-the-art specificity. In general, the insert match approach is very sensitive and thus is the best approach for paired-end reads. However, certain sequence motives and unbalanced base content can make sequencing difficult in one read direction or even in both read directions. In these difficult sequences, the adapter match approach can perform better than the insert match approach. Thus, we combine the two approaches to increase the sensitivity of SeqPurge. First, we try to find an insert match between forward read and the reverse-complement of the reverse read. To detect an insert match, each possible offset between both reads is tested for a match (see Fig. 1b). If we find a match, the adapters are trimmed and the insert remains. If we find several matches, we select the best offset, i.e. the one with the lowest probability of being random. Several matches occur primarily in reads of regions with simple repeats. To prevent overtrimming because of false-positive matches in simple repeat regions, we also require a match between the previously defined adapter sequence and the sequence flanking the putative insert. This additional adapter match is required in only one out of the two reads, which makes the algorithm more robust towards bad read quality in one read direction. If no insert match was found, we check for adapter matches in the forward and reverse read separately. Again, each possible offset is tested for an adapter sequence match. If an adapter match is detected, the read is trimmed starting at the offset position. If only one of the two reads has an adapter match, the other read is trimmed at the offset as well, because a ‘read through’ is always symmetrical. The rationale is again that one read could have bad quality due to sequencing problems. Theoretical runtime and runtime optimizations The theoretical runtime of the algorithm is nm 2 where n is the number of reads and m is the read length. We implemented several optimizations to reduce the runtime in practice. Because we need to calculate the match probability m 2 times for each read pair and three faculty values are required for the calculation, the faculty values are pre-calculated and stored in a hash for fast lookup. To avoid a large part of the match probability calculations, we added a minimum matching bases parameter (default is 80 %). Once the maximum allowed number of mismatches of a comparison is exceeded, the rest of the comparison can be skipped. This reduces the overall runtime by up to 75 %. Reading the data from file and processing the data in the same thread can slow down the analysis considerably when file I/O is slow. Thus, we pre-fetch reads in an I/O thread and use n additional threads for data analysis (n = 1 by default). This strategy of course increases the memory usage slightly. We are currently evaluating the possible speedup when using four bit-arrays instead of characters to store DNA sequences. This approach is also used by Skewer [5], the fastest tool in our comparison.",Trimming,"seqpurge highlysensitive adapter trim  pairedend ngs data
 go   algorithm detail   briefly define  problem  formally give forwardreverse adapter sequence   forwardreverse read produce  one dna fragment remove  base   read  stem   readthrough   adapters  present  base  must   trim   stem   actual sequence  interest  call insert   follow text calculation  nonrandom match probability  mention   introduction  algorithm   base  sequence alignment  use  rather simple probabilistic approach give two dnarna sequence  length   count  number  match base    sequence give  probability    single base match  calculate  probability   observe    match base  sequence  length  use  binomial distribution ∑𝑖𝑘𝑛𝑛𝑖𝑛𝑖 𝑝𝑖𝑝𝑛𝑖  call two sequence  nonrandom match    less   give threshold  threshold   main parameter  balance sensitivityspecificity   algorithm use  simple approach  possible  model indels   necessary   insertiondeletion   insert  present   read directions  use  fix match probability      base   assume   four base occur    rate base   performance evaluations  simplification cause  major problems algorithm description  primary design goal  seqpurge   achieve   high sensitivity  maintain  stateoftheart specificity  general  insert match approach   sensitive  thus   best approach  pairedend read however certain sequence motives  unbalance base content  make sequence difficult  one read direction  even   read directions   difficult sequence  adapter match approach  perform better   insert match approach thus  combine  two approach  increase  sensitivity  seqpurge first  try  find  insert match  forward read   reversecomplement   reverse read  detect  insert match  possible offset   read  test   match see fig    find  match  adapters  trim   insert remain   find several match  select  best offset   one   lowest probability   random several match occur primarily  read  regions  simple repeat  prevent overtrimming   falsepositive match  simple repeat regions  also require  match   previously define adapter sequence   sequence flank  putative insert  additional adapter match  require   one    two read  make  algorithm  robust towards bad read quality  one read direction   insert match  find  check  adapter match   forward  reverse read separately   possible offset  test   adapter sequence match   adapter match  detect  read  trim start   offset position   one   two read   adapter match   read  trim   offset  well   read   always symmetrical  rationale    one read could  bad quality due  sequence problems theoretical runtime  runtime optimizations  theoretical runtime   algorithm        number  read     read length  implement several optimizations  reduce  runtime  practice   need  calculate  match probability   time   read pair  three faculty value  require   calculation  faculty value  precalculated  store   hash  fast lookup  avoid  large part   match probability calculations  add  minimum match base parameter default      maximum allow number  mismatch   comparison  exceed  rest   comparison   skip  reduce  overall runtime      read  data  file  process  data    thread  slow   analysis considerably  file   slow thus  prefetch read    thread  use  additional thread  data analysis     default  strategy  course increase  memory usage slightly   currently evaluate  possible speedup  use four bitarrays instead  character  store dna sequence  approach  also use  skewer   fastest tool   comparison",7
161,Skewer,"Skewer: a fast and accurate adapter trimmer for next-generation sequencing paired-end reads
k-difference problem Given a sequence S=s1s2…s n , a query pattern P=p1p2…p m , and a threshold k(0≤k<m), search all substrings of S, noted as {P′}, such that ∥P′,P∥ l e v ≤k. Extended k-difference problem Given a sequence S=s1s2…s n , a query pattern P=p1p2…p m , and a threshold e(0≤e<1,⌊n×e⌋=k), search all substrings of S, noted as {P′}, such that ∥P′,P∥ l e v ≤k; and all suffixes of S, noted as {S′}, such that ∃ a prefix of P, noted as P′, and ∥S′,P′∥ l e v ≤|S′|×e Algorithms For the k-difference problem, the classic approach [2] computes a (m+1)×(n+1) dynamic programming matrix C[0..m,0..n] using the following recurrence: 𝐶[𝑖,𝑗]=𝑚𝑖𝑛⎧⎩⎨⎪⎪𝐶[𝑖−1,𝑗−1]+𝛿𝑖𝑗𝐶[𝑖−1,𝑗]+1𝐶[𝑖,𝑗−1]+1 where 𝛿𝑖𝑗={0,1,if𝑝𝑖=𝑠𝑗,otherwise. with initialization at the upper boundary by C[0,j]=0, and at the left boundary by C[i,0]=i, for i=1,2,…,m and j=1,2,…,n. Finally, the algorithm tests the last row of the matrix, i.e. C[m,j], and reports those elements that are no greater than k. This algorithm has O(m n) time and O(m n) space complexity. The space bound can be easily reduced to O(m) if matrix C is computed by columns, noted as C j for j=1,2,…n, and report a match each time C j [m]≤k, because computing column C j requires only the knowledge of previous column Cj−1. With careful design, C j and Cj−1 can share one column vector, as proposed by Ukkonen [4]. Ukkonen also observed that for columns that have the last element greater than k, there is a boundary index of C j , noted as l a c(C j ), such that C j [l a c(C j )]=k and C j [l]>k for l=l a c(C j )+1,…m. It is easy to prove that l a c(C j )≤l a c(Cj−1)+1. Using this observation, Ukkonen reduced the time from O(m n) to expected O(k n) [4]. Our algorithm was developed from Ukkonen’s algorithm; however, we use a queue instead of an array to store all elements of current column above the boundary index. When there is a new element that corresponds to the topmost element of the new column, all elements in the queue shift automatically to the next (lower) position, just as elements transfer in the diagonals of matrix C. This process inherently keeps the basic properties of Ukkonen’s algorithm and facilitates subsequent improvements. Lemma 1 In the dynamic programming matrix C for tackling the k-difference problem, the values of elements along each diagonal are monotonically non-decreasing. The proof is provided in Additional file 5 Appendix A. Theorem 1 All the matched elements of the query pattern and sequence are equal to their precursors in the diagonal and do not need to be updated in the dynamic programming process. Proof. This theorem is a direct consequence of Lemma 1 and the dynamic programming recurrence, when δ i j =0. In other words, only mismatched elements need to be updated in the dynamic programming process. If bit-vectors that denote mismatched positions of comparison between the adapter sequence and each of the four nucleotide characters are pre-computed and a bit-vector that marks all positions of the queue elements that exceed the k-difference constraint is maintained, then unnecessary computations in updating the column vector can be inhibited. This is the key point that led to the main improvement of our algorithm over Ukkonen’s algorithm. As listed in Algorithm 1, the bit-masked k-difference matching algorithmhas the following characteristics: Use a queue instead of an array to store all elements of the current column above boundary index. In preprocessing, calculate for each of four nucleotide characters a bit-vector that denotes the mismatched positions compared with the adapter. Mark the internal cells that exceed the k-difference constraint by a bit-vector which shifts as the queue pushes it. When processing the column starting from each input nucleotide, update only the cells that mismatch and have not been marked. This algorithm uses a queue of size m and several bit vectors of size ⌈m/w⌉, where w is the word length of the computer (for example w equals 64 for a 64-bit machine), and hence has a space of O(m). For each of the n characters in a target sequence, the character enters the queue once and exits from the queue at most once. For a random sequence, the expected size of the queue is O(k); hence, generally the algorithm has O(k n) expected time. However, because it is restricted by the bit-mask operations, each element in the queue usually updates at most k+1 times. Because bit operations are negligible compared with element update operations, this algorithm achieves O(k n) worst-case time in practice, which is better than the O(k n) expected time for Ukkonen’s algorithm. Algorithm 1 can be improved further by avoiding all unnecessary updates through constant time bit operations within each iteration cycle of the target nucleotide. The basic principle is that when an element in a diagonal of the original dynamic programming matrix has a value that is derived from an adjacent diagonal (i.e. an indel occurs in the corresponding path), the score and associated index of the element will remain unchanged if the precursor remains unchanged. Although the above improvement can reduce the theoretical time complexity from expected O(k n) to worst-case O(k n), experiments on large volumes of real data showed that the reduced element update operations did not compensate for the additional bit operations. For the extended k-difference problem, an additional step bounded by O(m) is performed to check all the elements remained in the queue if no hit was found in previous steps. Deal with base-call qualities The main advantage of the bit-masked k-difference matching algorithm over Myer’s bit-vector algorithm is that it can be extended to handle base-call quality values. To handle base-call quality values, we introduce the following parameters: P m i n =−l o g10(1/3), the minimum penalty for a mismatch; P m a x =−l o g10(1040/(−10)/3), the maximum penalty for a mismatch; d e l t a=P m a x , the penalty for an insertion or deletion. The penalty of a mismatch with quality value q is calculated as: 𝑃⎛⎝⎜⎜⎜𝑞⎞⎠⎟⎟⎟=⎧⎩⎨⎪⎪𝑃𝑚𝑖𝑛𝑃𝑚𝑖𝑛+𝑞/40×(𝑃𝑚𝑎𝑥−𝑃𝑚𝑖𝑛)𝑃𝑚𝑎𝑥𝑞≤00<𝑞<40𝑞≥40 It is trivial to prove that P(q)=−l o g10(10q/(−10)/3) when 0<q<40. This score is the negative logarithm of the probability that the corresponding base is actually a match to the adapter sequence with sequencing error. Note that the scoring scheme herein only induces penalties for mismatches and insertions/deletions. For matches, because the possibility of false matching based on sequence homology reduces exponentially as the matching length increases, the false matching possibility can be reduced by setting a longer alignment length. When PE information is available, the false matching possibility can be reduced to a minimum. The extended version of Algorithm 1 that can take quality values into consideration is outlined in Additional file 6 Appendix B. Deal with paired-end information Unlike the standard SE sequencing, PE sequencing reads out each DNA template twice, in opposite directions from different ends. The underlying fact is that all the PE reads that need to be trimmed must have the preserved paired sequences reverse-complement to each other, as illustrated in Figure 5 and Additional file 7 Appendix C. Figure 5 figure5 Layout of paired-end reads that have adapter contaminants. Full size image Using this property, the program first finds all k-difference occurrences of adapters in both paired reads using the extended version of Algorithm 1 with quality values considered. Then the reverse-complementary property of each trimmed paired sequences is checked. Next, all candidates are evaluated with a scoring scheme that takes into account the fitness of adapter sequences in paired reads and the alignment of reverse-complementary counter-parts. Finally, the program outputs the optimal occurrence, if any. The scoring scheme we used is as follows: 𝑠𝑐𝑜𝑟𝑒(𝑖𝑑𝑥)=+𝑝𝑠𝑐𝑜𝑟𝑒(𝑟𝑒𝑎𝑑2[𝑖𝑑𝑥…𝑟𝑒𝑎𝑑𝐿𝑒𝑛−1],𝑎𝑑𝑎𝑝𝑡𝑒𝑟2)+𝑝𝑠𝑐𝑜𝑟𝑒(𝑟𝑒𝑎𝑑1[0…𝑖𝑑𝑥−1],𝑟𝑒𝑣𝐶𝑜𝑚𝑝(𝑟𝑒𝑎𝑑2[0…𝑖𝑑𝑥−1]))𝑝𝑠𝑐𝑜𝑟𝑒(𝑟𝑒𝑎𝑑1[𝑖𝑑𝑥…𝑟𝑒𝑎𝑑𝐿𝑒𝑛−1],𝑎𝑑𝑎𝑝𝑡𝑒𝑟1) where idx is the start position for trimming, p s c o r e(x,y)=|x|×P m a x −p e n a l t y(x,y), P m a x is the maximum penalty for a difference, p e n a l t y(x,y) is the penalty for matching x and y as calculated by the k-difference matching algorithms, and r e v C o m p(x) denotes the reverse complementary sequence of x. The goal is to find the idx that meets the k-difference requirement and maximizes the score function. Deal with Nextera LMP information Mate-pair library sequencing allows the generation of long-insert PE libraries that are useful in the scaffolding process of de novo genome assembly and in the detection of long-range genome structural variations. In the Nextera LMP library construction process, there are additional reactions called “tagmentation” and “circularization” before the normal PE library construction. The tagmentation reaction uses a specially engineered transposome to fragment the DNA sample and tag the DNA fragments by attaching a pair of biotinylated junction adapters simultaneously to the ends. Next, the tagmented DNA molecules are circularized and sheared by ultrasonics, and the sub-fragments containing the original junction parts are enriched via the biotin tag in the junction adapter. Trimming adapters from Nextera LMP reads is like a reverse process of Nextera LMP library construction. To process Nextera mate-pair reads, the program first trims the adapters as if it is dealing with PE reads. Then, it trims junction adapters from the processed paired reads separately using the extended version of Algorithm 1",Trimming,"skewer  fast  accurate adapter trimmer  nextgeneration sequence pairedend reads
kdifference problem give  sequence ss1s2…    query pattern pp1p2…     threshold ≤ search  substrings   note  {′}   ∥′∥    ≤ extend kdifference problem give  sequence ss1s2…    query pattern pp1p2…     threshold ≤⌊⌋ search  substrings   note  {′}   ∥′∥    ≤   suffix   note  {′}   ∃  prefix   note  ′  ∥′′∥    ≤′ algorithms   kdifference problem  classic approach  compute   dynamic program matrix cmn use  follow recurrence 𝐶𝑖𝑗𝑚𝑖𝑛⎧⎩⎨⎪⎪𝐶𝑖𝑗𝛿𝑖𝑗𝐶𝑖𝑗1𝐶𝑖𝑗  𝛿𝑖𝑗{if𝑝𝑖𝑠𝑗otherwise  initialization   upper boundary      leave boundary  cii  …  … finally  algorithm test  last row   matrix  cmj  report  elements    greater    algorithm    time    space complexity  space bind   easily reduce    matrix   compute  columns note     …  report  match  time   ≤  compute column   require   knowledge  previous column   careful design      share one column vector  propose  ukkonen  ukkonen also observe   columns    last element greater      boundary index     note                         …   easy  prove      ≤  ccj use  observation ukkonen reduce  time     expect     algorithm  develop  ukkonens algorithm however  use  queue instead   array  store  elements  current column   boundary index     new element  correspond   topmost element   new column  elements   queue shift automatically   next lower position   elements transfer   diagonals  matrix   process inherently keep  basic properties  ukkonens algorithm  facilitate subsequent improvements lemma    dynamic program matrix   tackle  kdifference problem  value  elements along  diagonal  monotonically nondecreasing  proof  provide  additional file  appendix  theorem    match elements   query pattern  sequence  equal   precursors   diagonal    need   update   dynamic program process proof  theorem   direct consequence  lemma    dynamic program recurrence        word  mismatch elements need   update   dynamic program process  bitvectors  denote mismatch position  comparison   adapter sequence     four nucleotide character  precomputed   bitvector  mark  position   queue elements  exceed  kdifference constraint  maintain  unnecessary computations  update  column vector   inhibit    key point  lead   main improvement   algorithm  ukkonens algorithm  list  algorithm   bitmasked kdifference match algorithmhas  follow characteristics use  queue instead   array  store  elements   current column  boundary index  preprocessing calculate    four nucleotide character  bitvector  denote  mismatch position compare   adapter mark  internal cells  exceed  kdifference constraint   bitvector  shift   queue push   process  column start   input nucleotide update   cells  mismatch     mark  algorithm use  queue  size   several bite vectors  size ⌈⌉     word length   computer  example  equal    bite machine  hence   space        character   target sequence  character enter  queue   exit   queue      random sequence  expect size   queue   hence generally  algorithm    expect time however    restrict   bitmask operations  element   queue usually update    time  bite operations  negligible compare  element update operations  algorithm achieve   worstcase time  practice   better     expect time  ukkonens algorithm algorithm    improve   avoid  unnecessary update  constant time bite operations within  iteration cycle   target nucleotide  basic principle     element   diagonal   original dynamic program matrix   value   derive   adjacent diagonal   indel occur   correspond path  score  associate index   element  remain unchanged   precursor remain unchanged although   improvement  reduce  theoretical time complexity  expect    worstcase   experiment  large volumes  real data show   reduce element update operations   compensate   additional bite operations   extend kdifference problem  additional step bound    perform  check   elements remain   queue   hit  find  previous step deal  basecall qualities  main advantage   bitmasked kdifference match algorithm  myers bitvector algorithm      extend  handle basecall quality value  handle basecall quality value  introduce  follow parameters       g10  minimum penalty   mismatch       g10  maximum penalty   mismatch           penalty   insertion  deletion  penalty   mismatch  quality value   calculate  ⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟⎧⎩⎨⎪⎪𝑃𝑚𝑖𝑛𝑃𝑚𝑖𝑛𝑞𝑃𝑚𝑎𝑥𝑃𝑚𝑖𝑛𝑃𝑚𝑎𝑥𝑞≤𝑞40𝑞≥   trivial  prove  pql  g1010q    score   negative logarithm   probability   correspond base  actually  match   adapter sequence  sequence error note   score scheme herein  induce penalties  mismatch  insertionsdeletions  match   possibility  false match base  sequence homology reduce exponentially   match length increase  false match possibility   reduce  set  longer alignment length   information  available  false match possibility   reduce   minimum  extend version  algorithm    take quality value  consideration  outline  additional file  appendix  deal  pairedend information unlike  standard  sequence  sequence read   dna template twice  opposite directions  different end  underlie fact      read  need   trim must   preserve pair sequence reversecomplement     illustrate  figure   additional file  appendix  figure  figure5 layout  pairedend read   adapter contaminants full size image use  property  program first find  kdifference occurrences  adapters   pair read use  extend version  algorithm   quality value consider   reversecomplementary property   trim pair sequence  check next  candidates  evaluate   score scheme  take  account  fitness  adapter sequence  pair read   alignment  reversecomplementary counterparts finally  program output  optimal occurrence    score scheme  use   follow 𝑠𝑐𝑜𝑟𝑒𝑖𝑑𝑥𝑝𝑠𝑐𝑜𝑟𝑒𝑟𝑒𝑎𝑑2𝑖𝑑𝑥…𝑟𝑒𝑎𝑑𝐿𝑒𝑛𝑎𝑑𝑎𝑝𝑡𝑒𝑟2𝑝𝑠𝑐𝑜𝑟𝑒𝑟𝑒𝑎𝑑1…𝑖𝑑𝑥𝑟𝑒𝑣𝐶𝑜𝑚𝑝𝑟𝑒𝑎𝑑2…𝑖𝑑𝑥𝑝𝑠𝑐𝑜𝑟𝑒𝑟𝑒𝑎𝑑1𝑖𝑑𝑥…𝑟𝑒𝑎𝑑𝐿𝑒𝑛𝑎𝑑𝑎𝑝𝑡𝑒𝑟1  idx   start position  trim      exyxp          yxy       maximum penalty   difference       yxy   penalty  match     calculate   kdifference match algorithms         denote  reverse complementary sequence    goal   find  idx  meet  kdifference requirement  maximize  score function deal  nextera lmp information matepair library sequence allow  generation  longinsert  libraries   useful   scaffold process   novo genome assembly    detection  longrange genome structural variations   nextera lmp library construction process   additional reactions call “tagmentation”  “circularization”   normal  library construction  tagmentation reaction use  specially engineer transposome  fragment  dna sample  tag  dna fragment  attach  pair  biotinylated junction adapters simultaneously   end next  tagmented dna molecules  circularize  shear  ultrasonics   subfragments contain  original junction part  enrich via  biotin tag   junction adapter trim adapters  nextera lmp read  like  reverse process  nextera lmp library construction  process nextera matepair read  program first trim  adapters     deal   read   trim junction adapters   process pair read separately use  extend version  algorithm ",7
162,Ktrim,"Ktrim: an extra-fast and accurate adapter- and quality-trimmer for sequencing data
Ktrim provides both adapter- and quality-trimming of the sequencing data. The schematic workflow of Ktrim was shown in Supplementary Fig. S1. In the current implementation, Ktrim performs quality-trimming first. For each read, Ktrim screens the quality of the sequencing cycles from its tail to head and stops at a position where the quality of the corresponding cycle is higher than the given threshold; then Ktrim will trim all the cycles after this one. If the remaining sequence is shorter than the minimum acceptable size (which parameter could be set by the users), this read will be discarded; otherwise Ktrim will perform adapter-trimming on this read. Ktrim employs a dual-index scheme to search for adapters in the sequencing data. For paired-end reads, Ktrim uses the head 3 bp of the two adapters linked to read 1 and read 2, respectively, as indexes to find the potential ligating position (“seed”s) of the adapters; for single-end data, however, Ktrim uses the head 3 bp and the very next 3 bp of the adapter linked to read 1 as the indexes. This dual-index approach could tolerate sequencing errors in the adapter sequences thus improves sensitivity. After finding the “seed”s, Ktrim searches for adapters from the left-most “seed” cycle by comparing the sequencing data versus the adapters base-by-base until the end of the sequencing reads or the adapters, whichever occurs first. In this manner, Ktrim could handle both scenarios where adapters are completely included or partially covered in the sequencing data. If the number of mismatches identified during the comparison of sequencing reads and adapters is smaller than a dynamic threshold (which parameter could be set by the users), this “seed” position will be called as the adapter ligation site, then Ktrim trims all the cycles after the “seed” position and ignores all the rest “seed”s; otherwise the next “seed” position will be investigated until all the “seed”s get exhausted. After adapter-trimming, Ktrim checks the sizes of the reads again and only reports those longer than the minimum acceptable size specified by the user. For performance evaluations, 10 million paired-end DNA reads were in silico generated. The read length was set to 100 bp while DNA insert size ranged from 10 to 200 bp. Hence, the dataset was consisted of reads that contain complete or partial adapters as well as those not contaminated by adapters. Meanwhile, random mutations were also introduced into the reads to simulate sequencing errors. Notably, the adapter information is known for these in silico reads, therefore allows accuracy evaluation and comparison of adapter-trimming algorithms. Furthermore, the read-1 also served as the testing dataset of single-end reads. Three widely used tools, Trim Galore (a wrapper of the Cutadapt program (Martin, 2011)), Trimmomatic (Bolger, et al., 2014) and SeqPurge (Sturm, et al., 2016), were collected for performance comparisons. In addition, a real dataset collected from the literature (Zhang, et al., 2016) was also analyzed. Detailed information (including codes) on simulation data generation and performance evaluations could be found in Supplementary Method.",Trimming,"ktrim  extrafast  accurate adapter  qualitytrimmer  sequence data
ktrim provide  adapter  qualitytrimming   sequence data  schematic workflow  ktrim  show  supplementary fig    current implementation ktrim perform qualitytrimming first   read ktrim screen  quality   sequence cycle   tail  head  stop   position   quality   correspond cycle  higher   give threshold  ktrim  trim   cycle   one   remain sequence  shorter   minimum acceptable size  parameter could  set   users  read   discard otherwise ktrim  perform adaptertrimming   read ktrim employ  dualindex scheme  search  adapters   sequence data  pairedend read ktrim use  head     two adapters link  read   read  respectively  index  find  potential ligate position “seed”   adapters  singleend data however ktrim use  head      next     adapter link  read    index  dualindex approach could tolerate sequence errors   adapter sequence thus improve sensitivity  find  “seed” ktrim search  adapters   leftmost “seed” cycle  compare  sequence data versus  adapters basebybase   end   sequence read   adapters whichever occur first   manner ktrim could handle  scenarios  adapters  completely include  partially cover   sequence data   number  mismatch identify   comparison  sequence read  adapters  smaller   dynamic threshold  parameter could  set   users  “seed” position   call   adapter ligation site  ktrim trim   cycle   “seed” position  ignore   rest “seed” otherwise  next “seed” position   investigate    “seed” get exhaust  adaptertrimming ktrim check  size   read    report  longer   minimum acceptable size specify   user  performance evaluations  million pairedend dna read   silico generate  read length  set     dna insert size range      hence  dataset  consist  read  contain complete  partial adapters  well    contaminate  adapters meanwhile random mutations  also introduce   read  simulate sequence errors notably  adapter information  know    silico read therefore allow accuracy evaluation  comparison  adaptertrimming algorithms furthermore  read also serve   test dataset  singleend read three widely use tool trim galore  wrapper   cutadapt program martin  trimmomatic bolger     seqpurge sturm     collect  performance comparisons  addition  real dataset collect   literature zhang     also analyze detail information include cod  simulation data generation  performance evaluations could  find  supplementary method",7
163,ConDetri-a,"ConDeTri--a content dependent read trimmer for Illumina data
ConDeTri is implemented in Perl (required version 5.8.9 or above), is platform independent, has no additional hardware or library requirements, and is distributed under Artistic License/GPL. It is designed to run single-threaded on desktop computers or on cluster machines. In default mode, it can be run by giving only one Fastq file for single-end sequencing or two Fastq files for paired-end sequencing. More advanced options allow the user to control such things as the quality values used for trimming, trimming size, the fraction of a read containing high quality bases, and the quality format used (either Illumina/Solexa Fastq format or Sanger Fastq format is chosen by different offset scores). Our trimming approach does not correct the actual quality scores called by the Illumina pipeline. Instead, it removes bases with quality values lower than a threshold from the 3′-end of a read and checks the remaining read for internal low quality bases. ConDeTri applies two filtering steps on each read. First, each read is trimmed, one base at the time, in an iterative process. Starting from the 3′-end of the read, bases are removed if the corresponding quality score is lower than a threshold QH. When reaching a base with a quality score higher than QH, the base is kept temporarily while following bases are evaluated. After parsing a certain number of consecutive high quality bases, nH, the trimming is terminated. However, even bases with low quality scores below QH, recorded before nH is reached, are saved temporarily. Up to nL consecutive low quality bases are accepted when they are surrounded by high quality bases. If nL is overrun, all temporarily saved bases are removed, and the process starts over again. The trimming continues until either nH consecutive high quality bases are found, or the read is trimmed down to length L. For a trimmed read to be approved, it must contain more than a certain fraction f of bases with a quality score higher than QH, and no bases with a quality score less than a lower bound threshold QL. If a base has a quality score lower than QL the read is removed. When all reads have been trimmed, each read or each read pair is examined. If a single read passes the quality check, it is stored in a new Fastq file. For paired end reads, pairs where both the reads fulfill the quality demands are saved in new paired Fastq files. If a pair contains only one read passing the quality requirements, the high quality read is saved in an extra Fastq. These reads can be used as single end reads. Besides Fastq files, ConDeTri reports the number of scanned and removed reads and the number of reads that are present as paired-end and as single-end reads. Figure S1 summarizes the algorithm in a flowchart and Figure S2 gives two examples. Per default, the high quality score (QH) is set to 25, which is similar to a sequencing error probability of 0.0032. This value was chosen after inspecting quality score distributions from several data sets with different insert sizes from the collared flycatcher genome-sequencing project, as a level where the number of bases kept are of highest possible quality without having a considerable loss of reads. For the sets inspected, changing the quality threshold to 30 resulted in a loss of the majority of reads during filtering. On the other hand, lowering it to 20 did not increase the number of reads kept significantly, but the per base error probability of those reads will be up to three times higher (∼0.01). However, the default value is by no means universal, and the threshold should be set according to the data. The low quality score (QL) is set to 10, which equals a probability of a sequencing error of 0.0909, the fraction f of bases with a quality score higher than QH is set to 80% and L, the minimum number of bases after trimming, is set to 50, to prevent saving reads that are too short for de novo assembly. The parameters nH and nL are set to 5 and 1, respectively. This means that for each low quality-base there must be at least five high quality bases, which is more than the QH value of 80%. The connection between these numbers must be considered when tweaking the parameters – keeping nH and nL as 5 and 1 but increasing the QH to 95% results in removing a large proportion of reads in the second step. However, all these settings can be changed as desired. Quality score distribution along reads and read length distribution after trimming for the libraries used for choosing the default values are shown in Figures S3, S4, S5, S6, S7, S8, S9, S10, S11, S12, S13, S14, S15, S16, S17 and Table S1. ConDeTri can read all three different Fastq quality score standards: Illumina and Solexa (early Illumina) quality scores with an offset of 64 and Sanger standard with an offset of 33. Conclusion The main focus of our quality filtering approach was to provide an accurate, standardized and easy to use method for trimming Illumina sequencing data. In comparison to other programs, data filtered with ConDeTri gave better results with respect to the size of the assembled data and also the accuracy of the de novo assembly. In comparison to untrimmed data, less memory and time is needed for de novo assemblies. This is crucial for larger eukaryotic genomes, because affordable computational resources are still a limiting factor in performing assemblies of larger genomes using several insert sizes for paired-end sequencing. Using quality-filtered data reduces the de Bruijn graph in the assembly process and should improve downstream analyses of NGS data (e.g. SNP calling).",Trimming,"condetria content dependent read trimmer  illumina data
condetri  implement  perl require version     platform independent   additional hardware  library requirements   distribute  artistic licensegpl   design  run singlethreaded  desktop computers   cluster machine  default mode    run  give  one fastq file  singleend sequence  two fastq file  pairedend sequence  advance options allow  user  control  things   quality value use  trim trim size  fraction   read contain high quality base   quality format use either illuminasolexa fastq format  sanger fastq format  choose  different offset score  trim approach   correct  actual quality score call   illumina pipeline instead  remove base  quality value lower   threshold   ′end   read  check  remain read  internal low quality base condetri apply two filter step   read first  read  trim one base   time   iterative process start   ′end   read base  remove   correspond quality score  lower   threshold   reach  base   quality score higher    base  keep temporarily  follow base  evaluate  parse  certain number  consecutive high quality base   trim  terminate however even base  low quality score   record    reach  save temporarily    consecutive low quality base  accept    surround  high quality base    overrun  temporarily save base  remove   process start    trim continue  either  consecutive high quality base  find   read  trim   length    trim read   approve  must contain    certain fraction   base   quality score higher     base   quality score less   lower bind threshold    base   quality score lower    read  remove   read   trim  read   read pair  examine   single read pass  quality check   store   new fastq file  pair end read pair    read fulfill  quality demand  save  new pair fastq file   pair contain  one read pass  quality requirements  high quality read  save   extra fastq  read   use  single end read besides fastq file condetri report  number  scan  remove read   number  read   present  pairedend   singleend read figure  summarize  algorithm   flowchart  figure  give two examples per default  high quality score   set     similar   sequence error probability    value  choose  inspect quality score distributions  several data set  different insert size   collar flycatcher genomesequencing project   level   number  base keep   highest possible quality without   considerable loss  read   set inspect change  quality threshold   result   loss   majority  read  filter    hand lower      increase  number  read keep significantly   per base error probability   read     three time higher  however  default value    mean universal   threshold   set accord   data  low quality score   set    equal  probability   sequence error    fraction   base   quality score higher    set      minimum number  base  trim  set    prevent save read    short   novo assembly  parameters     set     respectively  mean    low qualitybase  must   least five high quality base       value    connection   number must  consider  tweak  parameters  keep         increase     result  remove  large proportion  read   second step however   settings   change  desire quality score distribution along read  read length distribution  trim   libraries use  choose  default value  show  figure        s10 s11 s12 s13 s14 s15 s16 s17  table  condetri  read  three different fastq quality score standards illumina  solexa early illumina quality score   offset    sanger standard   offset   conclusion  main focus   quality filter approach   provide  accurate standardize  easy  use method  trim illumina sequence data  comparison   program data filter  condetri give better result  respect   size   assemble data  also  accuracy    novo assembly  comparison  untrimmed data less memory  time  need   novo assemblies   crucial  larger eukaryotic genomes  affordable computational resources  still  limit factor  perform assemblies  larger genomes use several insert size  pairedend sequence use qualityfiltered data reduce   bruijn graph   assembly process   improve downstream analyse  ngs data  snp call",7
164,InfoTrim,"InfoTrim: A DNA read quality trimmer using entropy
InfoTrim is a multiprocess Python 2.7 program featuring a maximum information score used to cut DNA reads at the 5’ end. For a read d, the substring dl is the read starting at base position 0 and ending at base position l. Starting at the first base, the score Stotal(dl) is computed for all l starting at 0 and ending at the total length of the read d. The position l that maximizes the score is chosen and every base after l is discarded from the read. The InfoTrim maximum information model involves four terms. The first term, Slen(dl), is a logistic function that provides a length threshold m. This causes the trimmer to strongly prefer reads larger than m. The reason for this is that reads that are too small will have too little information to be useful. The default is m = 25, and the equation is the following. Slen(dl) = 1 1 + em−l The second term, Scov(dl), causes the trimmer to prefer longer reads to shorter reads in order to increase the coverage that the read represents. It is simply the length of the read, giving a linear relationship between the length and the score. It is described in the following. Scov(dl) = l The phred score, Sphred(dl), is the third term, and it measures base calling correctness. The phred score at base position i in read d is the probability that the base was called correctly as determined by the sequencing platform. The phred number, a positive integer, at base position i is given by Qi . The probability derived from Qi is given by Pcorrect(i) = 10 −Qi 10 . The phred term is given by the following formula. The rationale for using the product is given in the Trimmomatic paper [4]. Sphred(dl) = Y l i=1 Pcorrect(i) InfoTrim adds a fourth term for entropy. The probability Pl(b) is the frequency that base b is found in read dl . The Shannon entropy is multiplied by one half to give it a scaling between 0 and 1. The formula that InfoTrim uses is given as the following. Sent(dl) = − 1 2 X b∈{A,C,T ,G,N} Pl(b) log(Pl(b)) The last three terms are combined using the geometric mean so that proportional changes in each term will contribute equally to the final score. Parameters r (for entropy) and s (for phred) are used to weight the relative contributions of the last three terms. The values r, s, and 1 − r − s must all add up to one. For example, if r = 1, then only the entropy term will be used while the coverage and phred scores will not be used. Thus, the final score is the following.",Trimming,"infotrim  dna read quality trimmer use entropy
infotrim   multiprocess python  program feature  maximum information score use  cut dna read    end   read   substring    read start  base position   end  base position  start   first base  score stotaldl  compute    start    end   total length   read   position   maximize  score  choose  every base    discard   read  infotrim maximum information model involve four term  first term slendl   logistic function  provide  length threshold   cause  trimmer  strongly prefer read larger    reason     read    small    little information   useful  default       equation   follow slendl     eml  second term scovdl cause  trimmer  prefer longer read  shorter read  order  increase  coverage   read represent   simply  length   read give  linear relationship   length   score   describe   follow scovdl    phred score sphreddl   third term   measure base call correctness  phred score  base position   read    probability   base  call correctly  determine   sequence platform  phred number  positive integer  base position   give     probability derive    give  pcorrecti       phred term  give   follow formula  rationale  use  product  give   trimmomatic paper  sphreddl     pcorrecti infotrim add  fourth term  entropy  probability plb   frequency  base   find  read    shannon entropy  multiply  one half  give   scale      formula  infotrim use  give   follow sentdl      ∈{act } plb logplb  last three term  combine use  geometric mean   proportional change   term  contribute equally   final score parameters   entropy    phred  use  weight  relative contributions   last three term  value         must  add   one  example        entropy term   use   coverage  phred score    use thus  final score   follow",7
165,CutAdapt,"Cutadapt Removes Adapter Sequences From High-Throughput Sequencing Reads
Cutadapt is mainly written in Python, but for speed, the alignment algorithm is implemented in C as a Python extension module. The program was developed on Ubuntu Linux, but tested also on Windows and Mac OS X. It should also work on other platforms for which Python is available. Features The program was initially developed to trim 454 sequencing data collected by Zeschnigk et al. [5]. As insertions and deletions within homopolymer runs are common in 454 data, cutadapt supports gapped alignment. For small RNA data analysis by Schulte et al. [6], the program was modified to support trimming of colour-space reads. It has also been tested and works well on Illumina data. Cutadapt can search for multiple adapters in a single run of the program and will remove the best matching one. It can optionally search and remove an adapter multiple times, which is useful when (perhaps accidentally) library preparation has led to an adapter being appended multiple times. It can either trim or discard reads in which an adapter occurs. Reads that are outside a specified length range after trimming can also be discarded. To decrease the number of random hits, a minimum overlap between the read and adapter can be specified. In addition to adapter trimming, low-quality ends of reads can be trimmed using the same algorithm as BWA. Cutadapt is thoroughly unit-tested. The program is actively maintained, and many features have been added in response to requests by users. Performance In theory, adapter trimming with cutadapt is dominated by the time needed to compute alignments, which is O(nk), where n is the total number of the characters in all reads, and k is the sum of the length of the adapters. In practice, other operations, such as reading and parsing the input files, take up more than half of the time. With 35 bp colour-space reads and an adapter of length 18, cutadapt trims approximately 1 million reads per minute (0.06 ms per read) on a single core of a 2.66 GHz Intel Core 2 processor. Colour-space reads Cutadapt correctly deals with reads given in SOLiD colour space. When an adapter is found, the adapter and the colour preceding it must be removed, as that colour encodes the transition from the small RNA into the adapter sequence and could otherwise lead to a spurious mismatch during read mapping. Cutadapt can produce output compatible with MAQ [2] and BWA [3]. These tools need FASTQ files in which the colours are not encoded by the digits 0-3, but by the letters ACGT (so-called double encoding), and in which the last primer base (given in nucleotide space) and the first colour are removed. Usage example In this simple example, the adapter AACCGG is trimmed from reads in the compressed file in.fastq.gz. The result is written to out.fastq: cutadapt -a AACCGG in.fastq.gz > out.fastq Other use cases are documented in the README file in the cutadapt distribution, and on the website. Full documentation is available by typing “cutadapt --help” on the command line. Algorithm In the following, a character is a nucleotide or a colour (encoded by 0-3). The first step in processing a single read is to compute optimal alignments between the read and all given adapters. Cutadapt computes either ‘regular’ or slightly modified semi-global alignments. Regular semi-global alignments, also called end-space free alignments [7, Chapter 11.6.4], do not penalise initial or trailing gaps. This allows the two sequences to shift freely relative to each other. When the “-a” parameter is used to provide the sequence of an adapter, the adapter is assumed to be ligated to the 3’ end of the molecule, and the behaviour of cutadapt is therefore to remove the adapter and all characters after it. With regular semi-global alignments, a short, usually random, match that overlaps the beginning of a read would lead to the removal of the entire read. We therefore require that an adapter starts at the beginning or within the read. This is achieved by penalising initial gaps in the read sequence, which is the only modification to regular overlap alignment. See Figure 1, right column. 200-1607-1-SP.png Figure 1. This illustration shows all possible alignment configurations between the read and adapter sequence. There are two different trimming behaviours, triggered by whether option “-a” or “-b” is used to provide the adapter sequence. Note that the case “Partial adapter in the beginning” is not possible with option “-a”, as the alignment algorithm prevents it. Regular semi-global alignment is used when the location of the adapter is unknown (assumed when the “-b” parameter is used). Then, if the adapter is found to overlap the beginning of the read, all characters before the first non-adapter character are removed. See Figure 1 for an illustration of all possible cases. After aligning all adapters to the read, the alignment with the greatest number of characters that match between read and adapter is considered to be the best one. Next, the error rate e/l is computed, where e is the number of errors, and l is the length of the matching segment between read and adapter. Finally, if the error rate is below the allowed maximum, the read is trimmed",Trimming,"cutadapt remove adapter sequence  highthroughput sequence reads
cutadapt  mainly write  python   speed  alignment algorithm  implement     python extension module  program  develop  ubuntu linux  test also  windows  mac     also work   platforms   python  available feature  program  initially develop  trim  sequence data collect  zeschnigk     insertions  deletions within homopolymer run  common   data cutadapt support gap alignment  small rna data analysis  schulte     program  modify  support trim  colourspace read   also  test  work well  illumina data cutadapt  search  multiple adapters   single run   program   remove  best match one   optionally search  remove  adapter multiple time   useful  perhaps accidentally library preparation  lead   adapter  append multiple time   either trim  discard read    adapter occur read   outside  specify length range  trim  also  discard  decrease  number  random hit  minimum overlap   read  adapter   specify  addition  adapter trim lowquality end  read   trim use   algorithm  bwa cutadapt  thoroughly unittested  program  actively maintain  many feature   add  response  request  users performance  theory adapter trim  cutadapt  dominate   time need  compute alignments   onk     total number   character   read     sum   length   adapters  practice  operations   read  parse  input file take    half   time    colourspace read   adapter  length  cutadapt trim approximately  million read per minute   per read   single core    ghz intel core  processor colourspace read cutadapt correctly deal  read give  solid colour space   adapter  find  adapter   colour precede  must  remove   colour encode  transition   small rna   adapter sequence  could otherwise lead   spurious mismatch  read map cutadapt  produce output compatible  maq   bwa   tool need fastq file    colour   encode   digits     letter acgt socalled double encode     last primer base give  nucleotide space   first colour  remove usage example   simple example  adapter aaccgg  trim  read   compress file infastqgz  result  write  outfastq cutadapt  aaccgg infastqgz  outfastq  use case  document   readme file   cutadapt distribution    website full documentation  available  type “cutadapt help”   command line algorithm   follow  character   nucleotide   colour encode    first step  process  single read   compute optimal alignments   read   give adapters cutadapt compute either regular  slightly modify semiglobal alignments regular semiglobal alignments also call endspace free alignments  chapter    penalise initial  trail gap  allow  two sequence  shift freely relative      “” parameter  use  provide  sequence   adapter  adapter  assume   ligate    end   molecule   behaviour  cutadapt  therefore  remove  adapter   character    regular semiglobal alignments  short usually random match  overlap  begin   read would lead   removal   entire read  therefore require   adapter start   begin  within  read   achieve  penalise initial gap   read sequence     modification  regular overlap alignment see figure  right column sppng figure   illustration show  possible alignment configurations   read  adapter sequence   two different trim behaviours trigger  whether option “”  “”  use  provide  adapter sequence note   case “partial adapter   beginning”   possible  option “”   alignment algorithm prevent  regular semiglobal alignment  use   location   adapter  unknown assume   “” parameter  use    adapter  find  overlap  begin   read  character   first nonadapter character  remove see figure    illustration   possible case  align  adapters   read  alignment   greatest number  character  match  read  adapter  consider    best one next  error rate   compute     number  errors     length   match segment  read  adapter finally   error rate    allow maximum  read  trim",7
166,AlienTrimmer,"AlienTrimmer: A tool to quickly and accurately trim off multiple short contaminant sequences from high-throughput sequencing reads
The AlienTrimmer algorithm Decomposing DNA sequence into overlapping k-mers (i.e. sequences of length k) is a standard task in bioinformatics. Given a non-zero positive integer value k, each of the 4k k-mers can be stored with only nk bits by using an n-bit binary code bn (provided n > 1). AlienTrimmer uses a binary coding b2 defined as b2(A) = 00, b2(C) = 01, b2(G) = 10, and b2(T) = 11. In order to deal with degenerate nucleotides N, it also uses another binary coding b4 defined as b4(A) = 0001, b4(C) = 0010, b4(G) = 0100, and b4(T) = 1000 (see below for details about b4 coding of degenerate residues). Therefore, each k-mer is bijectively associated to a unique binary number of nk bits. Usual computer words being 32 or 64 bit long, such binary codes allow k-mers with k ≤ 16 to be easily computed and stored. Given an alien or read sequence, AlienTrimmer computes the binary representation of its k-mers by using three basic bit operations: bitwise OR (|), bitwise AND (&), and bit left shifting (<<). More formally, if wi is the binary representation of the k-mer starting at position i and ending at position i + k − 1, then the binary representation wi + 1 of the next k-mer is easily computed by the formula wi + 1 = ((wi << n) | bn(ci + k)) & maskn, where ci + k is the character state at position i + k, and maskn a constant binary representation of nk − 1 that allows setting to zero all bits shifted beyond the nkth one. Following this approach, the list of every k-mer from a nucleotide sequence of length L is computed in time O(L). Given a fixed integer value k (= 10 by default), AlienTrimmer first performs k-mer decomposition from each specified alien sequence, and the binary representation w of each extracted alien k-mer is computed following both binary codes b2 and b4 in order to deal with the presence of degenerate nucleotides (see below). Each binary representation of a k-mer encoded with b2 being equivalent to an integer lying between 0 (i.e. poly-A of length k) and 4k − 1 (i.e. poly-T of length k), each distinct alien k-mer is stored inside a bitset Β of size 4k (i.e. if an alien k-mer is b2-coded as w, the xth bit of Β is set to 1 where x is the integer value equivalent to w). Each alien k-mer binary representation encoded with b4 is stored in a sorted list Λ. Note that AlienTrimmer also stores the binary representations of the reverse-complement of each extracted alien k-mer, in order to be able to search for the reverse-complement of each specified alien sequence in read ends. This whole procedure allows computing and storing (in both bitset Β and sorted list Λ) the K = |Λ| distinct k-mers that can be extracted from the different specified alien sequences (see algorithm details in Supplementary materials S1.1). Denoting LA as the sum of the different alien sequence lengths, we have K ∈ O(LA) even if K is much smaller than LA in practice; therefore this first pre-computing step requires O(LA log LA) time complexity. Given a read sequence of length LR, AlienTrimmer proceeds its successive k-mer surveying following the same way as for alien sequences. However, for each read k-mer, AlienTrimmer searches whether it is present among the K alien ones. When a read k-mer only contains non-degenerate nucleotides (i.e. A, C, G and T), this search is directly performed by considering its b2 coding, and looking in the pre-computed alien k-mer bitset Β whether its corresponding bit is set to 1. In contrast, when a read k-mer contains at least one degenerate nucleotide, AlienTrimmer considers its b4 coding, which is able to deal with such character state. By setting b4(N) = 0000, a k-mer containing degenerate nucleotides is compatible with another k-mer with no degenerate nucleotide if their respective binary coding w and w′ verify the simple property w | w′ = w′; indeed, b4(s) | b4(s′) = b4(s′) only when s = s′ or s = N. When a read k-mer w contains at least one degenerate nucleotide, AlienTrimmer searches whether there exists one compatible alien k-mer w′ among the K ones. This leads to a worst case O(K) time complexity for each read k-mer with at least one degenerate nucleotide. However, as the K b4-coded alien k-mers are sorted in Λ (see above), this step is accelerated by searching first the largest alien binary representation w′min such that w′min < w. Recalling that b4(N) = 0000, the binary search algorithm (e.g. [21]) allows finding w′min among the K alien k-mers in time O(log K). Reciprocally, by updating a second binary representation  from w with the coding , AlienTrimmer also searches for the lowest alien binary representation w′max such that  with the binary search algorithm. Thanks to the two coding b4 and  of N, if there exists inside Λ at least one alien binary representation w′ compatible with the read one w containing at least one degenerate nucleotide, then one has w′min ≤ w′ ≤ w′max, and AlienTrimmer searches for w′ only between these two bounds. The two O(log K) binary searches of w′min and w′max do not modify the theoretical O(K) computational complexity, but allow observing faster running times in practice than crudely testing compatibility of w with each of the K alien w′ in Λ. Using the previously described method, AlienTrimmer is able to determine every nucleotide indexes of a read where an exact alien k-mer match occurs. By using these nucleotide indexes, AlienTrimmer easily estimates the alien coverage within the read, i.e. the number of times each read nucleotide is covered by alien k-mers. The alien k-mer coverage can be directly computed together with the alien k-mer matching process without modifying the overall time complexity (see algorithm details in Supplementary materials S1.2). Moreover, this simple approach can be easily extended to quality-based trimming by incrementing by 1 the coverage value of every nucleotide supported by a low Phred score value. Finally, AlienTrimmer performs trimming by removing the prefix or suffix sub-sequences of the read when their alien k-mer coverage is higher than zero. Fig. 4 represents four examples of read contamination, with graphical representations of the alien coverage estimated for each nucleotide with k = 10, and the resulting trimmed alien residues. Though fast and efficient when complete alien oligonucleotide sequences are present in at least one of both ends of a read (see Figs. 4A and B), this algorithm may not accurately perform its task when mismatches occur within the read ends to be trimmed. In practice, this situation corresponds to some single nucleotides with zero alien coverage occurring between nucleotides with non-zero alien coverage (see base ‘A’ with white background on Fig. 4C). Albeit infrequent when low quality nucleotides were trimmed off (see Section 2.2 and Fig. 3), such mismatches are easily accommodated by AlienTrimmer in the following manner. For 5′ end, AlienTrimmer first searches for the index i of the first nucleotide with non-zero alien coverage. Second, AlienTrimmer surveys the following alien covered nucleotides until a nucleotide of index j with zero alien coverage is reached. Given a specified number of mismatches m (= ⌈k/2⌉ by default), AlienTrimmer verifies whether the nucleotide at index j + m is covered by an alien k-mer; if any, AlienTrimmer re-iterates its search of the next index j such that nucleotides at both indexes j and j + m are not covered by any alien k-mer (see algorithm details in Supplementary materials S1.3). This approach allows identifying the sub-read defined between indexes i and j that is mainly covered by alien k-mers. If this sub-read is sufficiently close enough to the 5′ end (i.e. j ≥ 2i), then AlienTrimmer removes the read prefix up to index j. The 3′ end trimming is performed following the same approach by starting from the last nucleotide index i with non-zero alien coverage, and performs backward searching of the index j < i such that nucleotides at indexes j and j − m are not covered by any alien k-mer. Given a collection of alien sequences of total length LA, decomposing and storing them into K distinct alien k-mers require O(LA log LA) time complexity (see above), which is negligible in comparison with the overall alien trimming process. Given a read of length LR, AlienTrimmer is able to detect and remove alien sequences in 5′ and 3′ ends in time O(LR) or O(LR K) depending on the absence of degenerate nucleotide within the read or not, respectively. Knowing that K ∈ O(LA), the overall time complexity required by AlienTrimmer for processing each read of length LR is O(LA log LA + LR) in the best case scenario (i.e. no degenerate nucleotide), or O(LA(LR + log LA)) in the worst case scenario (i.e. presence of degenerate nucleotides within the read). Therefore, when dealing with standard HTS data with N reads with no degenerate nucleotide and N′ reads with degenerate nucleotides, the overall time complexity required by AlienTrimmer is O(LA log LA + N LR + N′ LA LR).",Trimming,"alientrimmer  tool  quickly  accurately trim  multiple short contaminant sequence  highthroughput sequence reads
 alientrimmer algorithm decompose dna sequence  overlap kmers  sequence  length    standard task  bioinformatics give  nonzero positive integer value      kmers   store    bits  use  nbit binary code  provide    alientrimmer use  binary cod  define  b2a   b2c   b2g    b2t    order  deal  degenerate nucleotides   also use another binary cod  define  b4a   b4c   b4g    b4t   see   detail   cod  degenerate residues therefore  kmer  bijectively associate   unique binary number   bits usual computer word     bite long  binary cod allow kmers   ≤    easily compute  store give  alien  read sequence alientrimmer compute  binary representation   kmers  use three basic bite operations bitwise   bitwise    bite leave shift   formally     binary representation   kmer start  position   end  position        binary representation      next kmer  easily compute   formula         bnci    maskn       character state  position     maskn  constant binary representation      allow set  zero  bits shift beyond  nkth one follow  approach  list  every kmer   nucleotide sequence  length   compute  time  give  fix integer value     default alientrimmer first perform kmer decomposition   specify alien sequence   binary representation    extract alien kmer  compute follow  binary cod     order  deal   presence  degenerate nucleotides see   binary representation   kmer encode    equivalent   integer lie    polya  length       polyt  length   distinct alien kmer  store inside  bitset   size     alien kmer  b2coded    xth bite    set       integer value equivalent    alien kmer binary representation encode    store   sort list  note  alientrimmer also store  binary representations   reversecomplement   extract alien kmer  order   able  search   reversecomplement   specify alien sequence  read end  whole procedure allow compute  store   bitset   sort list      distinct kmers    extract   different specify alien sequence see algorithm detail  supplementary materials  denote    sum   different alien sequence lengths    ∈ ola even    much smaller    practice therefore  first precomputing step require ola log  time complexity give  read sequence  length  alientrimmer proceed  successive kmer survey follow   way   alien sequence however   read kmer alientrimmer search whether   present among   alien ones   read kmer  contain nondegenerate nucleotides        search  directly perform  consider   cod  look   precomputed alien kmer bitset  whether  correspond bite  set    contrast   read kmer contain  least one degenerate nucleotide alientrimmer consider   cod   able  deal   character state  set b4n    kmer contain degenerate nucleotides  compatible  another kmer   degenerate nucleotide   respective binary cod   ′ verify  simple property   ′  ′ indeed b4s  b4s′  b4s′     ′       read kmer  contain  least one degenerate nucleotide alientrimmer search whether  exist one compatible alien kmer ′ among   ones  lead   worst case  time complexity   read kmer   least one degenerate nucleotide however    b4coded alien kmers  sort   see   step  accelerate  search first  largest alien binary representation ′min   ′min   recall  b4n    binary search algorithm   allow find ′min among   alien kmers  time olog  reciprocally  update  second binary representation      cod  alientrimmer also search   lowest alien binary representation ′max      binary search algorithm thank   two cod        exist inside   least one alien binary representation ′ compatible   read one  contain  least one degenerate nucleotide  one  ′min ≤ ′ ≤ ′max  alientrimmer search  ′    two bound  two olog  binary search  ′min  ′max   modify  theoretical  computational complexity  allow observe faster run time  practice  crudely test compatibility        alien ′   use  previously describe method alientrimmer  able  determine every nucleotide index   read   exact alien kmer match occur  use  nucleotide index alientrimmer easily estimate  alien coverage within  read   number  time  read nucleotide  cover  alien kmers  alien kmer coverage   directly compute together   alien kmer match process without modify  overall time complexity see algorithm detail  supplementary materials  moreover  simple approach   easily extend  qualitybased trim  incrementing    coverage value  every nucleotide support   low phred score value finally alientrimmer perform trim  remove  prefix  suffix subsequences   read   alien kmer coverage  higher  zero fig  represent four examples  read contamination  graphical representations   alien coverage estimate   nucleotide       result trim alien residues though fast  efficient  complete alien oligonucleotide sequence  present   least one   end   read see figs     algorithm may  accurately perform  task  mismatch occur within  read end   trim  practice  situation correspond   single nucleotides  zero alien coverage occur  nucleotides  nonzero alien coverage see base   white background  fig  albeit infrequent  low quality nucleotides  trim  see section   fig   mismatch  easily accommodate  alientrimmer   follow manner  ′ end alientrimmer first search   index    first nucleotide  nonzero alien coverage second alientrimmer survey  follow alien cover nucleotides   nucleotide  index   zero alien coverage  reach give  specify number  mismatch   ⌈⌉  default alientrimmer verify whether  nucleotide  index     cover   alien kmer   alientrimmer reiterate  search   next index    nucleotides   index        cover   alien kmer see algorithm detail  supplementary materials   approach allow identify  subread define  index      mainly cover  alien kmers   subread  sufficiently close enough   ′ end   ≥   alientrimmer remove  read prefix   index   ′ end trim  perform follow   approach  start   last nucleotide index   nonzero alien coverage  perform backward search   index      nucleotides  index        cover   alien kmer give  collection  alien sequence  total length  decompose  store    distinct alien kmers require ola log  time complexity see    negligible  comparison   overall alien trim process give  read  length  alientrimmer  able  detect  remove alien sequence  ′  ′ end  time olr  olr  depend   absence  degenerate nucleotide within  read   respectively know   ∈ ola  overall time complexity require  alientrimmer  process  read  length   ola log      best case scenario   degenerate nucleotide  olalr  log    worst case scenario  presence  degenerate nucleotides within  read therefore  deal  standard hts data   read   degenerate nucleotide  ′ read  degenerate nucleotides  overall time complexity require  alientrimmer  ola log      ′  ",7
167,PE-Trimmer,"An Efficient Trimming Algorithm based on Multi-Feature Fusion Scoring Model for NGS Data
The pipeline of PE-Trimmer can be divided into three major steps. The first one is to remove the technical sequences from the reads library. The technical sequences includes adapters, simple repeats, ploy-A/T tails and non-ATGC characters. The second one is to determine the range of reads that need to be trimmed. The last one is to determine the two sites with the lowest quality of the reads that need to be pruned by using the composite scoring model, and select the appropriate trimming strategy to trim the read based on the distribution of these two sites. The illustration of the pipeline of PE-Trimmer is shown in Fig.1, and the detailed steps are illustrated in the following subsections. 2.1 Removal of technical sequences There are many types of technical sequences in NGS data, such as base N or non-ATGC characters, poly-A or poly-T tails, simple repeats, adapters and so on. If these low quality sequences are not removed correctly, they can lead to either missed alignments or wrong genotyping and SNP calls in further downstream analyses[23],[24]. The following strategies are proposed to remove these low quality sequences in this study. 2.1.1 Removal of base N and non-ATGC characters Assume that Q(x) is a quality function, which represents the average quality score of string x. Let l be the length of read. Let ler be the threshold of the length of read after trimming(ler=l/3 by default). If a read consists only of the character N, this read is discarded directly. If a read ends with a series of characters N or non-AT GC characters, and the length of read after trimming is larger than or equal to the threshold ler, this strange tail is cut off. Conversely, if the length of a read after trimming is less than the threshold ler, this read is discarded as a whole. If the characters N or non-AT GC characters are scattered in a read, PE-Trimmer cuts the read into several normal segments according to the positions of these abnormal characters. After that ,it selects the segments whose length is larger than or equal to the threshold ler. Finally, it calculates the quality score of each segment by function Q(x), and selects the segment with the highest quality score as the retention segment. The principle of removing the characters N and non-AT GC characters in a read Removal of poly-A/T tails The poly-A/T tails must be removed since they (i).provide false sequences, (ii). mislead assembling, alignment and downstream sequence processing and analysis[25]. If a read ends with a series of characters A or T (poly-A or poly-T), then this strange tail is cut off. If a read consists only of poly-A/T sequences, it is discarded directly. The principle of removing the poly-A/T tails in a read Removal of simple repeats Simple repeats are part of the real sequence, they may also mislead further analysis[26]. If a read ends with a series of simple repeats, this strange tail is cut off. If a read consists only of simple repeats, it is discarded directly. The principle of removing the simple repeats in a read Removal of adapter Next generation sequencing requires DNA to be inserted into bacterial vectors for biological amplification. Adapter or linker oligonucleotides may also be attached to target DNA fragments to facilitate insertion into the vector. These vector and adapter sequences are sequenced concomitantly with the target, or insert sequence and represent contamination which must be removed from the dataset prior to analysis[17],[27]. Given a known adapter pattern and a read sequence, adapter trimming is usually modeled as a semi-global sequence alignment, also called end-space free alignment, where any space at the end of or beginning of the alignment does not incur penalties[28]. The accuracy of adapter trimming depends on the adapter sequence scanning on the error prone 3’ ends and 5’ ends. To overcome the impact of sequencing errors on the adapter sequence scanning, we introduces a fuzzy matching algorithm of strings based on the edit distance in this study. If the similarity of the first few bases or the last few bases of a read with a known adapter sequence is larger than the threshold(0.9 by default), these specific heads or tails are cut off. The principle of removing the adapter sequences in a read Determine the range of trimming Quality scores are usually given as phred-scaled probability[29] of the i-th base being wrong. The average value of the quality score of bases in a read can be used to measure the reliability of the read. The quality score statistics histogram of reads in the library is shown in Fig.6. Let |SR| represents the length of selected region, |RI| represents the length of reserved region, Pmin represents the minimum quality score, Pmax represents the maximum quality score, and Pth represents the x coordinate of trimming threshold. If the quality score of a read is less than Pmin + ε ∗ (Pmax-Pmin), it means that the read needs to be trimmed. On the contrary, this read does not need to be trimmed, Where ε is the influence factor of trimming. PE-Trimmer controls the trimming degree according to the value of ε. The larger ε value indicates that more reads need to be trimmed. The default value of ε is 0.3, its value also can be determined by the user according to the actual situation. 2.3 Sequence trimming To improve the accuracy of of this algorithm, we design a light-weight and easy-to-explain scoring model to evaluate candidates in the pattern of trimming step. Finally, PETrimmer selects the appropriate trimming strategy to process the low quality reads based on the location determined by the scoring model. 2.3.1 Sliding window for read quality detection The sliding window uses a standard approach. It works by scanning from 5’ end to 3’ end of the read. In the process of window sliding, PE-Trimmer uses the scoring model to calculate the score of each window. Afterwards, PE-Trimmer summarizes the scores of each window to form a bar chart. Finally, PE-Trimmer determines the trimming site of the low quality read according to the bar chart. Fig. 6: The quality score statistics histogram of reads in dataset of Rhodobacter sphaeroides(The dataset of R.sphaeroides is obtained from GAGE-B website). The xaxis refers to the quality score of reads; the y-axis refers to the frequency of quality scores. ′SR′ represent the selected region, ′RI′ represent the reserved region, ′T R′ represent the trim region. ′Pmin ′ represent the minimum quality score, ′Pmax ′ represent the maximum quality score, ′Pth ′ represent the x coordinate of trim threshold. 2.3.2 Scoring model Let x and Lx represent a read and its length, respectively. Let Qx(r) represents the quality score of the r-th base on read x, wi be the window appearing at the i-th position on read x, |wi | represents the length of wi(|wi | < Lx). The size of the window is fixed(11 by default). The correct probability of wi can be expressed as Pwi . Therefore, the error probability of wi can be expressed as 1-Pwi . PETrimmer uses the error probability of wi as the first scoring point, If the string contained in each window is treated as a k-mer, a read x can be represented as a list of (Lx- |wi |+1) k-mers. In the experiment, we found that k (the k-mer size) should be kept small to prevent the overuse of computer memory, while still large enough so that most k-mers are unique in genome. Once k is determined, the maximum number of k-mers is fixed as 4 k . In practice, we often require the space size of k-mer to be at least 5 times larger than the genome size (4 k > 5 ∗ G), and be less than the upper limit of system memory usage. In addition, as the value of k increases, the time required for the k-mer frequency statistics also increases. Therefore, when selecting the size of k, it is necessary to comprehensively consider the memory requirement, the time requirement and the accuracy requirement. In this study, we set the value of k to be 11, which have fully considered the above factors. When dealing with large genomes, the value of k can be increased appropriately. For example, when the number of reads exceeds 100 million, the value of k can be set to an odd number of 15 or more. It is well know that the low frequency k-mers are often caused by sequencing errors or bias[30],[31], especially in high-depth reads(reads from the high sequencing depth regions), the low frequency k-mer is more likely to result from sequencing errors. PE-Trimmer uses the frequency of k-mer as the second scoring point, just as shown in Equation(5). The lower the frequency of wi is, the higher the K Scorewi is. K Scorewi = 1 f requency(wi) (5) The sequencing errors occur more frequently in regions with an extremely high GC or AT content, such as constant heterochromatin regions that include centromeres, telomere or highly repetitive sequences[32],[33]. As above, PETrimmer considers the string contained in a window as a k-mer, uses the GC content of k-mer as the third scoring point, just as shown in Equation(6), where NGC represents the number of G/C characters in the k-mer. The higher the GC content of wi is, the higher the GC Scorewi is. GC Scorewi = NGC |wi | (6) Finally, PE-Trimmer combines three scoring formulas to form a comprehensive scoring model, just as shown in Equation(7). Scorewi = NGC × [1 − Q|wi| j=1(1 − 10−Qx(i+j)/10)] |wi | × f requency(wi) (7) After that, PE-Trimmer scores each window of the read that needs to be trimmed, and selects the two windows with the highest scores for the next judgment and processing. An example of applying the scoring model is shown in Figure 7. In Figure 7, the x-axis refers to the positions on the read, the y-axis refers to the score in four different situations, the subgraph (A) shows the k-mer frequency score of each window on the read, the subgraph (B) shows the quality score of each window on the read, the subgraph (C) shows the GC score of each window on the read, and the subgraph (D) shows the comprehensive score of each window on the read. After scoring the windows on the read, PE-Trimmer screens out the two highest scoring windows and records where they appear on the read. Next, PE-Trimmer selects the appropriate strategy based on these two recorded locations to trim the low quality reads. 2.3.3 Select trim strategy based on score information PE-Trimmer takes three trimming strategies according to the possible positions of the two windows with the highest score. In the following, we introduce these three trimming strategies in detail. Let l be the length of read. Let M1, M2 denote the positions of the two highest scoring windows in a read, respectively. Let |M1-M2| represents the distance between M1 and M2, and thd represents the threshold of the effective distance(l/3 by default). Let Q1, Q2, and Q3 indicate three candidate regions, respectively. Case1: If the two windows with the highest scores are located at both ends of the read (M1 < l/2 < M2 or M2 < l/2 < M1), and the distance between them is larger than ",Trimming," efficient trim algorithm base  multifeature fusion score model  ngs data
 pipeline  petrimmer   divide  three major step  first one   remove  technical sequence   read library  technical sequence include adapters simple repeat ployat tail  nonatgc character  second one   determine  range  read  need   trim  last one   determine  two sit   lowest quality   read  need   prune  use  composite score model  select  appropriate trim strategy  trim  read base   distribution   two sit  illustration   pipeline  petrimmer  show  fig   detail step  illustrate   follow subsections  removal  technical sequence   many type  technical sequence  ngs data   base   nonatgc character polya  polyt tail simple repeat adapters      low quality sequence   remove correctly   lead  either miss alignments  wrong genotyping  snp call   downstream analyse  follow strategies  propose  remove  low quality sequence   study  removal  base   nonatgc character assume     quality function  represent  average quality score  string  let    length  read let ler   threshold   length  read  trimminglerl  default   read consist    character   read  discard directly   read end   series  character   nonat  character   length  read  trim  larger   equal   threshold ler  strange tail  cut  conversely   length   read  trim  less   threshold ler  read  discard   whole   character   nonat  character  scatter   read petrimmer cut  read  several normal segment accord   position   abnormal character    select  segment whose length  larger   equal   threshold ler finally  calculate  quality score   segment  function   select  segment   highest quality score   retention segment  principle  remove  character   nonat  character   read removal  polyat tail  polyat tail must  remove since  iprovide false sequence  mislead assemble alignment  downstream sequence process  analysis   read end   series  character    polya  polyt   strange tail  cut    read consist   polyat sequence   discard directly  principle  remove  polyat tail   read removal  simple repeat simple repeat  part   real sequence  may also mislead  analysis   read end   series  simple repeat  strange tail  cut    read consist   simple repeat   discard directly  principle  remove  simple repeat   read removal  adapter next generation sequence require dna   insert  bacterial vectors  biological amplification adapter  linker oligonucleotides may also  attach  target dna fragment  facilitate insertion   vector  vector  adapter sequence  sequence concomitantly   target  insert sequence  represent contamination  must  remove   dataset prior  analysis give  know adapter pattern   read sequence adapter trim  usually model   semiglobal sequence alignment also call endspace free alignment   space   end   begin   alignment   incur penalties  accuracy  adapter trim depend   adapter sequence scan   error prone  end   end  overcome  impact  sequence errors   adapter sequence scan  introduce  fuzzy match algorithm  string base   edit distance   study   similarity   first  base   last  base   read   know adapter sequence  larger   threshold  default  specific head  tail  cut   principle  remove  adapter sequence   read determine  range  trim quality score  usually give  phredscaled probability   ith base  wrong  average value   quality score  base   read   use  measure  reliability   read  quality score statistics histogram  read   library  show  fig let  represent  length  select region  represent  length  reserve region pmin represent  minimum quality score pmax represent  maximum quality score  pth represent   coordinate  trim threshold   quality score   read  less  pmin   ∗ pmaxpmin  mean   read need   trim   contrary  read   need   trim     influence factor  trim petrimmer control  trim degree accord   value    larger  value indicate   read need   trim  default value      value also   determine   user accord   actual situation  sequence trim  improve  accuracy    algorithm  design  lightweight  easytoexplain score model  evaluate candidates   pattern  trim step finally petrimmer select  appropriate trim strategy  process  low quality read base   location determine   score model  slide window  read quality detection  slide window use  standard approach  work  scan   end   end   read   process  window slide petrimmer use  score model  calculate  score   window afterwards petrimmer summarize  score   window  form  bar chart finally petrimmer determine  trim site   low quality read accord   bar chart fig   quality score statistics histogram  read  dataset  rhodobacter sphaeroidesthe dataset  rsphaeroides  obtain  gageb website  xaxis refer   quality score  read  yaxis refer   frequency  quality score ′′ represent  select region ′′ represent  reserve region ′ ′ represent  trim region ′pmin ′ represent  minimum quality score ′pmax ′ represent  maximum quality score ′pth ′ represent   coordinate  trim threshold  score model let    represent  read   length respectively let qxr represent  quality score   rth base  read     window appear   ith position  read    represent  length  wiwi     size   window  fix  default  correct probability     express  pwi  therefore  error probability     express  pwi  petrimmer use  error probability     first score point   string contain   window  treat   kmer  read    represent   list     kmers   experiment  find    kmer size   keep small  prevent  overuse  computer memory  still large enough    kmers  unique  genome    determine  maximum number  kmers  fix      practice  often require  space size  kmer    least  time larger   genome size     ∗    less   upper limit  system memory usage  addition   value   increase  time require   kmer frequency statistics also increase therefore  select  size     necessary  comprehensively consider  memory requirement  time requirement   accuracy requirement   study  set  value        fully consider   factor  deal  large genomes  value     increase appropriately  example   number  read exceed  million  value     set   odd number       well know   low frequency kmers  often cause  sequence errors  bias especially  highdepth readsreads   high sequence depth regions  low frequency kmer   likely  result  sequence errors petrimmer use  frequency  kmer   second score point   show  equation  lower  frequency     higher   scorewi   scorewi    requencywi   sequence errors occur  frequently  regions   extremely high    content   constant heterochromatin regions  include centromeres telomere  highly repetitive sequence   petrimmer consider  string contain   window   kmer use   content  kmer   third score point   show  equation  ngc represent  number   character   kmer  higher   content     higher   scorewi   scorewi  ngc    finally petrimmer combine three score formulas  form  comprehensive score model   show  equation scorewi  ngc    qwi   qxij     requencywi    petrimmer score  window   read  need   trim  select  two windows   highest score   next judgment  process  example  apply  score model  show  figure   figure   xaxis refer   position   read  yaxis refer   score  four different situations  subgraph  show  kmer frequency score   window   read  subgraph  show  quality score   window   read  subgraph  show   score   window   read   subgraph  show  comprehensive score   window   read  score  windows   read petrimmer screen   two highest score windows  record   appear   read next petrimmer select  appropriate strategy base   two record locations  trim  low quality read  select trim strategy base  score information petrimmer take three trim strategies accord   possible position   two windows   highest score   follow  introduce  three trim strategies  detail let    length  read let   denote  position   two highest score windows   read respectively let m1m2 represent  distance      thd represent  threshold   effective distancel  default let     indicate three candidate regions respectively case1   two windows   highest score  locate   end   read              distance    larger  ",7
168,cutPrimers,"cutPrimers: A New Tool for Accurate Cutting of Primers from Reads of Targeted Next Generation Sequencing
Reads Primer sequences for amplification of BRCA1/2 genes were kindly provided by authors of hiplex-primer tool (Nguyen-Dumont et al., 2013). Primers were synthesized in the Institute of Chemical Biology and Fundamental Medicine (Novosibirsk, Russia). All primers were up to 33 nucleotides in length (mean is 23.7, minimum is 20) and were constructed in such a way that almost all amplicons without primer sequences had length of 100 nucleotides. All reads were obtained by sequencing of amplicons with MiSeq (Illumina) using MiSeq reagent kit v3 (300 cycles). Throughout we compared our tool with already existing tools using three pairs of FASTQ-files with different number of reads: 12,757, 24,379, and 89,919. Also we made a comparison with one whole run of 42 DNA samples (16,429,892 read pairs). As an input, the program cutPrimers uses four FASTA-files with primer sequences located on the 50 -end and 30 -end of R1 and R2 reads, respectively. Alternatively, it may trim primers only on the 50 -end (e.g., when amplicon length is 150 bp and read length is 2 · 75 bp). For input and output of FASTQ- and FASTAfiles, it uses biopython package SeqIO (Cock et al., 2009). For searching primer sequences in the reads, cutPrimers applies two Python modules: regex, which searches primer sequences with regular expressions and multiprocessing. The last one is necessary for multithreading. During processing of the reads, the program can save error statistics in the primers. If an error occurs in one of the reads of the read pair, it considers it as a sequencing error, if an error occurs in both reads, it considers it as a primer synthesis error. Such additional information is useful for evaluation of primer synthesis quality as well as for estimation of the amplicon coverage. Moreover, cutPrimers can detect primer dimers that may form during amplification. Our algorithm allows achieving high accuracy of identifying of the primer sequences in NGS reads by simultaneous search of the primer sequences in the pairs of the reads. We compared our tool with other available tools: cutadapt, BBDuk, and AlienTrimmer. Use of Trimmomatic has not yielded acceptable number of reads. Examples of commands used for execution are available in Supplementary Material.",Trimming,"cutprimers  new tool  accurate cut  primers  read  target next generation sequencing
reads primer sequence  amplification  brca1 genes  kindly provide  author  hiplexprimer tool nguyendumont    primers  synthesize   institute  chemical biology  fundamental medicine novosibirsk russia  primers     nucleotides  length mean   minimum     construct    way  almost  amplicons without primer sequence  length   nucleotides  read  obtain  sequence  amplicons  miseq illumina use miseq reagent kit   cycle throughout  compare  tool  already exist tool use three pair  fastqfiles  different number  read     also  make  comparison  one whole run   dna sample  read pair   input  program cutprimers use four fastafiles  primer sequence locate    end   end     read respectively alternatively  may trim primers     end   amplicon length     read length       input  output  fastq  fastafiles  use biopython package seqio cock     search primer sequence   read cutprimers apply two python modules regex  search primer sequence  regular expressions  multiprocessing  last one  necessary  multithreading  process   read  program  save error statistics   primers   error occur  one   read   read pair  consider    sequence error   error occur   read  consider    primer synthesis error  additional information  useful  evaluation  primer synthesis quality  well   estimation   amplicon coverage moreover cutprimers  detect primer dimers  may form  amplification  algorithm allow achieve high accuracy  identify   primer sequence  ngs read  simultaneous search   primer sequence   pair   read  compare  tool   available tool cutadapt bbduk  alientrimmer use  trimmomatic   yield acceptable number  read examples  command use  execution  available  supplementary material",7
169,AfterQC,"AfterQC: automatic filtering, trimming, error removing and quality control for fastq data
AfterQC is designed to process FastQ files in batches. It goes through a folder with all FastQ files (can be single-end or pair-end output), which are typically data of a sequencing run for different samples, and passes each FastQ file or pair into the QC and filtering pipeline. As described in Fig. 1, firstly, AfterQC will run a bubble detection to find the bubbles raised during the sequencing process. Secondly, a pre-filtering QC will be conducted to profile the data with per-cycle base content and quality curves. Thirdly, AfterQC will do automatic read trimming based on data quality profiling. Fourthly, each read will be filtered by bubble filter, polyX filter, quality filter and overlapping analysis filters, the ones failed to pass these filters will be discarded as bad reads. Fifthly, an error correction based on overlapping analysis will be applied for pair-end sequencing data. Finally, AfterQC will store the good reads, perform post-filtering QC profiling and generate HTML reports. Fig. 1 figure1 Pipeline diagram of AfterQC. For each single or pair of FastQ file(s), AfterQC will perform pre-filtering QC, automatic trimming, data filtering, error correction and post-filtering QC. Reads will be categorized as good or bad reads and stored separately, figures will be included in the final HTML report Full size image Bubble detection and visualisation For Illumina sequencers, especially for those using two-channel SBS sequencing technology [9], we observed a phenomenon that more polyX reads could be found in the bubble areas than the background. Based on this phenomenon, we developed a method deBubble to visualise and detect bubbles. Firstly, we detect all polyX reads, separate them by tiles, and filter them by their local density since bubble areas tend to have higher polyX density. Secondly, we cluster the polyX reads into small sets, filter the clusters by features like size, shape and number of polyX reads. Thirdly, for each polyX cluster, we fit a circle to include all its polyX reads, and we also perform circle filtering to remove false positive bubbles. Finally, we plot the polyX and circle figures, and use these circles to filter out all the reads located in them. Figure 2 shows how we implement deBubble algorithm. Fig. 2 figure2 Algorithm diagram of deBubble. The major steps of this algorithm are polyX detection, polyX clustering and filtering, circle fitting and filtering Full size image Bubble detection is optional in AfterQC and is not enabled by default. According to our study, Illumina NextSeq sequencers are more likely to raise bubbles, so we suggest enabling this option for NextSeq sequencer outputs and disabling it for HiSeq sequencer outputs. Figure 3 shows a part of debubble’s output, from which we can also find that NextSeq sequencers produce much more polyX reads. Fig. 3 figure3 The output images of AfterQC deBubble. a is a sub-image of a lane of NextSeq 500 sequencer, from which we can find 1 bubble detected. b shows enlarged details of the bubble. c shows a sub-image of a tile of HiSeq 3000 with similar resolution of (b), which has much fewer polyX reads Full size image Automatic trimming In the whole sequencing process, the first several cycles can have more biases or errors since the signal coordination hasn’t been established yet, and the last several cycles can also have errors due to error accumulation and lack of following correction. In some cases, the beginning or ending of the reads may have significant statistical biases. For example, library preparation bias or sequencing bias can cause GC percentage higher than 70% at some beginning or ending cycles, and these cycles should be considered as abnormal cycles, and surely should be removed by some methods. There are two strategies for trimming, namely local strategy and global strategy. Some tools, like Trimmomatic [6], apply local strategy, which perform trimming read by read. However local trimming has two drawbacks. The first drawback is that local trimming only uses the quality information for trimming, but cannot utilise the global statistical information to discover the abnormal cycles. The second drawback is local trimming results in unaligned trimming, which means duplicated reads may be trimmed differently, and consequently lead to failure of de-duplication tools like Picard [10]. Most of these de-duplication tools detect duplications only by clustering reads with same mapping positions. In contrast, AfterQC implements global trimming strategy, which means trimming all the reads identically. An algorithm is used to determine how many cycles to trim in the front and tail. The algorithm is based on such finding: the mean per-cycle base ratio curve is usually flat in the intermediate cycles, but may be fluctuant in the first and last several cycles. Also the intermediate cycles usually have higher mean quality score than the first and last cycles. Before trimming happens, AfterQC will do pre-filtering quality control to calculate the base content and quality curves. Our algorithm initialises the central cycle as a good cycle, and then expands the good region by scanning the base content and quality curves cycle by cycle, until it meets the front or end, or meet a cycle considered as abnormal. Then the cycles in the good region will be kept, and the rest cycles in the front and tail will be trimmed. Currently a cycle will be marked as abnormal if it meets at least one of following criteria: 1), too high or too low of mean base content percentages (i.e higher than 40%, or lower than 15%); 2), too significant change of mean base content percentages (i.e, ±10% change comparing to neighbour cycle); 3), too high or too low of mean GC percentages (i.e higher than 70%, or lower than 30%); 4), too low of mean quality (i.e. less then Q20). Figure 4 gives an example how automatic trimming works. Fig. 4 figure4 An example of how automatic trimming works. Data is obtained from a cell-free DNA quality control sample, and sequenced by Illumina NextSeq 500 sequencer. a is the base content percentage curve before trimming and filtering, from which we can find base contents change dramatically in front and tail; b is the curve after trimming and filtering, from which we can find that the bad cycles in the tail are all trimmed, while only part of the front is trimmed. This results from the fact that we use different thresholds for the front and tail, since unflatness in front is more probably caused by different fragmentation methods, while unflatness in tail is usually caused by lab preparation or sequencing artefacts Full size image According to our experiments, AfterQC only trims very few cycles for data with good sequencing quality (i.e. 1 base in front, and 1 base in tail), so normally it will not significantly affect the data utilisation rate. However, for some extreme cases, the sequencing quality is quite low, and the mean base content percentage or quality curves can be totally chaotic. To not trim too many data for such cases, AfterQC limits the trimming cycles both in front and tail. The default setting is no more than 10% in front and no more than 5% in tail. Filtering After trimming is done, AfterQC will apply a series of filters on the reads. AfterQC implements quality filters and polyX filters. Quality filters are trivial, which just count the number of low quality bases or N, calculate the mean quality of each read, and then determine whether to keep or discard this read. AfterQC implements an error-tolerantly method to detect polyX (X is one of A/T/C/G). Two arguments (P and L) are used to configure the polyX detection algorithm, P (default is 35) means how long the polyX sequence should have, while L (default is 2) refers to how many non-X bases can be tolerated in each polyX sub-sequence. According to our experiments, NextSeq sequencers are more likely to produce polyX reads, and most of them are polyG. The order of applying different filters is not important. If one read is filtered out, a new sequence name containing the filter name will be assigned, and then this read will be streamed into the bad output. Overlapping analysis and error correction Let T denote the length of a sequenced DNA template, and S denote the length of pair-end sequencing length, then the pair of reads will totally overlap if T≤S, will overlap with a length of 2S−T, if S<T<2S, and will not overlap if 2S≤T. Based on edit distance [11] optimisation, we developed a method to check how each pair of reads overlap, for data from pair-end sequencing. For a pair of reads R1 and R2, let O be the offset we place R2 under R1, then we’ll have vertically aligned subsequences R1 o and R2 o , and we can calculate their edit distance ed(R1 o ,R2 o ). Our method optimises offset O to obtain the minimal edit distance, ed(R1 o−1,R2 o−1)<ed(R1 o ,R2 o )<ed(R1 o+1,R2 o+1). We consider R1 and R2 overlapped at this offset O if this edit distance ed(R1 o ,R2 o ) and overlapped length L o meet the thresholds. If a pair is overlapped, AfterQC will do overlapping analysis and error correction for it. If ed(R1 o ,R2 o ) is 0, it indicates no mismatch and no obvious sequencing error in the overlapped bases. Otherwise we should correct the overlapped mismatch or discard the reads if they cannot be corrected. For each pair with mismatch bases in overlapping region, we calculate the hamming distance hd(R1 o ,R2 o ) and check if it is identical to ed(R1 o ,R2 o ). If yes, it means there is only substitution difference between R1 o and R2 o . For this case, we check the mismatch pairs to see if one base is of very high quality and the other is of very low quality. If it’s true, AfterQC will correct the low quality base according to its high quality mate. According to our results, most mismatch pairs have unbalanced quality scores. Figure 5 shows an example of overlapping analysis. Fig. 5 figure5 An example of overlapping analysis: the original DNA template is 60 bp long and sequencing length is 2×50, R1 and R2 have 40 bp overlap at offset 10, and the edit distance of the overlapped sub-sequences is 1. Brighter colour represents higher quality. A mismatch pair is found with high quality A and very low quality T, then T can be corrected Full size image Sequencing error profiling As described above, AfterQC can detect the mismatches in the overlapping regions. For those reads with very long overlap (i.e. overlap_len>50), the edit distance of overlapped subsequences is mainly caused by sequencing errors, because an error-free overlapping is usually completely identical (edit distance should be 0). Based on this assumption, we can count the total bases and the mismatched bases in all overlapping regions. And we can consider the ratio of (mismatch/total) reflecting the sequencing error rate, which can be called estimated sequencing error rate. Furthermore, a mismatched pair usually consists of one high quality base (i.e. Q30+) and one low quality base (i.e. < Q15). In this case, we can consider that the low quality base in this pair is a sequencing error, and furtherly profile the sequencing error transform distribution (i.e. how many T bases are sequenced as C). For each pair of pair-end sequenced FastQ files, AfterQC estimates such sequencing error rate and profiles the sequencing error transform distribution. By looking into the error distribution results from lots of sequencing data, we found an interesting phenomenon: error distribution is clearly sequencing platform dependent, different sequencing platforms have different error patterns, while the same sequencing platform’s different sequencing runs share similar patterns. Figure 6 shows an example of Illumina NextSeq sequencer patterns comparing with Illumina HiSeq sequencer patterns. An interesting phenomenon is that NextSeq sequencers produce very few A/G and C/T errors (the orange bars). We guess it is due to the two-colour system [9] adopted by NextSeq systems. In a Illumina two-colour system, base A, which requires both red and green light signals, is not easy to be mis-recognised as base G, which requires no light signals. Also base C, which requires only red light signal, can be clearly distinguished from base T, which requires green light signals. Fig. 6 figure6 a Illumina NextSeq 500 output run #1. b Illumina NextSeq 500 output run #2. c Illumina HiSeq X-tenoutput run #1. d Illumina HiSeq X-tenoutput run #2. Sequencing error transform distribution is platform associated. Data are obtained from internal quality control DNA samples, and sequenced by Illumina HiSeq X10 sequencer and Illumina NextSeq 500 sequencer. Values in X-axis represent the sequencing error, and the values in Y-axis represent the counts calculated from a pair of FastQ files. Fig. (a) and (b) are profiled from two different sequencing runs from same NextSeq sequencers, while Fig. (c) and (d) are profiled from two different runs from different HiSeq sequencers. We can find that the patterns of a) and b) are nearly identical, while patterns of (c) and (d) are similar, but with noticeable difference Full size image Automatic adapter cutting When the DNA template length is less than the sequencing cycles, a part of 3’ adapter will be sequenced in the tail. From Fig. 7, we can see that when the inserted DNA template length T is less than sequencing length S, the offset O for the best overlapping will be negative. On the other hand, if we find that the optimal offset O for aligning the pair of reads is negative, we consider that the length of inserted DNA is smaller than sequencing length. Based on this rule, AfterQC implements automatic adapter cutting for pair-end sequencing data. Fig. 7 figure7 An example of automatic adapter detection and cutting. The offset makes the best alignment for this pair of reads is negative, which indicates that the length of inserted DNA is less than the sequencing length. When the offset is detected, it is trivial to calculate the overlapping region, and cut the adapter bases (outside overlapping region) from 3’ of both read1 and read2 Full size image In the overlapping analysis process, we get the optimal offset O for the best local alignment of each pair. The overlap length of this pair can be directly calculated using the offset O. If O is found negative, the bases outside overlapping region will be considered as part of adapter sequences, and then be trimmed automatically. Quality profiling Besides normal per-cycle base content and quality profiling, AfterQC implements two novel methods to give more information about sequencing quality: strand bias profiling to reflect amplification bias, and per-cycle discontinuity profiling to reflect sequencing quality instability. The first one is based on a hypothesis: if the DNA amplification process and sequencing process have only little non-uniformity, the repeat count of a short K-MER should be close to the repeat count of its reverse complement. So we plot each K-MER and its reverse complement’s counts, and check whether most points are near the line y=x. Figure 8 gives an example of K-MER based strand bias evaluation. The second method is based on another hypothesis: the mean discontinuity should be more or less stable for all sequencing cycles. For a short window of sequencing cycles, we use the average discontinued base number in this window to calculate the discontinuity. For example, ATCGA has a discontinued base number of 4 because all of the neighbour bases are different, while AAAAA has a discontinued base number of 0. If discontinuity drops down significantly cycle by cycle, it usually reflects a sequencing issue, which may be caused by the per-cycle washing process not working well. Fig. 8 figure8 Two examples of strand bias profiling. X-axis is about the counts of relative forward strand K-MERs, while the Y-axis is about relative reverse ones. a shows a case of very little strand bias because most points are close to the line y=x, and (b) shows a case of serious strand bias because lots of points are close to X-axis and Y-axis, and repeat counts of some K-MERs are very high so the figure seems very sparse. Both files are downloaded from NCBI Sequence Read Archive (SRA), with accession numbers SRR1654347 and SRR2496735 [18] Full size image Software implementation AfterQC can be viewed as a mix of quality control tools (i.e. FastQC) and data filtering/trimming tools (i.e. Trimmomatic, cutadapt). Table 1 gives a simple feature comparison of AfterQC with some existing tools. AfterQC differs from other tools by those features like overlapping analysis, bubble detection and automatic trimming. And for figure plotting, AfterQC switched from using matplotlib [12] to plotly.js [13] for creating interactive figures. Table 1 A feature comparison of AfterQC with existing tools. From the table we can find that AfterQC is versatile on common quality control and data filtering tasks, and offers novel features not implemented by other tools before Full size table Since AfterQC provides some functions that other high throughput sequencing QC or filtering tools do not possess, it usually runs slower than those other tools. In our evaluation, for pair-end sequencing data, AfterQC can process 2*240K pair-end reads per minute, while FastQC can process 2*1.5M reads per minute, which is 6X faster as AfterQC. However, the most time consuming parts of AfterQC are overlapping analysis and error correction processes, which are very useful for pair-end data. Actually, for single-end data, AfterQC can run as fast as FastQC, since no overlapping analysis is involved. This tool is written in Python, with an edit distance module written in C. PyPy is supported for performance consideration. Currently, the fastest way to run AfterQC is using PyPy, but we are also re-implementing AfterQC using C/C++ only. The performance will be improved after the slow python code is replaced.",QualityControl,"afterqc automatic filter trim error remove  quality control  fastq data
afterqc  design  process fastq file  batch  go   folder   fastq file   singleend  pairend output   typically data   sequence run  different sample  pass  fastq file  pair     filter pipeline  describe  fig  firstly afterqc  run  bubble detection  find  bubble raise   sequence process secondly  prefiltering    conduct  profile  data  percycle base content  quality curve thirdly afterqc   automatic read trim base  data quality profile fourthly  read   filter  bubble filter polyx filter quality filter  overlap analysis filter  ones fail  pass  filter   discard  bad read fifthly  error correction base  overlap analysis   apply  pairend sequence data finally afterqc  store  good read perform postfiltering  profile  generate html report fig  figure1 pipeline diagram  afterqc   single  pair  fastq file afterqc  perform prefiltering  automatic trim data filter error correction  postfiltering  read   categorize  good  bad read  store separately figure   include   final html report full size image bubble detection  visualisation  illumina sequencers especially   use twochannel sbs sequence technology   observe  phenomenon   polyx read could  find   bubble areas   background base   phenomenon  develop  method debubble  visualise  detect bubble firstly  detect  polyx read separate   tile  filter    local density since bubble areas tend   higher polyx density secondly  cluster  polyx read  small set filter  cluster  feature like size shape  number  polyx read thirdly   polyx cluster  fit  circle  include   polyx read   also perform circle filter  remove false positive bubble finally  plot  polyx  circle figure  use  circle  filter    read locate   figure  show   implement debubble algorithm fig  figure2 algorithm diagram  debubble  major step   algorithm  polyx detection polyx cluster  filter circle fit  filter full size image bubble detection  optional  afterqc    enable  default accord   study illumina nextseq sequencers   likely  raise bubble   suggest enable  option  nextseq sequencer output  disable   hiseq sequencer output figure  show  part  debubbles output     also find  nextseq sequencers produce much  polyx read fig  figure3  output image  afterqc debubble    subimage   lane  nextseq  sequencer     find  bubble detect  show enlarge detail   bubble  show  subimage   tile  hiseq   similar resolution     much fewer polyx read full size image automatic trim   whole sequence process  first several cycle    bias  errors since  signal coordination hasnt  establish yet   last several cycle  also  errors due  error accumulation  lack  follow correction   case  begin  end   read may  significant statistical bias  example library preparation bias  sequence bias  cause  percentage higher     begin  end cycle   cycle   consider  abnormal cycle  surely   remove   methods   two strategies  trim namely local strategy  global strategy  tool like trimmomatic  apply local strategy  perform trim read  read however local trim  two drawbacks  first drawback   local trim  use  quality information  trim  cannot utilise  global statistical information  discover  abnormal cycle  second drawback  local trim result  unaligned trim  mean duplicate read may  trim differently  consequently lead  failure  deduplication tool like picard     deduplication tool detect duplications   cluster read   map position  contrast afterqc implement global trim strategy  mean trim   read identically  algorithm  use  determine  many cycle  trim   front  tail  algorithm  base   find  mean percycle base ratio curve  usually flat   intermediate cycle  may  fluctuant   first  last several cycle also  intermediate cycle usually  higher mean quality score   first  last cycle  trim happen afterqc   prefiltering quality control  calculate  base content  quality curve  algorithm initialise  central cycle   good cycle   expand  good region  scan  base content  quality curve cycle  cycle   meet  front  end  meet  cycle consider  abnormal   cycle   good region   keep   rest cycle   front  tail   trim currently  cycle   mark  abnormal   meet  least one  follow criteria   high   low  mean base content percentages  higher    lower     significant change  mean base content percentages  ± change compare  neighbour cycle   high   low  mean  percentages  higher    lower     low  mean quality  less  q20 figure  give  example  automatic trim work fig  figure4  example   automatic trim work data  obtain   cellfree dna quality control sample  sequence  illumina nextseq  sequencer    base content percentage curve  trim  filter     find base content change dramatically  front  tail    curve  trim  filter     find   bad cycle   tail   trim   part   front  trim  result   fact   use different thresholds   front  tail since unflatness  front   probably cause  different fragmentation methods  unflatness  tail  usually cause  lab preparation  sequence artefacts full size image accord   experiment afterqc  trim   cycle  data  good sequence quality   base  front   base  tail  normally    significantly affect  data utilisation rate however   extreme case  sequence quality  quite low   mean base content percentage  quality curve   totally chaotic   trim  many data   case afterqc limit  trim cycle   front  tail  default set       front       tail filter  trim   afterqc  apply  series  filter   read afterqc implement quality filter  polyx filter quality filter  trivial   count  number  low quality base   calculate  mean quality   read   determine whether  keep  discard  read afterqc implement  errortolerantly method  detect polyx   one  atcg two arguments     use  configure  polyx detection algorithm  default   mean  long  polyx sequence     default   refer   many nonx base   tolerate   polyx subsequence accord   experiment nextseq sequencers   likely  produce polyx read      polyg  order  apply different filter   important  one read  filter   new sequence name contain  filter name   assign    read   stream   bad output overlap analysis  error correction let  denote  length   sequence dna template   denote  length  pairend sequence length   pair  read  totally overlap  ≤  overlap   length  2st  st2s    overlap  ≤ base  edit distance  optimisation  develop  method  check   pair  read overlap  data  pairend sequence   pair  read    let    offset  place     well  vertically align subsequences          calculate  edit distance edr1      method optimise offset   obtain  minimal edit distance edr1 or2 oedr1    edr1 or2   consider    overlap   offset    edit distance edr1      overlap length   meet  thresholds   pair  overlap afterqc   overlap analysis  error correction    edr1        indicate  mismatch   obvious sequence error   overlap base otherwise   correct  overlap mismatch  discard  read   cannot  correct   pair  mismatch base  overlap region  calculate  ham distance hdr1      check    identical  edr1      yes  mean    substitution difference          case  check  mismatch pair  see  one base    high quality       low quality   true afterqc  correct  low quality base accord   high quality mate accord   result  mismatch pair  unbalance quality score figure  show  example  overlap analysis fig  figure5  example  overlap analysis  original dna template    long  sequence length         overlap  offset    edit distance   overlap subsequences   brighter colour represent higher quality  mismatch pair  find  high quality    low quality      correct full size image sequence error profile  describe  afterqc  detect  mismatch   overlap regions   read   long overlap  overlap_len  edit distance  overlap subsequences  mainly cause  sequence errors   errorfree overlap  usually completely identical edit distance    base   assumption   count  total base   mismatch base   overlap regions    consider  ratio  mismatchtotal reflect  sequence error rate    call estimate sequence error rate furthermore  mismatch pair usually consist  one high quality base  q30  one low quality base   q15   case   consider   low quality base   pair   sequence error  furtherly profile  sequence error transform distribution   many  base  sequence     pair  pairend sequence fastq file afterqc estimate  sequence error rate  profile  sequence error transform distribution  look   error distribution result  lot  sequence data  find  interest phenomenon error distribution  clearly sequence platform dependent different sequence platforms  different error pattern    sequence platforms different sequence run share similar pattern figure  show  example  illumina nextseq sequencer pattern compare  illumina hiseq sequencer pattern  interest phenomenon   nextseq sequencers produce      errors  orange bar  guess   due   twocolour system  adopt  nextseq systems   illumina twocolour system base   require  red  green light signal   easy   misrecognised  base   require  light signal also base   require  red light signal   clearly distinguish  base   require green light signal fig  figure6  illumina nextseq  output run #  illumina nextseq  output run #  illumina hiseq xtenoutput run #  illumina hiseq xtenoutput run # sequence error transform distribution  platform associate data  obtain  internal quality control dna sample  sequence  illumina hiseq x10 sequencer  illumina nextseq  sequencer value  xaxis represent  sequence error   value  yaxis represent  count calculate   pair  fastq file fig     profile  two different sequence run   nextseq sequencers  fig     profile  two different run  different hiseq sequencers   find   pattern      nearly identical  pattern      similar   noticeable difference full size image automatic adapter cut   dna template length  less   sequence cycle  part   adapter   sequence   tail  fig    see    insert dna template length   less  sequence length   offset    best overlap   negative    hand   find   optimal offset   align  pair  read  negative  consider   length  insert dna  smaller  sequence length base   rule afterqc implement automatic adapter cut  pairend sequence data fig  figure7  example  automatic adapter detection  cut  offset make  best alignment   pair  read  negative  indicate   length  insert dna  less   sequence length   offset  detect   trivial  calculate  overlap region  cut  adapter base outside overlap region     read1  read2 full size image   overlap analysis process  get  optimal offset    best local alignment   pair  overlap length   pair   directly calculate use  offset     find negative  base outside overlap region   consider  part  adapter sequence    trim automatically quality profile besides normal percycle base content  quality profile afterqc implement two novel methods  give  information  sequence quality strand bias profile  reflect amplification bias  percycle discontinuity profile  reflect sequence quality instability  first one  base   hypothesis   dna amplification process  sequence process   little nonuniformity  repeat count   short kmer   close   repeat count   reverse complement   plot  kmer   reverse complement count  check whether  point  near  line  figure  give  example  kmer base strand bias evaluation  second method  base  another hypothesis  mean discontinuity     less stable   sequence cycle   short window  sequence cycle  use  average discontinue base number   window  calculate  discontinuity  example atcga   discontinue base number       neighbour base  different  aaaaa   discontinue base number    discontinuity drop  significantly cycle  cycle  usually reflect  sequence issue  may  cause   percycle wash process  work well fig  figure8 two examples  strand bias profile xaxis    count  relative forward strand kmers   yaxis   relative reverse ones  show  case   little strand bias   point  close   line    show  case  serious strand bias  lot  point  close  xaxis  yaxis  repeat count   kmers   high   figure seem  sparse  file  download  ncbi sequence read archive sra  accession number srr1654347  srr2496735  full size image software implementation afterqc   view   mix  quality control tool  fastqc  data filteringtrimming tool  trimmomatic cutadapt table  give  simple feature comparison  afterqc   exist tool afterqc differ   tool   feature like overlap analysis bubble detection  automatic trim   figure plot afterqc switch  use matplotlib   plotlyjs   create interactive figure table   feature comparison  afterqc  exist tool   table   find  afterqc  versatile  common quality control  data filter task  offer novel feature  implement   tool  full size table since afterqc provide  function   high throughput sequence   filter tool   possess  usually run slower    tool   evaluation  pairend sequence data afterqc  process * pairend read per minute  fastqc  process * read per minute    faster  afterqc however   time consume part  afterqc  overlap analysis  error correction process    useful  pairend data actually  singleend data afterqc  run  fast  fastqc since  overlap analysis  involve  tool  write  python   edit distance module write   pypy  support  performance consideration currently  fastest way  run afterqc  use pypy    also reimplementing afterqc use    performance   improve   slow python code  replace",8
170,NGSQC,"NGS QC Toolkit: a toolkit for quality control of next generation sequencing data.
Implementation All tools in the toolkit have been developed using Perl programming language by implementing modularized structure using several sub-routines for various tasks, which allows better maintainability. GD module has been used to generate various graphs for statistics and String::Approx module for searching primer/adaptor sequence in input reads. Parallel::ForkManager and Threads modules have been utilized for parallelizing the QC tools. IO::Zlib module has been used to facilitate reading/writing compressed (gzip) files. QC reports are generated using Hypertext Markup Language (HTML) and Cascading Style Sheets (CSS). All the tools have been tested on Windows and Linux (CentOS) operating systems for full functionality. Availability, installation and usage NGS QC Toolkit is a standalone and open source application freely available at http://www.nipgr.res.in/ngsqctoolkit.html. Detailed description about installation and usage of the toolkit is available in user manual on the web site. In brief, to install the toolkit, user needs to download NGSQCToolkit_v2.2.zip (current version) file from the website and unzip it. Dependencies for using toolkit include Perl interpreter (usually supplied with OS for Linux and ActivePerl for Windows) and additional Perl modules, GD (optional; required to generate QC graphs) and String::Approx. Their installation instructions can be found on their respective websites. Different tools available in the toolkit, categorized on the basis of task they are meant to perform (Fig. 1), are available in different folders. These tools could be run using “perl <tool name> <options>” command on the command-line prompt. The parameter options for the input sequence data, QC analysis, processing and output, and their default settings can be viewed by using “-h” option in the above mentioned command. To check whether dependencies are resolved, QC tools can be run without any parameter, which will result into appropriate error/warning messages for the missing modules, if any. To input PE data into IlluQC, user needs to use “-pe” (“-se” for single-end data) option, followed by forward and reverse end read files, sequencing assay and FASTQ variant. In 454QC, “-i” option is used followed by read and quality files in FASTA format and sequencing assay. A switch “-p” is provided in IlluQC.pl and 454QC.pl to specify number of files to be processed simultaneously in parallel. Number of CPUs can be specified in multithreaded tools using switch “-c”. By default, tools generate output where the input files are located. Test data has been provided for download on the website, which include input data for Illumina (PE and SE) and Roche 454 platforms and output data from QC, trimming and statistics tools. Major enhancements First version (v1.0) of the NGS QC toolkit included QC tools (IlluQC.pl and 454QC.pl) with basic functionality of quality check and primer/adaptor contamination removal for Illumina and Roche 454 data generating textual QC statistics, and sequence statistics analysis tools. In a major update of the toolkit (v2.0), parallelization was introduced in the QC tools to speed up the analysis. In addition, the feature of generating QC statistics in the form of graphs was implemented. We have also added the feature of reading/writing of compressed files (gzip) and generating consolidated QC report in HTML format in our earlier update (v2.1). Recently, IlluQC tools were updated to generate a graph depicting percentage of reads falling into different quality score ranges at each base position, TrimmingReads tool was modified to provide an additional option for trimming reads based on quality score and a new tool has been incorporated for the QC of Roche 454 paired-end data in the current version (v2.2).",QualityControl,"ngs  toolkit  toolkit  quality control  next generation sequence data
implementation  tool   toolkit   develop use perl program language  implement modularized structure use several subroutines  various task  allow better maintainability  module   use  generate various graph  statistics  stringapprox module  search primeradaptor sequence  input read parallelforkmanager  thread modules   utilize  parallelize   tool iozlib module   use  facilitate readingwriting compress gzip file  report  generate use hypertext markup language html  cascade style sheet css   tool   test  windows  linux centos operate systems  full functionality availability installation  usage ngs  toolkit   standalone  open source application freely available   detail description  installation  usage   toolkit  available  user manual   web site  brief  install  toolkit user need  download ngsqctoolkit_v2zip current version file   website  unzip  dependencies  use toolkit include perl interpreter usually supply    linux  activeperl  windows  additional perl modules  optional require  generate  graph  stringapprox  installation instructions   find   respective websites different tool available   toolkit categorize   basis  task   mean  perform fig   available  different folders  tool could  run use “perl tool name options” command   commandline prompt  parameter options   input sequence data  analysis process  output   default settings   view  use “” option    mention command  check whether dependencies  resolve  tool   run without  parameter   result  appropriate errorwarning message   miss modules    input  data  illuqc user need  use “” “”  singleend data option follow  forward  reverse end read file sequence assay  fastq variant  454qc “” option  use follow  read  quality file  fasta format  sequence assay  switch “”  provide  illuqcpl  454qcpl  specify number  file   process simultaneously  parallel number  cpus   specify  multithreaded tool use switch “”  default tool generate output   input file  locate test data   provide  download   website  include input data  illumina     roche  platforms  output data   trim  statistics tool major enhancements first version    ngs  toolkit include  tool illuqcpl  454qcpl  basic functionality  quality check  primeradaptor contamination removal  illumina  roche  data generate textual  statistics  sequence statistics analysis tool   major update   toolkit  parallelization  introduce    tool  speed   analysis  addition  feature  generate  statistics   form  graph  implement   also add  feature  readingwriting  compress file gzip  generate consolidate  report  html format   earlier update  recently illuqc tool  update  generate  graph depict percentage  read fall  different quality score range   base position trimmingreads tool  modify  provide  additional option  trim read base  quality score   new tool   incorporate     roche  pairedend data   current version ",8
171,QC-Chain,"QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data
The Overall Quality Control Strategy The objectives of QC-Chain include (1) retrieving reads with high quality; (2) identifying and quantifying the source of contaminations, and filtering contaminating reads; (3) accomplishing the QC process in a relatively short time. To achieve these objectives, the overall method of QC-Chain includes sequencing quality assessment and trimming, and contamination screening and removal. Additionally, evaluation and comparison of downstream analysis results using reads after QC were also included as an important component for this holistic approach. The strategy and workflow of the method is shown in Figure 1, with detailed procedures as described below. Read Quality Assessment and Trimming by Parallel-QC The sequencing quality assessment and trimming is the first step for NGS data quality control, which requires both accuracy and efficiency. To accomplish this step, we developed a parallel quality control software, Parallel-QC, which could be used to trim, filter and remove low sequencing-quality reads from NGS data. Parallel-QC is developed by Linux C++ and multi-thread technology based on multi-core X86 CPU platform, and is compatible for X86 and X86-64 Linux. Specifically, by Parallel-QC, sequences could be trimmed to a specific length; low-quality bases within reads could be trimmed from both 59 and 39 ends; low-quality reads could be filtered by quality value with user defined percentage; duplications could be identified and removed. For tag sequences filtration, multiple tag sequences could be aligned and shifted on both 5’ and 3’ ends of the reads with mismatches allowed, and the positive aligned reads could be removed. To significantly accelerate the speed of computation, ParallelQC parallelizes the sequencing quality evaluation and filtration steps by assigning balanced and weighted tasks to independent threads, which could be executed on different CPU cores simultaneously. In addition, all progresses could be completed with only one disk I/O operation, which highly improves the efficiency of analysis. On the other hand, the multiple steps can be accomplished by using a single command line with user-friendly options. Therefore, Parallel-QC significantly shortens the processing time compared to traditional single core CPU based method, and simplifies user’s operation compared to using multiple single function QC tools. Identification and Removal of Contaminating Reads The aim of contamination screening is to identify and quantify the (mostly unknown) source of contaminations, filter the contaminating reads, and obtain the processed reads as clean as possible. We adopted two complementary strategies, both of which could provide (known and unknown) species information of the dataset. In the ‘‘rDNA-reads based’’ method, ribosomal DNA reads were used to qualitatively detect the taxonomical structures of the dataset quickly. Ribosomal RNAs, such as 16S (for prokaryote) and 18S (for eukaryotes) sequences, are good indicators to characterize prokaryotic and eukaryotic species and are commonly used in phylogenetic analysis. They are also widely used in metagenomic analysis to detect the community structure. Here we applied Parallel-META [6], a high-performance 16S/18S rRNA analysis pipeline to report the taxonomic classification, construction and distribution of NGS reads. Parallel-META is a GPU and Multi-Core CPU based software, which firstly extracts the (user selected) 16S or 18S rRNA sequences from the input data and aligns the obtained rRNA reads to several optional databases, including RDP [7], GREENGENES [8] and SILVA [9]. The taxonomy information is produced and then shown in a dynamic graphic view with corresponding species’ proportion. Additionally, in QC-Chain, Parallel-META was updated to be able to accomplish eukaryotic species screening and identification, but in previous version it could only identify prokaryotic species. Through this approach, all the possible species sources of the raw reads, including both prokaryotic and eukaryotic information, could be detected de novo. The other method is ‘‘random-reads based’’, which could quantitatively provide the species information. Generally, detecting all possible contaminations requires aligning reads to a comprehensive database, which includes species records as many as possible. The most popular and widely-used alignment method is BLAST against NCBI (National Center for Biotechnology Information) database (http://www.ncbi.nlm.nih.gov/ ). However, it is known that BLAST is a time-consuming process and the speed is a bottleneck, especially when analyzing immense amount of reads. An alternative is to reduce the size of the query data and perform BLAST to get the species information quickly. With such a consideration, we developed an in-house script which could randomly extract reads from the raw reads with a userdefined proportion of all reads. The extracted reads were then aligned to NCBI-NT database using BLASTn, to extract species information in a relatively short time. The above two approaches are complementary and synergetic to each other: rDNA-reads based method could quickly screen and identify the possible contaminating species. The random-reads based method could provide quantitative evaluation of the contaminations, and also help to verify the result of rDNA-reads based method. After confirming the contaminating sources by combining the results of the above two methods, the contaminating reads are filtered by the alignment tool Bowtie 0.12.8 [5] with default parameters: reads aligned to contaminating species’ genomes are filtered out. Assessment of the Overall QC Results by Downstream Analysis To evaluate the effect of QC-Chain, downstream analyses were performedto assessand comparethe results obtainedfrom reads both pre-QC and after-QC, respectively. In the following parts of this work, the combined original pre-QC reads were referred to as ‘‘total reads’’,the reads passedthe contamination screeningwere referredto as ‘‘clean reads’’, and the reads coming from the target genomic or metagenomic sources were referred to as ‘‘control reads’’. For the simulated genomic data, the genome assembly was performed by Velvet 1.2.03 [10] on total reads, clean reads and control reads, respectively. The parameters used were: ‘‘-exp_cov 70, -cov_cutoff 4, -ins_length 500’’ and others are set as default. Several indexes, including number of contigs, N50 size and assembly size were considered to evaluate the analysis result. Augustus 2.5.5 [11] was used to predict the open reading frames (ORFs) from the assembly result. The protein sequences of the reference genome were used as the reference to test the accuracy of the gene structure predicted. Specifically, the protein sequences predicted from the assembly of total reads, clean reads and control reads were aligned to reference proteins by BLASTp, respectively and the false positive rate (FPR) were calculated as: For the simulated metagenomic data, de novo metagenome assembly and functional analysis were performed with total reads, clean reads and control reads, respectively. Each dataset was firstly assembled using IDBA_UD [12], based on which ORFs were predicted by MetaGeneMark [13]. Simultaneously, those contigs with more than 50 bp in length were submitted to MG-RAST (http://metagenomics.anl.gov) for organism abundance analysis and functional gene annotation. GC distribution and rarefaction curves were generated from MG-RAST automatically. Functional analysis was performed by aligning the predicted ORFs of each dataset to COG database using a maximum e-value of 1e25 and a minimum identity of 60%. Differences were calculated using one-tailed paired t-test, with asterisks denoting statistical significance (NS: not significant; *: p,0.05; **: p,0.01).",QualityControl,"qcchain fast  holistic quality control method  nextgeneration sequence data
 overall quality control strategy  objectives  qcchain include  retrieve read  high quality  identify  quantify  source  contaminations  filter contaminate read  accomplish   process   relatively short time  achieve  objectives  overall method  qcchain include sequence quality assessment  trim  contamination screen  removal additionally evaluation  comparison  downstream analysis result use read    also include   important component   holistic approach  strategy  workflow   method  show  figure   detail procedures  describe  read quality assessment  trim  parallelqc  sequence quality assessment  trim   first step  ngs data quality control  require  accuracy  efficiency  accomplish  step  develop  parallel quality control software parallelqc  could  use  trim filter  remove low sequencingquality read  ngs data parallelqc  develop  linux   multithread technology base  multicore x86 cpu platform   compatible  x86  x86 linux specifically  parallelqc sequence could  trim   specific length lowquality base within read could  trim      end lowquality read could  filter  quality value  user define percentage duplications could  identify  remove  tag sequence filtration multiple tag sequence could  align  shift      end   read  mismatch allow   positive align read could  remove  significantly accelerate  speed  computation parallelqc parallelize  sequence quality evaluation  filtration step  assign balance  weight task  independent thread  could  execute  different cpu core simultaneously  addition  progress could  complete   one disk  operation  highly improve  efficiency  analysis    hand  multiple step   accomplish  use  single command line  userfriendly options therefore parallelqc significantly shorten  process time compare  traditional single core cpu base method  simplify users operation compare  use multiple single function  tool identification  removal  contaminate read  aim  contamination screen   identify  quantify  mostly unknown source  contaminations filter  contaminate read  obtain  process read  clean  possible  adopt two complementary strategies    could provide know  unknown species information   dataset   rdnareads base method ribosomal dna read  use  qualitatively detect  taxonomical structure   dataset quickly ribosomal rnas     prokaryote    eukaryotes sequence  good indicators  characterize prokaryotic  eukaryotic species   commonly use  phylogenetic analysis   also widely use  metagenomic analysis  detect  community structure   apply parallelmeta   highperformance 16s18s rrna analysis pipeline  report  taxonomic classification construction  distribution  ngs read parallelmeta   gpu  multicore cpu base software  firstly extract  user select    rrna sequence   input data  align  obtain rrna read  several optional databases include rdp  greengenes   silva   taxonomy information  produce   show   dynamic graphic view  correspond species proportion additionally  qcchain parallelmeta  update   able  accomplish eukaryotic species screen  identification   previous version  could  identify prokaryotic species   approach   possible species source   raw read include  prokaryotic  eukaryotic information could  detect  novo   method  randomreads base  could quantitatively provide  species information generally detect  possible contaminations require align read   comprehensive database  include species record  many  possible   popular  widelyused alignment method  blast  ncbi national center  biotechnology information database   however   know  blast   timeconsuming process   speed   bottleneck especially  analyze immense amount  read  alternative   reduce  size   query data  perform blast  get  species information quickly    consideration  develop  inhouse script  could randomly extract read   raw read   userdefined proportion   read  extract read   align  ncbint database use blastn  extract species information   relatively short time   two approach  complementary  synergetic    rdnareads base method could quickly screen  identify  possible contaminate species  randomreads base method could provide quantitative evaluation   contaminations  also help  verify  result  rdnareads base method  confirm  contaminate source  combine  result    two methods  contaminate read  filter   alignment tool bowtie    default parameters read align  contaminate species genomes  filter  assessment   overall  result  downstream analysis  evaluate  effect  qcchain downstream analyse  performedto assessand comparethe result obtainedfrom read  preqc  afterqc respectively   follow part   work  combine original preqc read  refer   total readsthe read passedthe contamination screeningwere referredto  clean read   read come   target genomic  metagenomic source  refer   control read   simulate genomic data  genome assembly  perform  velvet    total read clean read  control read respectively  parameters use  exp_cov  cov_cutoff  ins_length   others  set  default several index include number  contigs n50 size  assembly size  consider  evaluate  analysis result augustus    use  predict  open read frame orfs   assembly result  protein sequence   reference genome  use   reference  test  accuracy   gene structure predict specifically  protein sequence predict   assembly  total read clean read  control read  align  reference proteins  blastp respectively   false positive rate fpr  calculate    simulate metagenomic data  novo metagenome assembly  functional analysis  perform  total read clean read  control read respectively  dataset  firstly assemble use idba_ud  base   orfs  predict  metagenemark  simultaneously  contigs       length  submit  mgrast   organism abundance analysis  functional gene annotation  distribution  rarefaction curve  generate  mgrast automatically functional analysis  perform  align  predict orfs   dataset  cog database use  maximum evalue  1e25   minimum identity   differences  calculate use onetailed pair ttest  asterisk denote statistical significance   significant *  ** ",8
172,SAMStat,"SAMStat: monitoring biases in next generation sequencing data
SAMStat automatically recognizes the input files as either fasta, fastq, SAM or BAM and reports several basic properties of the sequences as listed in Table 1. Multiple input files can be given for batch processing. For each dataset, the output consists of a single html5 page containing several plots allowing non-specialists to visually inspect the results. Naturally, the html5 pages can be viewed both on- and off-line and easily be stored for future reference. All properties are plotted separately for different mapping quality intervals if those are present in the input file. For example, mismatch profiles are given for high-and low-quality alignments allowing users to verify whether poorly mapped reads contain a specific collection of mismatches. The latter may represent untrimmed linkers in a subset of reads. Dinucleotide overrepresentation is calculated as described by Frith et al. (2008). Overrepresented 10mers are calculated by comparing the frequency of 10mer within a mapping quality interval compared with the overall frequency of the 10mer. Table 1. Overview of SAMstat output Reported statistics Mapping ratea Read length distribution Nucleotide composition Mean base quality at each read position Overrepresented 10mers Overrepresented dinucleotides along read Mismatch, insertion and deletion profilea Open in a separate window aOnly reported for SAM files. 3 RESULTS AND DISCUSSION To demonstrate how SAMStat can be used to visualize mapping properties of a next generation datasets, we used data from a recently published transcriptome study (Plessy et al., 2010); (DDBJ short read archive: DRA000169). We mapped all 24 million 5′ reads to the human genome (GRCh37/hg19 assembly) using BWA (Li and Durbin, 2009) with default parameters. SAMStat parsed the alignment information in ~3 min which is comparable to the 2 min it takes to copy the SAM file from one directory to another. The majority of reads can be mapped with very high confidence (Fig. 1a). When inspecting the mismatch error profiles, we noticed that there are many mismatches involving a guanine residue at the very start of many reads (yellow bars in Fig. 1b–e). These 5′ added guanine residues are known to originate from the reverse transcriptase step in preparing the cDNAs (Carninci et al., 2006). When comparing the mismatch profiles for high (Fig. 1b) to low-quality alignments (Fig. 1e), it is clear that a proportion of reads contain multiple 5′ added G's which in turn pose a problem to the mapping. For example, at the lowest mapping quality (Fig. 1e), there are frequent mismatches involving G's at positions one, two and to a lesser extent until position five while in high-quality alignments the mismatches are confined to the first position of the reads (Fig. 1b). An external file that holds a picture, illustration, etc. Object name is btq614f1.jpg Open in a separate window Fig. 1. A selection of SAMStat's html output. (a) Mapping statistics. More than half of the reads are mapped with a high mapping accuracy (red) while 9.9% of the reads remain unmapped (black). (b) Barcharts showing the distribution of mismatches and insertions along the read for alignments with the highest mapping accuracy [shown in red in (a)]. The colors indicate the mismatched nucleotides found in the read or the nucleotides inserted into the read. (c,d and e) Frequency of mismatches at the start of reads with mapping accuracies 1e−3 ≤ P < 1e−2, 1e−2 ≤ P < 0.5 and 0.5 ≤ P < 1, respectively (shown in orange, yellow and blue in panel a). The fraction of mismatches involving G's at position 2–5 increases. (f) Percentage of ‘GG’ dinucleotides at positions 1–5 in reads split up by mapping quality intervals. The background color highlights large percentages. The first and last row for nucleotides ‘GT’ and ‘GC’ are shown for comparison. Alongside the mismatch profiles SAMstat gives a table listing the percentages of each dinucleotide at each position of the reads split up by mapping quality intervals (Fig. 1f). For the present dataset, 60.4% of unmapped reads start with ‘GG’ and 10.1 percent contain a ‘GG’ at position 4. Evidently, 5′ G residues are added during library preparation and the start positions of mappings should be adjusted accordingly. SAMStat is ideally suited to deal with the ever increasing amounts of data from second-and third-generation sequencing projects. Specific applications include the verification and quality control of processing pipelines, the tracking of data quality over time and the visualization of data properties derived from new protocols and approaches which in turn often leads to novel insight",QualityControl,"samstat monitor bias  next generation sequence data
samstat automatically recognize  input file  either fasta fastq sam  bam  report several basic properties   sequence  list  table  multiple input file   give  batch process   dataset  output consist   single html5 page contain several plot allow nonspecialists  visually inspect  result naturally  html5 page   view    offline  easily  store  future reference  properties  plot separately  different map quality intervals    present   input file  example mismatch profile  give  highand lowquality alignments allow users  verify whether poorly map read contain  specific collection  mismatch  latter may represent untrimmed linkers   subset  read dinucleotide overrepresentation  calculate  describe  frith    overrepresented 10mers  calculate  compare  frequency  10mer within  map quality interval compare   overall frequency   10mer table  overview  samstat output report statistics map ratea read length distribution nucleotide composition mean base quality   read position overrepresented 10mers overrepresented dinucleotides along read mismatch insertion  deletion profilea open   separate window aonly report  sam file  result  discussion  demonstrate  samstat   use  visualize map properties   next generation datasets  use data   recently publish transcriptome study plessy    ddbj short read archive dra000169  map   million ′ read   human genome grch37hg19 assembly use bwa   durbin   default parameters samstat parse  alignment information  ~ min   comparable    min  take  copy  sam file  one directory  another  majority  read   map   high confidence fig   inspect  mismatch error profile  notice    many mismatch involve  guanine residue    start  many read yellow bar  fig 1be  ′ add guanine residues  know  originate   reverse transcriptase step  prepare  cdnas carninci     compare  mismatch profile  high fig   lowquality alignments fig    clear   proportion  read contain multiple ′ add '   turn pose  problem   map  example   lowest map quality fig    frequent mismatch involve '  position one two    lesser extent  position five   highquality alignments  mismatch  confine   first position   read fig   external file  hold  picture illustration etc object name  btq614f1jpg open   separate window fig   selection  samstat' html output  map statistics   half   read  map   high map accuracy red     read remain unmapped black  barcharts show  distribution  mismatch  insertions along  read  alignments   highest map accuracy show  red    color indicate  mismatch nucleotides find   read   nucleotides insert   read    frequency  mismatch   start  read  map accuracies  ≤     ≤      ≤    respectively show  orange yellow  blue  panel   fraction  mismatch involve '  position  increase  percentage   dinucleotides  position   read split   map quality intervals  background color highlight large percentages  first  last row  nucleotides     show  comparison alongside  mismatch profile samstat give  table list  percentages   dinucleotide   position   read split   map quality intervals fig    present dataset   unmapped read start     percent contain    position  evidently ′  residues  add  library preparation   start position  mappings   adjust accordingly samstat  ideally suit  deal   ever increase amount  data  secondand thirdgeneration sequence project specific applications include  verification  quality control  process pipelines  track  data quality  time   visualization  data properties derive  new protocols  approach   turn often lead  novel insight",8
173,ClinQC,"ClinQC: a tool for quality control and cleaning of Sanger and NGS data in clinical research
ClinQC tool is developed in Python 2.7.9 (http://www.python.org) by using the multiprocessing capability. It uses four other tools including FASTQC [7], PRINSEQ [8], Alientrimmer [18], and TraceTuner [19]. The ClinQC workflow is depicted in Fig. 1 and consist of several sequential steps that lead from the raw sequencing reads to the high quality Sanger encoded FASTQ file for each patient/sample. All parameter settings can be specified in a single configuration file (Additional files 1 and 2). To achieve the optimized performance, ClinQC uses the available hardware (Physical memory and CPU) in a best possible way. A buffer file read write concept was implemented where input and output are partially stored in memory during the analysis, which reduces the computation time and reduces the disk reading and writing workload. An external file that holds a picture, illustration, etc. Object name is 12859_2016_915_Fig1_HTML.jpg Open in a separate window Fig. 1 The workflow of ClinQC pipeline. ClinQC tool can be run with a single command. The flow of analysis is depicted from top to bottom. BASE CALLING (violet color) step is only applicable for Sanger data analysis; DEMULTIPLEXING and DUPLICATE & CONATMINATION FILTERING (yellow color) steps are only applicable for NGS data analysis; all other steps (green color) are applicable for both analysis flows. ClinQC generates three final outputs Go to: Results and discussion ClinQC is an open-source, easy-to-use and integrated tool, which facilitates the analysis of Sanger and NGS sequencing data in a single platform with a common input output model. It supports the rapid analysis of hundreds of sample/patient data in parallel. This pipeline provides full flexibility to customize all parameters using the “ClinQCOptions” file for handling the sequencing platform specific errors and provides proper guidelines for the analysis. All components of ClinQC workflow and their inputs have been summarized in Fig. 1. ClinQC pipeline The ClinQC pipeline (Fig. 1) consists of nine sequential steps that starts with raw sequencing reads and ends up with three outputs: 1) QC summary table, 2) FASTQ files with high quality reads and 3) QC report. The detailed description of each step is given below: Base calling Due to unclear signal in Sanger pherogram files, the base caller of the sequencer always calls ambiguous nucleotide as N. However, it could output more specific ambiguous nucleotides, i.e., R, if signal is not clear between A or G; Y, if signal is not clear between C and T. Therefore, ClinQC uses the tool TraceTunner [19] to improve the base calling and assign more specific ambiguous nucleotides. 2. Format conversion In this step, ClinQC check the raw sequencing files and their formats and, if needed, converts from native file format to FASTQ with Sanger quality encoding (Fig. 2). Sanger sequencing files are accepted in AB1 and SCF format and NGS files are accepted in SFF, FASTA-QUAL and FASTQ format. 3. Demultiplexing An external file that holds a picture, illustration, etc. Object name is 12859_2016_915_Fig2_HTML.jpg Fig. 2 The format conversion workflow of ClinQC. ClinQC takes raw reads in any native file format of their sequencing platforms and returns a unified FASTQ files with Sanger (PHRED) quality encoding This step is only applicable for NGS data, where multiple samples are sequenced in a single sequencing run by using the multiplexing method. Based on the barcode sequences (MID: Multiplexed Identifier) provided in the “ClinQCTargetFile” file (as shown in Additional file 3), one FASTQ file per barcode is created. In case of paired-end sequencing, two FASTQ files (one for forward and one for reverse reads) are generated. This step will be skipped if the input data is already de-multiplexed. 4. Adapter and primer trimming In this step, ClinQC trim the forward and reverse adapter and primer sequences provided in the “PrimerAdapter” file (as shown in Additional file 4) by using the AlienTrimmer [17] tool. AlienTrimmer is a flexible and sensitive sequence trimmer with mismatch tolerance, which allows the customization of the number of mismatches and k-mers based on the data quality and user requirements. 5. Duplicate and contamination filtering PCR duplicates are a critical known problem, which arise when low abundant fragments are over amplified during the library preparation process. These duplicates can substantially inflate the allele frequency leading to wrong mutation detection and unexpected species richness in metagenomic analysis [20]. Therefore, ClinQC identify and remove duplicates using the PRINSEQ [8] tool to eliminate this technical artifact. Contamination is another problem particularly in metagenomic analysis [21] leading to wrong analysis when DNA from unknown sources is sequenced. Hence, ClinQC assesses and eliminates the contamination from the samples using the PRINSEQ [8] software. 6. Quality trimming As NGS short read sequencing errors increase with the position in the read [22], ClinQC trim the low quality stretch and Ns from the 5’ and 3’ end of the reads. 7. Read filtering In this Step ClinQC eliminate the reads, which do not meet the minimum average base quality and the minimum and maximum read length threshold. Thus, only high quality reads, which fulfill all quality trimming and filtering criteria, are kept in the final output file. 8. GC content assessment GC content is crucial parameter when analyzing NGS data as the under or over representation of GC content could effect the downstream analysis and biological conclusions. Therefore, ClinQC reports the average GC content before and after QC in the summary table for each dataset. 9. Output generation In this final step ClinQC write three output files: 1) summary output file in HTML format, 2) QC report, and 3) FASTQ files after filtering the low quality reads. ClinQC input ClinQC provide a uniform input and output data models for Sanger and NGS sequencing data analysis requiring a minimum of three input files: Target file: The target file contains experimental and sequencing information for each patient (Additional file 3). This file contains patient information including experiment details and raw sequencing files paths. The first column (Patient_ID) is mandatory and should be a unique identifier for each sample. Other patient information is optional and can be ignored for genomic data analysis. Adapter-Primer file: This input file is optional and is required only if primer and adapter sequences need to be trimmed. It is a tab-separated text file with four columns describing the feature-type, id, forward sequence and reverse sequence (see Additional file 4). ClinQCOptions file: The options file contains all input parameters for various parts of the pipeline and the path to the third party tools. A default ClinQCOptions file for Sanger and NGS data analysis is provided separately (Additional files 1 and 2). Sequencing reads: ClinQC support Sanger sequencing reads in AB1 and SCF file format, Illumina reads in FASTQ format, 454 reads in SFF and FASTQ-QUAL format and Ion Torrent reads in SFF and FASTQ format. ClinQC output ClinQC produces output files in the same format for Sanger and NGS, which make output handling and further downstream analysis more efficient. The output files are: QC summary table: The QC summary table (Fig. 3a) consists of one line for each sample/patient including references to the two other patient specific output files (QC report and FASTQ file). The QC summary table contains experimental, patient, and sequencing information along with QC summary, number of reads and average GC content before and after quality control and filtering. 2. QC report file: An external file that holds a picture, illustration, etc. Object name is 12859_2016_915_Fig3_HTML.jpg Fig. 3 ClinQC final output. a QC summary table generated for each run, which includes experimental, patient, sequencing and QC information, one row for each sample/patient, (b) QC report generated by FASTQC before (left) and after (right) quality control for each sample/patient and linked in summary table, (c) FASTQ files with high quality reads for each sample/patient and linked in summary table After quality trimming and filtering, an extensive and intuitive quality report is generated in HTML format by using the widely used FASTQC [7] tool. It generates various useful plots (i.e. read base quality, read length distribution, overrepresented sequences and sequence duplication levels) to get a detailed view of the quality of sequencing data. ClinQC generates two QC reports for each patient/sample before QC (Fig. 4a) and after QC (Fig. 4b), which can be used for direct comparison. These two QC report HTML files are linked in the variant summary table. 3. FASTQ file with high quality reads: An external file that holds a picture, illustration, etc. Object name is 12859_2016_915_Fig4_HTML.jpg Fig. 4 ClinQC quality control report generated by FASTQC. a Per base sequence quality before quality control and (b) per base sequence quality after quality control. ClinQC generates several useful QC plots for each patient’s FASTQ file before and after quality control. This feature enables to directly compare the data quality improvements and the number of filtered reads before and after quality control After all file preprocessing, quality filtering and trimming steps are completed, ClinQC creates a Sanger encoded FASTQ file with high quality reads for each patient/sample (Fig. 2c). This file can be directly used in further down-stream analysis (e.g., mutation screening, genome assembly and metagenomic).",QualityControl,"clinqc  tool  quality control  clean  sanger  ngs data  clinical research
clinqc tool  develop  python    use  multiprocessing capability  use four  tool include fastqc  prinseq  alientrimmer   tracetuner   clinqc workflow  depict  fig   consist  several sequential step  lead   raw sequence read   high quality sanger encode fastq file   patientsample  parameter settings   specify   single configuration file additional file     achieve  optimize performance clinqc use  available hardware physical memory  cpu   best possible way  buffer file read write concept  implement  input  output  partially store  memory   analysis  reduce  computation time  reduce  disk read  write workload  external file  hold  picture illustration etc object name  12859_2016_915_fig1_htmljpg open   separate window fig   workflow  clinqc pipeline clinqc tool   run   single command  flow  analysis  depict  top  bottom base call violet color step   applicable  sanger data analysis demultiplexing  duplicate  conatmination filter yellow color step   applicable  ngs data analysis   step green color  applicable   analysis flow clinqc generate three final output   result  discussion clinqc   opensource easytouse  integrate tool  facilitate  analysis  sanger  ngs sequence data   single platform   common input output model  support  rapid analysis  hundreds  samplepatient data  parallel  pipeline provide full flexibility  customize  parameters use  “clinqcoptions” file  handle  sequence platform specific errors  provide proper guidelines   analysis  components  clinqc workflow   input   summarize  fig  clinqc pipeline  clinqc pipeline fig  consist  nine sequential step  start  raw sequence read  end   three output   summary table  fastq file  high quality read    report  detail description   step  give  base call due  unclear signal  sanger pherogram file  base caller   sequencer always call ambiguous nucleotide   however  could output  specific ambiguous nucleotides    signal   clear       signal   clear     therefore clinqc use  tool tracetunner   improve  base call  assign  specific ambiguous nucleotides  format conversion   step clinqc check  raw sequence file   format   need convert  native file format  fastq  sanger quality encode fig  sanger sequence file  accept  ab1  scf format  ngs file  accept  sff fastaqual  fastq format  demultiplexing  external file  hold  picture illustration etc object name  12859_2016_915_fig2_htmljpg fig   format conversion workflow  clinqc clinqc take raw read   native file format   sequence platforms  return  unify fastq file  sanger phred quality encode  step   applicable  ngs data  multiple sample  sequence   single sequence run  use  multiplexing method base   barcode sequence mid multiplexed identifier provide   “clinqctargetfile” file  show  additional file  one fastq file per barcode  create  case  pairedend sequence two fastq file one  forward  one  reverse read  generate  step   skip   input data  already demultiplexed  adapter  primer trim   step clinqc trim  forward  reverse adapter  primer sequence provide   “primeradapter” file  show  additional file   use  alientrimmer  tool alientrimmer   flexible  sensitive sequence trimmer  mismatch tolerance  allow  customization   number  mismatch  kmers base   data quality  user requirements  duplicate  contamination filter pcr duplicate   critical know problem  arise  low abundant fragment   amplify   library preparation process  duplicate  substantially inflate  allele frequency lead  wrong mutation detection  unexpected species richness  metagenomic analysis  therefore clinqc identify  remove duplicate use  prinseq  tool  eliminate  technical artifact contamination  another problem particularly  metagenomic analysis  lead  wrong analysis  dna  unknown source  sequence hence clinqc assess  eliminate  contamination   sample use  prinseq  software  quality trim  ngs short read sequence errors increase   position   read  clinqc trim  low quality stretch        end   read  read filter   step clinqc eliminate  read    meet  minimum average base quality   minimum  maximum read length threshold thus  high quality read  fulfill  quality trim  filter criteria  keep   final output file   content assessment  content  crucial parameter  analyze ngs data      representation   content could effect  downstream analysis  biological conclusions therefore clinqc report  average  content       summary table   dataset  output generation   final step clinqc write three output file  summary output file  html format   report   fastq file  filter  low quality read clinqc input clinqc provide  uniform input  output data model  sanger  ngs sequence data analysis require  minimum  three input file target file  target file contain experimental  sequence information   patient additional file   file contain patient information include experiment detail  raw sequence file paths  first column patient_id  mandatory     unique identifier   sample  patient information  optional    ignore  genomic data analysis adapterprimer file  input file  optional   require   primer  adapter sequence need   trim    tabseparated text file  four columns describe  featuretype  forward sequence  reverse sequence see additional file  clinqcoptions file  options file contain  input parameters  various part   pipeline   path   third party tool  default clinqcoptions file  sanger  ngs data analysis  provide separately additional file    sequence read clinqc support sanger sequence read  ab1  scf file format illumina read  fastq format  read  sff  fastqqual format  ion torrent read  sff  fastq format clinqc output clinqc produce output file    format  sanger  ngs  make output handle   downstream analysis  efficient  output file   summary table   summary table fig  consist  one line   samplepatient include reference   two  patient specific output file  report  fastq file   summary table contain experimental patient  sequence information along   summary number  read  average  content    quality control  filter   report file  external file  hold  picture illustration etc object name  12859_2016_915_fig3_htmljpg fig  clinqc final output   summary table generate   run  include experimental patient sequence   information one row   samplepatient   report generate  fastqc  leave   right quality control   samplepatient  link  summary table  fastq file  high quality read   samplepatient  link  summary table  quality trim  filter  extensive  intuitive quality report  generate  html format  use  widely use fastqc  tool  generate various useful plot  read base quality read length distribution overrepresented sequence  sequence duplication level  get  detail view   quality  sequence data clinqc generate two  report   patientsample   fig     fig     use  direct comparison  two  report html file  link   variant summary table  fastq file  high quality read  external file  hold  picture illustration etc object name  12859_2016_915_fig4_htmljpg fig  clinqc quality control report generate  fastqc  per base sequence quality  quality control   per base sequence quality  quality control clinqc generate several useful  plot   patients fastq file    quality control  feature enable  directly compare  data quality improvements   number  filter read    quality control   file preprocessing quality filter  trim step  complete clinqc create  sanger encode fastq file  high quality read   patientsample fig   file   directly use   downstream analysis  mutation screen genome assembly  metagenomic",8
174,UrQt,"UrQt: an efficient software for the Unsupervised Quality trimming of NGS data.
In this section, we present the probabilistic model that we use to find the best position to trim a read to increase its quality without removing more nucleotides than necessary. We also present an extension of this model for homopolymer trimming. A read is defined as a vector (n 1,…,n m) of m nucleotides associated with a vector of phred scores (q 1,…,q m). We want to find the best cut-point k 1[set membership][1,m] in a read of length m between an informative segment for nucleotide n i,i[set membership][1,k 1] and a segment of unreliable quality for nucleotide n i,i[set membership][k 1+1,m] (Figure ​(Figure1).1). Then, having found k 1, we want to find the best cut-point k 2[set membership][1,k 1] between a segment of unreliable quality for nucleotide n i,i[set membership][1,k 2−1] and an informative segment for nucleotide n i,i[set membership][k 2,k 1]. Given the shape of the calling error probability distribution, there is less signal to find k 1 (the probability slowly increases at the extremity of the read) than k 2 (abruptly decreases). Therefore, we want to have the highest number of nucleotides to support the choice of k 1 when k 2 can be found with a subsequence of the read (Figure ​(Figure11). An external file that holds a picture, illustration, etc. Object name is 12859_2015_546_Fig1_HTML.jpg Open in a separate window Figure 1 Quality trimming. Position of the cut-points k 1 and k 2 in a read. After trimming, the retained part corresponds to the section with a green background, which indicates an informative segment of nucleotides between k 1 and k 2. With q the quality value of a nucleotide, the probability for this nucleotide to be correct is defined by: p a ( q ) = 1 − 1 0 − q 10 (A) which gives, for example, a probability p a(q)=0.99 for a phred q=20 [2]. However, in QC, the word “informative” is typically defined as a phred score above a certain threshold and not the probability of calling the correct nucleotide. From a probabilistic point of view, we need to discriminate informative nucleotides (with p a(q)≥p a(t) and t a given threshold) from other nucleotides, rather than discriminate fairly accurate nucleotides (with p a(q)≥0.5) from the others. Therefore, we propose to define the probability of having an informative nucleotide as p b ( q , t ) = 1 − 2 − q t with t the minimal phred score acceptable to be informative. This definition shifts the probability function such that for q=t, we have p b(q,t)=0.5. Therefore, at the threshold t, nucleotides with p b(q,t)≥0.5 are informative and the others are not. With t=3.0103, we go back to the classical phred function (Figure ​(Figure2)2) in which p b(q,t)=p a(q). An external file that holds a picture, illustration, etc. Object name is 12859_2015_546_Fig2_HTML.jpg Open in a separate window Figure 2 Probability-phred functions. p(q,t) according to the choice of t. The white, dark grey, light grey and black dots represent respectively the position of p 1,p 2,p 3 and p 4 for the corresponding probability-phred functions. Before p 1 we have the 1 − 2 − q t part of the function (B) and after p 1 the B(q [open star],p 1,p 2,p 3,p 4) part of the function (B). With the function p b(q,t), low phred scores are associated with a low probability to be correct (p b(0,t)=0), but for t≤20 a high phred score does not correspond to a high probability to be correct (for example, p b(40,20)=0.75). Therefore, from a probabilistic point of view, unreliable nucleotides will have more weight than informative ones. To associate a high phred score with a high probability of having an informative nucleotide, we constrain this probability to reach 1 for a phred score of 45 by using the following spline function (Figure ​(Figure22): p ( q , t ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 − 2 − q t if q ≤ max ( 20 , t ) , B ( q ⋆ , p 1 , p 2 , 1 , 1 ) otherwise (B) with B(q [open star],p 1,p 2,p 3,p 4) the cubic Bezier curve starting at p 1 toward p 2 and arriving at p 4 coming from the direction of p 3 for q [open star][set membership][0,1]. We have p 1=1−2− max(20,t)/t, p 2=g(1/3×(45− max(20,t))) with g(q) the tangent to the function 1 − 2 − q t in max(20,t). We scale the Bezier curve to the interval [t,45] with q [open star]=(q−t)/(45−t). The constraint max(20,t) ensures that d d q ⋆ B ( q ⋆ , p 1 , p 2 , p 3 , p 4 ) < 0 for q [open star][set membership][0,1] (see Figure ​Figure22). With the maximum likelihood framework, finding the position of the cut-point between a segment of informative nucleotides (q>t) and a segment of unreliable nucleotides (q<t) consists in estimating k 1 by: ˆ k 1 = arg max k k ∏ i = 1 1 k f 0 ( n i , t ) m ∏ i = k + 1 1 m − k − 1 f 1 ( n i , t ) (C) with f 0(n i,t) the probability that the nucleotide n i comes from the segment of informative nucleotides and f 1(n i,t) the probability that the nucleotide n i comes from the segment of unreliable nucleotides for a given t. Such that: f 0 ( n i , t ) = p ( q i , t ) ∏ N ∈ Ω Pr ( N ) 1 ( n i = N ) (D) f1(ni, t) = (1 − p(qi, t))¼        (E) with 1(n i=N) an indicator variable such that 1(n i=N)=1 if the nucleotide n i is equal to N and 0 otherwise, Pr ( N ) = ∑ k i = 1 1 ( n i = N ) / k the probability to observe the nucleotide N between 1 and k, and Ω the standard IUB/IUPAC dictionary [10]. Pr(N)N[set membership]Ω and k 1 are estimated with the complete data framework of the EM algorithm [11]. After finding ˆ k 1 , we apply the same procedure on the interval [ 1 , ˆ k 1 ] to estimate the best cut-point k 2 between a segment of unreliable nucleotides ahead of a segment of informative nucleotides. This double binary segmentation ensures to provide the best two cut-points for a given read [12]. For p(q,t)=p a(q), we can interpret the segment of informative nucleotides as a segment for which on average we are confident that a given nucleotide is the correct one, whereas the segment of unreliable nucleotides is composed of uninformative nucleotides in which on average any of the four nucleotides can be present at a given position. The cut-point k 1 maximizes the probability that the nucleotides n i,i[set membership][1,k 1] are informative and that nucleotides n i,i[set membership][k 1,m] are not. With our model, trimming nucleotides of unreliable quality is somewhat similar to removing homopolymers from the extremities of the reads. The task of removing homopolymers, such as polyA tails in RNA-Seq experiments, is not trivial, because the quality of a given nucleotide decreases both at the end of the read and with the size of the homopolymer. Therefore, because the number of incorrectly called nucleotides increases, we are less likely to observe As at the end of the polyA tail. UrQt implements a procedure for the unsupervised trimming of polyN with a straightforward modification of equation (E) such that: f1(ni, t) = pa(qi,t)1(ni=A)((1−pa(qi,t))¼)1(ni≠A)        (F) in which we can replace A by any letter of the standard IUB/IUPAC dictionary. With this definition of f 1, we consider the calling error probability of the nucleotide at position i if n i=A or if n i≠A, the probability that the nucleotide could have been an A",QualityControl,"urqt  efficient software   unsupervised quality trim  ngs data
  section  present  probabilistic model   use  find  best position  trim  read  increase  quality without remove  nucleotides  necessary  also present  extension   model  homopolymer trim  read  define   vector  …    nucleotides associate   vector  phred score  …   want  find  best cutpoint  set membershipm   read  length    informative segment  nucleotide  iiset membershipk    segment  unreliable quality  nucleotide  iiset membershipk  figure ​figure1   find    want  find  best cutpoint  set membershipk    segment  unreliable quality  nucleotide  iiset membershipk    informative segment  nucleotide  iiset membershipk   give  shape   call error probability distribution   less signal  find    probability slowly increase   extremity   read    abruptly decrease therefore  want    highest number  nucleotides  support  choice         find   subsequence   read figure ​figure11  external file  hold  picture illustration etc object name  12859_2015_546_fig1_htmljpg open   separate window figure  quality trim position   cutpoints        read  trim  retain part correspond   section   green background  indicate  informative segment  nucleotides          quality value   nucleotide  probability   nucleotide   correct  define                 give  example  probability     phred   however    word “informative”  typically define   phred score   certain threshold    probability  call  correct nucleotide   probabilistic point  view  need  discriminate informative nucleotides   ≥     give threshold   nucleotides rather  discriminate fairly accurate nucleotides   ≥   others therefore  propose  define  probability    informative nucleotide                   minimal phred score acceptable   informative  definition shift  probability function        bqt therefore   threshold  nucleotides   bqt≥  informative   others       back   classical phred function figure ​figure2    bqtp   external file  hold  picture illustration etc object name  12859_2015_546_fig2_htmljpg open   separate window figure  probabilityphred function pqt accord   choice    white dark grey light grey  black dot represent respectively  position           correspond probabilityphred function             part   function        open starp     part   function    function  bqt low phred score  associate   low probability   correct     ≤  high phred score   correspond   high probability   correct  example   therefore   probabilistic point  view unreliable nucleotides    weight  informative ones  associate  high phred score   high probability    informative nucleotide  constrain  probability  reach    phred score    use  follow spline function figure ​figure22        ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩         ≤ max          ⋆            otherwise    open starp      cubic bezier curve start    toward    arrive    come   direction      open starset membership     maxtt   maxt    tangent   function        maxt  scale  bezier curve   interval    open starqtt  constraint maxt ensure     ⋆    ⋆                  open starset membership see figure ​figure22   maximum likelihood framework find  position   cutpoint   segment  informative nucleotides    segment  unreliable nucleotides  consist  estimate        arg max   ∏               ∏                          probability   nucleotide   come   segment  informative nucleotides      probability   nucleotide   come   segment  unreliable nucleotides   give                    ∏  ∈              f1ni     pqi              indicator variable       nucleotide    equal     otherwise      ∑               probability  observe  nucleotide         standard iubiupac dictionary  prnnset membershipω     estimate   complete data framework    algorithm   find      apply   procedure   interval         estimate  best cutpoint     segment  unreliable nucleotides ahead   segment  informative nucleotides  double binary segmentation ensure  provide  best two cutpoints   give read   pqtp    interpret  segment  informative nucleotides   segment    average   confident   give nucleotide   correct one whereas  segment  unreliable nucleotides  compose  uninformative nucleotides    average    four nucleotides   present   give position  cutpoint   maximize  probability   nucleotides  iiset membershipk   informative   nucleotides  iiset membershipk      model trim nucleotides  unreliable quality  somewhat similar  remove homopolymers   extremities   read  task  remove homopolymers   polya tail  rnaseq experiment   trivial   quality   give nucleotide decrease    end   read    size   homopolymer therefore   number  incorrectly call nucleotides increase   less likely  observe    end   polya tail urqt implement  procedure   unsupervised trim  polyn   straightforward modification  equation    f1ni   paqitniapaqit¼ni≠             replace    letter   standard iubiupac dictionary   definition     consider  call error probability   nucleotide  position        ≠  probability   nucleotide could    ",8
175,FQC,"FQC Dashboard: integrates FastQC results into a web-based, interactive, and extensible FASTQ quality control tool
FQC Dashboard is a combination of a command line interface (CLI) written in Python 3, which depends on FastQC for processing FASTQ files, a frontend website written in JavaScript, HTML, and CSS, that utilizes Highcharts (http://www.highcharts.com) and D3 (https://d3js.org) Javascript libraries for plotting, and Bootstrap.js (https://getbootstrap.com) for styling and interactivity. These packages are included in the source code repository. The CLI is a single executable with submodules for (i) FASTQ processing of either single- or paired-end data, (ii) batch FASTQ processing based on directory searching and (iii) adding custom plots onto existing dashboard pages. The only dependencies outside of cloning the source code repository are Python 3, which can be installed using Anaconda (https://www.continuum.io/downloads), and FastQC, which can be installed using the Bioconda channel (https://github.com/bioconda/bioconda-recipes). Plots and tables are dynamically generated from CSV files and configured by the user to be displayed within FQC's supported visualizations: table, line plot, bar plot, area range plot, heatmap or plate heatmap. Configuration is set using JSON files which define the biological samples and sample groups as well as which plots to display and their respective user settings. The default settings for FQC will generate a standard QC analysis from FastQC without the need for the user to edit files, though by modifying the configuration files, the user can personalize and add plots relevant to their analysis protocol.",QualityControl,"fqc dashboard integrate fastqc result   webbased interactive  extensible fastq quality control tool
fqc dashboard   combination   command line interface cli write  python   depend  fastqc  process fastq file  frontend website write  javascript html  css  utilize highcharts     javascript libraries  plot  bootstrapjs   style  interactivity  package  include   source code repository  cli   single executable  submodules   fastq process  either single  pairedend data  batch fastq process base  directory search  iii add custom plot onto exist dashboard page   dependencies outside  clone  source code repository  python     instal use anaconda   fastqc    instal use  bioconda channel  plot  table  dynamically generate  csv file  configure   user   display within fqc' support visualizations table line plot bar plot area range plot heatmap  plate heatmap configuration  set use json file  define  biological sample  sample group  well   plot  display   respective user settings  default settings  fqc  generate  standard  analysis  fastqc without  need   user  edit file though  modify  configuration file  user  personalize  add plot relevant   analysis protocol",8
176,KAT,"KAT: a K-mer analysis toolkit to quality control NGS datasets and genome assemblies 
KAT is a C++11 application containing multiple tools, each of which exploits multi-core machines via multi-threading where possible. Core functionality is contained in a library designed to promote rapid development of new tools. Runtime and memory requirements depend on input data size, error and bias levels, and properties of the biological sample but as a rule of thumb, machines capable of de novo assembly of a dataset will be sufficient to run KAT on the dataset (see SI section 4 for details). K-mer counting in KAT is performed by an integrated and modified version of Jellyfish2 (Marc¸ais and Kingsford, 2011), which supports large k values and is among the fastest k-mer counters available (Zhang et al., 2014). 2.1 Assembly validation by comparison of read spectrum and assembly copy number The KAT comp tool generates a matrix, with a sequence set’s k-mer frequency on one axis, and another’s set frequency on the other, with cells holding distinct k-mers counts at the given frequencies. When comparing reads against an assembly, KAT highlights properties of assembly composition and quality. If represented in a stacked histogram, read k-mer spectrum is split by copy number in the assembly (see SI section 5 for a primer on how to interpret KAT’s stacked histograms). In addition, KAT provides the sect tool necessary to study specific assembled sequences and track the kmer coverage across both the read and the assembly spectra. This can help identify assembly artefacts such as collapsing or expanding events, or detect repeat regions. Figure 1 shows plots relating to two Fraxinus excelsior assemblies created from the same dataset using the comp and sect tools. The plots highlight different strategies taken by the assembler, in (a) and (c) we see some homozygous content being duplicated, and in (b) and (d) some heterozygous content eliminated. _excelsior. Read content in black is absent from the assembly, red occurs once, purple twice, etc. Both k-mer spectra show an error distribution under 25x, heterozygous content around 50X and homozygous content around 100X. (a) contains most (but not all) the heterozygous content, and introduces more duplications on homozygous content. (b) is more collapsed, including mostly a single copy of the homozygous content and less of the heterozygous content. (c) and (d), generated using KAT sect, show kmer coverage across example assembled loci. The assembly k-mer coverage (black line) of assembly (a) in plot (c) shows that the assembly has two copies of this locus, whereas the read k-mer coverage (red line) implies there should be only a single copy. This incorrect duplication has been corrected in assembly (b) with the read and assembly k-mer coverage agreeing in plot (d). The increased read and assembly k-mer coverage at positions 100 and 400 indicates small regions of repetitive sequence in the genome. The halved read k-mer coverage after position 400 indicates a heterozygous locus, which likely caused the duplication of this locus in the assembly (a). See SI Section 5 for a more extensive analysis of all sequences from this loci and their impact on (a) and (b). 2.2 Other KAT tools KAT also includes the hist tool for computing spectrum from a single sequence set, the gcp tool to analyse gc content against k-mer frequency. The filter tool can be used to isolate sequences from a set according to their k-mer coverage or gc content from a given spectrum (see SI section 1 for details on all the tools). These tools can be used for various tasks including contaminant detection and extraction both in raw reads and assemblies, analysis of the GC bias and consistency between paired end reads and several",QualityControl,"kat  kmer analysis toolkit  quality control ngs datasets  genome assemblies 
kat    application contain multiple tool    exploit multicore machine via multithreading  possible core functionality  contain   library design  promote rapid development  new tool runtime  memory requirements depend  input data size error  bias level  properties   biological sample    rule  thumb machine capable   novo assembly   dataset   sufficient  run kat   dataset see  section   detail kmer count  kat  perform   integrate  modify version  jellyfish2 marc¸ais  kingsford   support large  value   among  fastest kmer counter available zhang     assembly validation  comparison  read spectrum  assembly copy number  kat comp tool generate  matrix   sequence set kmer frequency  one axis  anothers set frequency     cells hold distinct kmers count   give frequencies  compare read   assembly kat highlight properties  assembly composition  quality  represent   stack histogram read kmer spectrum  split  copy number   assembly see  section    primer    interpret kats stack histograms  addition kat provide  sect tool necessary  study specific assemble sequence  track  kmer coverage across   read   assembly spectra   help identify assembly artefacts   collapse  expand events  detect repeat regions figure  show plot relate  two fraxinus excelsior assemblies create    dataset use  comp  sect tool  plot highlight different strategies take   assembler      see  homozygous content  duplicate       heterozygous content eliminate _excelsior read content  black  absent   assembly red occur  purple twice etc  kmer spectra show  error distribution   heterozygous content around   homozygous content around   contain      heterozygous content  introduce  duplications  homozygous content    collapse include mostly  single copy   homozygous content  less   heterozygous content    generate use kat sect show kmer coverage across example assemble loci  assembly kmer coverage black line  assembly   plot  show   assembly  two copy   locus whereas  read kmer coverage red line imply      single copy  incorrect duplication   correct  assembly    read  assembly kmer coverage agree  plot   increase read  assembly kmer coverage  position    indicate small regions  repetitive sequence   genome  halve read kmer coverage  position  indicate  heterozygous locus  likely cause  duplication   locus   assembly  see  section     extensive analysis   sequence   loci   impact       kat tool kat also include  hist tool  compute spectrum   single sequence set  gcp tool  analyse  content  kmer frequency  filter tool   use  isolate sequence   set accord   kmer coverage   content   give spectrum see  section   detail    tool  tool   use  various task include contaminant detection  extraction   raw read  assemblies analysis    bias  consistency  pair end read  several",8
177,NGS-QC generator,"NGS-QC Generator: A Quality Control System for ChIP-Seq and Related Deep Sequencing-Generated Datasets
Preliminary Sequencing Quality Assessment Before stepping into the analysis of experiment quality based on enrichment with the NGS-QC Generator, it is important to check the quality of the DNA sequencing itself, as this factor can directly influence the results. We currently use FastQC [7] to evaluate the DNA sequencing quality and potential contaminations. FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines prior to alignment. It can be used through simple GUI interface or through command-line for easy batch processing and the tool can handle different formats like FASTQ, SAM, and BAM file. FastQC generates a HTML output containing a variety of graphical displays illustrating the quality of the data under different aspects. Here we highlight the important modules that require close investigation from FastQC results to assess better the quality of the data. 3.1.1 Sequence Quality “Per base sequence quality” and “Per sequence quality scores” are the two important plots that has to be paid attention to understand the sequencing quality of the data. The first plot is a boxplot illustrating the average, median, and distribution of quality per base from all the reads in the data (Fig. 1a). As Illumina’s sequencing chemistry is sequencing per cycle approach, at each cycle one base of all reads are sequenced, thus the 36 cycle kit yields all reads with 36 bp length. While more recent versions of Illumina sequencing kits generate much longer reads (50 to even 150 bp length); a FastQC analysis will still be helpful to identify potential abnormalities during the sequencing process. Open image in new windowFig. 1 Fig. 1 (a) FastQC generated “Per base sequence quality” plot for GSM733720 NHEK (left panel) and GSM733737 HepG2 datasets (right panel). (b) FastQC generated “Sequence Duplication Level” for both datasets as in (a). Both the data show similar harmless level of clonal reads Hence it is expected to have dip or hike in quality at or from a particular base on in most of the sequences due to technical issues. This pattern can be easily observed in “Per base sequence quality” box plot with average and distribution of quality per base from all reads. It is important to remember that due to the reagents “burning” out over the period, quality of bases towards the end of the reads result in the gradual decrease of average quality in the plot. While an average view of the read sequences provides a first way to evaluate sequence quality, the presence of reads with poor quality levels will affect the alignment accuracy to the reference genome (false or multiple alignment outcomes). One can address this problem by using the “Per sequence quality scores” plot which shows the distribution of each read average quality to get an idea about altogether bad reads. Unlike the “per base distribution” approach, the analysis in a “per sequence quality” context allows to estimate the fraction of reads with low quality. It is important to note from Fig. 1a, that the NHEK dataset presents an overall poor sequencing quality, which is also reflected by the lower percentage of mapped reads in comparison to that observed for HepG2 datasets (Table 1). Note that tools like FASTX-toolkit [http://hannonlab.cshl.edu/fastx_toolkit/], NGSQC-toolkit [8], or SeqQC [http://genotypic.co.in/Products/7/Seq-QC.aspx] are dedicated to trim and filter reads presenting low quality scores (see Note 2). 3.1.2 Clonal Reads “Sequence duplication level” is important aspect to understand the quality of data rather than the sequencing itself. When there is low amount of starting material to start with or by over-amplifying the fragments during library preparation could lead to high levels of sequence duplication. As clonal reads bias the analysis by over-representing the same fragment, they can generate false-positive enrichments. Hence, it is highly recommended to remove duplicate reads and keep only one copy. Also it is important to point out that the currently existing methods to detect clonal reads from single-end sequencing assays may overestimate their presence. Clonal reads are defined as reads aligned with the same start and end position; indeed an analysis in which the two reads generated by paired-end sequencing from each DNA were treated separately as single reads demonstrated that reads considered as clonal in a single-end sequencing context can have different alignment coordinates in the corresponding pair-read analysis (data not shown). It is highly recommended to remove clonal reads after alignment as sequencing errors could bias the similarity estimation among reads when it is carried out prior alignment. Hence, the FastQC sequence duplication level can be used for a rough assumption only, as it is done with raw data and only the first million reads are considered for the calculation. This deviation can be observed between FastQC sequences duplication level from Fig. 1b with the actual unique reads count from Table 1 (see Note 3). 3.1.3 Adapter Contamination When fragmented DNA molecules are shorter than the applied sequencing length, DNA sequencing will continue over the adapter sequence at the end of the process. This is rare as most of the chromatin immunoprecipitated DNA is in average larger than several hundreds of nucleotides but when it happens (e.g., over-sonication; sequencing library primer-dimer contamination) it can be detected by the counts of “Overrepresented sequences.” If there are bulk of reads with adapter contamination, tools like Cutadapt [9], FASTX-toolkit, NGSQC-toolkit, or SeqQC can be used to trim the adapter sequence but retain the actual DNA sequence. 3.2 Exploring the NGS-QC Generator Portal NGS-QC can be accessed through the portal www.ngs-qc.org, which gives access to different elements of NGS-QC method. Among the various components available in the main page we can cite the direct access to the customized Galaxy instance hosting the NGS-QC Generator tool, the access to the NGS-QC database as well as to a detailed tutorial describing the principles in use for assessing quality scores and their interpretation in the context of the enrichment behavior (Fig. 2). Open image in new windowFig. 2 Fig. 2 Home page of ngs-qc.org website. Notice that as part of the main display, access to the NGS-QC Generator tool and to the dedicated database is available. Furthermore, an access to the database content statistics is illustrated such that visitors could have a glance over the variety of the publicly available datasets currently hosted by this website 3.3 Accessing to the NGS-QC Generator Tool via the Dedicated Galaxy Instance 3.3.1 An Overview of the Galaxy Interface Galaxy [10, 11, 12] is an open, Web-based platform for biological data analysis. It enables to run bioinformatics tools in a user-friendly point-and-click environment. The NGS-QC Generator is available through a local Galaxy server reachable at http://galaxy.ngs-qc.org/. It is worth to mention that any user requires to create a login account such that a dedicated space is allocated to his jobs in the NGS-QC Generator associated server. Once logged in, users can access to the main Galaxy interface composed of three sections (as illustrated in Fig. 3): Open image in new windowFig. 3 Fig. 3 Illustration of a classical Galaxy interface composed by the “tools” panel (left side, demarcated in red), the central panel where tool parameters and results are displayed, and the history panel (right side in purple) containing the datasets available. In this example, the current history panel contains two datasets imported from the “Shared Libraries” 1. A panel containing the list of available tools, grouped by field of analysis (left side). 2. A central panel where the interaction with the selected tools is enable as well as the display of the generated results. 3. A panel containing the history of data, requested jobs and results (right side). It is important to mention that this last panel provides the history of every dataset/job associated to the user, thus a continuous cleaning of its content is a good practice to avoid overloading the allocated space in our servers (see below). Running a tool on Galaxy requires importing the dataset of interest into their history. Locally stored files can be uploaded either through the user’s Web browser, or by using our FTP server; nevertheless we might strongly recommend using the second option in the case of large size datasets (larger than 2Gbs). In the case of first option, locally stored files can be uploaded to Galaxy by clicking on “Get Data” in the tool panel, and select “Upload File”. Either one can drag and drop files from their desktop file manager to the Galaxy interface, or select files from the local disk by clicking the “Choose local file” button. Once selected, click the “Start” button to start the upload. To upload files by FTP, a connection to our FTP server with an FTP client is required (e.g., FileZilla [https://filezilla-project.org/]; an open source and cross-platform tool). In the FTP client, the host has to be given as “galaxy.ngs-qc.org” and users should use their Galaxy credentials for login (Fig. 4). Once the files are uploaded, they can be imported into the history by clicking on “Get Data”, “Upload File”, and “Choose FTP file” on the Galaxy left panel. Datasets successfully imported will be shown in green color box in the history panel (right side). Open image in new windowFig. 4 Fig. 4 Screenshot of the FileZilla interface. An FTP connection can be established by filling the “Host” field with “galaxy.ngs-qc.org” and the Galaxy user/password in the “Username”/”Password” fields, respectively, then clicking the “Quickconnect” button. Dragging files from the left-side panel and dropping them onto the right-side panel will start the upload Though our Galaxy server is equipped with high computing capacity and storage space for handling multiple users, we cannot keep each and every dataset uploaded. For this reason, files, which are older than a month will be automatically removed from Galaxy. In case users are running out of space, they can reduce their disk usage by permanently deleting datasets they do not need anymore. This is a two steps action: delete the unwanted dataset by clicking the “Delete” (“X”) button in the history panel, then click on the history gear icon and click on “Purge deleted datasets”. The NGS-QC Galaxy instance is currently hosting a variety of tools generated by the Galaxy community (Text manipulation, Operate on Genomic Intervals) but also two dedicated tools for the assessment of Quality descriptors for ChIP-sequencing and enrichment related datasets, available under the “NGS: QC” group in the tool panel: 1. The NGS-QC Generator computes global and local quality descriptors by performing multiple random sub-samplings of the mapped reads retrieved in the provided dataset, to then use them for reconstructing and comparing their enrichment patterns with the original profile. 2. The Local QCs viewer allows users to visualize the read count intensity levels retrieved in their datasets in the context of the computed local quality descriptors. Regions to display can be chosen by the user either by (a) gene names, (b) genomic coordinates or (c) let the tool to select these regions based on local QC score ranking. 3.3.2 Performing a Global Quality Control Analysis As mentioned earlier, we are going to run NGS-QC Generator on NHEK and HepG2 datasets, available in the “Shared libraries”. To use these datasets from shared libraries, users have to import these datasets into their history panel as following: 1. Click “Shared Data” at the top of the page; 2. Navigate to “Data libraries”, “Examples”; 3. Select both “NHEK” and “HepG2” datasets; 4. Finally select “Import to current history” in the “For selected datasets” menu; click the “Go” button, and go back to the main page by clicking on “Analyze Data”. Now the datasets are into the history, we can run the NGS-QC Generator: 1. Click on “NGS-QC Generator” under “NGS: QC” in the tool panel 2. In the central panel, select one or more (batch mode) datasets, and the genome assembly; in this case hg19. The following options are also available: (a) Generate up to three technical replicates, to evaluate the results variability. (b) Remove the clonal reads (potential PCR duplicates; by default it is not removed). (c) Exclude background noise (users can switch-off this option if interested in assessing its influence in QC scoring; but by default it is active). (d) Select the resolution for the read-counts intensity representation of displayed genomic regions. (e) Select the sample’s antibody target. If the user provides the target identity, the output report will contain a scatter-plot showing a comparison of the evaluated dataset’s quality with those retrieved in the NGS-QC collection. 3. Click the “Execute” button to launch the job. For each run the NGS-QC Generator produces two outputs which are made available in the history panel. The first one is an HTML page linking to the PDF report that summarizes the quality descriptors, and a ZIP archive containing several supplementary files. A NGS-QC Generator report (Fig. 5) is composed by three major elements: Open image in new windowFig. 5 Fig. 5 First page of the NGS-QC report for the HepG2 dataset (GSM733737; replicate 1). On the upper right side of the page, the global quality stamp is displayed along with a colored bar representing the proportion of unique reads(URs) in the dataset 1. Dataset information where characteristics of the dataset under analysis like the total mapped reads, the fraction of unique mapped reads (i.e., without the clonal sequences), reads’ length size, the genome assembly, and the target molecule (when available) are indicated. 2. QC parameters where a detailed overview of the various parameters used for generating the corresponding QC report are listed such that it could be reproduced if required. 3. QC results where a compilation of the global quality indicators assessed in the context of the considered mapped reads are displayed (either with or without clonal reads). The result panel is complemented with a scatter-plot displaying the read counts per genomic region (500 bp bin; x-axis) in comparison to the recovered counts after multiple reads’ random subsampling (90 %, 70 %, and 50 % subsampled reads; y-axis). Furthermore a global QC certification score (from “AAA” to “DDD” for designating from high and low quality datasets) is stamped over the main page of the QC report such that the quality of the analyzed dataset is expressed in a rather intuitive manner without the need of getting deep into the assessed quality scores. Each QC report is further complemented with series of genomic regions displays illustrating the enrichment patterns associated to the dataset under analysis complemented by the demarcation of genomic areas presenting high enrichment robustness (local QC indicators; heatmap display). In addition, when the target molecule identity is provided, a scatter-plot displaying the total mapped reads versus the quality descriptors assessed for entries available in the NGS-QC database in comparison with evaluated dataset is included at the end of the QC report (Fig. 6). Open image in new windowFig. 6 Fig. 6 Quality scores (y-axis) relative to their total mapped reads (x-axis) assessed for several public datasets (blue) in comparison to the dataset under evaluation (red). In the left panel this analysis has been performed for the HepG2 dataset; while in the right side it is illustrated for the NHEK dataset. This type of displays are included int he last page of the QC-report when the target molecule identity is provided In addition to the QC report a set of supplementary files are generated (ZIP archive format). This supplementary folder contains further genomic regions display and local QC regions in either wiggle or BED format files such that they can be uploaded into a genome browsers like IGB [13] or on the UCSC Genome Browser [14]. Furthermore, a text file, referred here as “local Qci file”, containing all genomic regions presenting enriched regions with high robustness (dRCI < 10 %) is also included in this supplementary file. This last file is also created in the current history, since it is required to generate multiple genomic regions displays (see Subheading 3.3.3). 3.3.3 Visualize Local Enrichment Patterns in the Context of Their Quality Considering the interest of users to visualize genomic regions as a way to confirm the potential low or high quality of the dataset under study, we have developed a convenient way to display them together with a demarcation enriched regions based on their robustness or local QC indicators score. For it, our new tool called “local QCs viewer”, takes alignment file (BED or BAM format) and the previously generated local QCi file as input to produce up to 25 genomic regions displays (PDF report format), with three selection strategies. By default, regions to be displayed are selected based on the local QC scores, the read count intensities and the presence of genes near a highly enriched region. Users have also the possibility to provide a list of genes or genomic positions as a way to customize these displays. As performed for the QC reports, genomic regions display enrichment patterns associated to the dataset under analysis complemented by the demarcation of genomic areas presenting high enrichment robustness (local QC indicators; heatmap display) (Fig. 7). Open image in new windowFig. 7 Fig. 7 Comparison of three genomic regions from HepG2 (top) and NHEK (bottom). Regions were selected using the RASL11A, WRAP53, and ZNF37A genes. Each plots contains three parts: (1) top, the representation of the read-counts intensity, (2) center, the position of the local regions with a dispersion between 0 % (yellow) and 10 % (black) and the genes’ positions. The HepG2 plots present conserved local qc regions with highly peak intensity. On the contrary, NFEK plots are poor in enriched regions 3.4 Exploring the NGS-QC Generator Database 3.4.1 A General Overview In addition to the access to NGS-QC Generator tool, users can retrieve a large collection of quality indicators computed for a variety of publicly available datasets in the dedicated website. To access it, users have to select either “Database” on the top navigation bar or “NGS-QC Database” on the main page of the NGS-QC portal. There are two ways to browse our results; either by using the proposed query panel or through the interactive boxplots table located below. The query panel (Fig. 8a) allows specifying the request by multiple options like the model organism, target molecule, quality grades and also a public identifier from GEO database (GSM or GSE) or from ENCODE consortium (wgEncode). Importantly, each of these query options can be used in combinations. For example users can query for all the qualified datasets corresponding to mouse (Mus musculus) and human (Homo sapiens) model systems targeting the histone modification mark H3K4me3 and presenting quality grades between “A” and “B” (Fig. 8). Open image in new windowFig. 8 Fig. 8 Display illustrating the database page showing the search panel (top) and boxplots/violin plots table (bottom) The boxplot table displays quality scores (QC-Stamp; dRCI < 10 %) distribution assessed over the whole database content (currently more than 21,000 datasets) as well as the discretized QC-STAMP intervals (from A to D). Furthermore, the quality score distribution per target molecule are displayed such that users might have a global overview of their associated quality scores. It is worth to mention that in addition to display each of these distributions under a boxplot format, a violin plot is also displayed such that a more detailed view of the distribution of population is available. Each target molecule in boxplot is complemented by a legend indicating its identity as well as the number of entries available (target molecules represented by less than 10 datasets are categorized as “Others”). By clicking on any of the target molecule boxplots, users are redirected to the results panel associated to that target molecule such that further refinement queries could be performed if required. For instance by clicking on the boxplot associated to histone modification mark H3K4me3, users are redirected to the results page where the QC scores for the datasets associated to this target are displayed. The results page (Fig. 9) is composed of five elements: Open image in new windowFig. 9 Fig. 9 Display illustrating the results obtained after performing a query in the NGS-QC database. (a) scatter-plot displaying the QC-indicators relative to the total mapped reads. (b) Boxplot/vioplots displaying the different target molecule retrieved on the query. (c) Results table including several additional information for each dataset (right panel) and the refinement panel (left panel) 1. Query panel displaying the current request made. 2. Boxplot table displaying the QC scores distribution for each of the targets included in the request. 3. a Scatter-plot displaying the quality scores (QC-stamp) for each dataset in the context of their total mapped reads (TMRs). 4. Results table presenting an important number of information for each dataset retrieved. 5. Refinement panel providing further query options to be applied over the initial request to refine the results to specific interest. All the described elements are meant to provide complementary information to the user, as well as means to narrow down the mining to users’ specific interest. For instance, when multiple targets are part of the initial query, the boxplot panel provides the possibility to select one of them (Fig. 9b). Furthermore, the scatter-plot display provides a powerful way to visualize the quality scores associated to each dataset in the context of their related TMRs. For instance, in the context of the query targeting the histone modification marks H3K27me3, the user can pass the pointer over the scatter-plot to have a pop-up displaying element like the dataset identifier, its associated TMRs, the target identity, the model organism and its computed quality grade (Fig. 10). Considering that the quality scores are computed at three different read count intensity dispersion conditions (dRCI), each of them are summarized in one of the letters corresponding to the QC-STAMP grade (e.g., A, A and A—represented in short AAA—for dRCI < 2.5 %, <5 % and <10 % respectively). In this context, users can switch the displays among all three corresponding scatter-plots using the buttons located on the top of the panel. Furthermore, users can zoom on each scatter-plot to perform for further refinement of the results for a desired group of datasets. To do so, users might select the “Zoom” icon located on the right top corner of the plot and then press and hold mouse button to select the region. Alternatively, zooming on the scatter-plot is possible by scrolling the wheel mouse button. Open image in new windowFig. 10 Fig. 10 Scatter-plot illustrating the quality scores computed for several H3K27me3 generated datasets in the context of their total mapped reads. The identity of one of these datasets presenting more than 70 million TMRs are highlighted as done in the NGS-QC Generator website The results table (Fig. 9c), located under the scatter-plot provides further information for each of the datasets retrieved like public identifier (ID); study ID, associated publication ID (PMID), model organism, data type, target molecule, TMRs complemented by the fraction of unique mapped reads (i.e., by excluding the clonal reads), global quality grades (QC-Stamps) and a Global QC indicators report available in a pdf format. Further supplementary information is available under a ZIP file format available for downloading. Taken in consideration that users might systematically wish to associate a qualified dataset to an article describing it, users can access to the abstract of the related publication (when available) by placing the mouse pointer over the corresponding PMID. Finally, some times the initial query might include too many outputs, such that users might wish to focus their view on a defined subset; thus a separate refinement panel is provided at the left of the results table (Fig. 9c). Importantly the refinement panel is composed by a list of 13 elements aiming at providing a large flexibility for this task such as Data type, target molecule, model organism, sequencing platform, TMRs, submission date, Abstract content, Author and Public ID. Other elements like the Cell line/tissue, the potential cell/tissue treatment or the antibody reference and batch available under a nonacademic access (see below). 3.4.2 The NGS-QC Database as a Source of Information for Providing Potential Hints to Interpret ChIP-Sequencing Assays with Low Quality Performances An important application of the database is to provide potential hints to understand the reasons for the low quality grade associated to certain ChIP-seq or related datasets. To illustrate this aspect the user could query for the dataset “GSM733754” as described in the Subheading 3.4.1. In result, two entries associated to the histone modification mark “H3K27me3” are retrieved from the database, which correspond to two replicates retrieved under the same unique identifier (in general a unique ID is associated to a single dataset; but in some cases GEO entries include replicates under the same ID). Surprisingly, the computed quality grades are significantly different between these two entries, suggesting that though they are attributed as replicates, technical differences in preparation or sequencing could have led to these differences. In fact the dataset presenting “CBB” quality grade has 26 million TMRs, whereas “DDD” QC-stamp has only 9.2 million. This difference in the total mapped reads among datasets can at least partially explain their difference in their quality grades and strongly suggest that replicate datasets might also be sequenced at similar depths to avoid such potential sources of quality differences. To further illustrate the use of the NGS-QC database, user could perform a query targeting all entries associated to the histone modification mark “H3K27me3” for the human model system. This time the scatter-plot is populated with more than 500 datasets in which a direct correlation between the quality grades and the TMRs per dataset can be observed. From this observed pattern, for instance it is possible to infer a minimal sequencing depth from which the majority of the retrieved datasets might present high quality grades (e.g., for TMRs higher than 30 million, most of the datasets are “CCC” or higher). It is important to mention that even for datasets generated with high TMRs levels, significant differences in quality scores can be observed. For instance, as illustrated in Fig. 10, datasets generated with more than 70 million reads could still present “CCC” quality grades. 3.4.3 Identifying ChIP-seq Grade Antibodies from the NGS-QC Database Content While this in silico approach could provide guidelines concerning the sequencing-depth in use for a given factor, it is not guaranteed that all higher sequencing-depth experiment will be successful. In fact several other parameters like the antibody in use, the cell line/tissue under analysis, as well as the performance of the experimenter are crucial factors in ChIP-seq pipeline that could have influence in the quality. To illustrate this important aspect, users might query for datasets generated with the histone modification mark H3K9me3 in the context of human studies (Fig. 11). Then, by taken advantage of the zoom option available in the results scatter-plot we can refine the query to datasets presenting quality grades of “A”. Finally, the refined output can be sorted based on TMRs using the results table such that high quality datasets (AAA) generated with the least number of TMRs might appear on the top of the result table panel. At this point, users might be interested on retrieving the Antibody source that gave rise to such optimal results, this is indeed possible by accessing the corresponding public repository page in GEO database by using the link embedded in the public ID displayed in the results table. Additionally, we are currently annotating the antibody source per QC-certified datasets in the NGS-QC database, such that users might directly access to this information through the previously described refinement panel. Open image in new windowFig. 11 Fig. 11 Example illustrating the procedure by which the NGS-QC database can be used for retrieving the antibody in use for high quality grades datasets. A refined query has been performed for retrieving high quality datasets associated to the histone mark H3K9me3; followed by the identification of the best dataset (“AAA” QC grade with the least TMRs). Finally the corresponding GSM ID link has been used to explore over the original information retrieved in GEO",QualityControl,"ngsqc generator  quality control system  chipseq  relate deep sequencinggenerated datasets
preliminary sequence quality assessment  step   analysis  experiment quality base  enrichment   ngsqc generator   important  check  quality   dna sequence    factor  directly influence  result  currently use fastqc   evaluate  dna sequence quality  potential contaminations fastqc aim  provide  simple way    quality control check  raw sequence data come  high throughput sequence pipelines prior  alignment    use  simple gui interface   commandline  easy batch process   tool  handle different format like fastq sam  bam file fastqc generate  html output contain  variety  graphical display illustrate  quality   data  different aspects   highlight  important modules  require close investigation  fastqc result  assess better  quality   data  sequence quality “per base sequence quality”  “per sequence quality scores”   two important plot     pay attention  understand  sequence quality   data  first plot   boxplot illustrate  average median  distribution  quality per base    read   data fig   illuminas sequence chemistry  sequence per cycle approach   cycle one base   read  sequence thus   cycle kit yield  read    length   recent versions  illumina sequence kit generate much longer read   even   length  fastqc analysis  still  helpful  identify potential abnormalities   sequence process open image  new windowfig  fig   fastqc generate “per base sequence quality” plot  gsm733720 nhek leave panel  gsm733737 hepg2 datasets right panel  fastqc generate “sequence duplication level”   datasets      data show similar harmless level  clonal read hence   expect   dip  hike  quality     particular base      sequence due  technical issue  pattern   easily observe  “per base sequence quality” box plot  average  distribution  quality per base   read   important  remember  due   reagents “burning”    period quality  base towards  end   read result   gradual decrease  average quality   plot   average view   read sequence provide  first way  evaluate sequence quality  presence  read  poor quality level  affect  alignment accuracy   reference genome false  multiple alignment outcomes one  address  problem  use  “per sequence quality scores” plot  show  distribution   read average quality  get  idea  altogether bad read unlike  “per base distribution” approach  analysis   “per sequence quality” context allow  estimate  fraction  read  low quality   important  note  fig    nhek dataset present  overall poor sequence quality   also reflect   lower percentage  map read  comparison   observe  hepg2 datasets table  note  tool like fastxtoolkit  ngsqctoolkit   seqqc   dedicate  trim  filter read present low quality score see note   clonal read “sequence duplication level”  important aspect  understand  quality  data rather   sequence     low amount  start material  start    overamplifying  fragment  library preparation could lead  high level  sequence duplication  clonal read bias  analysis  overrepresenting   fragment   generate falsepositive enrichments hence   highly recommend  remove duplicate read  keep  one copy also   important  point    currently exist methods  detect clonal read  singleend sequence assay may overestimate  presence clonal read  define  read align    start  end position indeed  analysis    two read generate  pairedend sequence   dna  treat separately  single read demonstrate  read consider  clonal   singleend sequence context   different alignment coordinate   correspond pairread analysis data  show   highly recommend  remove clonal read  alignment  sequence errors could bias  similarity estimation among read    carry  prior alignment hence  fastqc sequence duplication level   use   rough assumption       raw data    first million read  consider   calculation  deviation   observe  fastqc sequence duplication level  fig    actual unique read count  table  see note   adapter contamination  fragment dna molecules  shorter   apply sequence length dna sequence  continue   adapter sequence   end   process   rare     chromatin immunoprecipitated dna   average larger  several hundreds  nucleotides    happen  oversonication sequence library primerdimer contamination    detect   count  “overrepresented sequences”    bulk  read  adapter contamination tool like cutadapt  fastxtoolkit ngsqctoolkit  seqqc   use  trim  adapter sequence  retain  actual dna sequence  explore  ngsqc generator portal ngsqc   access   portal wwwngsqcorg  give access  different elements  ngsqc method among  various components available   main page   cite  direct access   customize galaxy instance host  ngsqc generator tool  access   ngsqc database  well    detail tutorial describe  principles  use  assess quality score   interpretation   context   enrichment behavior fig  open image  new windowfig  fig  home page  ngsqcorg website notice   part   main display access   ngsqc generator tool    dedicate database  available furthermore  access   database content statistics  illustrate   visitors could   glance   variety   publicly available datasets currently host   website  access   ngsqc generator tool via  dedicate galaxy instance   overview   galaxy interface galaxy      open webbased platform  biological data analysis  enable  run bioinformatics tool   userfriendly pointandclick environment  ngsqc generator  available   local galaxy server reachable     worth  mention   user require  create  login account    dedicate space  allocate   job   ngsqc generator associate server  log  users  access   main galaxy interface compose  three section  illustrate  fig  open image  new windowfig  fig  illustration   classical galaxy interface compose   “tools” panel leave side demarcate  red  central panel  tool parameters  result  display   history panel right side  purple contain  datasets available   example  current history panel contain two datasets import   “shared libraries”   panel contain  list  available tool group  field  analysis leave side   central panel   interaction   select tool  enable  well   display   generate result   panel contain  history  data request job  result right side   important  mention   last panel provide  history  every datasetjob associate   user thus  continuous clean   content   good practice  avoid overload  allocate space   servers see  run  tool  galaxy require import  dataset  interest   history locally store file   upload either   users web browser   use  ftp server nevertheless  might strongly recommend use  second option   case  large size datasets larger  2gbs   case  first option locally store file   upload  galaxy  click  “get data”   tool panel  select “upload file” either one  drag  drop file   desktop file manager   galaxy interface  select file   local disk  click  “choose local file” button  select click  “start” button  start  upload  upload file  ftp  connection   ftp server   ftp client  require  filezilla   open source  crossplatform tool   ftp client  host    give  “galaxyngsqcorg”  users  use  galaxy credentials  login fig    file  upload    import   history  click  “get data” “upload file”  “choose ftp file”   galaxy leave panel datasets successfully import   show  green color box   history panel right side open image  new windowfig  fig  screenshot   filezilla interface  ftp connection   establish  fill  “host” field  “galaxyngsqcorg”   galaxy userpassword   “username””password” field respectively  click  “quickconnect” button drag file   leftside panel  drop  onto  rightside panel  start  upload though  galaxy server  equip  high compute capacity  storage space  handle multiple users  cannot keep   every dataset upload   reason file   older   month   automatically remove  galaxy  case users  run   space   reduce  disk usage  permanently delete datasets    need anymore    two step action delete  unwanted dataset  click  “delete” “” button   history panel  click   history gear icon  click  “purge delete datasets”  ngsqc galaxy instance  currently host  variety  tool generate   galaxy community text manipulation operate  genomic intervals  also two dedicate tool   assessment  quality descriptors  chipsequencing  enrichment relate datasets available   “ngs ” group   tool panel   ngsqc generator compute global  local quality descriptors  perform multiple random subsamplings   map read retrieve   provide dataset   use   reconstruct  compare  enrichment pattern   original profile   local qcs viewer allow users  visualize  read count intensity level retrieve   datasets   context   compute local quality descriptors regions  display   choose   user either   gene name  genomic coordinate   let  tool  select  regions base  local  score rank  perform  global quality control analysis  mention earlier   go  run ngsqc generator  nhek  hepg2 datasets available   “shared libraries”  use  datasets  share libraries users   import  datasets   history panel  follow  click “shared data”   top   page  navigate  “data libraries” “examples”  select  “nhek”  “hepg2” datasets  finally select “import  current history”   “ select datasets” menu click  “” button   back   main page  click  “analyze data”   datasets    history   run  ngsqc generator  click  “ngsqc generator”  “ngs ”   tool panel    central panel select one   batch mode datasets   genome assembly   case hg19  follow options  also available  generate   three technical replicate  evaluate  result variability  remove  clonal read potential pcr duplicate  default    remove  exclude background noise users  switchoff  option  interest  assess  influence   score   default   active  select  resolution   readcounts intensity representation  display genomic regions  select  sample antibody target   user provide  target identity  output report  contain  scatterplot show  comparison   evaluate datasets quality   retrieve   ngsqc collection  click  “execute” button  launch  job   run  ngsqc generator produce two output   make available   history panel  first one   html page link   pdf report  summarize  quality descriptors   zip archive contain several supplementary file  ngsqc generator report fig   compose  three major elements open image  new windowfig  fig  first page   ngsqc report   hepg2 dataset gsm733737 replicate    upper right side   page  global quality stamp  display along   color bar represent  proportion  unique readsurs   dataset  dataset information  characteristics   dataset  analysis like  total map read  fraction  unique map read  without  clonal sequence read length size  genome assembly   target molecule  available  indicate   parameters   detail overview   various parameters use  generate  correspond  report  list    could  reproduce  require   result   compilation   global quality indicators assess   context   consider map read  display either   without clonal read  result panel  complement   scatterplot display  read count per genomic region   bin xaxis  comparison   recover count  multiple read random subsampling        subsampled read yaxis furthermore  global  certification score  “aaa”  “ddd”  designate  high  low quality datasets  stamp   main page    report    quality   analyze dataset  express   rather intuitive manner without  need  get deep   assess quality score   report   complement  series  genomic regions display illustrate  enrichment pattern associate   dataset  analysis complement   demarcation  genomic areas present high enrichment robustness local  indicators heatmap display  addition   target molecule identity  provide  scatterplot display  total map read versus  quality descriptors assess  entries available   ngsqc database  comparison  evaluate dataset  include   end    report fig  open image  new windowfig  fig  quality score yaxis relative   total map read xaxis assess  several public datasets blue  comparison   dataset  evaluation red   leave panel  analysis   perform   hepg2 dataset    right side   illustrate   nhek dataset  type  display  include int  last page   qcreport   target molecule identity  provide  addition    report  set  supplementary file  generate zip archive format  supplementary folder contain  genomic regions display  local  regions  either wiggle  bed format file      upload   genome browsers like igb     ucsc genome browser  furthermore  text file refer   “local qci file” contain  genomic regions present enrich regions  high robustness drci     also include   supplementary file  last file  also create   current history since   require  generate multiple genomic regions display see subheading   visualize local enrichment pattern   context   quality consider  interest  users  visualize genomic regions   way  confirm  potential low  high quality   dataset  study   develop  convenient way  display  together   demarcation enrich regions base   robustness  local  indicators score    new tool call “local qcs viewer” take alignment file bed  bam format   previously generate local qci file  input  produce    genomic regions display pdf report format  three selection strategies  default regions   display  select base   local  score  read count intensities   presence  genes near  highly enrich region users  also  possibility  provide  list  genes  genomic position   way  customize  display  perform    report genomic regions display enrichment pattern associate   dataset  analysis complement   demarcation  genomic areas present high enrichment robustness local  indicators heatmap display fig  open image  new windowfig  fig  comparison  three genomic regions  hepg2 top  nhek bottom regions  select use  rasl11a wrap53  znf37a genes  plot contain three part  top  representation   readcounts intensity  center  position   local regions   dispersion    yellow    black   genes position  hepg2 plot present conserve local  regions  highly peak intensity   contrary nfek plot  poor  enrich regions  explore  ngsqc generator database   general overview  addition   access  ngsqc generator tool users  retrieve  large collection  quality indicators compute   variety  publicly available datasets   dedicate website  access  users   select either “database”   top navigation bar  “ngsqc database”   main page   ngsqc portal   two ways  browse  result either  use  propose query panel    interactive boxplots table locate   query panel fig  allow specify  request  multiple options like  model organism target molecule quality grade  also  public identifier  geo database gsm  gse   encode consortium wgencode importantly    query options   use  combinations  example users  query    qualify datasets correspond  mouse mus musculus  human homo sapiens model systems target  histone modification mark h3k4me3  present quality grade  “”  “” fig  open image  new windowfig  fig  display illustrate  database page show  search panel top  boxplotsviolin plot table bottom  boxplot table display quality score qcstamp drci    distribution assess   whole database content currently    datasets  well   discretized qcstamp intervals     furthermore  quality score distribution per target molecule  display   users might   global overview   associate quality score   worth  mention   addition  display    distributions   boxplot format  violin plot  also display     detail view   distribution  population  available  target molecule  boxplot  complement   legend indicate  identity  well   number  entries available target molecules represent  less   datasets  categorize  “others”  click     target molecule boxplots users  redirect   result panel associate   target molecule    refinement query could  perform  require  instance  click   boxplot associate  histone modification mark h3k4me3 users  redirect   result page    score   datasets associate   target  display  result page fig   compose  five elements open image  new windowfig  fig  display illustrate  result obtain  perform  query   ngsqc database  scatterplot display  qcindicators relative   total map read  boxplotvioplots display  different target molecule retrieve   query  result table include several additional information   dataset right panel   refinement panel leave panel  query panel display  current request make  boxplot table display   score distribution     target include   request   scatterplot display  quality score qcstamp   dataset   context   total map read tmrs  result table present  important number  information   dataset retrieve  refinement panel provide  query options   apply   initial request  refine  result  specific interest   describe elements  mean  provide complementary information   user  well  mean  narrow   mine  users specific interest  instance  multiple target  part   initial query  boxplot panel provide  possibility  select one   fig  furthermore  scatterplot display provide  powerful way  visualize  quality score associate   dataset   context   relate tmrs  instance   context   query target  histone modification mark h3k27me3  user  pass  pointer   scatterplot    popup display element like  dataset identifier  associate tmrs  target identity  model organism   compute quality grade fig  consider   quality score  compute  three different read count intensity dispersion condition drci     summarize  one   letter correspond   qcstamp grade     —represented  short aaa— drci         respectively   context users  switch  display among  three correspond scatterplots use  button locate   top   panel furthermore users  zoom   scatterplot  perform   refinement   result   desire group  datasets    users might select  “zoom” icon locate   right top corner   plot   press  hold mouse button  select  region alternatively zoom   scatterplot  possible  scroll  wheel mouse button open image  new windowfig  fig  scatterplot illustrate  quality score compute  several h3k27me3 generate datasets   context   total map read  identity  one   datasets present    million tmrs  highlight     ngsqc generator website  result table fig  locate   scatterplot provide  information     datasets retrieve like public identifier  study  associate publication  pmid model organism data type target molecule tmrs complement   fraction  unique map read   exclude  clonal read global quality grade qcstamps   global  indicators report available   pdf format  supplementary information  available   zip file format available  download take  consideration  users might systematically wish  associate  qualify dataset   article describe  users  access   abstract   relate publication  available  place  mouse pointer   correspond pmid finally  time  initial query might include  many output   users might wish  focus  view   define subset thus  separate refinement panel  provide   leave   result table fig  importantly  refinement panel  compose   list   elements aim  provide  large flexibility   task   data type target molecule model organism sequence platform tmrs submission date abstract content author  public   elements like  cell linetissue  potential celltissue treatment   antibody reference  batch available   nonacademic access see    ngsqc database   source  information  provide potential hint  interpret chipsequencing assay  low quality performances  important application   database   provide potential hint  understand  reason   low quality grade associate  certain chipseq  relate datasets  illustrate  aspect  user could query   dataset “gsm733754”  describe   subheading   result two entries associate   histone modification mark “h3k27me3”  retrieve   database  correspond  two replicate retrieve    unique identifier  general  unique   associate   single dataset    case geo entries include replicate     surprisingly  compute quality grade  significantly different   two entries suggest  though   attribute  replicate technical differences  preparation  sequence could  lead   differences  fact  dataset present “cbb” quality grade   million tmrs whereas “ddd” qcstamp    million  difference   total map read among datasets   least partially explain  difference   quality grade  strongly suggest  replicate datasets might also  sequence  similar depths  avoid  potential source  quality differences   illustrate  use   ngsqc database user could perform  query target  entries associate   histone modification mark “h3k27me3”   human model system  time  scatterplot  populate     datasets    direct correlation   quality grade   tmrs per dataset   observe   observe pattern  instance   possible  infer  minimal sequence depth    majority   retrieve datasets might present high quality grade   tmrs higher   million    datasets  “ccc”  higher   important  mention  even  datasets generate  high tmrs level significant differences  quality score   observe  instance  illustrate  fig  datasets generate     million read could still present “ccc” quality grade  identify chipseq grade antibodies   ngsqc database content    silico approach could provide guidelines concern  sequencingdepth  use   give factor    guarantee   higher sequencingdepth experiment   successful  fact several  parameters like  antibody  use  cell linetissue  analysis  well   performance   experimenter  crucial factor  chipseq pipeline  could  influence   quality  illustrate  important aspect users might query  datasets generate   histone modification mark h3k9me3   context  human study fig    take advantage   zoom option available   result scatterplot   refine  query  datasets present quality grade  “” finally  refine output   sort base  tmrs use  result table   high quality datasets aaa generate   least number  tmrs might appear   top   result table panel   point users might  interest  retrieve  antibody source  give rise   optimal result   indeed possible  access  correspond public repository page  geo database  use  link embed   public  display   result table additionally   currently annotate  antibody source per qccertified datasets   ngsqc database   users might directly access   information   previously describe refinement panel open image  new windowfig  fig  example illustrate  procedure    ngsqc database   use  retrieve  antibody  use  high quality grade datasets  refine query   perform  retrieve high quality datasets associate   histone mark h3k9me3 follow   identification   best dataset “aaa”  grade   least tmrs finally  correspond gsm  link   use  explore   original information retrieve  geo",8
178,EasyQC,"EasyQC: Tool with Interactive User Interface for Efficient Next-Generation Sequencing Data Quality Control
EasyQC is an open source desktop tool with cross-operating system compatibility, facilitating the QC process of huge NGS data sets with great accuracy. EasyQC includes various modules such as format converter, adapter trimmer, and QC graph generator apart from the pipeline that carries out filtering based on length, quality, ambiguous bases, and homopolymer repeats as well. The key features of EasyQC are described hereunder. 3.1. EasyQC features and pipeline 3.1.1. Format converter. This module of EasyQC mediates the conversion of .fastq files to .fasta files. It verifies the accuracy of the input FASTQ file and converts it to FASTA file upon successful format check. 2 RANGAMARAN ET AL. Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. 3.1.2. Adapter trimmer. Adapter or primer trimming is a vital step in NGS data QC. It is important to remove the forward and reverse adapter sequences from the data set. The adapter trimmer module of EasyQC takes the forward and reverse adapter sequences as input and trims the sequences accordingly. 3.1.3. Paired end merging. Many sequencing protocols that are in use today sequence the DNA fragments from both ends. Merging of these paired end reads is important in genome assembly and other analyses. The paired end merging module of EasyQC uses FLASH (Magoc and Salzberg, 2011) to merge the input FASTQ files. 3.1.4. Graph generator. This feature of EasyQC enables the user to obtain graphical outputs of length distribution, average quality distribution, ATGC composition, and GC content of the input sequence data set. 3.1.5. Quality-based trimming. Removal of low quality sequences is essential, since they can have a significant impact on the downstream analyses of NGS data. This module of EasyQC gets the quality threshold for each base along with the percentage threshold for each sequence as input. Sequences that contain lesser percentage of high-quality bases than the threshold will be removed from the data set. 3.1.6. Length-based trimming. At times, data sets possess sequences with a very wide range of read lengths and it is important to remove those sequences that fall short of the required length. This module trims those sequences that are shorter than the input threshold length value. 3.1.7. Ambiguous-based trimming. Presence of ambiguous bases has significant impact on the subsequent alignment process. This feature fetches the threshold percentage of the ambiguous bases that shall be permitted in a sequence and those with a greater percentage will be removed. FIG. 1. The workflow of EasyQC pipeline. EASYQC: A STANDALONE TOOL FOR NGS QC 3 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. FIG. 2. User interface of EasyQC containing multiple tabs enabling the user to execute required process. (A) QC pipeline that enables the user to set the threshold values for various filtering parameters. (B) Paired end merge interface that facilitates merging of two fastq files based on the user-specified thresholds. (C) The adapter trimmer module that enables the user to remove the adapter sequences from the 50 and 30 ends of the sequence. (D) Graph generator module of EasyQC that provides sequence quality, length and GC distributions, along with base composition graphs. GC, DNA nucleotides; QC, quality control. 4 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. FIG. 3. Results of the trimming pipeline in EasyQC. The graph represents the number of sequences filtered at each stage of the trimming process. 5 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. FIG. 4. Comparison of per base quality of (A) raw reads and (B) reads processed using EasyQC with a quality filter of Q30. The graph shows significant increase in quality of processed reads. 6 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. 3.1.8. Homopolymer trimming. Homopolymer repeats in a sequence shall also cause misalignment of the sequences leading to false results. This module takes the cutoff value for the homopolymers to be present in a sequence and those that exceed this cutoff value will be taken off from the data set. The trimming modules are designed as a single pipeline in EasyQC. The user shall upload the input sequence data set and specify the threshold values of all the modules at the same time and execute the pipeline. 3.2. Input and output of EasyQC EasyQC takes FASTQ files generated from any NGS platform as input for all its modules. The outputs from EasyQC are as hereunder. 3.2.1. Paired end merging. The output FASTQ file containing the stitched paired end sequences based on user input using FLASH gets stored in a new folder with the statistics of the number of sequences that were merged. FIG. 5. Graphical output of an NGS data set obtained from EasyQC. (A) Base composition, (B) sequence quality distribution, (C) GC percentage, and (D) sequence length distribution. NGS, next-generation sequencing. EASYQC: A STANDALONE TOOL FOR NGS QC 7 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. 3.2.2. QC statistics. The QC pipeline of EasyQC provides the statistics of the total sequences that got filtered in each step of the QC processes based on the filter parameters provided by the user (Fig. 3). The output sequences in FASTQ and FASTA format are generated at every step of the pipeline. These sequences can directly be used for downstream analyses. The comparison of quality of raw sequence data set with that processed by EasyQC with a quality cutoff of Q30 was carried out using FastQC, and the results indicated significant increase in the overall quality of the reads (Fig. 4). 3.2.3. Graphical output. The graphs module generates four different graphs as described earlier (Fig. 5) and is stored in a separate folder in the same location from where the input files were uploaded. 3.3. Comparison with existing tools The comparison of various features available in EasyQC with that of other QC tools is described in Table 1. Although many toolkits are available, not many exist as desktop standalones with a user interface. EasyQC provides that interactive facility to the users through its interface with a flexibility to work across Windows and Linux platforms. 3.4. Performance evaluation The performance of EasyQC was validated using various Illumina sequence data sets containing sequences ranging from 0.3 to 2.8 million. These evaluations were carried out on a standalone Dell precision T7600 workstation with Intel Xeon(R) E5-2560 processor and 32GB RAM (operating system: Windows 7). The time taken for the execution of QC pipeline for various sequence data sets is given in Table 2. Comparison of EasyQC with other tools such as NGSQC and PRINSEQ for filtering of sequences based on quality score and length was carried out using a data set containing 2.3 million sequences. This was carried out in HP Z620 Workstation with Intel(R) Xeon(R) CPU E5-2620 v2 processor and 32GB RAM (operating system: Ubuntu v16.04). The quality cutoff was set at Q30, whereas the length threshold was set to 100 bp. In addition, the percentage of high-quality bases per sequence was set at 90 (which is the default value) in EasyQC. Comparison of the post-QC data sets (Fig. 6) indicated that the filtering of sequences by EasyQC was more stringent and there were no significant deviation in the quality distribution as well. The time taken for the QC process was 4 minutes 28 seconds for EasyQC, whereas it took 1 minute 20 seconds Table 1. Comparison of Various Features in EasyQC with Other Quality Control Tools Features EasyQC FastQC ClinQC FASTX-Toolkit NGSQC Toolkit QC-Chain PRINSEQ TagCleaner Format conversion Yes No Yes Yes Yes Yes No No Adapter trimming Yes No Yes Yes Yes Yes Yes Yes Paired-end merging Yes No No No No No No No Quality filtering Yes No No Yes Yes Yes Yes No Length filtering Yes No No Yes Yes No Yes No Ambiguous base filtering Yes No No Yes Yes Yes Yes Yes Homopolymers filtering Yes No No No Yes No No No Graphical output Yes Yes Yes Yes Yes Yes Yes Yes Graphical user interface Yes Yes No No No No No No Table 2. Execution Time of the Quality Control Pipeline of EasyQC with Next-Generation Sequencing Data Sets S. no. Total sequences Time (minutes) 1 3,19,095 2.08 2 10,53,732 5.15 3 20,44,728 9.51 4 28,56,099 14.39 8 RANGAMARAN ET AL. Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. FIG. 6. Comparison of post-QC evaluation of sample data set using EasyQC (A) with PRINSEQ (B) and NGSQC (C). The number of sequences filtered during the QC process from each tool is indicated in the graph (D). 9 Downloaded by Univ Of Western Ontario from www.liebertpub.com at 09/14/18. For personal use only. for NGSQC and 1 minute 40 seconds for PRINSEQ. The relatively higher time taken by EasyQC can be attributed to the execution of percentage of high-quality base filter module, which is not present in the other two tools, in addition to the overall quality filter option. Furthermore, EasyQC provides the output data set containing the sequences that passed and failed in each step of the QC process that adds to its execution time, which is not provided by the tools compared herewith.",QualityControl,"easyqc tool  interactive user interface  efficient nextgeneration sequence data quality control
easyqc   open source desktop tool  crossoperating system compatibility facilitate   process  huge ngs data set  great accuracy easyqc include various modules   format converter adapter trimmer   graph generator apart   pipeline  carry  filter base  length quality ambiguous base  homopolymer repeat  well  key feature  easyqc  describe hereunder  easyqc feature  pipeline  format converter  module  easyqc mediate  conversion  fastq file  fasta file  verify  accuracy   input fastq file  convert   fasta file upon successful format check  rangamaran   download  univ  western ontario  wwwliebertpubcom    personal use   adapter trimmer adapter  primer trim   vital step  ngs data    important  remove  forward  reverse adapter sequence   data set  adapter trimmer module  easyqc take  forward  reverse adapter sequence  input  trim  sequence accordingly  pair end merge many sequence protocols    use today sequence  dna fragment   end merge   pair end read  important  genome assembly   analyse  pair end merge module  easyqc use flash magoc  salzberg   merge  input fastq file  graph generator  feature  easyqc enable  user  obtain graphical output  length distribution average quality distribution atgc composition   content   input sequence data set  qualitybased trim removal  low quality sequence  essential since     significant impact   downstream analyse  ngs data  module  easyqc get  quality threshold   base along   percentage threshold   sequence  input sequence  contain lesser percentage  highquality base   threshold   remove   data set  lengthbased trim  time data set possess sequence    wide range  read lengths    important  remove  sequence  fall short   require length  module trim  sequence   shorter   input threshold length value  ambiguousbased trim presence  ambiguous base  significant impact   subsequent alignment process  feature fetch  threshold percentage   ambiguous base  shall  permit   sequence     greater percentage   remove fig   workflow  easyqc pipeline easyqc  standalone tool  ngs   download  univ  western ontario  wwwliebertpubcom    personal use  fig  user interface  easyqc contain multiple tabs enable  user  execute require process   pipeline  enable  user  set  threshold value  various filter parameters  pair end merge interface  facilitate merge  two fastq file base   userspecified thresholds   adapter trimmer module  enable  user  remove  adapter sequence      end   sequence  graph generator module  easyqc  provide sequence quality length   distributions along  base composition graph  dna nucleotides  quality control  download  univ  western ontario  wwwliebertpubcom    personal use  fig  result   trim pipeline  easyqc  graph represent  number  sequence filter   stage   trim process  download  univ  western ontario  wwwliebertpubcom    personal use  fig  comparison  per base quality   raw read   read process use easyqc   quality filter  q30  graph show significant increase  quality  process read  download  univ  western ontario  wwwliebertpubcom    personal use   homopolymer trim homopolymer repeat   sequence shall also cause misalignment   sequence lead  false result  module take  cutoff value   homopolymers   present   sequence    exceed  cutoff value   take    data set  trim modules  design   single pipeline  easyqc  user shall upload  input sequence data set  specify  threshold value    modules    time  execute  pipeline  input  output  easyqc easyqc take fastq file generate   ngs platform  input    modules  output  easyqc   hereunder  pair end merge  output fastq file contain  stitch pair end sequence base  user input use flash get store   new folder   statistics   number  sequence   merge fig  graphical output   ngs data set obtain  easyqc  base composition  sequence quality distribution   percentage   sequence length distribution ngs nextgeneration sequence easyqc  standalone tool  ngs   download  univ  western ontario  wwwliebertpubcom    personal use    statistics   pipeline  easyqc provide  statistics   total sequence  get filter   step    process base   filter parameters provide   user fig   output sequence  fastq  fasta format  generate  every step   pipeline  sequence  directly  use  downstream analyse  comparison  quality  raw sequence data set   process  easyqc   quality cutoff  q30  carry  use fastqc   result indicate significant increase   overall quality   read fig   graphical output  graph module generate four different graph  describe earlier fig    store   separate folder    location    input file  upload  comparison  exist tool  comparison  various feature available  easyqc      tool  describe  table  although many toolkits  available  many exist  desktop standalones   user interface easyqc provide  interactive facility   users   interface   flexibility  work across windows  linux platforms  performance evaluation  performance  easyqc  validate use various illumina sequence data set contain sequence range     million  evaluations  carry    standalone dell precision t7600 workstation  intel xeonr  processor  32gb ram operate system windows   time take   execution   pipeline  various sequence data set  give  table  comparison  easyqc   tool   ngsqc  prinseq  filter  sequence base  quality score  length  carry  use  data set contain  million sequence   carry    z620 workstation  intelr xeonr cpu   processor  32gb ram operate system ubuntu v16  quality cutoff  set  q30 whereas  length threshold  set     addition  percentage  highquality base per sequence  set      default value  easyqc comparison   postqc data set fig  indicate   filter  sequence  easyqc   stringent     significant deviation   quality distribution  well  time take    process   minutes  second  easyqc whereas  take  minute  second table  comparison  various feature  easyqc   quality control tool feature easyqc fastqc clinqc fastxtoolkit ngsqc toolkit qcchain prinseq tagcleaner format conversion yes  yes yes yes yes   adapter trim yes  yes yes yes yes yes yes pairedend merge yes        quality filter yes   yes yes yes yes  length filter yes   yes yes  yes  ambiguous base filter yes   yes yes yes yes yes homopolymers filter yes    yes    graphical output yes yes yes yes yes yes yes yes graphical user interface yes yes       table  execution time   quality control pipeline  easyqc  nextgeneration sequence data set   total sequence time minutes              rangamaran   download  univ  western ontario  wwwliebertpubcom    personal use  fig  comparison  postqc evaluation  sample data set use easyqc   prinseq   ngsqc   number  sequence filter    process   tool  indicate   graph   download  univ  western ontario  wwwliebertpubcom    personal use   ngsqc   minute  second  prinseq  relatively higher time take  easyqc   attribute   execution  percentage  highquality base filter module    present    two tool  addition   overall quality filter option furthermore easyqc provide  output data set contain  sequence  pass  fail   step    process  add   execution time    provide   tool compare herewith",8
179,FastQ Screen,"FastQ Screen: A tool for multi-genome mapping and quality control
Implementation The program utilises a short read sequence aligner to map FASTQ reads against pre-defined reference genomes. The tool records against which genome or genomes each read maps and summarises the results in graphical and text formats. Operation We coded FastQ Screen in Perl and made use of the CPAN module GD::Graph for the generation of summary bar plots. The software requires a functional version of Bowtie, Bowtie 2 or BWA, and should be run on a Linux-based operating system. Amendments from Version 1 Corrected typographical errors in the Figure 1 caption text. See referee reports REVISED Figure 1. Graphical output from FastQ Screen after mapping a publicly available RNA-Seq sample (SRR5100711) against several reference genomes. Reads either i) mapped uniquely to one genome only (light blue), ii) multi-mapped to one genome only (dark blue), iii) mapped uniquely to a given genome and mapped to at least one other genome (light red), or iv) multi-mapped to a given genome and mapped to at least one other genome (dark red). The reads represented by blue shading are significant since these are sequences that align only to one genome, and consequently, if are observed in an unexpected genome they suggest contamination. Page 3 of 13 F1000Research 2018, 7:1338 Last updated: 17 MAY 2019 FASTQ Screen uses Plotly to enable visualisation of results in a web browser. The tool takes as input a text configuration file and FASTQ files, which are sub-sampled by default to 100,000 reads to reduce running times, and then mapped to a panel of pre-specified genomes.",QualityControl,"fastq screen  tool  multigenome map  quality control
implementation  program utilise  short read sequence aligner  map fastq read  predefined reference genomes  tool record   genome  genomes  read map  summarise  result  graphical  text format operation  cod fastq screen  perl  make use   cpan module gdgraph   generation  summary bar plot  software require  functional version  bowtie bowtie   bwa    run   linuxbased operate system amendments  version  correct typographical errors   figure  caption text see referee report revise figure  graphical output  fastq screen  map  publicly available rnaseq sample srr5100711  several reference genomes read either  map uniquely  one genome  light blue  multimapped  one genome  dark blue iii map uniquely   give genome  map   least one  genome light red   multimapped   give genome  map   least one  genome dark red  read represent  blue shade  significant since   sequence  align   one genome  consequently   observe   unexpected genome  suggest contamination page    f1000research   last update  may  fastq screen use plotly  enable visualisation  result   web browser  tool take  input  text configuration file  fastq file   subsampled  default   read  reduce run time   map   panel  prespecified genomes",8
180,antiSMASH,"antiSMASH: rapid identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genome sequences
File and options input The input front end of the antiSMASH web server allows uploading of sequence files of a variety of types (FASTA, GBK, or EMBL files). Alternatively, a GenBank/RefSeq accession number can be provided, which is used by the web server to automatically obtain the associated file from GenBank. If the user chooses to use a FASTA input file, gene prediction is performed by Glimmer3 (9)—using its long-orfs tool to construct a gene model based on the input sequence itself—or by GlimmerHMM (10) when eukaryotic input data is submitted. Before starting the antiSMASH analysis run, the user can select the gene cluster types he or she wants to search for. Additionally, he can select which of the downstream analysis modules to include. For those users who, e.g. work with proprietary data, a stand-alone version with a Java graphical user interface is available with the same input options as the web version. Finally, expert users may choose to directly run the Python-based pipeline program from the command line in order to batch analyze a larger number of inputs. Detection of secondary metabolite biosynthesis gene clusters Using the HMMer3 tool (http://hmmer.janelia.org/), the amino acid sequence translations of all protein-encoding genes are searched with profile Hidden Markov Models (pHMMs) based on multiple sequence alignments of experimentally characterized signature proteins or protein domains (proteins, protein subtypes or protein domains which are each exclusively present in a certain type of biosynthetic gene clusters). Using both existing pHMMs (5,11–13) and new pHMMs from seed alignments, we constructed a library of models specific for type I, II and III PK, NRP, terpene, lantibiotic, bacteriocin, aminoglycoside/aminocyclitol, beta-lactam, aminocoumarin, indole, butyrolactone, ectoine, siderophore, phosphoglycolipid, melanin and aminoglycoside biosynthesis signature genes. Additionally, we constructed a number of pHMMs specific for false positives, such as the different types of fatty acid synthases which show homology to PKSs. The final detection stage operates a filtering logic of negative and positive pHMMs and their cut-offs. The logic is based on knowledge of the minimal core components of each gene cluster type taken from the scientific literature. The cut-offs were determined by manual studies of the pHMM results when run against the NCBI non-redundant (nr) protein sequence database (ftp://ftp.ncbi.nlm.nih.gov/blast/db). All technical details on the pHMM library and the detection rules are available in Supplementary Tables S1 and S2, respectively. Gene clusters are defined by locating clusters of signature gene pHMM hits spaced within <10 kb mutual distance. To include flanking accessory genes, gene clusters are extended by 5, 10 or 20 kb on each side of the last signature gene pHMM hit, depending on the gene cluster type detected. As a consequence of this greedy methodology, gene clusters that are spaced very closely together may be merged into ‘superclusters’. These gene clusters are indicated in the output as ‘hybrid clusters’; they may either represent a single gene cluster which produces a hybrid compound that combines two or more chemical scaffold types, or they may represent two separate gene clusters which just happen to be spaced very closely together. NRPS/PKS domain architecture analysis NRPS/PKS domain architectures are analyzed (Figure 2) using another pHMM library comprising existing models (8,11–15) as well as newly constructed models specific for NRPS/PKS protein domains and functional/phylogenetic subgroups of these domains (Supplementary Table S3). An external file that holds a picture, illustration, etc. Object name is gkr466f2.jpg Open in a separate window Figure 2. Interactive XHTML visualization of results. The numbers below the banner represent the gene clusters that were detected, the type of which is shown to the left of them at mouse-over. Once a gene cluster has been selected, the ‘Gene cluster description’ tab will display an SVG image with all genes within the approximate gene cluster, with the detected signature genes displayed in red. Locus tags appear on mouse-over, and on clicking a gene a small panel pops up with annotation information and cross-links to other web services. If PKS/NRPS proteins are encoded in the gene cluster, their domain annotations are given in the ‘PKS/NRPS domain annotation’ tab. More detailed domain annotation information and cross-links are provided on mouse-over. In the ‘Predicted core structure’ tab, a prediction of the core chemical structure is given for PKS or NRPS gene clusters based on the predictions displayed below it. All tabs contain a wide range of links to pop-ups which further detail the prediction information. Conserved motifs within key PKS and NRPS domains are also detected using the pHMMs described earlier in the CLUSEAN package (8), and are written to the detailed downloadable EMBL output. PKS/NRPS gene names are annotated according to the domains and domain subtypes that the genes contain (e.g. ‘hybrid NRPS-PKS’, ‘enediyne PKS’, ‘glycopeptide NRPS’, ‘trans-AT PKS’, etc.). Substrate specificity, stereochemistry and final structure predictions Substrate specificity prediction of PKS and NRPS modules, based on the active sites of their respective acyltransferase (AT) and adenylation (A) domains, is performed by various available methods. PKS AT domain specificities are predicted using a 24 amino acid signature sequence of the active site (16), as well as with pHMMs based on the method of Minowa et al. (17), which is also used to predict co-enzyme A ligase domain specificities. NRPS A domain specificities are predicted using both the signature sequence method and the support-vector machines-based method of NRPSPredictor2 (18,19), and using the method of Minowa et al. (17). Finally, all predictions are integrated into a consensus prediction by a majority vote. Ketoreductase domain-based stereochemistry predictions for PKSs (2) are performed as well. An estimate of the biosynthetic order of PKS/NRPS modules is predicted based on PKS docking domain sequence residue matching [for type I modular PKSs, (3)] or assumed colinearity, and a final predicted core chemical structure is generated as a SMILES string (20), i.e. a unique text description of the chemical structure, and visualized in a picture file (Figure 2). To increase the reliability of the core structure prediction, monomers for which there was no consensus in the predictions are represented as generic amino acids or ketides with unspecified R-groups. Secondary metabolite clusters of orthologous groups In order to rapidly annotate the accessory genes surrounding the detected core signature genes in the various types of secondary metabolite biosynthesis gene clusters, we constructed a database of all gene clusters contained in the latest NCBI nt database (15 February 2011). To do so, pHMMs described above were used to detect all secondary metabolite biosynthesis gene cluster signature genes in the nr database. The accession numbers of all hits meeting the described cut-offs were extracted and used to download the corresponding GenPept files. If the taxonomy identifier included ‘bacteria’ or ‘fungi’, the nucleotide source accession number was extracted. The corresponding nucleotide GenBank files were then downloaded as well, and cross-checked for presence of the queried protein accession number. For each nucleotide GenBank file, gene clusters were detected as described above. Amino acid sequences of all genes contained within the gene clusters were written to a FASTA file with headers containing key information, and a summary of all detected gene clusters (nucleotide accession, nucleotide description, cluster number, cluster type, protein accession numbers) was written to a text file. To construct the smCOGs, clustering of all gene cluster proteins was performed using OrthoMCL (21), and consensus annotations were manually assigned based on the frequencies of the five most prevalent annotations of each smCOG in GenBank. For each smCOG, a seed alignment was created from 100 randomly picked sequences using MUSCLE 3.5 (22), and a pHMM of each smCOG was generated based on the conserved core of each alignment (Supplementary Figure S1). Within the antiSMASH software pipeline, the smCOG pHMMs are used for functional annotation of all accessory genes within the gene clusters. After assignment of an smCOG to a gene—based on the highest-scoring pHMM on its sequence above a certain e-value threshold—the predicted protein sequence is aligned to the smCOG seed alignment, and a rough neighbor-joining phylogenetic tree is calculated using FastTree 2 (23) and visualized with TreeGraph 2 (24) (Supplementary Figure S1). ClusterBlast comparative gene cluster analysis Secondary metabolite biosynthesis gene clusters are highly modular, and their genes are transferred frequently from one gene cluster to another during evolution (25,26). Therefore, when trying to obtain a functional understanding of a gene cluster, it is highly beneficial to be able to compare it with (parts of) other gene clusters which show similarity to it and which may have been characterized experimentally. In order to facilitate this, we applied our annotated database of gene clusters to link up protein sequences with their parent gene clusters and create a comparison tool—based on the most recent BLAST+ implementation (27)—which ranks gene clusters by similarity to a queried gene cluster. Clusters are sorted first based on an empirical similarity score S = h + H + s + S + B, in which h is the number of query genes with a significant hit, H is the number of core query genes with a significant hit, s is the number of gene pairs with conserved synteny, S is the number of gene pairs with conserved synteny involving a core gene, and B is a core gene bonus (three points given when at least one core gene has a hit in the subject cluster). If the similarity scores are equal, the hits are subsequently ranked based on the cumulative BlastP bit scores between the gene clusters. This feature enables a rapid assessment of the comparative genomics for each annotated cluster (Figure 3). An external file that holds a picture, illustration, etc. Object name is gkr466f3.jpg Open in a separate window Figure 3. Example of ClusterBlast alignment of gene clusters homologous to the query gene cluster. In this case, the ten best hits to the calcium-dependent antibiotic NRPS gene cluster from Streptomyces coelicolor A3(2) are displayed. Homologous genes (BLAST e-value < 1E-05; 30% minimal sequence identity; shortest BLAST alignment covers over >25% of the sequence) are given the same colors. The ‘select gene cluster alignment’ drop-down menu provides links to one-by-one gene cluster alignments to each gene cluster hit. In the one-by-one gene cluster alignments, PubMed and/or PubChem links are provided for gene clusters associated with a known compound. Genome-wide BLAST and Pfam analysis and prediction of potential unknown secondary metabolite biosynthesis gene cluster types To facilitate further thorough manual genome analysis, antiSMASH has also been linked up to the whole-genome BLAST and Pfam analysis modules from the previously published CLUSEAN framework (8). The CLUSEAN results are integrated into an EMBL output file. Furthermore, as unknown biosynthetic gene cluster types are likely to exist which may be missed by the antiSMASH gene cluster detection module, the Pfam results are also used to predict genomic regions with a high probability of constituting secondary metabolite biosynthesis gene clusters in a more generalized fashion than the signature genes pHMMs method. For this, the genome sequence is converted to a string of predicted Pfam domains which is fed to a hidden Markov model (P. Cimermancic et al., manuscript in preparation) with transitions between a gene cluster state and a rest-of-the-genome state. This model was trained on Pfam domain frequencies from a set of 473 cloned gene clusters (gene cluster state) and from the set of ~1100 genomes currently in the JGI IMG database (rest-of-the-genome state). The result of this analysis is visualized in a PNG graph. Output and visualization All pipeline analysis results are visualized in a user-friendly interactive XHTML page (Figure 2), which can be used to browse through the different gene clusters. For PKS and NRPS gene clusters, the predicted core chemical structures are shown as images. Gene cluster maps are drawn with scalable vector graphics (SVGs), to which interactive on-click and mouse-over functions are added through JavaScript to provide annotation information, pipeline result scores, and BLAST hyperlinks. Detected signature genes on which the gene cluster identification is based are shown in a distinct color. ClusterBlast results are displayed in a similar way, as aligned gene cluster maps in which genes with mutual BLAST hits are given identical colors. Additionally, available at the bottom right of the page, fully annotated EMBL output files provide the user with the additional possibility to browse their genome in a genome browser such as Artemis (28).",Annotation,"antismash rapid identification annotation  analysis  secondary metabolite biosynthesis gene cluster  bacterial  fungal genome sequences
file  options input  input front end   antismash web server allow upload  sequence file   variety  type fasta gbk  embl file alternatively  genbankrefseq accession number   provide   use   web server  automatically obtain  associate file  genbank   user choose  use  fasta input file gene prediction  perform  glimmer3 —using  longorfs tool  construct  gene model base   input sequence —  glimmerhmm   eukaryotic input data  submit  start  antismash analysis run  user  select  gene cluster type    want  search  additionally   select    downstream analysis modules  include   users   work  proprietary data  standalone version   java graphical user interface  available    input options   web version finally expert users may choose  directly run  pythonbased pipeline program   command line  order  batch analyze  larger number  input detection  secondary metabolite biosynthesis gene cluster use  hmmer3 tool   amino acid sequence translations   proteinencoding genes  search  profile hide markov model phmms base  multiple sequence alignments  experimentally characterize signature proteins  protein domains proteins protein subtypes  protein domains    exclusively present   certain type  biosynthetic gene cluster use  exist phmms   new phmms  seed alignments  construct  library  model specific  type    iii  nrp terpene lantibiotic bacteriocin aminoglycosideaminocyclitol betalactam aminocoumarin indole butyrolactone ectoine siderophore phosphoglycolipid melanin  aminoglycoside biosynthesis signature genes additionally  construct  number  phmms specific  false positives    different type  fatty acid synthases  show homology  pkss  final detection stage operate  filter logic  negative  positive phmms   cutoffs  logic  base  knowledge   minimal core components   gene cluster type take   scientific literature  cutoffs  determine  manual study   phmm result  run   ncbi nonredundant  protein sequence database ftpftpncbinlmnihgovblastdb  technical detail   phmm library   detection rule  available  supplementary table    respectively gene cluster  define  locate cluster  signature gene phmm hit space within   mutual distance  include flank accessory genes gene cluster  extend         side   last signature gene phmm hit depend   gene cluster type detect   consequence   greedy methodology gene cluster   space  closely together may  merge  superclusters  gene cluster  indicate   output  hybrid cluster  may either represent  single gene cluster  produce  hybrid compound  combine two   chemical scaffold type   may represent two separate gene cluster   happen   space  closely together nrpspks domain architecture analysis nrpspks domain architectures  analyze figure  use another phmm library comprise exist model   well  newly construct model specific  nrpspks protein domains  functionalphylogenetic subgroups   domains supplementary table   external file  hold  picture illustration etc object name  gkr466f2jpg open   separate window figure  interactive xhtml visualization  result  number   banner represent  gene cluster   detect  type    show   leave    mouseover   gene cluster   select  gene cluster description tab  display  svg image   genes within  approximate gene cluster   detect signature genes display  red locus tag appear  mouseover   click  gene  small panel pop   annotation information  crosslinks   web service  pksnrps proteins  encode   gene cluster  domain annotations  give   pksnrps domain annotation tab  detail domain annotation information  crosslinks  provide  mouseover   predict core structure tab  prediction   core chemical structure  give  pks  nrps gene cluster base   predictions display    tabs contain  wide range  link  popups   detail  prediction information conserve motifs within key pks  nrps domains  also detect use  phmms describe earlier   clusean package    write   detail downloadable embl output pksnrps gene name  annotate accord   domains  domain subtypes   genes contain  hybrid nrpspks enediyne pks glycopeptide nrps transat pks etc substrate specificity stereochemistry  final structure predictions substrate specificity prediction  pks  nrps modules base   active sit   respective acyltransferase   adenylation  domains  perform  various available methods pks  domain specificities  predict use   amino acid signature sequence   active site   well   phmms base   method  minowa      also use  predict coenzyme  ligase domain specificities nrps  domain specificities  predict use   signature sequence method   supportvector machinesbased method  nrpspredictor2   use  method  minowa    finally  predictions  integrate   consensus prediction   majority vote ketoreductase domainbased stereochemistry predictions  pkss   perform  well  estimate   biosynthetic order  pksnrps modules  predict base  pks dock domain sequence residue match  type  modular pkss   assume colinearity   final predict core chemical structure  generate   smile string    unique text description   chemical structure  visualize   picture file figure   increase  reliability   core structure prediction monomers      consensus   predictions  represent  generic amino acids  ketides  unspecified rgroups secondary metabolite cluster  orthologous group  order  rapidly annotate  accessory genes surround  detect core signature genes   various type  secondary metabolite biosynthesis gene cluster  construct  database   gene cluster contain   latest ncbi  database  february     phmms describe   use  detect  secondary metabolite biosynthesis gene cluster signature genes    database  accession number   hit meet  describe cutoffs  extract  use  download  correspond genpept file   taxonomy identifier include bacteria  fungi  nucleotide source accession number  extract  correspond nucleotide genbank file   download  well  crosschecked  presence   query protein accession number   nucleotide genbank file gene cluster  detect  describe  amino acid sequence   genes contain within  gene cluster  write   fasta file  headers contain key information   summary   detect gene cluster nucleotide accession nucleotide description cluster number cluster type protein accession number  write   text file  construct  smcogs cluster   gene cluster proteins  perform use orthomcl   consensus annotations  manually assign base   frequencies   five  prevalent annotations   smcog  genbank   smcog  seed alignment  create   randomly pick sequence use muscle     phmm   smcog  generate base   conserve core   alignment supplementary figure  within  antismash software pipeline  smcog phmms  use  functional annotation   accessory genes within  gene cluster  assignment   smcog   gene—based   highestscoring phmm   sequence   certain evalue threshold— predict protein sequence  align   smcog seed alignment   rough neighborjoining phylogenetic tree  calculate use fasttree    visualize  treegraph   supplementary figure  clusterblast comparative gene cluster analysis secondary metabolite biosynthesis gene cluster  highly modular   genes  transfer frequently  one gene cluster  another  evolution  therefore  try  obtain  functional understand   gene cluster   highly beneficial   able  compare   part   gene cluster  show similarity     may   characterize experimentally  order  facilitate   apply  annotate database  gene cluster  link  protein sequence   parent gene cluster  create  comparison tool—based    recent blast implementation — rank gene cluster  similarity   query gene cluster cluster  sort first base   empirical similarity score                 number  query genes   significant hit    number  core query genes   significant hit    number  gene pair  conserve synteny    number  gene pair  conserve synteny involve  core gene     core gene bonus three point give   least one core gene   hit   subject cluster   similarity score  equal  hit  subsequently rank base   cumulative blastp bite score   gene cluster  feature enable  rapid assessment   comparative genomics   annotate cluster figure   external file  hold  picture illustration etc object name  gkr466f3jpg open   separate window figure  example  clusterblast alignment  gene cluster homologous   query gene cluster   case  ten best hit   calciumdependent antibiotic nrps gene cluster  streptomyces coelicolor   display homologous genes blast evalue    minimal sequence identity shortest blast alignment cover     sequence  give   color  select gene cluster alignment dropdown menu provide link  onebyone gene cluster alignments   gene cluster hit   onebyone gene cluster alignments pubmed andor pubchem link  provide  gene cluster associate   know compound genomewide blast  pfam analysis  prediction  potential unknown secondary metabolite biosynthesis gene cluster type  facilitate  thorough manual genome analysis antismash  also  link    wholegenome blast  pfam analysis modules   previously publish clusean framework   clusean result  integrate   embl output file furthermore  unknown biosynthetic gene cluster type  likely  exist  may  miss   antismash gene cluster detection module  pfam result  also use  predict genomic regions   high probability  constitute secondary metabolite biosynthesis gene cluster    generalize fashion   signature genes phmms method    genome sequence  convert   string  predict pfam domains   feed   hide markov model  cimermancic   manuscript  preparation  transition   gene cluster state   restofthegenome state  model  train  pfam domain frequencies   set   clone gene cluster gene cluster state    set  ~ genomes currently   jgi img database restofthegenome state  result   analysis  visualize   png graph output  visualization  pipeline analysis result  visualize   userfriendly interactive xhtml page figure     use  browse   different gene cluster  pks  nrps gene cluster  predict core chemical structure  show  image gene cluster map  draw  scalable vector graphics svgs   interactive onclick  mouseover function  add  javascript  provide annotation information pipeline result score  blast hyperlinks detect signature genes    gene cluster identification  base  show   distinct color clusterblast result  display   similar way  align gene cluster map   genes  mutual blast hit  give identical color additionally available   bottom right   page fully annotate embl output file provide  user   additional possibility  browse  genome   genome browser   artemis ",9
181,CSN,"CSN: unsupervised approach for inferring biological networks based on the genome alone
Methods Algorithm description This subsection gives an in-depth look at the different steps that generate the CSN graph efficiently (see Fig. 1v and supplementary section Running time and Space complexity). For a given set of genomic sequences, the algorithm first calculates the common-chimeraARS scores for all pairs of genomic sequences. Then it summarized the scores in a matrix (score matrix). This matrix is then transformed into a correlation matrix (which based on correlations between rows in the scores matrix). The correlations matrix represents an undirected weighted graph where an edge weight corresponds to a score in the correlation matrix. In this graph, edges with low values (related to low similarity) are removed to make sure that the edges represent high similarity scores and thus are meaningful ones. Next, we greedily added a minimal number of edges to the graph between connected components to make the graph connected. Finally, the sequence-based graph was embedded in a two-dimensional layout and was displayed as the sequence’s Common Substring Network. Calculating normalized chimeraARS score A unique sequence similarity measurement called Chimera Average Repetitive Substring (ChimeraARS) [45] gives a more significant impact to long substrings that are shared between pairs of sequences. It was shown that a version of this measure could be used for ranking different regions of genes according to their expression levels, suggesting that this measure may capture regulatory signals and signals related to the gene’s functionality, which are encoded in different parts of the genes [45]. Genes with similar functionality and similar protein expression are likely to share sub-sequences, and thus will have a higher chimeraARS score. In this study, the chimeraARS score was computed for each pair of sequences, by comparing a target gene S, to a reference gene R. The algorithm scans the target gene, nucleotide-by-nucleotide if this is a non-translated region, or codon-by-codon if it is a coding region. For each position, it finds the length (li) of the longest common substrings that starts from position i that also appears at the reference gene. The average length of those substrings is calculated (defined here as ARS(S, R)). Next, analysis repeats with R as the target gene and S as the reference, to get ARS(R, S). The final Normalized chimeraARS score for a pair of genes is the average of these two ARS scores (see example in Fig. 1e). This score is computed for all pairs of sequences in the set and arranged as symmetric matrix M’. figurea Algorithm implementation To calculate the Normalized chimera ARS score for a given pair of sequences efficiently, we created a different Suffix Array [SA] data structure for each sequence [61]. Specifically, for every sequence in the input set, the algorithm creates its unique SA that will be used to calculate the longest common substring between each pair of sequences (Supplemental Fig. S2). One SA is considered the target and the other- the reference SA. For each position in the target sequence (a suffix in the target SA), we searched, using a binary search, the longest matching prefix in the reference SA. Combining chimeraARS scores The normalized ARS scores mentioned above were computed separately for the UTR/promoter region, and the CDS sequence. The UTR/promoter region includes 100 nucleotides and 50 nucleotides before the start codon for S. cerevisiae and E. coli, respectively; the CDS sequence includes the first 500 and 250 nucleotides for S. cerevisiae and E. coli respectively. In the second sequence type, the score is calculated based on codons/nucleotide triplets (Fig. 1c). However, the results reported here are robust to changes in this length threshold (Supplemental section: Determine sequence range and Fig. S1). To combine the two scores, we performed weighted arithmetic mean where the coding regions are weighted five times higher than that of the UTR/promoter weight to reflect the relatively longer region of the coding region that was used: Aggregated ARS score(R,S) = (|UTR_length| * Normalized ARS scores(R_UTR, S_UTR) + |CDS_length| * Normalized ARS scores(R_CDS, S_CDS))/ (|UTR_length| + |CDS_length|). The reported results are robust to changes in the relative lengths of these two segments (see Supplemental section: CSN based on coding and regulatory regions separately, and Supplemental Fig. S11). This combined score is computed for all pairs of sequences in the input set and arranged as symmetric matrix M. From scores matrix to correlation matrix In the next step of the algorithm, Spearman’s correlation for each pair of rows x, y in M (Aggregated ChimeraARS score matrix) is computed to generate a correlation-based scoring matrix Mr where Mr(x, y) = rho = Spearman_correlation(x, y). The algorithm compares rows that represent the sequences’ scores sets. Note that comparing columns would yield the same result due to matrix symmetrically. We used this measure as it compares for each pair of sequences (x,y) the set of normalized chimeraARS scores related to sequence x to the set of normalized chimeraARS scores related to sequence y. This type of comparison includes more information than just using one normalized ARS scores related to x and y since it considers N relations instead of only one (where N denotes the number of input sequences). We used Spearman’s rank correlations in this case because we do not expect a linear relationship between pairs of analyzed variables, and we do expect to see monotonic relations (See example in Fig. 1b, Step 3). figureb From Correlation’s matrix to a CSN graph The matrix Mr can be represented as a complete graph with edges representing ‘similarity’ score among pairs of genes; however, such a graph is ‘noisy’ if it includes edges with very low weights. Thus, we filtered edges based on their corresponding correlation scores: Only edges above a minimal weight (i.e., specific correlation) were included. We reported here results related to a threshold of RHO = 0.6 (note that the p-value related to all these edges was significant). However, the results are robust to changes in this threshold (Supplemental Fig. S4). figurec Determine edge cutoff In S. cerevisiae, the edge weight threshold is 0.6; to keep the graph sparse enough to visualize it properly, In E. coli, the minimum correlation score is set to 0.42 to achieve the same edges density as S. cerevisiae CSN (Supplemental Fig. S12F). Ensuring graph connectivity The previous step may generate a disconnected graph that cannot be efficiently dealt with the network embedding algorithm (see next sub-section). Thus, in the next step, we transformed the graphs obtained in the previous step to a connected graph by adding a minimum number of edges that were not included in the initial graph while greedily choosing at each step the maximal additional edge’s weights. This algorithm is a modification of the Kruskal algorithm that connects graph components instead of nodes. figured Network embedding and visualization Finally, to visualize the graph in 2D, we used Cytoscape software [62]. We used a force-directed layout algorithm as the embedding algorithm for setting the node and edge’s locations in 2D [63]. The force-directed layout algorithm sets the graph topology by force equation where nodes push each other away, but edges between nodes pull them together. The attraction between two nodes is correlated to the weight of the edge between them. This way, nodes with heavier weight (in our case, higher similarity) tend to be physically closer in the graph. Randomization and validation To estimate the importance of silent aspects of the genes on CSN performance, we created a reference network we called ppCSN, which maintains the amino acid sequence of the gene, its GC content, and the codon frequencies but not the exact nucleotide order. Randomization was done as follows: (1) To randomize the gene’s non-translated sequences (e.g., promoters and introns), we performed permutations on their nucleotides. (2) To randomize the gene’s translated sequence (e.g., CDS), while keeping the amino acid chain and genomic codon bias, we rearranged all synonymous codons within and between sequences (Fig. 1d). (3) Then we used the same pipeline as CSN on this partly-shuffled sequence (Fig. 1b); (4) we made sure that the number of edges (and nodes) in the CSN and ppCSN was identical by adding edges according to their weights in addition to ppCSN edges.",Annotation,"csn unsupervised approach  infer biological network base   genome alone
methods algorithm description  subsection give  indepth look   different step  generate  csn graph efficiently see fig   supplementary section run time  space complexity   give set  genomic sequence  algorithm first calculate  commonchimeraars score   pair  genomic sequence   summarize  score   matrix score matrix  matrix   transform   correlation matrix  base  correlations  row   score matrix  correlations matrix represent  undirected weight graph   edge weight correspond   score   correlation matrix   graph edge  low value relate  low similarity  remove  make sure   edge represent high similarity score  thus  meaningful ones next  greedily add  minimal number  edge   graph  connect components  make  graph connect finally  sequencebased graph  embed   twodimensional layout   display   sequence common substring network calculate normalize chimeraars score  unique sequence similarity measurement call chimera average repetitive substring chimeraars  give   significant impact  long substrings   share  pair  sequence   show   version   measure could  use  rank different regions  genes accord   expression level suggest   measure may capture regulatory signal  signal relate   genes functionality   encode  different part   genes  genes  similar functionality  similar protein expression  likely  share subsequences  thus    higher chimeraars score   study  chimeraars score  compute   pair  sequence  compare  target gene    reference gene   algorithm scan  target gene nucleotidebynucleotide     nontranslated region  codonbycodon     cod region   position  find  length    longest common substrings  start  position   also appear   reference gene  average length   substrings  calculate define   arss  next analysis repeat     target gene     reference  get arsr   final normalize chimeraars score   pair  genes   average   two ars score see example  fig   score  compute   pair  sequence   set  arrange  symmetric matrix  figurea algorithm implementation  calculate  normalize chimera ars score   give pair  sequence efficiently  create  different suffix array  data structure   sequence  specifically  every sequence   input set  algorithm create  unique     use  calculate  longest common substring   pair  sequence supplemental fig  one   consider  target     reference    position   target sequence  suffix   target   search use  binary search  longest match prefix   reference  combine chimeraars score  normalize ars score mention   compute separately   utrpromoter region   cds sequence  utrpromoter region include  nucleotides   nucleotides   start codon   cerevisiae   coli respectively  cds sequence include  first    nucleotides   cerevisiae   coli respectively   second sequence type  score  calculate base  codonsnucleotide triplets fig  however  result report   robust  change   length threshold supplemental section determine sequence range  fig   combine  two score  perform weight arithmetic mean   cod regions  weight five time higher     utrpromoter weight  reflect  relatively longer region   cod region   use aggregate ars scorers  utr_length * normalize ars scoresr_utr s_utr  cds_length * normalize ars scoresr_cds s_cds utr_length  cds_length  report result  robust  change   relative lengths   two segment see supplemental section csn base  cod  regulatory regions separately  supplemental fig s11  combine score  compute   pair  sequence   input set  arrange  symmetric matrix   score matrix  correlation matrix   next step   algorithm spearmans correlation   pair  row     aggregate chimeraars score matrix  compute  generate  correlationbased score matrix   mrx   rho  spearman_correlationx   algorithm compare row  represent  sequence score set note  compare columns would yield   result due  matrix symmetrically  use  measure   compare   pair  sequence   set  normalize chimeraars score relate  sequence    set  normalize chimeraars score relate  sequence   type  comparison include  information   use one normalize ars score relate     since  consider  relations instead   one   denote  number  input sequence  use spearmans rank correlations   case     expect  linear relationship  pair  analyze variables    expect  see monotonic relations see example  fig  step  figureb  correlations matrix   csn graph  matrix    represent   complete graph  edge represent similarity score among pair  genes however   graph  noisy   include edge   low weight thus  filter edge base   correspond correlation score  edge   minimal weight  specific correlation  include  report  result relate   threshold  rho   note   pvalue relate    edge  significant however  result  robust  change   threshold supplemental fig  figurec determine edge cutoff   cerevisiae  edge weight threshold    keep  graph sparse enough  visualize  properly   coli  minimum correlation score  set    achieve   edge density   cerevisiae csn supplemental fig s12f ensure graph connectivity  previous step may generate  disconnect graph  cannot  efficiently deal   network embed algorithm see next subsection thus   next step  transform  graph obtain   previous step   connect graph  add  minimum number  edge    include   initial graph  greedily choose   step  maximal additional edge weight  algorithm   modification   kruskal algorithm  connect graph components instead  nod figure network embed  visualization finally  visualize  graph    use cytoscape software   use  forcedirected layout algorithm   embed algorithm  set  node  edge locations     forcedirected layout algorithm set  graph topology  force equation  nod push   away  edge  nod pull  together  attraction  two nod  correlate   weight   edge    way nod  heavier weight   case higher similarity tend   physically closer   graph randomization  validation  estimate  importance  silent aspects   genes  csn performance  create  reference network  call ppcsn  maintain  amino acid sequence   gene   content   codon frequencies    exact nucleotide order randomization    follow   randomize  genes nontranslated sequence  promoters  introns  perform permutations   nucleotides   randomize  genes translate sequence  cds  keep  amino acid chain  genomic codon bias  rearrange  synonymous codons within   sequence fig     use   pipeline  csn   partlyshuffled sequence fig    make sure   number  edge  nod   csn  ppcsn  identical  add edge accord   weight  addition  ppcsn edge",9
182,CAVA,"CSN and CAVA: variant annotation tools for rapid, robust next-generation sequencing analysis in the clinical setting
Clinical sequencing nomenclature We developed a standardized clinical sequencing nomenclature (CSN) for DNA sequence variant annotation. The aims of CSN are a) to provide a fixed, standardized system in which every variant has a single notation, b) to be identical for all mutation detection methods, c) to use a logical terminology understandable to non-experts, and d) to provide a nomenclature that allows easy visual discrimination between the major classes of variant in clinical genomics. The CSN follows the principles of the HGVS nomenclature, with some minor amendments to ensure compatibility and integration with historical clinical data, whilst also allowing high-throughput automated output from NGS platforms. The CSN is fully detailed in Additional file 1. Clinical Annotation of VAriants (CAVA) To provide CSN annotation in a robust and automated fashion, we developed a tool called CAVA (Clinical Annotation of VAriants) which is written in Python. CAVA is DNA ‘strand-aware’, performing coding transcript-dependent alignment so all indels are consistently reported at the most 3′ position in the coding transcript, in line with the HGVS recommendation. CAVA also classifies variants based on their impact on the protein according to a simple ontology (Table 1). Within the CAVA classification system each variant is assigned to a single class to ensure consistency. To facilitate data utilization and comparison with other datasets the Sequence Ontology (SO) classes are also given [18]. CAVA further provides an impact flag which stratifies variants into categories according to predicted severity of impact on protein function, with three default classes: category 1 = ESS, FS, SG; category 2 = NSY, SS5, IF, IM, SL, EE; and category 3 = SY, SS, INT, 5PU, 3PU. Table 1 CAVA variant classification system Full size table Default variant annotations outputted by CAVA include the CSN call, variant type (substitution, insertion, deletion or complex), HUGO Gene Nomenclature Committee (HGNC) symbol(s) of affected gene(s), Ensembl transcript identifier(s), within-transcript location(s) (i.e., the exon/intron number or 5′/3′ untranslated region (UTR)), the CAVA class, the SO term, the impact category, and the alternative most 5′ annotation (where appropriate). A SNP database can also be used to assign dbSNP identifiers [2]. The user can specify the set of Ensembl transcripts used for variant annotation instead of, or in addition to, a default whole exome canonical transcript set provided on installation. CAVA supports overlapping Ensembl transcripts, i.e., a single variant call can be annotated according to multiple transcripts. CAVA also provides various filtering options, including removing intergenic variant calls, i.e., calls not overlapping with any included transcripts, or only outputting calls affecting specific genes or genomic regions. CAVA is lightweight and is easily added to NGS pipelines as it reads variants from VCF files and outputs either a VCF with annotations appended to the original input or an easily parsable tab-separated text file, and both can be written to the standard output. Processing speed can be further increased by parallelization as each line in the VCF file is processed independently. CAVA is fully detailed in Additional file 2. CAVA is freely available and can be downloaded from the CAVA webpage [19]. CAVA exome data annotation The Exome Aggregation Consortium (ExAC) is a collaborative effort to reanalyze germline exome sequencing data from 61,486 unrelated individuals contributed by a number of disease-specific and population genetic studies [20]. The VCF file containing 10,313,034 variants in version 0.2 was downloaded and annotated by CAVA using a single core. In-house exome sequencing data were available from 1000 individuals obtained from the 1958 Birth Cohort Collection (the ICR1000 UK exome series) [21]. We used the Illumina TruSeq Exome and sequencing was performed with an Illumina HiSeq2000 generating 2 × 101 bp reads. Reads were mapped to hg19 using Stampy [22] and duplicate reads were flagged with Picard [23]. Variants were called with Platypus [24], generating raw VCF files. The ICR1000 UK exome data are available from the European Genome-phenome Archive [25]. Annotation of the 1000 VCF files was performed by CAVA in five independent jobs. Each job utilized 15 of the 16 available cores to process files in batches of 15 in parallel with one core per file. Four jobs processed 195 files each, and the fifth processed the remaining 220 files. CAVA indel annotation To evaluate CAVA indel annotation in a typical clinical scenario we used the raw VCF data from a single individual from the ICR1000 series. We excluded intergenic variants and those which only affected intronic or UTR sequence (CAVA classes INT, 3PU, or 5PU). CAVA clinical sequence data analysis We used data from a clinical gene testing laboratory, TGLclinical [26], from 25 individuals with BRCA1 mutations and 25 individuals with BRCA2 mutations. The mutations had been identified by NGS using the Illumina TruSight Cancer panel (TSCP) [27] and each mutation was then verified by Sanger sequencing and the Sanger data were used to generate the clinical report. NGS analysis of TSCP used Stampy for alignment [22] and Platypus for variant calling [24]. The default VCF file output from Platypus was used as input for CAVA (v.1.0), VEP (v.77), ANNOVAR (v.2014Jul14) and SnpEff (v.4.0), which were the most recent versions available in November 2014 when the analysis was performed.",Annotation,"csn  cava variant annotation tool  rapid robust nextgeneration sequence analysis   clinical setting
clinical sequence nomenclature  develop  standardize clinical sequence nomenclature csn  dna sequence variant annotation  aim  csn    provide  fix standardize system   every variant   single notation    identical   mutation detection methods   use  logical terminology understandable  nonexperts    provide  nomenclature  allow easy visual discrimination   major class  variant  clinical genomics  csn follow  principles   hgvs nomenclature   minor amendments  ensure compatibility  integration  historical clinical data whilst also allow highthroughput automate output  ngs platforms  csn  fully detail  additional file  clinical annotation  variants cava  provide csn annotation   robust  automate fashion  develop  tool call cava clinical annotation  variants   write  python cava  dna strandaware perform cod transcriptdependent alignment   indels  consistently report    ′ position   cod transcript  line   hgvs recommendation cava also classify variants base   impact   protein accord   simple ontology table  within  cava classification system  variant  assign   single class  ensure consistency  facilitate data utilization  comparison   datasets  sequence ontology  class  also give  cava  provide  impact flag  stratify variants  categories accord  predict severity  impact  protein function  three default class category   ess   category   nsy ss5      category     int 5pu 3pu table  cava variant classification system full size table default variant annotations output  cava include  csn call variant type substitution insertion deletion  complex hugo gene nomenclature committee hgnc symbols  affect genes ensembl transcript identifiers withintranscript locations   exonintron number  ′′ untranslated region utr  cava class   term  impact category   alternative  ′ annotation  appropriate  snp database  also  use  assign dbsnp identifiers   user  specify  set  ensembl transcripts use  variant annotation instead    addition   default whole exome canonical transcript set provide  installation cava support overlap ensembl transcripts   single variant call   annotate accord  multiple transcripts cava also provide various filter options include remove intergenic variant call  call  overlap   include transcripts   output call affect specific genes  genomic regions cava  lightweight   easily add  ngs pipelines   read variants  vcf file  output either  vcf  annotations append   original input   easily parsable tabseparated text file     write   standard output process speed    increase  parallelization   line   vcf file  process independently cava  fully detail  additional file  cava  freely available    download   cava webpage  cava exome data annotation  exome aggregation consortium exac   collaborative effort  reanalyze germline exome sequence data   unrelated individuals contribute   number  diseasespecific  population genetic study   vcf file contain  variants  version   download  annotate  cava use  single core inhouse exome sequence data  available   individuals obtain    birth cohort collection  icr1000  exome series   use  illumina truseq exome  sequence  perform   illumina hiseq2000 generate     read read  map  hg19 use stampy   duplicate read  flag  picard  variants  call  platypus  generate raw vcf file  icr1000  exome data  available   european genomephenome archive  annotation    vcf file  perform  cava  five independent job  job utilize     available core  process file  batch    parallel  one core per file four job process  file    fifth process  remain  file cava indel annotation  evaluate cava indel annotation   typical clinical scenario  use  raw vcf data   single individual   icr1000 series  exclude intergenic variants     affect intronic  utr sequence cava class int 3pu  5pu cava clinical sequence data analysis  use data   clinical gene test laboratory tglclinical    individuals  brca1 mutations   individuals  brca2 mutations  mutations   identify  ngs use  illumina trusight cancer panel tscp    mutation   verify  sanger sequence   sanger data  use  generate  clinical report ngs analysis  tscp use stampy  alignment   platypus  variant call   default vcf file output  platypus  use  input  cava  vep  annovar v2014jul14  snpeff      recent versions available  november    analysis  perform",9
183,VariantDB,"VariantDB: a flexible annotation and filtering portal for next generation sequencing data
Interface and database VariantDB consists of a PHP (5.3.2) based web interface, driving a CGI (5.10.1) backend. All data are stored in a MySQL (5.1.41) database on solid state drives (Figure 1). Structurally, data are ordered in sample and variant specific tables (Additional file 1). One additional table links variants to samples and holds quality information from GATK. Variant annotations are stored in separate tables based on the annotation source. This structure optionally allows VariantDB to retrieve annotation or filtering data from multiple sources in parallel, using the Perl Parallel::ForkManager library. Further improvements in performance can be achieved by enabling Memcached. The Perl Cache::Memcached::Fast library can reduce database load by caching and preloading frequently used data in memory. Queries, sources, and documentation for all filters and annotations are stored in XML files. Additional filtering rules can be specified as separate nodes in these configuration files. Figure 1 figure1 Schematic representation of VariantDB implementation. Depending on the expected platform load, server elements can be hosted either on a single machine (default) or on separate physical hosts. If high performance computing (HPC) infrastructure is available, annotation processes can be distributed. HPO, Human Phenotype Ontology. Full size image A public VariantDB instance is available for academic use. Furthermore, local installation is supported through either a downloadable virtualbox application or full installation on local infrastructure. Instructions for both approaches are available in the online documentation. To keep local installations up to date, automatic updating through the web interface is possible for the local administrator. Data import VCF files can be imported from an FTP server, accessible using VariantDB user credentials, or directly from a Galaxy server using the VariantDB tool (Additional file 2; for installation see [34]). Imported VCF files should comply with the VCF4.0 standards. Quality annotations generated by the GATK-based genotypers [7] are extracted and stored. VariantDB provides the option to store the imported VCF file and associated BAM file. If available, direct links are presented to load VCF and BAM files into Integrative Genomics Viewer (IGV) for visualization of filtering results [35]. Annotation Data annotation within VariantDB is available at sample and variant levels. With regard to sample annotation, family and experimental relations can be provided, which can later be applied to formulate inheritance patterns for variant filtering. Second, gender and phenotype information based on the Human Phenotype Ontology [36] is available. Finally, samples can be labeled as controls, which allow exclusion of common variants in filtering. Variant annotation is triggered by importing VCF files. Annotation proceeds by collecting variants missing a respective annotation, annotating the list of variants, and storing the results in the database. The annotation-specific tables in the database structure allow this process to be parallelized. If a high performance computing infrastructure is available, VariantDB can be configured to distribute these processes using the Perl Schedule::DRMAAc module (0.81). In total, 110 annotations are added to each variant (Table 1), taken from eight sources. The annotation engine utilizes ANNOVAR, snpEff, the Perl WWW::Mechanize library (for web tools) and a set of in-house parsers to retrieve the annotations [25],[28]. All annotations are presented by checkboxes in VariantDB for inclusion into the results (Figure 2). Users can also define sets of annotations that can be loaded simultaneously. Table 1 Summary of annotations available in VariantDB Full size table Figure 2 figure2 Selection of annotations. Top left: sample selection box, using either a dropdown menu, or auto-completion. Top right: when raw data files are available, hyperlinks are presented to download VCF/BAM files or load the files into IGV. Bottom left: all available annotations are listed. Users can select annotations using checkboxes for inclusion into the filtering results. Bottom right: previously saved sets of annotations can be enabled at once by selecting the checkbox and pressing `Add Annotations’. Full size image GATK genotyping modules provide a set of quality parameters for each identified variant. VariantDB stores the values of the allelic ratio, Phred score of the polymorphism (QUAL), Phred-based genotype quality (GQ), genotype (GT), allelic depths (AD), quality by depth (QD), mapping quality (MQ), strand bias (FS) and rank sums (BaseQRankSum, MQRankSum, ReadPosRankSum). If available, filter entries such as the VQSR tranches filter, are also stored. Minor allele frequencies (MAFs) are available from the 1000 Genomes Project (v.2012apr) and the exome sequencing project (v.esp5400.2012Jul11, v.esp6500.2013Jan22), both global and population specific [42],[43]. Second, dbSNP rsIDs, MAFs and population size values are available for versions 130, 135 and 137 [44]. Starting from version 135, the clinical association label is also extracted. Transcript information is extracted in UCSC, RefSeq and Ensembl-based format. Available information includes gene symbol or ID, transcript ID in case of multiple variants, affected position on cDNA and protein level and the effect on the protein level (intron/exon, missense/synonymous/nonsense, splicing). Predictions with regard to pathogenicity are included from several tools. Using ANNOVAR, dbSNFP annotations for LRT, MutationTaster, PhyloP, PolyPhen2 and SIFT are included [45]. GERP++ [46] and CADD [47] scores are added from the respective tool data. Up-to-date scores of PROVEAN, SIFT, Grantham and MutationTaster are retrieved using the respective web tools [37],[38]. Finally, the SnpEff annotations also provide an estimate of the variant impact on the protein function [25]. Two sources are provided for functional annotation. First, Gene Ontology terms and the first level parental terms associated with affected genes are provided [40]. Second, a summary of the information available in ClinVar is available [41]. This summary includes hyperlinks to the ClinVar entry of variants that exactly match or overlap the variant in the queried sample, the type of variant in ClinVar (SNP/indel), the affected gene and transcript, latest update, evidence type, pathogenicity classification and associated disease. For gene, disease and alleles listed in ClinVar, hyperlinks are provided to several external databases. Finally, users can specify additional information on inheritance, experimental validation and diagnostic classification on a per variant level. Annotation updates VariantDB provides two functionality layers to automatically keep annotation sources up to date. First, using scheduled execution at a frequency specified by the system administrator, third-party resources are checked for updated releases. When new data are available, all variants are re-annotated using the new release. To maintain data traceability, all discarded annotations are archived and all changes to variant annotation are logged. Finally, users are informed by email of possibly relevant novel annotations. Second, VariantDB automates the conversion between genome builds from the web interface. Upon conversion, the platform administrator needs information on the new build, including ANNOVAR, snpEff and IGV genome versions (hg19, GRC37.66 and hg19, respectively, for the current VariantDB version). Availability of the requested build is checked and, if available, all annotation tables are downloaded. Genome coordinates of currently stored variants are converted using the UCSC LiftOver tool, and failed conversions are presented to the platform administrator for manual curation [48]. Finally, all variants are re-annotated with regard to the new coordinates and users are informed. Previous genome versions remain accessible with their final annotations in read-only mode. The current genome build is always stated in the user interface. Also, when importing data from external pipelines such as galaxy, VariantDB requires the source genome build version to be passed along with the variant files, and will generate an error message on conflicting versions. Variant filtering VariantDB allows filtering on a combination of any of the available annotations listed in Table 1. To set filters, users select the criteria from dropdown menus (Figure 3) and optionally group them into a multi-level decision scheme (Figure 4). Successful filter settings can be saved for future usage. Next to the functional filtering criteria, parental and sibling relationships enable filtering for de novo, dominant and recessive inheritance models. Population-based variant selection can be performed on two levels. First, users can select variants that are present at least, or no more than, a specified number of times in a selection of samples. Second, genes can be selected for mutation burden by specifying the minimal or maximal number of samples containing a mutation in the same gene. Figure 3 figure3 Selection of filters. Left: filtering criteria are organized in high-level categories. Filters are added by selecting the relevant filter and settings from dropdown menus. Numeric (for example, quality control values) or textual (for example, Gene Symbol) criteria can be added in text fields where appropriate. Right: previously saved filtering schemes can be enabled at once by selecting the checkbox and pressing `Apply Filter’. Full size image Figure 4 figure4 Graphical representation of the selected filtering scheme. Individual filters can be grouped using logic AND/OR rules. Grouping and ordering is handled using a drag-and-drop interface. Full size image Next to general gene and population level information, users can create in silico gene panels for targeted evaluation of candidate genes. A gene panel exists of a set of RefSeq identifiers, optionally augmented with additional comments. Gene panels are private at the user level, but can be made available as a public resource to all users. Visualisation By default, results are presented in a tabular overview (Figure 5) with selected annotations and IGV hyperlinks [35]. VariantDB aims at presenting all information related to a variant in a compact single screen view. Alternatively, a classic, wide table format is available, presenting all annotations on a single line per variant (Additional file 3). Results can also be exported to CSV files for downstream analysis. Finally, various charts are available to review the quality or characteristics of the resulting variant set. These charts include, among others, the Tr/Tv ratio, known versus novel ratio, MAF distribution and SNP versus indel ratio. Figure 5 figure5 Results table. For each of the resulting variants, selected annotations are presented. On top, genomic position (which is also a hyperlink to the position in IGV), and other essential variant information is provided. If relevant, annotations are grouped in sub-tables on affected feature. User-specified information related to validation and classification is presented in a separate box on the left-hand side.",Annotation,"variantdb  flexible annotation  filter portal  next generation sequence data
interface  database variantdb consist   php  base web interface drive  cgi  backend  data  store   mysql  database  solid state drive figure  structurally data  order  sample  variant specific table additional file  one additional table link variants  sample  hold quality information  gatk variant annotations  store  separate table base   annotation source  structure optionally allow variantdb  retrieve annotation  filter data  multiple source  parallel use  perl parallelforkmanager library  improvements  performance   achieve  enable memcached  perl cachememcachedfast library  reduce database load  cache  preloading frequently use data  memory query source  documentation   filter  annotations  store  xml file additional filter rule   specify  separate nod   configuration file figure  figure1 schematic representation  variantdb implementation depend   expect platform load server elements   host either   single machine default   separate physical host  high performance compute hpc infrastructure  available annotation process   distribute hpo human phenotype ontology full size image  public variantdb instance  available  academic use furthermore local installation  support  either  downloadable virtualbox application  full installation  local infrastructure instructions   approach  available   online documentation  keep local installations   date automatic update   web interface  possible   local administrator data import vcf file   import   ftp server accessible use variantdb user credentials  directly   galaxy server use  variantdb tool additional file   installation see  import vcf file  comply   vcf4 standards quality annotations generate   gatkbased genotypers   extract  store variantdb provide  option  store  import vcf file  associate bam file  available direct link  present  load vcf  bam file  integrative genomics viewer igv  visualization  filter result  annotation data annotation within variantdb  available  sample  variant level  regard  sample annotation family  experimental relations   provide   later  apply  formulate inheritance pattern  variant filter second gender  phenotype information base   human phenotype ontology   available finally sample   label  control  allow exclusion  common variants  filter variant annotation  trigger  import vcf file annotation proceed  collect variants miss  respective annotation annotate  list  variants  store  result   database  annotationspecific table   database structure allow  process   parallelize   high performance compute infrastructure  available variantdb   configure  distribute  process use  perl scheduledrmaac module   total  annotations  add   variant table  take  eight source  annotation engine utilize annovar snpeff  perl wwwmechanize library  web tool   set  inhouse parsers  retrieve  annotations   annotations  present  checkboxes  variantdb  inclusion   result figure  users  also define set  annotations    load simultaneously table  summary  annotations available  variantdb full size table figure  figure2 selection  annotations top leave sample selection box use either  dropdown menu  autocompletion top right  raw data file  available hyperlinks  present  download vcfbam file  load  file  igv bottom leave  available annotations  list users  select annotations use checkboxes  inclusion   filter result bottom right previously save set  annotations   enable    select  checkbox  press `add annotations full size image gatk genotyping modules provide  set  quality parameters   identify variant variantdb store  value   allelic ratio phred score   polymorphism qual phredbased genotype quality  genotype  allelic depths  quality  depth  map quality  strand bias   rank sum baseqranksum mqranksum readposranksum  available filter entries    vqsr tranches filter  also store minor allele frequencies mafs  available    genomes project v2012apr   exome sequence project vesp54002012jul11 vesp65002013jan22  global  population specific  second dbsnp rsids mafs  population size value  available  versions      start  version   clinical association label  also extract transcript information  extract  ucsc refseq  ensemblbased format available information include gene symbol   transcript   case  multiple variants affect position  cdna  protein level   effect   protein level intronexon missensesynonymousnonsense splice predictions  regard  pathogenicity  include  several tool use annovar dbsnfp annotations  lrt mutationtaster phylop polyphen2  sift  include  gerp   cadd  score  add   respective tool data uptodate score  provean sift grantham  mutationtaster  retrieve use  respective web tool  finally  snpeff annotations also provide  estimate   variant impact   protein function  two source  provide  functional annotation first gene ontology term   first level parental term associate  affect genes  provide  second  summary   information available  clinvar  available   summary include hyperlinks   clinvar entry  variants  exactly match  overlap  variant   query sample  type  variant  clinvar snpindel  affect gene  transcript latest update evidence type pathogenicity classification  associate disease  gene disease  alleles list  clinvar hyperlinks  provide  several external databases finally users  specify additional information  inheritance experimental validation  diagnostic classification   per variant level annotation update variantdb provide two functionality layer  automatically keep annotation source   date first use schedule execution   frequency specify   system administrator thirdparty resources  check  update release  new data  available  variants  reannotated use  new release  maintain data traceability  discard annotations  archive   change  variant annotation  log finally users  inform  email  possibly relevant novel annotations second variantdb automate  conversion  genome build   web interface upon conversion  platform administrator need information   new build include annovar snpeff  igv genome versions hg19 grc37  hg19 respectively   current variantdb version availability   request build  check   available  annotation table  download genome coordinate  currently store variants  convert use  ucsc liftover tool  fail conversions  present   platform administrator  manual curation  finally  variants  reannotated  regard   new coordinate  users  inform previous genome versions remain accessible   final annotations  readonly mode  current genome build  always state   user interface also  import data  external pipelines   galaxy variantdb require  source genome build version   pass along   variant file   generate  error message  conflict versions variant filter variantdb allow filter   combination     available annotations list  table   set filter users select  criteria  dropdown menus figure   optionally group    multilevel decision scheme figure  successful filter settings   save  future usage next   functional filter criteria parental  sibling relationships enable filter   novo dominant  recessive inheritance model populationbased variant selection   perform  two level first users  select variants   present  least      specify number  time   selection  sample second genes   select  mutation burden  specify  minimal  maximal number  sample contain  mutation    gene figure  figure3 selection  filter leave filter criteria  organize  highlevel categories filter  add  select  relevant filter  settings  dropdown menus numeric  example quality control value  textual  example gene symbol criteria   add  text field  appropriate right previously save filter scheme   enable    select  checkbox  press `apply filter full size image figure  figure4 graphical representation   select filter scheme individual filter   group use logic andor rule group  order  handle use  draganddrop interface full size image next  general gene  population level information users  create  silico gene panel  target evaluation  candidate genes  gene panel exist   set  refseq identifiers optionally augment  additional comment gene panel  private   user level    make available   public resource   users visualisation  default result  present   tabular overview figure   select annotations  igv hyperlinks  variantdb aim  present  information relate   variant   compact single screen view alternatively  classic wide table format  available present  annotations   single line per variant additional file  result  also  export  csv file  downstream analysis finally various chart  available  review  quality  characteristics   result variant set  chart include among others  trtv ratio know versus novel ratio maf distribution  snp versus indel ratio figure  figure5 result table     result variants select annotations  present  top genomic position   also  hyperlink   position  igv   essential variant information  provide  relevant annotations  group  subtables  affect feature userspecified information relate  validation  classification  present   separate box   lefthand side",9
184,MGRASt,"MG-RAST, a Metagenomics Service for Analysis of Microbial Community Structure and Function
The pipeline diverges after upload for 16S ribosomal amplicon and whole-genome shotgun (WGS) samples. The WGS pipeline is composed of several steps from the removal of low-quality reads, dereplication, gene calling, and annotation to creation of functional abundance profiles. rRNA samples run through RNA detection, clustering, and identification, and the production of taxonomic abundance profiles. Subheading 4 found at the end of this chapter includes additional details. 3.1 The WGS Pipeline 1. Preprocessing. After upload, data are preprocessed by using SolexaQA [20] to trim low-quality regions from FASTQ data. Platform-specific approaches are used for 454 data submitted in FASTA format, reads more than two standard deviations away from the mean read length are discarded [21]. All sequences submitted to the system are available, but discarded reads are not analyzed further. 2. Dereplication . For shotgun metagenome and shotgun metatranscriptome data sets, we perform a dereplication step. We use a simple k-mer approach to rapidly identify all 20 character prefix identical sequences. This step is required in order to remove artificial duplicate reads (ADRs) [22]. Instead of simply discarding the ADRs, we set them aside and use them later as a means to assess sample quality. We note that dereplication is not suitable for amplicon data sets that are likely to share common prefixes. 3. DRISEE . MG-RAST v3 uses DRISEE (Duplicate Read Inferred Sequencing Error Estimation) [23] to analyze the sets of ADRs and determine the degree of variation among prefix-identical sequences derived from the same template. See below for details. 4. Screening . The pipeline provides the option of removing reads that are near-exact matches to the genomes of a handful of model organisms, including fly, mouse, cow, and human. The screening stage uses Bowtie [24] (a fast, memory-efficient, short read aligner), and only reads that do not match the model organisms pass into the next stage of the annotation pipeline. Note that this option will remove all reads similar to the human genome and render them inaccessible. This decision was made in order to avoid storing any human DNA on MG-RAST. 5. Gene calling . The previous version of MG-RAST used nucleotide-based similarity for annotation of WGS data, an approach that is significantly more expensive computationally than de novo gene prediction followed by protein similarity-based annotation. After an in-depth investigation of tool performance [25], we have moved to a machine learning approach that utilizes FragGeneScan [26] to predict proteins/protein fragments from de novo sequence data (FragGeneScan uses a well tested algorithm [25] to perform in silico translation of predicted protein coding nucleic acid sequences). Utilizing this approach, we can predict coding regions in DNA sequences that are 75 base pairs or longer. Our novel approach also enables the analysis of user-provided assembled contigs. We note that FragGeneScan is trained for prokaryotes only. While it will identify proteins for eukaryotic sequences, the results should be viewed critically. 6. AA clustering. MG-RAST builds clusters of proteins at the 90 % identity level using the uclust [27] implementation in QIIME [28], preserving the relative abundances. These clusters greatly reduce the computational burden of comparing all pairs of short reads, while clustering at 90 % identity preserves sufficient biological signals. 7. Protein identification . Once created, a representative (the longest sequence) for each cluster is subjected to similarity analysis. Functional identification of representative sequences does not use BLAST, instead we use a much more efficient algorithm, sBLAT, an implementation of the BLAT algorithm, which we parallelized using OpenMPI. We reconstruct the putative species composition of WGS data by looking at the phylogenetic origin of the database sequences hit by the protein-based similarity searches. Note that processing of rRNA 16S amplicon data is covered in Subheading 3.2 below. 8. Annotation mapping . Sequence similarity searches are computed against a protein database derived from the M5NR, which provides nonredundant integration of many databases. Users can easily change views without recomputation. For example, COG and KEGG views can be displayed, which both show the relative abundances of histidine biosynthesis in a data set of four cow rumen metagenomes. Help in interpreting results, MG-RAST searches the nonredundant M5NR and M5RNA databases in which each sequence is unique. These two databases are built from multiple sequence database sources, and the individual sequences may occur multiple times in different strains and species (and sometimes genera) with 100 % identity. In these circumstances, choosing the “right” taxonomic information is not a straightforward process. To optimally serve a number of different use cases, we have implemented three methods for end users to determine the number of hits (occurrences of the input sequence in the database) in their samples. Best hit, The best hit classification reports the functional and taxonomic annotation of the best hit in the M5NR for each feature. In those cases where the similarity search yields multiple same-scoring hits for a feature, we do not choose any single “correct” label. For this reason MG-RAST double counts all annotations with identical match properties and leaves determination of truth to our users. While this approach aims to inform about the functional and taxonomic potential of a microbial community by preserving all information, subsequent analysis can be biased (e.g., a single feature may have multiple annotations), leading to inflated hit counts. For users looking for a specific species or function in their results, the best hit classification is likely what is wanted. Representative hit, The representative hit classification selects a single, unambiguous annotation for each feature. The annotation is based on the first hit in the homology search and the first annotation for that hit in the database. This approach makes counts additive across functional and taxonomic levels and is better suited for comparisons of functional and taxonomic profiles of multiple metagenomes. Lowest Common Ancestor (LCA) , To avoid the problem of multiple taxonomic annotations for a single feature, MG-RAST provides taxonomic annotations based on the widely used LCA method introduced by MEGAN [29]. In this method, all hits are collected that have a bit score close to the bit score of the best hit. The taxonomic annotation of the feature is then determined by computing the LCA of all species in this set. This replaces all taxonomic annotations from ambiguous hits with a single higher-level annotation in the NCBI taxonomy tree. 9. Abundance profiles. Abundance profiles (essentially tables that indicate detected taxa or functions and their relative abundance as determined by the methods described in Subheading 3.1, step 8—examples can be found in the MG-RAST user manual, see the “additional documentation” in Subheading 4 found at the end of this chapter) are the primary data product that the MG-RAST’s user interface uses to display information in annotated data sets. Using the abundance profiles, the MG-RAST system defers to the user to select several parameters that will define their abundance data, e-value, percent identity, and minimal alignment length. As it is not possible to arbitrarily select thresholds suitable for all use cases, users can select their own thresholds for each of these values. Taxonomic profiles use the NCBI taxonomy. All taxonomic information is projected against the NCBI taxonomy. Functional profiles are available for data sources that provide hierarchical information. These currently include SEED subsystems, KEGG orthologs, and COGs. SEED subsystems represent an independent reannotation effort utilized by RAST [30] and MG-RAST. Manual curation of subsystems makes them an extremely valuable data source. The current subsystems hierarchy can be viewed at http://pubseed.theseed.org//SubsysEditor.cgi which allows browsing the subsystems. Subsystems represent a four-level hierarchy, 1. Subsystem level 1—highest level 2. Subsystem level 2—intermediate level 3. Subsystem level 3—similar to a KEGG pathway 4. Subsystem level 4—actual functional assignment to the feature in question KEGG Orthologs. MG-RAST uses the KEGG enzyme number to implement a four-level hierarchy. We note that KEGG data are no longer available for free download; therefore, we rely on the latest freely downloadable version of these data. 1. KEGG level 1—first digit of the EC number (EC,X.*.*.*) 2. KEGG level 2—first two digits of the EC number (EC,X.Y.*.*) 3. KEGG level 3—first three digits of the EC number (EC,X,Y,Z,.*) 4. KEGG level 4—entire four digits of the EC number The high-level KEGG categories are as follows: 1. Cellular Processes 2. Environmental Information Processing 3. Genetic Information Processing 4. Human Diseases 5. Metabolism 6. Organizational Systems COG and EGGNOG Categories. The high-level COG and EGGNOG categories are as follows: 1. Cellular Processes 2. Information Storage and Processing 3. Metabolism 4. Poorly Characterized 3.2 The rRNA Pipeline The rRNA pipeline starts with upload of rRNA reads and proceeds through the following steps: 1. rRNA detection . Reads are identified as rRNA through a simple rRNA detection. An initial BLAT search against a reduced RNA database efficiently identifies RNA. The reduced database is a 90 % identity clustered version of the SILVA database and is used merely to differentiate samples containing solely rRNA data from other samples (e.g., WGS or transcriptomic samples). 2. rRNA clustering . The rRNA-similar reads are clustered at 97 % identity, and the longest sequence is picked as the cluster representative. 3. rRNA identification . A nucleotide BLAT similarity search for the longest cluster representative is performed against the M5rna database, integrating SILVA [31], Greengenes [32], and RDP [33]. 3.3 Using the MG-RAST User Interface The MG-RAST system provides a rich web user interface that covers all aspects of metagenome analysis, from data upload to ordination analysis of annotation abundances. The web interface can also be used for data discovery. Metagenomic data sets can be easily selected individually or on the basis of filters such as technology (including read length), quality, sample type, and keyword, with dynamic filtering of results based on similarity to known reference proteins or taxonomy. For example, a user may want to perform a search such as “phylum eq ‘actinobacteria’ and function in KEGG pathway Lysine Biosynthesis and sample in ‘Ocean’” to extract sets of reads matching the appropriate functions and taxa across metagenomes. The results can be displayed in familiar formats, including bar charts, trees that incorporate abundance information, heatmaps, principal component analyses, or raw abundance tables exported in tabular form. The raw or processed data can be recovered via download pages or with the matR package for R (see Subheading 4 below). Metabolic reconstructions based on mapping to KEGG pathways are also provided. Sample selection is crucial for understanding large-scale patterns when multiple metagenomes are compared. Accordingly, MG-RAST supports MIxS and MIMARKS [34] (as well as domain-specific plug-ins for specialized environments not extending the minimal GSC standards); several projects, including TerraGenome, HMP, TARA, and EMP, use these GSC standards, enabling standardized queries that integrate new samples into these massive data sets. One key aspect of the MG-RAST approach is the creation of smart data products enabling the user at the time of analysis to determine the best parameters for, for example, a comparison between samples. This is done without the need for recomputation of results. 3.3.1 Navigation The MG-RAST website is rich with functionality and offers several options. The site at http://metagenomics.anl.gov has five main pages and a home page, shown in blue in Fig. 2. Open image in new windowFig. 2 Fig. 2 Sitemap for the MG-RAST version 3 website. On the site map the main pages are shown in blue, management pages in orange. The green boxes represent pages that are not directly accessible from the home page Download page—lists all publicly available data for download. The data are structured into projects. Browse page—allows interactive browsing of all data sets and is powered by metadata. Search page—allows identifier, taxonomy, and function-driven searches against all public data. Analysis page—enables in-depth analyses and comparisons between data sets. Upload page—allows users to provide their samples and metadata to MG-RAST. Home (Metagene Overview) page—provides an overview for each individual data set. 3.3.2 Upload Page Data and metadata can be uploaded in the form of spreadsheets along with the sequence data by using both the ftp and the http protocols. The web uploader will automatically split large files and also allows parallel uploads. MG-RAST supports data sets that are augmented with rich metadata using the standards and technology developed by the GSC. Each user has a temporary storage location inside the MG-RAST system. This inbox provides temporary storage for data and metadata to be submitted to the system. Using the inbox, users can extract compressed files, convert a number of vendor-specific formats to MG-RAST submission-compliant formats, and obtain an MD5 checksum for verifying that transmission to MG-RAST has not altered the data. The web uploader has been optimized for large data sets of over 100 giga-base-pairs, often resulting in file sizes in excess of 150 GB. 3.3.3 Browse Page: Metadata-Enabled Data Discovery The Browse page lists all data sets visible to the user (the users own data sets as well as all public data and all data shared by other users). This page also provides an overview of the nonpublic data sets submitted by the user or shared with users. The interactive metagenome browse table provides an interactive graphical means to discover data based on technical data (e.g., sequence type or data set size) or metadata (e.g., location or biome). 3.3.4 Project Page The project page provides a list of data sets and metadata for a project. The table at the bottom of the Project page provides access to the individual metagenomes by clicking on the identifiers in the first column. In addition, the final column provides downloads for metadata, submitted data, and the analysis results via the three labeled arrows. For the data set owners, the Project page provides an editing capability using a number of menu entries at the top of the page. Figure 3 shows the available options. Open image in new windowFig. 3 Fig. 3 Project page, providing a summary of all data in the project and an interface for downloads Share Project—make the data in this project available to third parties via sending them access tokens. Add Jobs—add additional data sets to this project. Edit Project Data—edit the contents of this page. Upload Info—upload information to be displayed on this page. Upload MetaData—upload a metadata spreadsheet for the project. Export MetaData2—export the metadata spreadsheet for this project. 3.3.5 Overview Page MG-RAST automatically creates an individual summary page for each data set. This metagenome overview page provides a summary of the annotations for a single data set. The page is made available by the automated pipeline once the computation is finished. This page is a good starting point for looking at a particular data set. It provides information regarding technical detail and biological content. The page is intended as a single point of reference for metadata, quality, and data. It also provides an initial overview of the analysis results for individual data sets with default parameters. Further analyses are available on the Analysis page. 3.3.5.1 Technical Details on Sequencing and Analysis The Overview page provides the MG-RAST ID for a data set, a unique identifier that is usable as an accession number for publications. Additional information, such as the name of the submitting PI, organization, and a user-provided metagenome name are displayed at the top of the page. A static URL for linking to the system that will be stable across changes to the MG-RAST web interface is provided as additional information (Fig. 7). MG-RAST provides an automatically generated paragraph of text describing the submitted data and the results computed by the pipeline. By means of the project information, we display additional information provided by the data submitters at the time of submission or later. One of the key diagrams in MG-RAST is the sequence breakdown pie chart (Fig. 4) classifying the submitted sequences submitted into several categories according to their annotation status. As detailed in the description of the MG-RAST v3 pipeline above, the features annotated in MG-RAST are protein coding genes and ribosomal proteins. Open image in new windowFig. 4 Fig. 4 Sequences to the pipeline are classified into one of five categories: grey = failed the QC, red = unknown sequences, yellow = unknown function but protein coding, green = protein coding with known function, and blue = ribosomal RNA. For this example, over 50 % of sequences were either filtered by QC or failed to be recognized as either protein coding or ribosomal Note that for performance reasons no other sequence features are annotated by the default pipeline. Other feature types such as small RNAs or regulatory motifs (e.g., CRISPRS [35]) not only will require significantly higher computational resources but also are frequently not supported by the unassembled short reads that constitute the vast majority of today’s metagenomic data in MG-RAST. The quality of the sequence data coming from next-generation instruments requires careful design of experiments, lest the sensitivity of the methods is greater than the signal-to-noise ratio the data supports. The overview page also provides metadata for each data set to the extent that such information has been made available. Metadata enables other researchers to discover data sets and compare annotations. MG-RAST requires standard metadata for data sharing and data publication. This is implemented using the standards developed by the Genomics Standards Consortium. All metadata stored for a specific data set is available in MG-RAST; we merely display a standardized subset in this table. A link at the bottom of the table (“More Metadata”) provides access to a table with the complete metadata. This enables users to provide extended metadata going beyond the GSC minimal standards. A mechanism to provide community consensus extensions to the minimal checklists and the environmental packages are explicitly encouraged but not required when using MG-RAST. 3.3.5.2 Metagenome Quality Control The analysis flowchart and analysis statistics provide an overview of the number of sequences at each stage in the pipeline. The text block next to the analysis flowchart presents the numbers next to their definitions. 3.3.5.3 Source Hits Distribution The source hits distribution shows the percentage of the predicted protein features annotated with similarity to a protein of known function per source database. In addition, ribosomal RNA genes are mapped to the rRNA databases. In addition, this display will print the number of records in the M5NR protein database and in the M5RNA ribosomal databases. 3.3.5.4 Other Statistics MG-RAST also provides a quick link to other statistics. For example, the Analysis Statistics and Analysis Flowchart provide sequence statistics for the main steps in the pipeline from raw data to annotation, describing the transformation of the data between steps. Sequence length and GC histograms display the distribution before and after quality control steps. Metadata is presented in a searchable table that contains contextual metadata describing sample location, acquisition, library construction, and sequencing using GSC compliant metadata. All metadata can be downloaded from the table. 3.3.6 Biological Part of the Overview Page The taxonomic hit distribution display divides taxonomic units into a series of pie charts of all the annotations grouped at various taxonomic ranks (domain, phylum, class, order, family, genus). The subsets are selectable for downstream analysis; this also enables downloads of subsets of reads, for example, those hitting a specific taxonomic unit. 3.3.6.1 Rank Abundance The rank abundance plot provides a rank-ordered list of taxonomic units at a user-defined taxonomic level, ordered by their abundance in the annotations. 3.3.6.2 Rarefaction The rarefaction curve of annotated species richness is a plot (see Fig. 5) of the total number of distinct species annotations as a function of the number of sequences sampled. The slope of the right-hand part of the curve is related to the fraction of sampled species that are rare. On the left, a steep slope indicates that a large fraction of the species diversity remains to be discovered. If the curve becomes flatter to the right, a reasonable number of individuals is sampled, more intensive sampling is likely to yield only few additional species. Sampling curves generally rise quickly at first and then level off toward an asymptote as fewer new species are found per unit of individuals collected. Open image in new windowFig. 5 Fig. 5 Rarefaction plot showing a curve of annotated species richness (i.e., the number of unique species). This curve is a plot of the total number of distinct species annotations as a function of the number of sequences sampled The rarefaction curve is derived from the protein taxonomic annotations and is subject to problems stemming from technical artifacts. These artifacts can be similar to the ones affecting amplicon sequencing [36], but the process of inferring species from protein similarities may introduce additional uncertainty. 3.3.6.3 Alpha Diversity In this section, we display an estimate of the alpha diversity based on the taxonomic annotations for the predicted proteins. The alpha diversity is presented in context of other metagenomes in the same project (see Fig. 6). Open image in new windowFig. 6 Fig. 6 Alpha diversity plot showing the range of -diversity values in the project the data set belongs to. The min, max, and mean values are shown, with the standard deviation ranges in different shades. The alpha-diversity of this metagenome is shown in red. The species-level annotations are from all the annotation source databases used by MG-RAST The alpha diversity estimate is a single number that summarizes the distribution of species-level annotations in a data set. The Shannon diversity index is an abundance-weighted average of the logarithm of the relative abundances of annotated species. We compute the species richness as the antilog of the Shannon diversity. 3.3.6.4 Functional Categories This section contains four pie charts providing a breakdown of the functional categories for KEGG, COG, SEED Subsystems, and EggNOGs. Clicking on the individual pie chart slices will save the respective sequences to the workbench. The relative abundance of sequences per functional category can be downloaded as a spreadsheet, and users can browse the functional breakdowns. A more detailed functional analysis, allowing the user to manipulate parameters for sequence similarity matches, is available from the Analysis page. 3.3.7 Analysis Page The MG-RAST annotation pipeline produces a set of annotations for each sample; these annotations can be interpreted as functional or taxonomic abundance profiles. The analysis page can be used to view these profiles for a single metagenome or to compare profiles from multiple metagenomes using various visualizations (e.g., heatmap) and statistics (e.g., PCoA, normalization). The page is divided into three parts following a typical workflow (Fig. 7). Open image in new windowFig. 7 Fig. 7 Three-step process in using the Analysis page: (1) select a profile and hit (see text) type; (2) select a list of metagenomes and set annotation source and similarity parameters; (3) choose a comparison 1. Data type Selection of an MG-RAST analysis scheme, that is, selection of a particular taxonomic or functional abundance profile mapping. For taxonomic annotations, since there is not always a unique mapping from hit to annotation, we provide three interpretations: best hit, representative hit, and lowest common ancestor. When choosing the LCA annotations, not all downstream tools are available. The reason is the fact that for the LCA annotations not all sequences will be annotated to the same level, classifications are returned on different taxonomic levels. Functional annotations can be grouped into mappings to functional hierarchies or can be displayed without a hierarchy. In addition, the recruitment plot displays the recruitment of protein sequences against a reference genome. Each selected data type has data selections and data visualizations specific for it. 2. Data selection Selection of sample and parameters. This dialog allows the selection of multiple metagenomes that can be compared individually or selected and compared as groups. Comparison is always relative to the annotation source, e-value, and percent identity cutoffs selectable in this section. In addition to the metagenomes available in MG-RAST, sets of sequences previously saved in the workbench can be selected for visualization. 3. Data visualization Data visualization and comparison. Depending on the selected profile type, the profiles for the metagenomes can be visualized and compared by using barcharts, trees, spreadsheet-like tables, heatmaps, PCoA, rarefaction plots, circular recruitment plot, and KEGG maps. The data selection dialog provides access to data sets in four ways. The four categories can be selected from a pulldown menu. private data—list of private or shared data sets for browsing under available metagenomes. collections—defined sets of metagenomes grouped for easier analysis. This is the recommended way of working with the analysis page. projects—global groups of data sets grouped by the submitting user. The project name will be displayed. public data—display of all public data sets. When using collections or projects, data can also be grouped into one set per collection or project and subsequently compared or added. 3.3.7.1 Normalization Normalization refers to a transformation that attempts to reshape an underlying distribution. A large number of biological variables exhibit a log-normal distribution, meaning that when the data are transformed with a log transformation, the values exhibit a normal distribution. Log transformation of the counts data makes a normalized data product that is more likely to satisfy the assumptions of additional downstream tests such as ANOVA or t-tests. Standardization is a transformation applied to each distribution in a group of distributions so that all distributions exhibit the same mean and the same standard deviation. This removes some aspects of inter-sample variability and can make data more comparable. This sort of procedure is analogous to commonly practiced scaling procedures but is more robust in that it controls for both scale and location. 3.3.7.2 Rarefaction The rarefaction view is available only for taxonomic data. The rarefaction curve of annotated species richness is a plot (see Fig. 8) of the total number of distinct species annotations as a function of the number of sequences sampled. As shown in the figure, multiple data sets can be included. Open image in new windowFig. 8 Fig. 8 Rarefaction plot showing a curve of annotated species richness. This curve is a plot of the total number of distinct species annotations as a function of the number of sequences sampled The slope of the right-hand part of the curve is related to the fraction of sampled species that are rare. When the rarefaction curve is flat, more intensive sampling is likely to yield only a few additional species. The rarefaction curve is derived from the protein taxonomic annotations and is subject to problems stemming from technical artifacts. These artifacts can be similar to the ones affecting amplicon sequencing [31], but the process of inferring species from protein similarities may introduce additional uncertainty. On the Analysis page, the rarefaction plot serves as a means of comparing species richness between samples in a way independent of the sampling depth. On the left, a steep slope indicates that a large fraction of the species diversity remains to be discovered. If the curve becomes flatter to the right, a reasonable number of individuals is sampled, more intensive sampling is likely to yield only a few additional species. Sampling curves generally rise very quickly at first and then level off toward an asymptote as fewer new species are found per unit of individuals collected. These rarefaction curves are calculated from the table of species abundance. The curves represent the average number of different species annotations for subsamples of the complete data set. 3.3.7.3 Heatmap/Dendrogram The heatmap/dendrogram allows an enormous amount of information to be presented in a visual form that is amenable to human interpretation. Dendrograms are trees that indicate similarities between annotation vectors. The MG-RAST heatmap/dendrogram has two dendrograms, one indicating the similarity/dissimilarity among metagenomic samples (x-axis dendrogram) and another indicating the similarity/dissimilarity among annotation categories (e.g., functional roles; the y-axis dendrogram). A distance metric is evaluated between every possible pair of sample abundance profiles. A clustering algorithm (e.g., ward-based clustering) then produces the dendrogram trees. Each square in the heatmap dendrogram represents the abundance level of a single category in a single sample. The values used to generate the heatmap/dendrogram figure can be downloaded as a table by clicking on the download button. 3.3.7.4 Ordination MG-RAST uses Principle Coordinate Analysis (PCoA) to reduce the dimensionality of comparisons of multiple samples that consider functional or taxonomic annotations. Dimensionality reduction is a process that allows the complex variation found in a large data sets (e.g., the abundance values of thousands of functional roles or annotated species across dozens of metagenomic samples) to be reduced to a much smaller number of variables that can be visualized as simple two or three-dimensional scatter plots. The plots enable interpretation of the multidimensional data in a human-friendly presentation. Samples that exhibit similar abundance profiles (taxonomic or functional) group together, whereas those that differ are found farther apart. A key feature of PCoA-based analyses is that users can compare components not just to each other but to metadata recorded variables (e.g., sample pH, biome, DNA extraction protocol) to reveal correlations between extracted variation and metadata-defined characteristics of the samples. It is also possible to couple PCoA with higher-resolution statistical methods in order to identify individual sample features (taxa or functions) that drive correlations observed in PCoA visualizations. This coupling can be accomplished with permutation-based statistics applied directly to the data before calculation of distance measures used to produce PCoAs; alternatively, one can apply conventional statistical approaches (e.g., ANOVA or Kruskal–Wallis test) to groups observed in PCoA-based visualizations. 3.3.7.5 Bar Charts The bar chart visualization option on the Analysis page has a built-in ability to drill down by clicking on a specific category. You can expand the categories to show the normalized abundance (adjusted for sample sizes) at various levels. The abundance information displayed can be downloaded into a local spreadsheet. Once a sub-selection has been made (e.g., the domain Bacteria selected), data can be sent to the workbench for detailed analysis. In addition, reads from a specific level can be added into the workbench. 3.3.7.6 Tree Diagram The tree diagram allows comparison of data sets against a hierarchy (e.g., Subsystems or the NCBI taxonomy). The hierarchy is displayed as a rooted tree, and the abundance (normalized for data set size or raw) for each data set in the various categories is displayed as a bar chart for each category. By clicking on a category (inside the circle), detailed information can be requested for that node 3.3.7.7 Table The table tool creates a spreadsheet-based abundance table that can be searched and restricted by the user. Tables can be generated at user-selected levels of phylogenetic or functional resolution. Table data can be visualized by using Krona [37] or can be exported in BIOM [24] format to be used in other tools (e.g., QIIME). The tables also can be exported as tab-separated text. Abundance tables serve as the basis for all comparative analysis tools in MG-RAST, from PCoA to heatmap/dendrograms. 3.3.7.8 Workbench The workbench was designed to allow users to select subsets of the data for comparison or export. Specifically, the workbench supports selecting sequence features and submitting them to further analysis or other analysis. A number of use cases are described below. An important limitation with the current implementation is that data sent to the workbench exist only until the current session is closed. 3.3.8 Metadata, Publishing, and Sharing MG-RAST is both an analytical platform and a data integration system. To enable data reuse, for example for meta-analyses, we require that all data being made available to third parties contain at least minimal metadata. The MG-RAST team has decided to follow the minimal checklist approach used by the GSC. MG-RAST provides a mechanism to make data and analyses publicly accessible. Only the submitting user can make data public on MG-RAST. As stated above, metadata is mandatory for data set publication. Metazen [39] is a web based tool for assisting end-users in the creation of metadata with the correct controlled vocabularies and in the correct format. In addition to publishing, data and analysis can also be shared with specific users (Fig. 9). To share data, users simply enter their email address via clicking sharing on the Overview page. Open image in new windowFig. 9 Fig. 9 Data sets shared in MG-RAST by users (orange dots), shown as connecting edges 4 matR, Metagenomic analysis tools for R We have recently produced a package for the R environment for statistical computing (www.r-project.org/) that provides accessory analytical capabilities to complement those already available through the MG-RAST website. The matR package is primarily designed for download and analysis of MG-RAST-based annotation abundance profiles. It makes it possible to download annotation abundance data from MG-RAST into R friendly data objects suitable for analysis with included analysis functions. We note that matR has been specifically designed to perform large-scale analyses on abundance profiles from dozens to thousands of data sets with suitable pre-processing, normalization, statistics, and visualization tools. Users can utilize these built-in tools or any of the enormous variety of tools available within the R universe. The release version of matR is available through CRAN (http://cran.r-project.org/web/packages/matR/); pre-release and development versions are available on github (https://github.com/MG-RAST/matR/); a google group is available (https://groups.google.com/forum/#!forum/matr-forum); a publication demonstrating the ease with which matR can be used to conduct large-scale analyses is forthcoming.",Annotation,"mgrast  metagenomics service  analysis  microbial community structure  function
 pipeline diverge  upload   ribosomal amplicon  wholegenome shotgun wgs sample  wgs pipeline  compose  several step   removal  lowquality read dereplication gene call  annotation  creation  functional abundance profile rrna sample run  rna detection cluster  identification   production  taxonomic abundance profile subheading  find   end   chapter include additional detail   wgs pipeline  preprocessing  upload data  preprocessed  use solexaqa   trim lowquality regions  fastq data platformspecific approach  use   data submit  fasta format read   two standard deviations away   mean read length  discard   sequence submit   system  available  discard read   analyze   dereplication   shotgun metagenome  shotgun metatranscriptome data set  perform  dereplication step  use  simple kmer approach  rapidly identify   character prefix identical sequence  step  require  order  remove artificial duplicate read adrs  instead  simply discard  adrs  set  aside  use  later   mean  assess sample quality  note  dereplication   suitable  amplicon data set   likely  share common prefix  drisee  mgrast  use drisee duplicate read infer sequence error estimation   analyze  set  adrs  determine  degree  variation among prefixidentical sequence derive    template see   detail  screen   pipeline provide  option  remove read   nearexact match   genomes   handful  model organisms include fly mouse cow  human  screen stage use bowtie   fast memoryefficient short read aligner   read    match  model organisms pass   next stage   annotation pipeline note   option  remove  read similar   human genome  render  inaccessible  decision  make  order  avoid store  human dna  mgrast  gene call   previous version  mgrast use nucleotidebased similarity  annotation  wgs data  approach   significantly  expensive computationally   novo gene prediction follow  protein similaritybased annotation   indepth investigation  tool performance    move   machine learn approach  utilize fraggenescan   predict proteinsprotein fragment   novo sequence data fraggenescan use  well test algorithm   perform  silico translation  predict protein cod nucleic acid sequence utilize  approach   predict cod regions  dna sequence    base pair  longer  novel approach also enable  analysis  userprovided assemble contigs  note  fraggenescan  train  prokaryotes     identify proteins  eukaryotic sequence  result   view critically   cluster mgrast build cluster  proteins     identity level use  uclust  implementation  qiime  preserve  relative abundances  cluster greatly reduce  computational burden  compare  pair  short read  cluster    identity preserve sufficient biological signal  protein identification   create  representative  longest sequence   cluster  subject  similarity analysis functional identification  representative sequence   use blast instead  use  much  efficient algorithm sblat  implementation   blat algorithm   parallelize use openmpi  reconstruct  putative species composition  wgs data  look   phylogenetic origin   database sequence hit   proteinbased similarity search note  process  rrna  amplicon data  cover  subheading    annotation map  sequence similarity search  compute   protein database derive   m5nr  provide nonredundant integration  many databases users  easily change view without recomputation  example cog  kegg view   display   show  relative abundances  histidine biosynthesis   data set  four cow rumen metagenomes help  interpret result mgrast search  nonredundant m5nr  m5rna databases    sequence  unique  two databases  build  multiple sequence database source   individual sequence may occur multiple time  different strain  species  sometimes genera    identity   circumstances choose  “right” taxonomic information    straightforward process  optimally serve  number  different use case   implement three methods  end users  determine  number  hit occurrences   input sequence   database   sample best hit  best hit classification report  functional  taxonomic annotation   best hit   m5nr   feature   case   similarity search yield multiple samescoring hit   feature    choose  single “correct” label   reason mgrast double count  annotations  identical match properties  leave determination  truth   users   approach aim  inform   functional  taxonomic potential   microbial community  preserve  information subsequent analysis   bias   single feature may  multiple annotations lead  inflate hit count  users look   specific species  function   result  best hit classification  likely   want representative hit  representative hit classification select  single unambiguous annotation   feature  annotation  base   first hit   homology search   first annotation   hit   database  approach make count additive across functional  taxonomic level   better suit  comparisons  functional  taxonomic profile  multiple metagenomes lowest common ancestor lca   avoid  problem  multiple taxonomic annotations   single feature mgrast provide taxonomic annotations base   widely use lca method introduce  megan    method  hit  collect    bite score close   bite score   best hit  taxonomic annotation   feature   determine  compute  lca   species   set  replace  taxonomic annotations  ambiguous hit   single higherlevel annotation   ncbi taxonomy tree  abundance profile abundance profile essentially table  indicate detect taxa  function   relative abundance  determine   methods describe  subheading  step —examples   find   mgrast user manual see  “additional documentation”  subheading  find   end   chapter   primary data product   mgrasts user interface use  display information  annotate data set use  abundance profile  mgrast system defer   user  select several parameters   define  abundance data evalue percent identity  minimal alignment length     possible  arbitrarily select thresholds suitable   use case users  select   thresholds     value taxonomic profile use  ncbi taxonomy  taxonomic information  project   ncbi taxonomy functional profile  available  data source  provide hierarchical information  currently include seed subsystems kegg orthologs  cog seed subsystems represent  independent reannotation effort utilize  rast   mgrast manual curation  subsystems make   extremely valuable data source  current subsystems hierarchy   view    allow browse  subsystems subsystems represent  fourlevel hierarchy  subsystem level —highest level  subsystem level —intermediate level  subsystem level —similar   kegg pathway  subsystem level —actual functional assignment   feature  question kegg orthologs mgrast use  kegg enzyme number  implement  fourlevel hierarchy  note  kegg data   longer available  free download therefore  rely   latest freely downloadable version   data  kegg level —first digit    number ecx***  kegg level —first two digits    number ecxy**  kegg level —first three digits    number ecxyz*  kegg level —entire four digits    number  highlevel kegg categories   follow  cellular process  environmental information process  genetic information process  human diseases  metabolism  organizational systems cog  eggnog categories  highlevel cog  eggnog categories   follow  cellular process  information storage  process  metabolism  poorly characterize   rrna pipeline  rrna pipeline start  upload  rrna read  proceed   follow step  rrna detection  read  identify  rrna   simple rrna detection  initial blat search   reduce rna database efficiently identify rna  reduce database     identity cluster version   silva database   use merely  differentiate sample contain solely rrna data   sample  wgs  transcriptomic sample  rrna cluster   rrnasimilar read  cluster    identity   longest sequence  pick   cluster representative  rrna identification   nucleotide blat similarity search   longest cluster representative  perform   m5rna database integrate silva  greengenes   rdp   use  mgrast user interface  mgrast system provide  rich web user interface  cover  aspects  metagenome analysis  data upload  ordination analysis  annotation abundances  web interface  also  use  data discovery metagenomic data set   easily select individually    basis  filter   technology include read length quality sample type  keyword  dynamic filter  result base  similarity  know reference proteins  taxonomy  example  user may want  perform  search   “phylum  actinobacteria  function  kegg pathway lysine biosynthesis  sample  ocean”  extract set  read match  appropriate function  taxa across metagenomes  result   display  familiar format include bar chart tree  incorporate abundance information heatmaps principal component analyse  raw abundance table export  tabular form  raw  process data   recover via download page    matr package   see subheading   metabolic reconstructions base  map  kegg pathways  also provide sample selection  crucial  understand largescale pattern  multiple metagenomes  compare accordingly mgrast support mix  mimarks   well  domainspecific plugins  specialize environments  extend  minimal gsc standards several project include terragenome hmp tara  emp use  gsc standards enable standardize query  integrate new sample   massive data set one key aspect   mgrast approach   creation  smart data products enable  user   time  analysis  determine  best parameters   example  comparison  sample    without  need  recomputation  result  navigation  mgrast website  rich  functionality  offer several options  site    five main page   home page show  blue  fig  open image  new windowfig  fig  sitemap   mgrast version  website   site map  main page  show  blue management page  orange  green box represent page    directly accessible   home page download page—lists  publicly available data  download  data  structure  project browse page—allows interactive browse   data set   power  metadata search page—allows identifier taxonomy  functiondriven search   public data analysis page—enables indepth analyse  comparisons  data set upload page—allows users  provide  sample  metadata  mgrast home metagene overview page—provides  overview   individual data set  upload page data  metadata   upload   form  spreadsheets along   sequence data  use   ftp   http protocols  web uploader  automatically split large file  also allow parallel upload mgrast support data set   augment  rich metadata use  standards  technology develop   gsc  user   temporary storage location inside  mgrast system  inbox provide temporary storage  data  metadata   submit   system use  inbox users  extract compress file convert  number  vendorspecific format  mgrast submissioncompliant format  obtain  md5 checksum  verify  transmission  mgrast   alter  data  web uploader   optimize  large data set    gigabasepairs often result  file size  excess     browse page metadataenabled data discovery  browse page list  data set visible   user  users  data set  well   public data   data share   users  page also provide  overview   nonpublic data set submit   user  share  users  interactive metagenome browse table provide  interactive graphical mean  discover data base  technical data  sequence type  data set size  metadata  location  biome  project page  project page provide  list  data set  metadata   project  table   bottom   project page provide access   individual metagenomes  click   identifiers   first column  addition  final column provide download  metadata submit data   analysis result via  three label arrows   data set owners  project page provide  edit capability use  number  menu entries   top   page figure  show  available options open image  new windowfig  fig  project page provide  summary   data   project   interface  download share project—make  data   project available  third party via send  access tokens add jobs—add additional data set   project edit project data—edit  content   page upload info—upload information   display   page upload metadata—upload  metadata spreadsheet   project export metadata2—export  metadata spreadsheet   project  overview page mgrast automatically create  individual summary page   data set  metagenome overview page provide  summary   annotations   single data set  page  make available   automate pipeline   computation  finish  page   good start point  look   particular data set  provide information regard technical detail  biological content  page  intend   single point  reference  metadata quality  data  also provide  initial overview   analysis result  individual data set  default parameters  analyse  available   analysis page  technical detail  sequence  analysis  overview page provide  mgrast    data set  unique identifier   usable   accession number  publications additional information    name   submit  organization   userprovided metagenome name  display   top   page  static url  link   system    stable across change   mgrast web interface  provide  additional information fig  mgrast provide  automatically generate paragraph  text describe  submit data   result compute   pipeline  mean   project information  display additional information provide   data submitters   time  submission  later one   key diagram  mgrast   sequence breakdown pie chart fig  classify  submit sequence submit  several categories accord   annotation status  detail   description   mgrast  pipeline   feature annotate  mgrast  protein cod genes  ribosomal proteins open image  new windowfig  fig  sequence   pipeline  classify  one  five categories grey  failed   red  unknown sequence yellow  unknown function  protein cod green  protein cod  know function  blue  ribosomal rna   example     sequence  either filter    fail   recognize  either protein cod  ribosomal note   performance reason   sequence feature  annotate   default pipeline  feature type   small rnas  regulatory motifs  crisprs     require significantly higher computational resources  also  frequently  support   unassembled short read  constitute  vast majority  todays metagenomic data  mgrast  quality   sequence data come  nextgeneration instrument require careful design  experiment lest  sensitivity   methods  greater   signaltonoise ratio  data support  overview page also provide metadata   data set   extent   information   make available metadata enable  researchers  discover data set  compare annotations mgrast require standard metadata  data share  data publication   implement use  standards develop   genomics standards consortium  metadata store   specific data set  available  mgrast  merely display  standardize subset   table  link   bottom   table “ metadata” provide access   table   complete metadata  enable users  provide extend metadata go beyond  gsc minimal standards  mechanism  provide community consensus extensions   minimal checklists   environmental package  explicitly encourage   require  use mgrast  metagenome quality control  analysis flowchart  analysis statistics provide  overview   number  sequence   stage   pipeline  text block next   analysis flowchart present  number next   definitions  source hit distribution  source hit distribution show  percentage   predict protein feature annotate  similarity   protein  know function per source database  addition ribosomal rna genes  map   rrna databases  addition  display  print  number  record   m5nr protein database    m5rna ribosomal databases   statistics mgrast also provide  quick link   statistics  example  analysis statistics  analysis flowchart provide sequence statistics   main step   pipeline  raw data  annotation describe  transformation   data  step sequence length   histograms display  distribution    quality control step metadata  present   searchable table  contain contextual metadata describe sample location acquisition library construction  sequence use gsc compliant metadata  metadata   download   table  biological part   overview page  taxonomic hit distribution display divide taxonomic units   series  pie chart    annotations group  various taxonomic rank domain phylum class order family genus  subsets  selectable  downstream analysis  also enable download  subsets  read  example  hit  specific taxonomic unit  rank abundance  rank abundance plot provide  rankordered list  taxonomic units   userdefined taxonomic level order   abundance   annotations  rarefaction  rarefaction curve  annotate species richness   plot see fig    total number  distinct species annotations   function   number  sequence sample  slope   righthand part   curve  relate   fraction  sample species   rare   leave  steep slope indicate   large fraction   species diversity remain   discover   curve become flatter   right  reasonable number  individuals  sample  intensive sample  likely  yield   additional species sample curve generally rise quickly  first   level  toward  asymptote  fewer new species  find per unit  individuals collect open image  new windowfig  fig  rarefaction plot show  curve  annotate species richness   number  unique species  curve   plot   total number  distinct species annotations   function   number  sequence sample  rarefaction curve  derive   protein taxonomic annotations   subject  problems stem  technical artifacts  artifacts   similar   ones affect amplicon sequence    process  infer species  protein similarities may introduce additional uncertainty  alpha diversity   section  display  estimate   alpha diversity base   taxonomic annotations   predict proteins  alpha diversity  present  context   metagenomes    project see fig  open image  new windowfig  fig  alpha diversity plot show  range  diversity value   project  data set belong   min max  mean value  show   standard deviation range  different shade  alphadiversity   metagenome  show  red  specieslevel annotations     annotation source databases use  mgrast  alpha diversity estimate   single number  summarize  distribution  specieslevel annotations   data set  shannon diversity index   abundanceweighted average   logarithm   relative abundances  annotate species  compute  species richness   antilog   shannon diversity  functional categories  section contain four pie chart provide  breakdown   functional categories  kegg cog seed subsystems  eggnogs click   individual pie chart slice  save  respective sequence   workbench  relative abundance  sequence per functional category   download   spreadsheet  users  browse  functional breakdowns   detail functional analysis allow  user  manipulate parameters  sequence similarity match  available   analysis page  analysis page  mgrast annotation pipeline produce  set  annotations   sample  annotations   interpret  functional  taxonomic abundance profile  analysis page   use  view  profile   single metagenome   compare profile  multiple metagenomes use various visualizations  heatmap  statistics  pcoa normalization  page  divide  three part follow  typical workflow fig  open image  new windowfig  fig  threestep process  use  analysis page  select  profile  hit see text type  select  list  metagenomes  set annotation source  similarity parameters  choose  comparison  data type selection   mgrast analysis scheme   selection   particular taxonomic  functional abundance profile map  taxonomic annotations since    always  unique map  hit  annotation  provide three interpretations best hit representative hit  lowest common ancestor  choose  lca annotations   downstream tool  available  reason   fact    lca annotations   sequence   annotate    level classifications  return  different taxonomic level functional annotations   group  mappings  functional hierarchies    display without  hierarchy  addition  recruitment plot display  recruitment  protein sequence   reference genome  select data type  data selections  data visualizations specific    data selection selection  sample  parameters  dialog allow  selection  multiple metagenomes    compare individually  select  compare  group comparison  always relative   annotation source evalue  percent identity cutoffs selectable   section  addition   metagenomes available  mgrast set  sequence previously save   workbench   select  visualization  data visualization data visualization  comparison depend   select profile type  profile   metagenomes   visualize  compare  use barcharts tree spreadsheetlike table heatmaps pcoa rarefaction plot circular recruitment plot  kegg map  data selection dialog provide access  data set  four ways  four categories   select   pulldown menu private data—list  private  share data set  browse  available metagenomes collections—defined set  metagenomes group  easier analysis    recommend way  work   analysis page projects—global group  data set group   submit user  project name   display public data—display   public data set  use collections  project data  also  group  one set per collection  project  subsequently compare  add  normalization normalization refer   transformation  attempt  reshape  underlie distribution  large number  biological variables exhibit  lognormal distribution mean    data  transform   log transformation  value exhibit  normal distribution log transformation   count data make  normalize data product    likely  satisfy  assumptions  additional downstream test   anova  ttests standardization   transformation apply   distribution   group  distributions    distributions exhibit   mean    standard deviation  remove  aspects  intersample variability   make data  comparable  sort  procedure  analogous  commonly practice scale procedures    robust    control   scale  location  rarefaction  rarefaction view  available   taxonomic data  rarefaction curve  annotate species richness   plot see fig    total number  distinct species annotations   function   number  sequence sample  show   figure multiple data set   include open image  new windowfig  fig  rarefaction plot show  curve  annotate species richness  curve   plot   total number  distinct species annotations   function   number  sequence sample  slope   righthand part   curve  relate   fraction  sample species   rare   rarefaction curve  flat  intensive sample  likely  yield    additional species  rarefaction curve  derive   protein taxonomic annotations   subject  problems stem  technical artifacts  artifacts   similar   ones affect amplicon sequence    process  infer species  protein similarities may introduce additional uncertainty   analysis page  rarefaction plot serve   mean  compare species richness  sample   way independent   sample depth   leave  steep slope indicate   large fraction   species diversity remain   discover   curve become flatter   right  reasonable number  individuals  sample  intensive sample  likely  yield    additional species sample curve generally rise  quickly  first   level  toward  asymptote  fewer new species  find per unit  individuals collect  rarefaction curve  calculate   table  species abundance  curve represent  average number  different species annotations  subsamples   complete data set  heatmapdendrogram  heatmapdendrogram allow  enormous amount  information   present   visual form   amenable  human interpretation dendrograms  tree  indicate similarities  annotation vectors  mgrast heatmapdendrogram  two dendrograms one indicate  similaritydissimilarity among metagenomic sample xaxis dendrogram  another indicate  similaritydissimilarity among annotation categories  functional roles  yaxis dendrogram  distance metric  evaluate  every possible pair  sample abundance profile  cluster algorithm  wardbased cluster  produce  dendrogram tree  square   heatmap dendrogram represent  abundance level   single category   single sample  value use  generate  heatmapdendrogram figure   download   table  click   download button  ordination mgrast use principle coordinate analysis pcoa  reduce  dimensionality  comparisons  multiple sample  consider functional  taxonomic annotations dimensionality reduction   process  allow  complex variation find   large data set   abundance value  thousands  functional roles  annotate species across dozens  metagenomic sample   reduce   much smaller number  variables    visualize  simple two  threedimensional scatter plot  plot enable interpretation   multidimensional data   humanfriendly presentation sample  exhibit similar abundance profile taxonomic  functional group together whereas   differ  find farther apart  key feature  pcoabased analyse   users  compare components        metadata record variables  sample  biome dna extraction protocol  reveal correlations  extract variation  metadatadefined characteristics   sample   also possible  couple pcoa  higherresolution statistical methods  order  identify individual sample feature taxa  function  drive correlations observe  pcoa visualizations  couple   accomplish  permutationbased statistics apply directly   data  calculation  distance measure use  produce pcoas alternatively one  apply conventional statistical approach  anova  kruskalwallis test  group observe  pcoabased visualizations  bar chart  bar chart visualization option   analysis page   builtin ability  drill   click   specific category   expand  categories  show  normalize abundance adjust  sample size  various level  abundance information display   download   local spreadsheet   subselection   make   domain bacteria select data   send   workbench  detail analysis  addition read   specific level   add   workbench  tree diagram  tree diagram allow comparison  data set   hierarchy  subsystems   ncbi taxonomy  hierarchy  display   root tree   abundance normalize  data set size  raw   data set   various categories  display   bar chart   category  click   category inside  circle detail information   request   node  table  table tool create  spreadsheetbased abundance table    search  restrict   user table   generate  userselected level  phylogenetic  functional resolution table data   visualize  use krona     export  biom  format   use   tool  qiime  table also   export  tabseparated text abundance table serve   basis   comparative analysis tool  mgrast  pcoa  heatmapdendrograms  workbench  workbench  design  allow users  select subsets   data  comparison  export specifically  workbench support select sequence feature  submit    analysis   analysis  number  use case  describe   important limitation   current implementation   data send   workbench exist    current session  close  metadata publish  share mgrast    analytical platform   data integration system  enable data reuse  example  metaanalyses  require   data  make available  third party contain  least minimal metadata  mgrast team  decide  follow  minimal checklist approach use   gsc mgrast provide  mechanism  make data  analyse publicly accessible   submit user  make data public  mgrast  state  metadata  mandatory  data set publication metazen    web base tool  assist endusers   creation  metadata   correct control vocabularies    correct format  addition  publish data  analysis  also  share  specific users fig   share data users simply enter  email address via click share   overview page open image  new windowfig  fig  data set share  mgrast  users orange dot show  connect edge  matr metagenomic analysis tool     recently produce  package    environment  statistical compute wwwrprojectorg  provide accessory analytical capabilities  complement  already available   mgrast website  matr package  primarily design  download  analysis  mgrastbased annotation abundance profile  make  possible  download annotation abundance data  mgrast   friendly data object suitable  analysis  include analysis function  note  matr   specifically design  perform largescale analyse  abundance profile  dozens  thousands  data set  suitable preprocessing normalization statistics  visualization tool users  utilize  builtin tool     enormous variety  tool available within   universe  release version  matr  available  cran  prerelease  development versions  available  github   google group  available #forummatrforum  publication demonstrate  ease   matr   use  conduct largescale analyse  forthcoming",9
185,VaPid,"VAPiD: a lightweight cross-platform viral annotation pipeline and identification tool to facilitate virus genome submissions to NCBI GenBank
VAPiD can be downloaded at https://github.com/rcs333/VAPiD. An installation guide, usage instructions, and test data can also be found at the above webpage. The invocation of VAPiD is shown in Fig. 1, users must provide a standard FASTA file containing all of the viral genomes they wish to annotate. Users also must provide a GenBank submission template (.sbt file) that includes author, publication, and project metadata. The GenBank submission template can be used for multiple viral sequences or submissions and is easily created at the NCBI Submission portal (https://submit.ncbi.nlm.nih.gov/genbank/template/submission/). An optional sample metadata file (.csv file) can be provided to VAPiD to expedite the process of incorporating sample metadata. This optional file can also be used to include any of the Source Modifiers supported by NCBI (https://www.ncbi.nlm.nih.gov/Sequin/modifiers.html). If no sample metadata file is provided, VAPiD will prompt the user to input the required sample metadata at runtime. Additionally, users can provide a specified reference from which to annotate all viruses in a run, as well as provide their own BLASTn database or force VAPiD to search NCBI’s NT database over the internet. An external file that holds a picture, illustration, etc. Object name is 12859_2019_2606_Fig1_HTML.jpg Fig. 1 Example usage of VAPiD. The two required files are shown as genome.fasta and author_info.sbt. Genome.fasta is all of the viral genomes you wish to submit, named as you want them to appear on GenBank. In the example code provided in the github repository this example file is called example.fasta. The author_info.sbt file is an NCBI specific file for attaching sequence author names to sequin files and is a required part of properly submitting sequences to NCBI. This file can be generated at (https://submit.ncbi.nlm.nih.gov/genbank/template/submission/ ). The first optional command is a comma separated file in which you can include all relevant metadata. You can create additional columns here so long as they correspond to NCBI approved sequence metadata. A list and formatting requirements can be found at (https://www.ncbi.nlm.nih.gov/Sequin/modifiers.html). Note that FASTA sequence names must be identical to names in the optional metadata sheet. Additionally, one could omit the metadata sheet and VAPiD will prompt for strain name, collection-date, country, and coverage data automatically at runtime. The second optional argument is a location of a local BLASTn database, which will force VAPiD to use the specified database instead of the included database. The last optional argument will force VAPiD to send an online search query to NCBI’s NT database The VAPiD pipeline is summarized in Fig. 2. The first step is finding the correct reference sequence. This is accomplished in three ways 1) using the provided reference database (default), 2) forcing VAPiD to execute an online BLASTn search of NCBI’s NT database, or 3) inputting the accession number of a single NCBI sequence to use as the reference. An external file that holds a picture, illustration, etc. Object name is 12859_2019_2606_Fig2_HTML.jpg Fig. 2 General design and information flow of VAPiD. First the provided sequences are used as queries for a local BLAST search (default) or an online BLASTn search. After results have been returned a reference annotation is downloaded, if a specific reference accession number is given then this reference is downloaded. Next the original FASTA file is aligned with the reference FASTA and the resulting alignment is used to map the reference annotations onto the new FASTA. Then custom code runs through the file and handles RNA editing, ribosomal slippage and splicing. These finalized annotations are then plugged into NCBI’s tbl2asn with the author information and sequin files are generated as well as .gbk files which can be used to manually verify accuracy of new annotations. Quality checked .sqn files can be emailed directly to GenBank In the default case, NCBI’s BLAST+ tools are called from the command line to search against a reference database that is included with the VAPiD installation. This database was generated by downloading all complete viral genomes in NCBI on May 1, 2018. The best result from this search is passed as the reference into the next steps. If the online option for finding the reference is specified, VAPiD finds an appropriate reference sequence for each genome to be annotated by performing an online BLASTn search with a word size of 28 using BioPython’s NCBI WWW.qblast() function against the online NCBI NT database. The BLASTn output is parsed for the best scoring alignment among the top 15 results that contains “complete genome” in the reference definition line. If no complete genome is found in the top 15 BLASTn results, the top-scoring hit is used as the reference sequence. If a specific reference is provided, VAPiD simply downloads it directly from NCBI. After the correct reference is downloaded, gene locations are stripped from the reference and a pairwise nucleotide alignment between the reference and the submitted sequence is generated using MAFFT [21]. The relative locations of the genes on the reference sequence are then mapped onto the new sequence based off the alignment. This putative alignment only requires that start codons are in regions of high homology and does not rely on intergenic spacing or gene lengths. Gene names are taken from the annotated reference sequence GenBank entry. Spellchecking is performed using NCBI’s ESpell module. This module provides spellchecking of many biological strings including protein product names. An optional argument can be provided at execution that enables this step. The diverse array of methods viruses use to encode genes can present problems for any viral genome annotator. Ribosomal slippage allows viruses to produce two proteins from a single mRNA transcript by having the ribosome ‘slip’ one or two nucleotides along the mRNA transcript, thus changing the reading frame. Since ribosomal slippage is well conserved within viral species and complete reference genomes often list exactly where it occurs, custom code was used to strip the correct junction site and include it in the annotation. RNA editing is another process by which viruses can include multiple proteins in a single gene. In RNA editing, the RNA polymerase co-transcriptionally adds one or two nucleotides that are not on the template. These changes are specifically created during viral mRNA transcription and not during viral genome replication. RNA editing presents an annotation issue because the annotated protein sequence does not match the expected translated nucleotide sequence. To correctly annotate genes with RNA editing, VAPiD parses the reference genome viral species, detects the RNA editing locus, and mimics the RNA polymerase. VAPiD adds the correct number of non-templated nucleotides for the viral species and provides an alternative protein translation. This process is hard-coded for human parainfluenza 2–4, Nipah virus, Sendai virus, measles virus, and mumps virus. Although RNA editing occurs in Ebola virus, references for Ebola virus are annotated in the same way as ribosomal slippage, so code written for ribosomal slippage handles Ebola virus annotations. After ribosomal slippage and RNA editing are processed, files required for GenBank submission are generated with the provided author and sample metadata. VAPiD first generates the .fsa file, .tbl file, and optional .cmt file. Submission files for each viral genome are packaged into a separate folder for each sequence. VAPiD then runs tbl2asn on each folder using the provided GenBank submission template file (.sbt). tbl2asn generates error reports and Sequin (.sqn) and GenBank (.gbk) files for manual verification and GenBank submission via email attachment to gb-admin@ncbi.nlm.nih.gov.",Annotation,"vapid  lightweight crossplatform viral annotation pipeline  identification tool  facilitate virus genome submissions  ncbi genbank
vapid   download    installation guide usage instructions  test data  also  find    webpage  invocation  vapid  show  fig  users must provide  standard fasta file contain    viral genomes  wish  annotate users also must provide  genbank submission template sbt file  include author publication  project metadata  genbank submission template   use  multiple viral sequence  submissions   easily create   ncbi submission portal   optional sample metadata file csv file   provide  vapid  expedite  process  incorporate sample metadata  optional file  also  use  include    source modifiers support  ncbi    sample metadata file  provide vapid  prompt  user  input  require sample metadata  runtime additionally users  provide  specify reference    annotate  viruses   run  well  provide   blastn database  force vapid  search ncbis  database   internet  external file  hold  picture illustration etc object name  12859_2019_2606_fig1_htmljpg fig  example usage  vapid  two require file  show  genomefasta  author_infosbt genomefasta     viral genomes  wish  submit name   want   appear  genbank   example code provide   github repository  example file  call examplefasta  author_infosbt file   ncbi specific file  attach sequence author name  sequin file    require part  properly submit sequence  ncbi  file   generate     first optional command   comma separate file     include  relevant metadata   create additional columns   long   correspond  ncbi approve sequence metadata  list  format requirements   find   note  fasta sequence name must  identical  name   optional metadata sheet additionally one could omit  metadata sheet  vapid  prompt  strain name collectiondate country  coverage data automatically  runtime  second optional argument   location   local blastn database   force vapid  use  specify database instead   include database  last optional argument  force vapid  send  online search query  ncbis  database  vapid pipeline  summarize  fig   first step  find  correct reference sequence   accomplish  three ways  use  provide reference database default  force vapid  execute  online blastn search  ncbis  database   inputting  accession number   single ncbi sequence  use   reference  external file  hold  picture illustration etc object name  12859_2019_2606_fig2_htmljpg fig  general design  information flow  vapid first  provide sequence  use  query   local blast search default   online blastn search  result   return  reference annotation  download   specific reference accession number  give   reference  download next  original fasta file  align   reference fasta   result alignment  use  map  reference annotations onto  new fasta  custom code run   file  handle rna edit ribosomal slippage  splice  finalize annotations   plug  ncbis tbl2asn   author information  sequin file  generate  well  gbk file    use  manually verify accuracy  new annotations quality check sqn file   email directly  genbank   default case ncbis blast tool  call   command line  search   reference database   include   vapid installation  database  generate  download  complete viral genomes  ncbi  may    best result   search  pass   reference   next step   online option  find  reference  specify vapid find  appropriate reference sequence   genome   annotate  perform  online blastn search   word size   use biopythons ncbi wwwqblast function   online ncbi  database  blastn output  parse   best score alignment among  top  result  contain “complete genome”   reference definition line   complete genome  find   top  blastn result  topscoring hit  use   reference sequence   specific reference  provide vapid simply download  directly  ncbi   correct reference  download gene locations  strip   reference   pairwise nucleotide alignment   reference   submit sequence  generate use mafft   relative locations   genes   reference sequence   map onto  new sequence base   alignment  putative alignment  require  start codons   regions  high homology    rely  intergenic space  gene lengths gene name  take   annotate reference sequence genbank entry spellchecking  perform use ncbis espell module  module provide spellchecking  many biological string include protein product name  optional argument   provide  execution  enable  step  diverse array  methods viruses use  encode genes  present problems   viral genome annotator ribosomal slippage allow viruses  produce two proteins   single mrna transcript    ribosome slip one  two nucleotides along  mrna transcript thus change  read frame since ribosomal slippage  well conserve within viral species  complete reference genomes often list exactly   occur custom code  use  strip  correct junction site  include    annotation rna edit  another process   viruses  include multiple proteins   single gene  rna edit  rna polymerase cotranscriptionally add one  two nucleotides      template  change  specifically create  viral mrna transcription    viral genome replication rna edit present  annotation issue   annotate protein sequence   match  expect translate nucleotide sequence  correctly annotate genes  rna edit vapid parse  reference genome viral species detect  rna edit locus  mimic  rna polymerase vapid add  correct number  nontemplated nucleotides   viral species  provide  alternative protein translation  process  hardcoded  human parainfluenza  nipah virus sendai virus measles virus  mumps virus although rna edit occur  ebola virus reference  ebola virus  annotate    way  ribosomal slippage  code write  ribosomal slippage handle ebola virus annotations  ribosomal slippage  rna edit  process file require  genbank submission  generate   provide author  sample metadata vapid first generate  fsa file tbl file  optional cmt file submission file   viral genome  package   separate folder   sequence vapid  run tbl2asn   folder use  provide genbank submission template file sbt tbl2asn generate error report  sequin sqn  genbank gbk file  manual verification  genbank submission via email attachment  gbadmin@ncbinlmnihgov",9
186,DNAScan,"DNAscan: personal computer compatible NGS analysis, annotation and visualisation
Pipeline description The DNAscan pipeline consists of four stages: Alignment, Analysis, Annotation and Report generation, and can be run in three modes: Fast, Normal and Intensive, according to user requirements (Fig. 1 and Table 1). These modes have been designed to optimize computational effort without compromising performance for the type of genetic variant the user is testing (see mode recommendations in Table 2). The user can restrict the analysis to any sub-region of the human genome by proving either a region file in bed format, a list of gene names, or using the whole-exome option, reducing the processing time and generating region specific reports. Fig. 1 figure1 Pipeline overview. Central panel: DNAscan accepts sequencing data, and optionally variant files. The pipeline firstly performs an alignment step (details in the left panel), followed by a customisable data analysis protocol (details in the right panel). Finally, results are annotated and user-friendly QC and result reports are generated. The annotation step uses Annovar to enrich the results with functional information from external databases. Right panel: detailed description of the post alignment analysis pipeline. Aligned reads are used by the variant calling pipeline (Freebayes + GATK HC); both aligned and unaligned reads are used by Manta and ExpensionHunter (for which repeat description files have to be provided) to look for structural variants. The unaligned reads are mapped to a database of known viral genomes (NCBI database) to screen for their DNA in the input sequencing data. Left panel: Alignment stage description. Raw reads are aligned with HISAT2. Resulting soft-clipped and unaligned reads are realigned with BWA mem and then merged with the others using Samtools Full size image Table 1 Key tools used by DNAscan in the three modes Full size table Table 2 DNAscan mode usage recommendations Full size table Alignment DNAscan accepts sequencing data in fastq.gz and as a Sequence Alignment Map (SAM) file (and its compressed version BAM). HISAT2 and BWA mem [6, 7] are used to map the reads to the reference genome (Fig. 1, left panel). This step is skipped if the user provides data in SAM or BAM formats. HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads to a reference genome. HISAT2 uses a new reference indexing scheme called a Hierarchical Graph FM index (HGFM) [8], thanks to which it can guarantee a high performance, comparable to state-of-the-art tools, in approximately one quarter of the time of BWA and Bowtie2 [9] (see Additional file 1). Variant calling pipelines based on HISAT2 generally perform poorly on indels [10]. To address this issue, DNAscan uses BWA to realign soft-clipped and unaligned reads. This alignment refinement step is skipped if DNAscan is run in Fast mode. Samblaster [11] is used to mark duplicates during the alignment step and Sambamba [12] to sort the aligned reads. Both the variant callers, Freebayes [13] and GATK Haplotype Caller (HC) [5] used in the following step, are duplicate-aware, meaning that they automatically ignore reads marked as duplicate. The user can optionally exclude it from the workflow according to the study design, e.g. when an intensive Polymerase Chain Reaction (PCR) amplification of small regions is required. Analysis Various analyses are performed on the mapped sequencing data (Fig. 1, right panel): SNV and small indel calling is performed using Freebayes, whose reliability is well reported [14, 15]. However, taking advantage of the documented better performance of GATK HC in small indel calling, we decided to add a customised indel calling step to DNAscan, called Intensive mode. This step firstly extracts the genome positions for which an insertion or a deletion is present on the cigar of at least one read, and secondly calls indels using GATK HC on these selected positions. The reduced number of positions where this occurs allows for a targeted use of GATK HC, limiting the required computational effort and time. The resulting SNVs and small indel calls with genotype quality smaller then 20 and depth smaller than 10 are discarded. The user can customize these filters according to their needs (see GitHub [16] for details and a complete list of available filters). Two Illumina developed tools, Manta [17] and Expansion Hunter [18] are used for detecting medium and large structural variants (> 50 bp) including insertions, deletions, translocations, duplications and known repeat expansions. These tools are optimised for high speed and can analyse a 40x WGS sample in about one hour using 4 threads, maintaining a very high performance. DNAscan also has options to scan the sequencing data for microbial genetic material. It performs a computational subtraction of human host sequences to identify sequences of infectious agents including viruses, bacteria or fungi, by aligning the non-human or unaligned reads to the whole NCBI database [19,20,21] of known viral, bacterial or any custom set of microbial genomes and reporting the number of reads aligned to each non-human genome, its length and the number of bases covered by at least one read. Annotation Variant calls are then annotated using Annovar [22]. The annotation includes the use of databases such as ClinVar [23], Exac [24], dbSNP [25] and dbNSFP [26] (more information about how to customise the annotation, e.g. by selecting alternative databases and/or focusing on specific genome regions, are available on GitHub). Reports and visualization utilities DNAscan produces a wide set of quality control (QC) and result reports and provides utilities for visualisation and interpretation of the results. MultiQC [27] is used to wrap up and visualise QC results. FastQC [28], Samtools [29] and Bcftools [30] are used to perform QC on the sequencing data, its alignment and the called variants. An example is available on GitHub [31]. A tab delimited file including all variants found within the selected region is also generated [32]. This report would include all annotations performed by Annovar [22] in a format that is easy to handle with any Excel-like software by users of all levels of expertise. Three iobio services (bam.iobio, vcf.iobio and gene.iobio) are locally provided with the pipeline allowing for the visualisation of the alignment file [33], the called variants [34] and for a gene based visualisation and interpretation of the results [35]. DNAscan benchmark Benchmarking every DNAscan component is not needed since a range of literature is available [14, 15, 17, 36, 37]. However, to our knowledge, none exists assessing HISAT2 [8] (the short-read mapper used by the pipeline) either for DNA read mapping or as part of DNA variant calling pipelines. In this manuscript, we both assess the performance of HISAT2 with BWA and Bowtie2 [9] mapping 1.25 billion WGS reads sequenced with the Illumina Hiseq X and 150 million simulated reads (see Additional file 1), and compare our SNV/indel calling pipeline in Fast, Normal and Intensive modes with the GATK BPW [5] and SpeedSeq [4] over the whole exome sequencing of NA12878. Illumina platinum calls are used as true positives [38]. We also show how DNAscan represents a powerful tool for medical and scientific use by analysing real DNA sequence data from two patients affected by Amyotrophic Lateral Sclerosis (ALS) and of HIV infected human cells. For the ALS patients we use both a gene panel of 10 ALS-related genes, whose feasibility for diagnostic medicine has been previously investigated [2], sequenced with the Illumina Miseq platform, and the WGS data from the Project MinE sequencing dataset [39]. DNAscan was used to look for SNVs, small indels, structural variants, and known repeat expansions. The WGS of an HIV infected human cell sample [40] was used to test DNAscan for virus detection. Variant calling assessment To assess the performance of DNAscan in calling SNVs and indels, we used the Illumina Genome Analyzer II whole exome sequencing of NA12878. Illumina platinum calls [38] were used as true positives. GATK BPW calls were generated using default parameters and following the indications on the GATK website [41] for germline SNVs and indels calling. These include the pre-processing and variant discovery steps for single sample, i.e. skipping the Merge and Join Genotype steps. SpeedSeq calls were generated running the “align” and “var” commands as described on GitHub [42]. RTG Tools [43] (“vcfeval” command) was used to evaluate the calls. F-measure, Precision and Sensitivity are defined as in the following: 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=𝑇𝑝 𝑇𝑝+𝐹𝑝, 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦=𝑇𝑝 𝑇𝑝+𝐹𝑛 and 𝐹−𝑚𝑒𝑎𝑠𝑢𝑟𝑒=2×𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛×𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦, where Tp is true positives, Fp false positives and Fn false negatives. ALS Miseq and whole genome sequencing test cases Using DNAscan in Fast mode, we analysed real DNA sequence data from two ALS patients (case A and case B). Case A carries a non-synonymous mutation in the FUS gene [44] (variant C1561T, amino acid change R521C, variant dbSNP id rs121909670 [45]) known to be a cause of ALS (ClinVar id RCV000017611.25). A panel of 10 ALS related genes was sequenced with the Illumina Miseq platform for case A. The Miseq gene panel was designed and tested for diagnostic purposes [2]. For these 10 genes (BSCL2, CEP112, FUS, MATR3, OPTN, SOD1, SPG11, TARDBP, UBQLN2, and VCP), the full exon set was sequenced, generating over 825,000,222-base-long paired reads. DNAscan was used to call SNVs, indels, and structural variants on case A. Case B has a confirmed C9orf72 expansion mutation on one allele, also known to be causative of ALS [46]. This expansion mutation is thousands of repeats long. and 40x WGS data was generated with the Illumina Hiseq X for case B. The WGS sample (paired reads, read length = 150, average coverage depth = 40) was sequenced as part of the Project MinE sequencing dataset [39]. For this sample we ran DNAscan on the whole genome. However, both for practical reasons and to simulate a specific medical diagnostic interest, we focused our analysis report on the 126 ALS related genes reported on the ALSoD webserver [47] and also looked for the C9orf72 repeat. For both samples, we also reported variants linked to frontotemporal dementia, which is a neurodegenerative disease that causes neuronal loss, predominantly involving the frontal or temporal lobes, with a genetic and clinical overlap with ALS",Annotation,"dnascan personal computer compatible ngs analysis annotation  visualisation
pipeline description  dnascan pipeline consist  four stag alignment analysis annotation  report generation    run  three modes fast normal  intensive accord  user requirements fig   table   modes   design  optimize computational effort without compromise performance   type  genetic variant  user  test see mode recommendations  table   user  restrict  analysis   subregion   human genome  prove either  region file  bed format  list  gene name  use  wholeexome option reduce  process time  generate region specific report fig  figure1 pipeline overview central panel dnascan accept sequence data  optionally variant file  pipeline firstly perform  alignment step detail   leave panel follow   customisable data analysis protocol detail   right panel finally result  annotate  userfriendly   result report  generate  annotation step use annovar  enrich  result  functional information  external databases right panel detail description   post alignment analysis pipeline align read  use   variant call pipeline freebayes  gatk   align  unaligned read  use  manta  expensionhunter   repeat description file    provide  look  structural variants  unaligned read  map   database  know viral genomes ncbi database  screen   dna   input sequence data leave panel alignment stage description raw read  align  hisat2 result softclipped  unaligned read  realign  bwa mem   merge   others use samtools full size image table  key tool use  dnascan   three modes full size table table  dnascan mode usage recommendations full size table alignment dnascan accept sequence data  fastqgz    sequence alignment map sam file   compress version bam hisat2  bwa mem    use  map  read   reference genome fig  leave panel  step  skip   user provide data  sam  bam format hisat2   fast  sensitive alignment program  map nextgeneration sequence read   reference genome hisat2 use  new reference index scheme call  hierarchical graph  index hgfm  thank     guarantee  high performance comparable  stateoftheart tool  approximately one quarter   time  bwa  bowtie2  see additional file  variant call pipelines base  hisat2 generally perform poorly  indels   address  issue dnascan use bwa  realign softclipped  unaligned read  alignment refinement step  skip  dnascan  run  fast mode samblaster   use  mark duplicate   alignment step  sambamba   sort  align read   variant callers freebayes   gatk haplotype caller   use   follow step  duplicateaware mean   automatically ignore read mark  duplicate  user  optionally exclude    workflow accord   study design    intensive polymerase chain reaction pcr amplification  small regions  require analysis various analyse  perform   map sequence data fig  right panel snv  small indel call  perform use freebayes whose reliability  well report   however take advantage   document better performance  gatk   small indel call  decide  add  customise indel call step  dnascan call intensive mode  step firstly extract  genome position    insertion   deletion  present   cigar   least one read  secondly call indels use gatk    select position  reduce number  position   occur allow   target use  gatk  limit  require computational effort  time  result snvs  small indel call  genotype quality smaller    depth smaller    discard  user  customize  filter accord   need see github   detail   complete list  available filter two illumina develop tool manta   expansion hunter   use  detect medium  large structural variants    include insertions deletions translocations duplications  know repeat expansions  tool  optimise  high speed   analyse   wgs sample   one hour use  thread maintain   high performance dnascan also  options  scan  sequence data  microbial genetic material  perform  computational subtraction  human host sequence  identify sequence  infectious agents include viruses bacteria  fungi  align  nonhuman  unaligned read   whole ncbi database   know viral bacterial   custom set  microbial genomes  report  number  read align   nonhuman genome  length   number  base cover   least one read annotation variant call   annotate use annovar   annotation include  use  databases   clinvar  exac  dbsnp   dbnsfp   information    customise  annotation   select alternative databases andor focus  specific genome regions  available  github report  visualization utilities dnascan produce  wide set  quality control   result report  provide utilities  visualisation  interpretation   result multiqc   use  wrap   visualise  result fastqc  samtools   bcftools   use  perform    sequence data  alignment   call variants  example  available  github   tab delimit file include  variants find within  select region  also generate   report would include  annotations perform  annovar    format   easy  handle   excellike software  users   level  expertise three iobio service bamiobio vcfiobio  geneiobio  locally provide   pipeline allow   visualisation   alignment file   call variants     gene base visualisation  interpretation   result  dnascan benchmark benchmarking every dnascan component   need since  range  literature  available      however   knowledge none exist assess hisat2   shortread mapper use   pipeline either  dna read map   part  dna variant call pipelines   manuscript   assess  performance  hisat2  bwa  bowtie2  map  billion wgs read sequence   illumina hiseq    million simulate read see additional file   compare  snvindel call pipeline  fast normal  intensive modes   gatk bpw   speedseq    whole exome sequence  na12878 illumina platinum call  use  true positives   also show  dnascan represent  powerful tool  medical  scientific use  analyse real dna sequence data  two patients affect  amyotrophic lateral sclerosis als   hiv infect human cells   als patients  use   gene panel   alsrelated genes whose feasibility  diagnostic medicine   previously investigate  sequence   illumina miseq platform   wgs data   project mine sequence dataset  dnascan  use  look  snvs small indels structural variants  know repeat expansions  wgs   hiv infect human cell sample   use  test dnascan  virus detection variant call assessment  assess  performance  dnascan  call snvs  indels  use  illumina genome analyzer  whole exome sequence  na12878 illumina platinum call   use  true positives gatk bpw call  generate use default parameters  follow  indications   gatk website   germline snvs  indels call  include  preprocessing  variant discovery step  single sample  skip  merge  join genotype step speedseq call  generate run  “align”  “var” command  describe  github  rtg tool  “vcfeval” command  use  evaluate  call fmeasure precision  sensitivity  define    follow 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑇𝑝 𝑇𝑝𝐹𝑝 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦𝑇𝑝 𝑇𝑝𝐹𝑛  𝐹𝑚𝑒𝑎𝑠𝑢𝑟𝑒𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦    true positives  false positives   false negative als miseq  whole genome sequence test case use dnascan  fast mode  analyse real dna sequence data  two als patients case   case  case  carry  nonsynonymous mutation   fus gene  variant c1561t amino acid change r521c variant dbsnp  rs121909670  know    cause  als clinvar  rcv000017611  panel   als relate genes  sequence   illumina miseq platform  case   miseq gene panel  design  test  diagnostic purpose     genes bscl2 cep112 fus matr3 optn sod1 spg11 tardbp ubqln2  vcp  full exon set  sequence generate  baselong pair read dnascan  use  call snvs indels  structural variants  case  case    confirm c9orf72 expansion mutation  one allele also know   causative  als   expansion mutation  thousands  repeat long   wgs data  generate   illumina hiseq   case   wgs sample pair read read length   average coverage depth    sequence  part   project mine sequence dataset    sample  run dnascan   whole genome however   practical reason   simulate  specific medical diagnostic interest  focus  analysis report    als relate genes report   alsod webserver   also look   c9orf72 repeat   sample  also report variants link  frontotemporal dementia    neurodegenerative disease  cause neuronal loss predominantly involve  frontal  temporal lob   genetic  clinical overlap  als",9
187,Savant,"Savant: genome browser for high-throughput sequencing data
Savant retrieves and renders data every time a range change is requested by the user. Together, these processes happen quickly so as to confer seamless navigation around the genome. The renderer for each track is adaptive to both the display mode and the length of the viewed region chosen by the user. 3.1 Dynamic resolution changes Savant dynamically adjusts its resolution—the amount of information it displays—to optimize both nucleotide- and genome-scale visualization of tracks. For example, read alignments can be visualized as a coverage track when the number of base pairs within a region is too large to enable the visualization of individual reads. Once the region is small enough, Savant seamlessly switches to a read-alignment view, as shown in Figure 2. In addition to presenting a more intuitive visualization, this feature also reduces the program's memory footprint and improves overall speed. Fig. 2. Read alignments, visualized at various resolutions and using two modes. (A) Chromosome-wide view of read mappings, showing the overall coverage (with no coverage in the centromere). (B) Regional view, still visualized as a coverage map, showing higher coverage in certain regions of the genome. (C) Local view, the reads are shown separately and differences between the reads and the reference genome are colored. Reads on the forward and reverse strand are shown with different shades of blue. (D) Matepair (arc) mode, showing the relative distance between the two reads of a pair. Taller arcs indicate larger distances between the pairs. Open in new tabDownload slide Read alignments, visualized at various resolutions and using two modes. (A) Chromosome-wide view of read mappings, showing the overall coverage (with no coverage in the centromere). (B) Regional view, still visualized as a coverage map, showing higher coverage in certain regions of the genome. (C) Local view, the reads are shown separately and differences between the reads and the reference genome are colored. Reads on the forward and reverse strand are shown with different shades of blue. (D) Matepair (arc) mode, showing the relative distance between the two reads of a pair. Taller arcs indicate larger distances between the pairs. Individual tracks can also be locked to a particular range so that they are not updated until they are unlocked. Locked tracks can be used as overview profiles from which subregions can be selected to specify range changes for other tracks. Track locking also enables simultaneous viewing of high- and low-resolution profiles. 3.2 Display modes Particular data types can be displayed in different modes. For example, interval annotations can be squished together on a single line or packed neatly so that none overlap (mimicking the squish and pack modes of the UCSC browser). The representation can be dynamically toggled within the browser, with each representation option emphasizing different aspects of the data. The variant and strand modes for read alignments, for instance, use colors to emphasize mismatches in reads and the strands to which reads are mapped, respectively. A novel mode for representing matepairs shows arcs between the mapped locations of paired reads, where the height of each arc is proportional to the inferred insert size. Arcs for anomalously mapped pairs, such as those suggestive of inversions or duplications, are colored differently. The various modes for read alignments are illustrated in Figure 2. 4 FILE FORMATTING AND PERFORMANCE Savant supports a number of common text-based formats, which are described in Table 2. However, because text-files do not enable fast random access, Savant formats and saves each file so as to provide very efficient search operations at runtime. The speed with which Savant can sift through large datasets is enabled by the way in which it formats and indexes data. In particular, formatting involves converting text records into an indexed binary data structure specific to each data type. Sequence and continuous tracks are stored as fixed-width records, enabling direct lookup of records of interest. Annotation ranges (such as genes) are stored using a binning scheme similar to the one used in the UCSC Browser (Kent et al., 2002) and in BAM files (Li et al., 2009), so retrieving all ranges corresponding to some region usually requires only one, and at most O(log n) disk seeks. File formatting can be done directly through the application itself. Savant keeps its memory usage low by adjusting its sampling rate depending on the size of the visualized range. Table 2. Supported file formats File Format        Description FASTA         Standard format for nucleotide sequences BED         Format for describing coordinates of localized features on genomes, such as genes SAM/BAM         Relatively standard format for large sets of nucleotide sequence alignments WIG         Standard format for continuous-valued data. Useful for GC percent, probability scores and transcriptome data GFF         General feature format for annotations of ranges Tab delimited         Any tab-delimited file containing point, interval or continuous genome annotations Open in new tab The time and space requirements for the processes of data formatting and visualization were measured for a collection of human chromosome 1 datasets, including a genetic sequence, genes, SNPs, mammalian conservation and alignments of sequenced reads from an individual from the 1000 Genomes Project (∼40× coverage). The tests were performed on a Lenovo T61p laptop computer with an Intel Core 2 Duo CPU at 2.40 GHz and 3.0 GB of RAM. The results are summarized in Table 3. Formatting of the gene, SNP, sequence and conservation tracks took less than 10 min total, while the computation of a coverage track from a large set of read alignments took about an additional 40 min. The latter conversion is optional, and allows for dynamic switching between an alignment view and a coverage view as illustrated in Figure 2. Runtime performance was assessed by measuring the time taken to navigate to ranges of various sizes. Each measurement was performed on a newly started instance of Savant and the start location of the range was randomized. For seeking arbitrary ranges of sizes 10 million to 10K bp, Savant took less than a second to fetch and render data. The performance was worst for ranges just slightly shorter than 10K long, where the large number of BAM records that are displayed require 2 s to be fetched from disk. Savant renders virtually instantaneously for regions having sizes on the order of hundreds of base pairs, where most fine-scale visualization is done. Table 3. Time and space requirements for file formatting and visualization Data files Formatting Retrieval and visualization  Read alignments shown at these ranges  The Data Files section describes input files. The Formatting section shows time required to format each input file and the resulting formatted file size. The Retrieval and visualization section shows the time taken to retrieve data from ranges of various sizes and the number of records retrieved and drawn. All operations require <50 MB of memory. aTracks with continuous values are smoothed before rendering at large ranges. Sequence and point tracks are not rendered beyond certain ranges, denoted with a hyphen (–). Read alignments (BAM file) are replaced by coverage (precomputed from the BAM file) when visualizing longer regions. Open in new tab 5 PLUG-IN FRAMEWORK Savant is able to integrate user-defined plug-ins, allowing one to accomplish very specific tasks. Each plug-in can be one of two general types. Interactive plug-ins are allocated dockable modules on which graphical user interface (GUI) elements such as buttons or text fields can be placed to respond to user input and retrieve data. Non-interactive plug-ins are not designated a GUI component but still have extensive access to the innards of the browser. A number of helper functions are provided to plug-ins which are summarized in Table 4. Plug-ins can be used, for example, to prototype a SNP finder by identifying variable columns currently in view, or for computing genome-wide statistics, such as the fraction of SNPs in exons. Table 4. Helper functions provided to plug-ins Category        Function descriptions Range         Change range Tracks         Add, remove, retrieve data, change display modes and resolutions Bookmarks         Add, remove and seek UI         Rearrange modules Other         Take screenshot, export data, etc. Open in new tab Plug-in development is straight-forward and requires implementation of one Java interface. The Bookmark Intersection Plug-in, shown in Figure 3, is an example of an interactive plug-in. The plug-in allows the user to select two tracks, intersect them and load the intersecting regions into the list of bookmarks, enabling easy navigation to all of the regions of interest. Once developed, plug-ins can be shared among users through the Plugin section of the Savant web site. Fig. 3. Code used to make Bookmark Intersection Plug-in. The details of the UI that allows the user to select two tracks have been omitted. Once the two tracks are selected, the bookMarkTrackIntersections() method is run, which, for each interval of one track, finds overlapping intervals of the other, and saves intervals with overlap to the bookmark panel. Open in new tabDownload slide Code used to make Bookmark Intersection Plug-in. The details of the UI that allows the user to select two tracks have been omitted. Once the two tracks are selected, the bookMarkTrackIntersections() method is run, which, for each interval of one track, finds overlapping intervals of the other, and saves intervals with overlap to the bookmark panel.",Annotation,"savant genome browser  highthroughput sequence data
savant retrieve  render data every time  range change  request   user together  process happen quickly    confer seamless navigation around  genome  renderer   track  adaptive    display mode   length   view region choose   user  dynamic resolution change savant dynamically adjust  resolution— amount  information  displays— optimize  nucleotide  genomescale visualization  track  example read alignments   visualize   coverage track   number  base pair within  region   large  enable  visualization  individual read   region  small enough savant seamlessly switch   readalignment view  show  figure   addition  present   intuitive visualization  feature also reduce  program' memory footprint  improve overall speed fig  read alignments visualize  various resolutions  use two modes  chromosomewide view  read mappings show  overall coverage   coverage   centromere  regional view still visualize   coverage map show higher coverage  certain regions   genome  local view  read  show separately  differences   read   reference genome  color read   forward  reverse strand  show  different shade  blue  matepair arc mode show  relative distance   two read   pair taller arc indicate larger distance   pair open  new tabdownload slide read alignments visualize  various resolutions  use two modes  chromosomewide view  read mappings show  overall coverage   coverage   centromere  regional view still visualize   coverage map show higher coverage  certain regions   genome  local view  read  show separately  differences   read   reference genome  color read   forward  reverse strand  show  different shade  blue  matepair arc mode show  relative distance   two read   pair taller arc indicate larger distance   pair individual track  also  lock   particular range      update    unlock lock track   use  overview profile   subregions   select  specify range change   track track lock also enable simultaneous view  high  lowresolution profile  display modes particular data type   display  different modes  example interval annotations   squish together   single line  pack neatly   none overlap mimic  squish  pack modes   ucsc browser  representation   dynamically toggle within  browser   representation option emphasize different aspects   data  variant  strand modes  read alignments  instance use color  emphasize mismatch  read   strand   read  map respectively  novel mode  represent matepairs show arc   map locations  pair read   height   arc  proportional   infer insert size arc  anomalously map pair    suggestive  inversions  duplications  color differently  various modes  read alignments  illustrate  figure   file format  performance savant support  number  common textbased format   describe  table  however  textfiles   enable fast random access savant format  save  file    provide  efficient search operations  runtime  speed   savant  sift  large datasets  enable   way    format  index data  particular format involve convert text record   index binary data structure specific   data type sequence  continuous track  store  fixedwidth record enable direct lookup  record  interest annotation range   genes  store use  bin scheme similar   one use   ucsc browser kent      bam file      retrieve  range correspond   region usually require  one    olog  disk seek file format    directly   application  savant keep  memory usage low  adjust  sample rate depend   size   visualize range table  support file format file format        description fasta         standard format  nucleotide sequence bed         format  describe coordinate  localize feature  genomes   genes sambam         relatively standard format  large set  nucleotide sequence alignments wig         standard format  continuousvalued data useful   percent probability score  transcriptome data gff         general feature format  annotations  range tab delimit          tabdelimited file contain point interval  continuous genome annotations open  new tab  time  space requirements   process  data format  visualization  measure   collection  human chromosome  datasets include  genetic sequence genes snps mammalian conservation  alignments  sequence read   individual    genomes project  coverage  test  perform   lenovo t61p laptop computer   intel core  duo cpu   ghz     ram  result  summarize  table  format   gene snp sequence  conservation track take less   min total   computation   coverage track   large set  read alignments take   additional  min  latter conversion  optional  allow  dynamic switch   alignment view   coverage view  illustrate  figure  runtime performance  assess  measure  time take  navigate  range  various size  measurement  perform   newly start instance  savant   start location   range  randomize  seek arbitrary range  size  million    savant take less   second  fetch  render data  performance  worst  range  slightly shorter   long   large number  bam record   display require     fetch  disk savant render virtually instantaneously  regions  size   order  hundreds  base pair   finescale visualization   table  time  space requirements  file format  visualization data file format retrieval  visualization  read alignments show   range   data file section describe input file  format section show time require  format  input file   result format file size  retrieval  visualization section show  time take  retrieve data  range  various size   number  record retrieve  draw  operations require    memory atracks  continuous value  smooth  render  large range sequence  point track   render beyond certain range denote   hyphen  read alignments bam file  replace  coverage precomputed   bam file  visualize longer regions open  new tab  plugin framework savant  able  integrate userdefined plugins allow one  accomplish  specific task  plugin   one  two general type interactive plugins  allocate dockable modules   graphical user interface gui elements   button  text field   place  respond  user input  retrieve data noninteractive plugins   designate  gui component  still  extensive access   innards   browser  number  helper function  provide  plugins   summarize  table  plugins   use  example  prototype  snp finder  identify variable columns currently  view   compute genomewide statistics    fraction  snps  exons table  helper function provide  plugins category        function descriptions range         change range track         add remove retrieve data change display modes  resolutions bookmarks         add remove  seek          rearrange modules          take screenshot export data etc open  new tab plugin development  straightforward  require implementation  one java interface  bookmark intersection plugin show  figure    example   interactive plugin  plugin allow  user  select two track intersect   load  intersect regions   list  bookmarks enable easy navigation     regions  interest  develop plugins   share among users   plugin section   savant web site fig  code use  make bookmark intersection plugin  detail     allow  user  select two track   omit   two track  select  bookmarktrackintersections method  run    interval  one track find overlap intervals     save intervals  overlap   bookmark panel open  new tabdownload slide code use  make bookmark intersection plugin  detail     allow  user  select two track   omit   two track  select  bookmarktrackintersections method  run    interval  one track find overlap intervals     save intervals  overlap   bookmark panel",9
188,FastAnnotator,"FastAnnotator- an efficient transcript annotation web tool
The annotation process in FastAnnotator consists of four main parts: finding the best hit in the NCBI non-redundant database, assignment of GO terms, identification of enzymes, and identification of domains. The assignment of the GO terms requires the result from LAST searching against the NCBI non-redundant database, and these four main steps can therefore be divided into three independent modules: GO term assignment, enzyme identification, and domain identification (Figure ​(Figure1).1). FastAnnotator runs these three steps in parallel to accelerate the annotation procedure and also calculates the basic statistics of the input sequences and provides a statistical report in the results. An external file that holds a picture, illustration, etc. Object name is 1471-2164-13-S7-S9-1.jpg Figure 1 Flowchart of the FastAnnotator pipeline. After users upload sequences to the FastAnnotator server, three different processes, LAST-Blast2GO, PRIAM and domain identification, are executed to determine the gene ontology, enzyme and domain annotation of the submitted sequences. We implemented these three processes in parallel to accelerate the annotation procedure. After all of these annotation programs are completed, FastAnnotator presents GO terms, the best hits in the nr database, enzyme annotations and domain annotations together with a statistical report on the website. In addition to exploring the annotation results online, users can also download all of the annotation results as a zip file. The identification of GO terms LAST [22-24] and Blast2GO (B2G4Pipe) [5] were used together to identify GO annotations for the query sequences. We downloaded the non-redundant protein sequences from the NCBI database [21] and constructed a local database for Blast2GO. In FastAnnotator, LAST is used to find the similar sequences within the non-redundant protein sequence database. The output of LAST is then transformed into the equivalent BLASTX XML output format, which is the required input file format for Blast2GO. We used the statistical parameters λ and κ provided by the LAST output result to derive the bit score based on the following definition [25]: 𝖲' = λ𝖲 - 𝖨𝗇𝖨𝗇𝟤 The alignment result generated by LAST is then used as an input to Blast2GO. The final assignments of GO terms are extracted and presented in a table on the website, which is made available for download. The identification of domains We downloaded the standalone BLAST+ (v2.2.25) program from NCBI [26] and used as database the 13,672 domain models (Pfam v26) from the Conserved Domains Database (CDD) [19,20]. FastAnnotator applies the rpstblastn to identify domains in the query nucleotide sequences by searching against the preformatted domain database with mostly default parameters except the expectation value (e-value) which is set to be less than 0.01, and the hit aligned length which is longer than 50% of the domain PSSM [27]. After the domains of the query sequences are identified, FastAnnotator calculates the length coverage and presents the percentage of coverage for the domain in the report table. The identification of EC numbers FastAnnotator utilizes PRIAM [6] to identify potential enzyme functions. PRIAM can detect specific enzymes patterns and annotate these enzymes with EC numbers. The latest version of the enzyme profiles (released on 19 Oct, 2011) was downloaded from the PRIAM website. The nucleotide sequence inputs were translated into protein sequences in six frames using Transeq, a tool a part of the European Molecular Biology Open Software Suite (EMBOSS) [28]. These translated protein sequences are then used as inputs to search against the enzyme profile database. FastAnnotator identifies transcripts that may act as enzymes and presents the transcripts together with EC numbers in the output table.",Annotation,"fastannotator  efficient transcript annotation web tool
 annotation process  fastannotator consist  four main part find  best hit   ncbi nonredundant database assignment   term identification  enzymes  identification  domains  assignment    term require  result  last search   ncbi nonredundant database   four main step  therefore  divide  three independent modules  term assignment enzyme identification  domain identification figure ​figure1 fastannotator run  three step  parallel  accelerate  annotation procedure  also calculate  basic statistics   input sequence  provide  statistical report   result  external file  hold  picture illustration etc object name  s7s9jpg figure  flowchart   fastannotator pipeline  users upload sequence   fastannotator server three different process lastblast2go priam  domain identification  execute  determine  gene ontology enzyme  domain annotation   submit sequence  implement  three process  parallel  accelerate  annotation procedure     annotation program  complete fastannotator present  term  best hit    database enzyme annotations  domain annotations together   statistical report   website  addition  explore  annotation result online users  also download    annotation result   zip file  identification   term last   blast2go b2g4pipe   use together  identify  annotations   query sequence  download  nonredundant protein sequence   ncbi database   construct  local database  blast2go  fastannotator last  use  find  similar sequence within  nonredundant protein sequence database  output  last   transform   equivalent blastx xml output format    require input file format  blast2go  use  statistical parameters    provide   last output result  derive  bite score base   follow definition  '    𝖨𝗇𝟤  alignment result generate  last   use   input  blast2go  final assignments   term  extract  present   table   website   make available  download  identification  domains  download  standalone blast  program  ncbi   use  database   domain model pfam v26   conserve domains database cdd  fastannotator apply  rpstblastn  identify domains   query nucleotide sequence  search   preformatted domain database  mostly default parameters except  expectation value evalue   set   less     hit align length   longer     domain pssm    domains   query sequence  identify fastannotator calculate  length coverage  present  percentage  coverage   domain   report table  identification   number fastannotator utilize priam   identify potential enzyme function priam  detect specific enzymes pattern  annotate  enzymes   number  latest version   enzyme profile release   oct   download   priam website  nucleotide sequence input  translate  protein sequence  six frame use transeq  tool  part   european molecular biology open software suite emboss   translate protein sequence   use  input  search   enzyme profile database fastannotator identify transcripts  may act  enzymes  present  transcripts together   number   output table",9
189,VMGAP,"TheViral MetaGenome Annotation Pipeline(VMGAP):an automated tool for the functional annotation of viral Metagenomic shotgun sequencing data
The JCVI VMGAP consists of two consecutive steps: (1) database searches and (2) functional assignments. The pipeline uses as input a multifasta file containing the translations of all open reading frames (ORFs) predicted in a metagenomic sample. Protein coding genes are predicted using the structural annotation pipeline [29], that is based on a combination of naïve 6-frame translations and MetaGeneAnnotator [30,31], an ab initio gene finder program that uses empirical data including sequence-based composition, distance and orientation of genes of completely sequenced genomes to identify protein coding genes. Once uploaded, protein sequences are used to query several databases to identify protein features and similarities as schematically represented in Figure 1. During step 1, the VMGAP performs the following sequence similarity searches: An external file that holds a picture, illustration, etc. Object name is sigs.1694706-f1.jpg Figure 1 Naming rules used for functional annotation of the VMGAP. 1) Blastp searches against a non-redundant protein database The non-redundant protein database encompasses several public protein databases (GenBank NR, UniProt, PIR and OMNIOME) where each set of redundant peptides are condensed into a single database entry without losing useful information recorded in the fasta headers, such as EC numbers, product names, and taxon identification number. The VMGAP reports the top 50 hits with e-values ≤1x10-5. 2) Blastp searches against the ACLAME database ACLAME is a public protein database of mobile genetic elements (MGEs), including bacteriophages, transposons and plasmids [27]. Proteins are organized into families based on their function and sequence similarity, and families of 4 or more members are manually annotated with functional assignments using GO and MeGO terms (an ontology dedicated to MGEs developed by ACLAME). All blastp hits with e-values ≤1x10-5 are reported. 3) Blastp and tblastn searches against environmental protein databases The VMGAP queries three different environmental composite databases at the amino acid level: (i) ENV_NR, a GenBank non-redundant protein database that includes many environmental datasets, (ii) an in-house database (SANGER_PEP) composed of proteins coded by Sanger-based viral metagenomic samples not represented in ENV_NR (Table 1), and (iii) ENV_NT, a collection of nucleotide sequences from metagenomic datasets deposited in GenBank. The purpose of these analyses is to determine how similar the viruses are within the query metagenomic samples to viruses and microbes that inhabit the different environments represented in the subject databases. The VMGAP reports all blast hits with e-values ≤1x10-3. Table 1 Metagenomic libraries incorporated into the Sanger environmental protein database Library Name        Reference Viral metagenomes from Yellowstone hot springs (Bear Paw)        [32] RNA viral community in human feces        [33] viral metagenomes from yellowstone hot springs (Octopus)        [32] Virus from Human Blood        [34] Virus from Human Feces        [35] Virus from Marine Sediments        [36] Uncultured marine viral communities (Mission Bay)        [37] Uncultured marine viral communities (Scripps Pier)        [37] Coastal RNA virus communities        [38] Chesapeake Bay virioplankton        [39] Virus from equine feces        [40] 4) HMM searches against PFAM/TIGRFAM and ACLAME HMM In addition to similarity searches against protein databases, the VMGAP looks for the presence of HMMs from two databases, PFAM/TIGRFAM (a database of HMMs representing conserved protein domains) and ACLAME-HMMs (a compilation of HMMs that describe each of the protein families found in ACLAME). PFAM/TIGRFAM HMM searches are carried out in two different ways, either requiring a global or local alignment to the HMMs. Local HMM alignments increase sensitivity in the detection of conserved protein domains, particularly when the predicted peptide is truncated and extends to the end of the read, which is noted frequently in metagenomic datasets. All HMM hit with e-values ≤l1x10-5 are recorded for further analysis. 5) RPS-Blast against NCBI CDD database The NCBI Conserved Domain Database (CDD) database is a collection of position specific scoring matrices representing conserved protein domains, protein families and superfamilies compiled from NCBI-curated domains [41], PFAM/TIGRFAM, SMART [42] and COG [43]. In spite of the overlap, PSSMs derived from PFAM/TIGRFAM do not behave exactly the same as their HMM counterparts, and in some cases these searches can identify domains where HMMs fail. The VMGAP stores all hits with e-values ≤ 1x10-5. 6) Identification of transmembrane domains and signal peptides To discover transmembrane proteins and signal peptides that could be associated with the surface of viral particles, the VMGAP utilizes two programs, SignalP for the identification of signal peptides, and TMHMM, a program that detects candidate transmembrane domains. 7) Assignment of EC numbers To aid in the metabolic reconstruction of metagenomes, the VMGAP makes use of PRIAM, a collection of PSSMs where each matrix represents an enzymatic function and is assigned to a particular EC number. Metagenomic samples are scanned for the presence of these PSSMs with RPS-Blast recording only those hits with e-values ≤1x10-10. 8) Rules hierarchy Functional assignments of predicted peptides are carried out by retrieving the functional information produced from the results of the analyses performed in the previous steps following a series of pre-defined rules (Figure 1). Rules prioritize the use of a certain piece of evidence over another based on how informative, trustful and accurate that evidence is. As shown in Figure 1, hits against equivalog TIGRFAM HMMs [26]are the highest ranked supporting evidence for functional assignments in the VMGAP. Therefore, any protein that hits above the trusted cutoff of one entire copy (100% coverage with respect to the length of the HMM) of an equivalog TIGRFAM will automatically inherit the functional annotation associated to that particular HMM. The second and third tiers of evidence are constituted by highly significant BLASTP hits against ACLAME DB and the non-redundant protein database respectively; having at least 80% coverage (with respect to the shortest sequence), 50% identity and an e-value ≤ 1x10-10. Although proteins from ACLAME DB are also included in the non-redundant protein database, entries in the former have a higher priority since they are curated and therefore provide better functional annotation. Hits against HMMs describing ACLAME protein families and PFAM/non-equivalog TIGRFAM HMMs comprise the 4th and 5th layers of functional evidence, giving higher priority to those HMMs representing protein families against those describing conserved protein domains. Ranked 6th and 7th in the rule list are respectively RPS-BLAST hits with at least 90% coverage, percent identity ≥ 35% and e-value ≤ 1x10-10 against NCBI-CDD profiles and local-local hits against PFAM/TIGRFAM HMMs with e-values ≤ 1x10-5. Finally, low-confidence BLASTP hits with at least 70% coverage, percent identity ≥ 30% and e-value ≤ 1x10-5 against ACLAME DB and the non-redundant protein database occupy tiers 8 and 9 in the priority list respectively. Proteins that lack the evidence types described above, but still contain some other evidence such as hits against the environmental DBs are named “hypothetical protein”. Otherwise, proteins are labeled as “unknown protein”. Go to: Implementation The VMGAP consists of three major modules implemented in Perl (Figure 2): (i) the control module, which initializes the pipeline, creates a sqlite DB [44] to store the status of computations and their results, coordinates the other modules, and allows interrupted pipelines to be resumed from the point of interruption, (ii) the compute module, which tracks the status of the individual computations and loads completed computations into the sqlite database, and (iii) the annotation module, which reads the computational results from the sqlite DB and applies a set of predefined rules to generate a tab-delimited annotation file containing the final annotation for each peptide (e.g. EC/GO assignments and protein names), and a tab-delimited evidence file that stores all the evidence that supports the annotation. Each line in the annotation file contains the functional annotation for an individual peptide, while in the evidence file each line represents one particular evidence for a single protein (Table 2 and Table 3). Additionally, the VMGAP contains an optional module, also implemented in Perl, called Com2GO (Common-Name-to-Go Mappings). Com2GO can be run after the annotation module to attempt to classify the protein names using the GO hierarchy. An external file that holds a picture, illustration, etc. Object name is sigs.1694706-f2.jpg Figure 2 Schematic representation of the implementation of the VMGAP. The three main modules of the pipeline are depicted by yellow squares. Orange and red circles represent input and output files respectively. VICS stands for Venter Institute Compute Services; SGE stands for Sun Grid Engine job scheduler. Single and double-headed arrows indicate information flowing in one or both directions respectively. Table 2 Description of the contents of the evidence file generated by the VMGAP 1        2          3        4        5        6        7        8           9 ID          CDD_RPS          Subject definition           % cov           % ident           e-value                   % ident ID          ALL GROUP_PEP          Subject ID           Subject definition           Query length           Subject length           % cov           % ident           e-value ID          ACLAME_PEP          Subject ID           Subject definition           Query length           Subject length           % cov           % ident           e-value ID          SANGER_PEP          Subject ID           Subject definition           Query length           Subject length           % cov           % ident           e-value ID          ENV_NT          Subject ID           Subject definition           Query length           Subject length           % cov           % ident           e-value ID          ENV_NR          Subject ID           Subject definition           Query length           Subject length           % cov           HMM description           e-value ID          FRAG_HMM          HMM begin           HMM end           % cov           Total e-value           HMM accession           HMM description           HMM length ID          PFAM/TIGRFAM_HMM          HMM begin           HMM end           % cov           Total e-value           HMM accession                   HMM length ID          PRIAM          EC Number           e-value                                   HMM description ID          ACLAME_HMM          HMM begin           HMM end           % cov           Total e-value           HMM accession                   HMM length ID          PEPSTATS          Molecular weight           Isoelectric point ID          TMHMM          Number predicted helixes ID          SIGNALP          signal pep           cleavage site position Open in a separate window Fields 1 and 2 correspond to the protein identifier and a flag specific for each analysis, respectively. % cov, percent coverage; % ident, percent identity; CDD_RPS, RPS-Blast vs. CDD DB; ALLGROUP_PEP, Blastp vs. protein NR DB; ACLAME_PEP, Blastp vs. ACLAME protein DB; SANGER_PEP, Blastp vs. in-house viral metagenomic DB; ENV_NT, Tblastn vs. ENV_NT DB; ENV_NR, Blastp vs. ENV_NR DB; FRAG_HMM, HMM searches vs. local PFAM/TIGRFAM HMM DB; PFAM/TIGRFAM_HMM, HMM searches vs. global PFAM/TIGRFAM HMM DB; PRIAM, RPS-Blast vs. PRIAM profile DB; ACLAME_HMM, HMM searches vs. global ACLAME HMM DB; PEPSTATS, peptide statistics; TMHMM, transmembrane domain searches; SIGNALP, signal peptide searches. Table 3 Explanation of the annotation file generated by the VMGAP Column           Description           Example 1           Unique peptide ID           JCVI_PEP_metagenomic.orf.112038372243 2.1 2           Protein common name tag           common_name 3           Functional description (s)           phosphonate C-P lyase system protein PhnL, putative 4           Source of functional description assignment           AllGroup High:rf|YP_001889651.1 5           GO tag           GO 6           Gene Ontology ID (s)           go:0016887||go:0005524 7           Source of Gene Ontology assignment           PF00005||PF00005 8           EC tag           EC 9           Enzyme Commission number ID (s)           3.6.3.28 10           Source of Enzyme Commission ID           PRIAM 11           Hits against ENV_NT DB tag           ENV_NT 12           ENV_NT DB libraries hit with e-values ≤1 × 10-3           Hydrothermal vent metagenome FOSS10958.y2, whole genome shotgun sequence || Lake Washington Formate SIP Enrichment Freshwater Metagenome || Human Gut Metagenome (healthy human sample In-M, Infant, Female) 13           Best hit e-value per environmental library           6.65676e-54 || 2.14066e-44 || 1.34265e-46 14           Number of hits with e-value ≤1 × 10-3 per environmental DB library           1 || 1 || 4 15           HMM DB tag           PFAM/TIGRFAM_HMM 16           PFAM/TIGRFAM HMM hit above trusted cutoff           PF000005 17           Signalp tag           SIGNALP 18           Presence (Y) or absence (N) of predicted signal peptide           Y 19           Cleavage site position           16 20           Transmembrane domain tag           TMHMM 21           Number of predicted transmembrane domains           2 22           Protein statistics tag           PEPSTATS 23           Molecular weight           17369.86 24           Isoelectric point           9.9423 Open in a separate window Each lane contains the annotation for a single predicted peptide. Multiple values within a field are separated by the symbol “||”. The heart of the VMGAP is the compute module (Figure 2). This module accepts a compute configuration file (see Table 4 for the current configuration) and a sqlite results database. It compares the computations specified in the configuration with the results loaded into the sqlite results database. Missing computations are initiated, stale computations (outdated reference dataset or obsolete program options) are refreshed, and interrupted computations are resumed. The computations themselves are executed either in a local machine (for jobs that are not very computational intensive such as SignalP), or through the JCVI high-throughput computing platform named VICS web-services. VICS is a J2EE server backed by a 1600 node SGE-grid and a 2 Terabyte scratch-disk. All of the computations are started (or restarted) and then the compute module waits for them to complete. As a computation is completed, its results are parsed and loaded into the sqlite database and the status of the computation is updated. When all computations have completed, the module exits and allows the controller to proceed. The module may be interrupted manually and restarted at a later time.",Annotation,"theviral metagenome annotation pipelinevmgapan automate tool   functional annotation  viral metagenomic shotgun sequence data
 jcvi vmgap consist  two consecutive step  database search   functional assignments  pipeline use  input  multifasta file contain  translations   open read frame orfs predict   metagenomic sample protein cod genes  predict use  structural annotation pipeline    base   combination  naïve frame translations  metageneannotator    initio gene finder program  use empirical data include sequencebased composition distance  orientation  genes  completely sequence genomes  identify protein cod genes  upload protein sequence  use  query several databases  identify protein feature  similarities  schematically represent  figure   step   vmgap perform  follow sequence similarity search  external file  hold  picture illustration etc object name  sigsf1jpg figure  name rule use  functional annotation   vmgap  blastp search   nonredundant protein database  nonredundant protein database encompass several public protein databases genbank  uniprot pir  omniome   set  redundant peptides  condense   single database entry without lose useful information record   fasta headers    number product name  taxon identification number  vmgap report  top  hit  evalues ≤1x10  blastp search   aclame database aclame   public protein database  mobile genetic elements mges include bacteriophages transposons  plasmids  proteins  organize  families base   function  sequence similarity  families     members  manually annotate  functional assignments use   mego term  ontology dedicate  mges develop  aclame  blastp hit  evalues ≤1x10  report  blastp  tblastn search  environmental protein databases  vmgap query three different environmental composite databases   amino acid level  env_nr  genbank nonredundant protein database  include many environmental datasets   inhouse database sanger_pep compose  proteins cod  sangerbased viral metagenomic sample  represent  env_nr table   iii env_nt  collection  nucleotide sequence  metagenomic datasets deposit  genbank  purpose   analyse   determine  similar  viruses  within  query metagenomic sample  viruses  microbes  inhabit  different environments represent   subject databases  vmgap report  blast hit  evalues ≤1x10 table  metagenomic libraries incorporate   sanger environmental protein database library name        reference viral metagenomes  yellowstone hot spring bear paw         rna viral community  human feces         viral metagenomes  yellowstone hot spring octopus         virus  human blood         virus  human feces         virus  marine sediment         uncultured marine viral communities mission bay         uncultured marine viral communities scripps pier         coastal rna virus communities         chesapeake bay virioplankton         virus  equine feces          hmm search  pfamtigrfam  aclame hmm  addition  similarity search  protein databases  vmgap look   presence  hmms  two databases pfamtigrfam  database  hmms represent conserve protein domains  aclamehmms  compilation  hmms  describe    protein families find  aclame pfamtigrfam hmm search  carry   two different ways either require  global  local alignment   hmms local hmm alignments increase sensitivity   detection  conserve protein domains particularly   predict peptide  truncate  extend   end   read   note frequently  metagenomic datasets  hmm hit  evalues ≤l1x10  record   analysis  rpsblast  ncbi cdd database  ncbi conserve domain database cdd database   collection  position specific score matrices represent conserve protein domains protein families  superfamilies compile  ncbicurated domains  pfamtigrfam smart   cog   spite   overlap pssms derive  pfamtigrfam   behave exactly     hmm counterparts    case  search  identify domains  hmms fail  vmgap store  hit  evalues ≤ 1x10  identification  transmembrane domains  signal peptides  discover transmembrane proteins  signal peptides  could  associate   surface  viral particles  vmgap utilize two program signalp   identification  signal peptides  tmhmm  program  detect candidate transmembrane domains  assignment   number  aid   metabolic reconstruction  metagenomes  vmgap make use  priam  collection  pssms   matrix represent  enzymatic function   assign   particular  number metagenomic sample  scan   presence   pssms  rpsblast record   hit  evalues ≤1x10  rule hierarchy functional assignments  predict peptides  carry   retrieve  functional information produce   result   analyse perform   previous step follow  series  predefined rule figure  rule prioritize  use   certain piece  evidence  another base   informative trustful  accurate  evidence   show  figure  hit  equivalog tigrfam hmms   highest rank support evidence  functional assignments   vmgap therefore  protein  hit   trust cutoff  one entire copy  coverage  respect   length   hmm   equivalog tigrfam  automatically inherit  functional annotation associate   particular hmm  second  third tiers  evidence  constitute  highly significant blastp hit  aclame    nonredundant protein database respectively   least  coverage  respect   shortest sequence  identity   evalue ≤ 1x10 although proteins  aclame   also include   nonredundant protein database entries   former   higher priority since   curated  therefore provide better functional annotation hit  hmms describe aclame protein families  pfamnonequivalog tigrfam hmms comprise  4th  5th layer  functional evidence give higher priority   hmms represent protein families   describe conserve protein domains rank 6th  7th   rule list  respectively rpsblast hit   least  coverage percent identity ≥   evalue ≤ 1x10  ncbicdd profile  locallocal hit  pfamtigrfam hmms  evalues ≤ 1x10 finally lowconfidence blastp hit   least  coverage percent identity ≥   evalue ≤ 1x10  aclame    nonredundant protein database occupy tiers      priority list respectively proteins  lack  evidence type describe   still contain   evidence   hit   environmental dbs  name “hypothetical protein” otherwise proteins  label  “unknown protein”   implementation  vmgap consist  three major modules implement  perl figure    control module  initialize  pipeline create  sqlite    store  status  computations   result coordinate   modules  allow interrupt pipelines   resume   point  interruption   compute module  track  status   individual computations  load complete computations   sqlite database  iii  annotation module  read  computational result   sqlite   apply  set  predefined rule  generate  tabdelimited annotation file contain  final annotation   peptide  ecgo assignments  protein name   tabdelimited evidence file  store   evidence  support  annotation  line   annotation file contain  functional annotation   individual peptide    evidence file  line represent one particular evidence   single protein table   table  additionally  vmgap contain  optional module also implement  perl call com2go commonnametogo mappings com2go   run   annotation module  attempt  classify  protein name use   hierarchy  external file  hold  picture illustration etc object name  sigsf2jpg figure  schematic representation   implementation   vmgap  three main modules   pipeline  depict  yellow square orange  red circle represent input  output file respectively vics stand  venter institute compute service sge stand  sun grid engine job scheduler single  doubleheaded arrows indicate information flow  one   directions respectively table  description   content   evidence file generate   vmgap                                                                                 cdd_rps          subject definition            cov            ident           evalue                    ident            group_pep          subject            subject definition           query length           subject length            cov            ident           evalue           aclame_pep          subject            subject definition           query length           subject length            cov            ident           evalue           sanger_pep          subject            subject definition           query length           subject length            cov            ident           evalue           env_nt          subject            subject definition           query length           subject length            cov            ident           evalue           env_nr          subject            subject definition           query length           subject length            cov           hmm description           evalue           frag_hmm          hmm begin           hmm end            cov           total evalue           hmm accession           hmm description           hmm length           pfamtigrfam_hmm          hmm begin           hmm end            cov           total evalue           hmm accession                   hmm length           priam           number           evalue                                   hmm description           aclame_hmm          hmm begin           hmm end            cov           total evalue           hmm accession                   hmm length           pepstats          molecular weight           isoelectric point           tmhmm          number predict helixes           signalp          signal pep           cleavage site position open   separate window field    correspond   protein identifier   flag specific   analysis respectively  cov percent coverage  ident percent identity cdd_rps rpsblast  cdd  allgroup_pep blastp  protein   aclame_pep blastp  aclame protein  sanger_pep blastp  inhouse viral metagenomic  env_nt tblastn  env_nt  env_nr blastp  env_nr  frag_hmm hmm search  local pfamtigrfam hmm  pfamtigrfam_hmm hmm search  global pfamtigrfam hmm  priam rpsblast  priam profile  aclame_hmm hmm search  global aclame hmm  pepstats peptide statistics tmhmm transmembrane domain search signalp signal peptide search table  explanation   annotation file generate   vmgap column           description           example            unique peptide            jcvi_pep_metagenomicorf             protein common name tag           common_name            functional description            phosphonate  lyase system protein phnl putative            source  functional description assignment           allgroup highrfyp_001889651             tag                       gene ontology             gogo            source  gene ontology assignment           pf00005pf00005             tag                       enzyme commission number                         source  enzyme commission            priam            hit  env_nt  tag           env_nt            env_nt  libraries hit  evalues ≤             hydrothermal vent metagenome foss10958y2 whole genome shotgun sequence  lake washington formate sip enrichment freshwater metagenome  human gut metagenome healthy human sample inm infant female            best hit evalue per environmental library                           number  hit  evalue ≤   per environmental  library                           hmm  tag           pfamtigrfam_hmm            pfamtigrfam hmm hit  trust cutoff           pf000005            signalp tag           signalp            presence   absence   predict signal peptide                       cleavage site position                       transmembrane domain tag           tmhmm            number  predict transmembrane domains                       protein statistics tag           pepstats            molecular weight                       isoelectric point            open   separate window  lane contain  annotation   single predict peptide multiple value within  field  separate   symbol “”  heart   vmgap   compute module figure   module accept  compute configuration file see table    current configuration   sqlite result database  compare  computations specify   configuration   result load   sqlite result database miss computations  initiate stale computations outdated reference dataset  obsolete program options  refresh  interrupt computations  resume  computations   execute either   local machine  job     computational intensive   signalp    jcvi highthroughput compute platform name vics webservices vics   j2ee server back    node sgegrid    terabyte scratchdisk    computations  start  restart    compute module wait    complete   computation  complete  result  parse  load   sqlite database   status   computation  update   computations  complete  module exit  allow  controller  proceed  module may  interrupt manually  restart   later time",9
190,PyroBayes,"Pyrobayes: an improved base caller for SNP discovery in pyrosequences
The sequencing reads produced by the 454 Life Sciences pyrosequencers are the result of cyclical nucleotide tests in which ideally all nucleotides within a homopolymer (for example, AAA) are incorporated in a single test, and the light intensity signal observed in each cycle is proportional to the actual number of incorporated nucleotides1. In reality, the signal for a fixed number of incorporated bases varies substantially, and there is usually a nonzero signal even when no base is incorporated (Supplementary Fig. 1a online). This makes accurate base calling difficult and leads to nucleotide over-calls and under-calls that manifest as insertion and deletion errors2,3,4. Such errors often lead to misalignments that artificially inflate sequencing error estimates and cause the assignment of lower estimates of the base calls' accuracy (which we refer to as base quality) than warranted by their true accuracy (Fig. 1). Figure 1: Comparison of the error profiles of Pyrobayes and the native 454 base caller. figure1 (a) Illustration of the effects of calling too few or too many bases on the alignment of a read (gray) to the reference sequence (black). Top, too few thymines were called, resulting in two spurious mismatches (arrows) by misaligning the correctly called cytosine and the inserted guanine in the 454 read. Middle, the correct number of thymines was called, resulting in the correct read alignment of the single insertion error (red) in the 454 read. Bottom, too many thymines were called, resulting in the correct read alignment of the two base insertion errors (red) in the 454 read. (b) Base error rates for Pyrobayes and the native 454 base caller. The relative contribution of each error type based on Pyrobayes calls is shown in the pie chart. Full size image Accurate base qualities are crucial for resequencing applications in which true allelic variation must be distinguished from sequencing error. Reliable SNP calls can only be made if the base error rate for the called allele is substantially lower than the expected polymorphism rate. For example, in human studies for which the average pairwise polymorphism rate is on the order of 1 in 1,000 bp, no SNP call should be made from a single allele with a base quality lower than 30 (1 in 1,000 bp error rate). However, if most base calls in resequencing reads are well above such a threshold, SNPs can be detected with high confidence even in single-read coverage. Unfortunately, we found that the majority of the base qualities assigned by the native 454 base caller (version 1.0.52) were not sufficiently high for SNP calling in low-coverage conditions, as only 24% of the native 454 base calls were above 30 (Fig. 2a). However, we found that 454 reads can be called accurately, but the base qualities assigned by the native base caller underestimate the actual base accuracy (Fig. 2b). We developed a new base calling program, Pyrobayes, to produce more accurate (higher) base qualities and hence make more high-quality base calls in 454 pyrosequences. Figure 2: Comparison of the base qualities assigned by Pyrobayes and the native 454 base caller. figure2 (a) The cumulative distribution of base qualities assigned by each program. (b) Comparison between assigned base quality and the base quality calculated from measured base accuracy. A value of 50 was assigned when no errors were found. (c) The distribution of base calls according to base quality. Full size image Our base caller first determines the most likely number of incorporated bases from the measured incorporation signal for each nucleotide test. Our Bayesian strategy (Supplementary Methods and Supplementary Fig. 1 online) requires 'data likelihoods', that is, the distribution of observed nucleotide incorporation signals for every possible homopolymer length. We estimated these by collecting shotgun resequencing data with the 454 Life Sciences GS20 instrument from a finished mouse bacterial artificial chromosome (BAC) clone and extrapolating to higher homopolymer lengths for which few or no examples could be found (Supplementary Fig. 1a and Supplementary Fig. 2 online). For 'prior probabilities', we used the relative frequency of homopolymer lengths tabulated from several different reference genome sequences. We found that these frequencies were consistently different from the theoretical expectation that they are proportional to 1/4n, where n is the homopolymer length (Supplementary Fig. 1b). In the software we used a single distribution because the frequencies are very similar across all eukaryotic genomes we considered. Using data likelihoods and prior distributions, we determined the 'Bayesian posterior probability' of the correct number of bases given the measured incorporation signal (Supplementary Fig. 1c). The called base sequence was produced by concatenating the most likely number of bases for every consecutive incorporation test. The base quality assigned to each base is the probability that the base in question is not an over-call. We found it also useful to call one extra base, as long as the presence of that base is above a minimum probability (see below). We compared the Pyrobayes and native base calling accuracy in 299,654 reads from the inbred reference (iso-1) strain of Drosophila melanogaster (Supplementary Methods). The overall base accuracy (Fig. 1b) was quite high for both Pyrobayes and the native base caller (99.60% versus 99.61%). Notably, 96% of all sequencing errors were insertions or deletions. The Pyrobayes insertion error rate was higher (0.29% versus 0.24%), but its deletion rate was lower (0.09% versus 0.10%). Most importantly for SNP discovery, the Pyrobayes substitution error rate was 60% lower (0.017% versus 0.042%) than that of the native base caller. A large fraction (74%) of the base calling errors was shared between the two methods. Characteristically, 86% of the errors solely made by Pyrobayes were insertions whereas 82% of the unique 454 base caller errors were deletions or substitutions (Supplementary Fig. 3). The Pyrobayes base qualities corresponded substantially better to the actual base accuracy than the native base qualities (Fig. 2b), and therefore our base qualities were typically higher (Fig. 2c). For example, 56% of the Pyrobayes base calls were assigned base qualities of 30 or higher, as compared to 24% of the native base calls (Fig. 2a,c). Additionally, Pyrobayes produced base qualities up to 50, whereas the highest native base quality was 38. We investigated the effect of our higher overall base qualities on SNP detection. First, we searched for single-base-pair differences between the 454-sequenced iso-1 reads and the iso-1 reference sequence. We expected few true polymorphisms as these sequences were from the same inbred D. melanogaster strain, and the overall accuracy of the D. melanogaster genome reference sequence is very high. Therefore, SNPs discovered in this comparison estimate the false positive SNP rate. This rate was 1.22/10,000 bp using the native base calls, but only 0.97/10,000 bp using the Pyrobayes base calls. It is important to consider that the false SNP discovery rate depends on the polymorphism rate in the resequenced organism. For example, in D. melanogaster, where the pairwise polymorphism rate is ∼1/200 bp (ref. 5), our results corresponded to a false SNP discovery rate of 1.9%. To estimate SNP calling error rates directly, we also sequenced an inbred D. melanogaster isolate from Malawi with a single 454 run. In the alignments of the 454 reads base called with Pyrobayes we found 1,118 SNP candidates at or above the Polybayes SNP probability6 cutoff value of 0.7. The validation rate for these candidates was 93% (1,036 of 1,118). The corresponding 7% false positive SNP rate observed in this experiment is a composite effect of false SNP calls, emulsion PCR errors before 454 sequencing and the usual artifacts associated with capillary sequence validation experiments7. We also estimated that we missed 14.8% of the SNPs (Supplementary Methods). We repeated the SNP discovery experiment in the alignments processed with the native 454 base caller: the false positive rates were identical, but twice as many (30.0%) SNPs were missed. The primary cause of spurious substitution errors in 454 reads is the erroneous alignment of a base under-call followed by an over-call (or vice versa) as a base substitution (Fig. 1a and Supplementary Fig. 3d). Our alignment algorithm, Mosaik (Supplementary Methods), uses gap penalties that properly align reads in such situations. Additionally, we found that calling more bases in homopolymer runs often also improves the alignment (Fig. 1a). Eliminating spurious base errors resulting from alignment artifacts leads to assignment of higher base qualities. Higher base qualities increase SNP calling sensitivity. The cost of tending toward calling more bases in homopolymer runs is a slightly increased insertion rate (Fig. 1b) even though the extra called bases are typically assigned very low base qualities. This is a logical choice for SNP discovery applications. However, it is not yet clear what effect such extra called bases will have for de novo sequence assembly of 454 reads. A natural, although undesirable consequence of having to determine homopolymer length from a single incorporation signal is that the likelihood of over-calling error increases with every consecutive nucleotide. Accordingly, the first called base in a homopolymer run is assigned the highest base quality, and the last called base, the lowest (Supplementary Fig. 4a online). This introduces an unintended directionality for the base qualities in the sequence alignment (Supplementary Fig. 4b). Clearly, it is not possible for the base calling program to resolve this ambiguity within the standard base quality framework defined by the Phred8,9 base calling program. Consequently, one must rely on alignment and SNP calling software to account for this phenomenon. We also evaluated base calling accuracy on the new 454 Life Sciences FLX sequencing machine model using two sequencing runs from the K12 strain of Escherichia coli and found that both base callers underestimate the FLX base accuracy (Supplementary Fig. 5 online). The primary reason for this is that the overall error rate of the FLX machine (0.12%) was much lower than that of the GS20 (0.40%). Although the fact that the Pyrobayes base qualities were much closer to the actual accuracy suggests that our calibration procedure is robust, there is clearly a need to recalibrate our method for the FLX and future models. The increased accuracy of our base qualities will likely permit more sensitive biological studies using the 454 machines. Although our data only illustrate this directly for low-coverage, survey-type applications, statistical fluctuations10 will result in regions of shallow read depth even in deeper nominal coverage. The ability to call SNPs in such regions without a substantial loss of accuracy will permit more complete analyses of whole-genome alignments. Pyrobayes can process a single sequencing run in under 2 min.",SNPDiscovery,"pyrobayes  improve base caller  snp discovery  pyrosequences
 sequence read produce    life sciences pyrosequencers   result  cyclical nucleotide test   ideally  nucleotides within  homopolymer  example aaa  incorporate   single test   light intensity signal observe   cycle  proportional   actual number  incorporate nucleotides1  reality  signal   fix number  incorporate base vary substantially    usually  nonzero signal even   base  incorporate supplementary fig  online  make accurate base call difficult  lead  nucleotide overcalls  undercalls  manifest  insertion  deletion errors2  errors often lead  misalignments  artificially inflate sequence error estimate  cause  assignment  lower estimate   base calls' accuracy   refer   base quality  warrant   true accuracy fig  figure  comparison   error profile  pyrobayes   native  base caller figure1  illustration   effect  call     many base   alignment   read gray   reference sequence black top   thymines  call result  two spurious mismatch arrows  misalign  correctly call cytosine   insert guanine    read middle  correct number  thymines  call result   correct read alignment   single insertion error red    read bottom  many thymines  call result   correct read alignment   two base insertion errors red    read  base error rat  pyrobayes   native  base caller  relative contribution   error type base  pyrobayes call  show   pie chart full size image accurate base qualities  crucial  resequencing applications   true allelic variation must  distinguish  sequence error reliable snp call    make   base error rate   call allele  substantially lower   expect polymorphism rate  example  human study    average pairwise polymorphism rate    order       snp call   make   single allele   base quality lower       error rate however   base call  resequencing read  well    threshold snps   detect  high confidence even  singleread coverage unfortunately  find   majority   base qualities assign   native  base caller version    sufficiently high  snp call  lowcoverage condition      native  base call    fig  however  find   read   call accurately   base qualities assign   native base caller underestimate  actual base accuracy fig   develop  new base call program pyrobayes  produce  accurate higher base qualities  hence make  highquality base call   pyrosequences figure  comparison   base qualities assign  pyrobayes   native  base caller figure2   cumulative distribution  base qualities assign   program  comparison  assign base quality   base quality calculate  measure base accuracy  value    assign   errors  find   distribution  base call accord  base quality full size image  base caller first determine   likely number  incorporate base   measure incorporation signal   nucleotide test  bayesian strategy supplementary methods  supplementary fig  online require 'data likelihoods'    distribution  observe nucleotide incorporation signal  every possible homopolymer length  estimate   collect shotgun resequencing data    life sciences gs20 instrument   finish mouse bacterial artificial chromosome bac clone  extrapolate  higher homopolymer lengths      examples could  find supplementary fig   supplementary fig  online  'prior probabilities'  use  relative frequency  homopolymer lengths tabulate  several different reference genome sequence  find   frequencies  consistently different   theoretical expectation    proportional       homopolymer length supplementary fig    software  use  single distribution   frequencies   similar across  eukaryotic genomes  consider use data likelihoods  prior distributions  determine  'bayesian posterior probability'   correct number  base give  measure incorporation signal supplementary fig   call base sequence  produce  concatenate   likely number  base  every consecutive incorporation test  base quality assign   base   probability   base  question    overcall  find  also useful  call one extra base  long   presence   base    minimum probability see   compare  pyrobayes  native base call accuracy   read   inbred reference iso strain  drosophila melanogaster supplementary methods  overall base accuracy fig   quite high   pyrobayes   native base caller  versus  notably    sequence errors  insertions  deletions  pyrobayes insertion error rate  higher  versus    deletion rate  lower  versus   importantly  snp discovery  pyrobayes substitution error rate   lower  versus      native base caller  large fraction    base call errors  share   two methods characteristically    errors solely make  pyrobayes  insertions whereas    unique  base caller errors  deletions  substitutions supplementary fig   pyrobayes base qualities correspond substantially better   actual base accuracy   native base qualities fig   therefore  base qualities  typically higher fig   example    pyrobayes base call  assign base qualities    higher  compare     native base call fig 2ac additionally pyrobayes produce base qualities    whereas  highest native base quality    investigate  effect   higher overall base qualities  snp detection first  search  singlebasepair differences   sequence iso read   iso reference sequence  expect  true polymorphisms   sequence     inbred  melanogaster strain   overall accuracy    melanogaster genome reference sequence   high therefore snps discover   comparison estimate  false positive snp rate  rate    use  native base call     use  pyrobayes base call   important  consider   false snp discovery rate depend   polymorphism rate   resequenced organism  example   melanogaster   pairwise polymorphism rate    ref   result correspond   false snp discovery rate    estimate snp call error rat directly  also sequence  inbred  melanogaster isolate  malawi   single  run   alignments    read base call  pyrobayes  find  snp candidates     polybayes snp probability6 cutoff value    validation rate   candidates       correspond  false positive snp rate observe   experiment   composite effect  false snp call emulsion pcr errors   sequence   usual artifacts associate  capillary sequence validation experiments7  also estimate   miss    snps supplementary methods  repeat  snp discovery experiment   alignments process   native  base caller  false positive rat  identical  twice  many  snps  miss  primary cause  spurious substitution errors   read   erroneous alignment   base undercall follow   overcall  vice versa   base substitution fig   supplementary fig   alignment algorithm mosaik supplementary methods use gap penalties  properly align read   situations additionally  find  call  base  homopolymer run often also improve  alignment fig  eliminate spurious base errors result  alignment artifacts lead  assignment  higher base qualities higher base qualities increase snp call sensitivity  cost  tend toward call  base  homopolymer run   slightly increase insertion rate fig  even though  extra call base  typically assign  low base qualities    logical choice  snp discovery applications however    yet clear  effect  extra call base     novo sequence assembly   read  natural although undesirable consequence    determine homopolymer length   single incorporation signal    likelihood  overcalling error increase  every consecutive nucleotide accordingly  first call base   homopolymer run  assign  highest base quality   last call base  lowest supplementary fig  online  introduce  unintended directionality   base qualities   sequence alignment supplementary fig  clearly    possible   base call program  resolve  ambiguity within  standard base quality framework define   phred8 base call program consequently one must rely  alignment  snp call software  account   phenomenon  also evaluate base call accuracy   new  life sciences flx sequence machine model use two sequence run   k12 strain  escherichia coli  find   base callers underestimate  flx base accuracy supplementary fig  online  primary reason      overall error rate   flx machine   much lower     gs20  although  fact   pyrobayes base qualities  much closer   actual accuracy suggest   calibration procedure  robust   clearly  need  recalibrate  method   flx  future model  increase accuracy   base qualities  likely permit  sensitive biological study use   machine although  data  illustrate  directly  lowcoverage surveytype applications statistical fluctuations10  result  regions  shallow read depth even  deeper nominal coverage  ability  call snps   regions without  substantial loss  accuracy  permit  complete analyse  wholegenome alignments pyrobayes  process  single sequence run    min",10
191,AutoSNP,"Redundancy based detection of sequence polymorphisms in expressed sequence tag data using autoSNP 
The AutoSNP script is run from the command line. On start-up, the user is asked to supply FASTA format input file name together with a similarity cut-off for d2cluster and cap3. Default values are 80% similarity for d2cluster and 95% for cap3. PROGRAM FLOW AND DEPENDENCIES Initial clustering is carried out by d2cluster (Burke et al., 1999). AutoSNP reads the output table created by d2cluster, and uses the information to build sequence cluster files in FASTA format. These clusters are then passed to the sequence assembly program cap3 (Huang and Madan, 1999). AutoSNP reads the ACE format output file from each cap3 run, and generates gapped FASTA format alignment files which are finally passed to the SNP detection and co-segregation subroutines. PROGRAM OUTPUT The primary output of AutoSNP is a set of linked HTML format SNP reports, prefaced by an index page containing statistical information relating to the sequence contig assembly and candidate SNP/indel identification. The SNP report pages have three components: (i) A key to the sequences in the alignment, (ii) A summary table showing the candidate SNPs/indels, together with confidance scores, and (iii) A full vertical alignment of the sequences, with the SNPs highlighted (Figure 1). Each SNP report also has a hyperlink to the underlying sequence alignment in FASTA format. In addition to the main report, several supporting files are produced which hold information such as the frequency distribution of cap3 sequence contig sizes, and the number of SNPs associated with each size of sequence contig, nucleotide substitution ratios and tables of indel sequence and size frequency. PERFORMANCE WITH THE MAIZE TEST DATA An input file containing 102 551 maize ESTs was downloaded from ZmDB (http://www.zmdb.iastate.edu/), and the AutoSNP program executed on a 1 GHz Intel Pentium III PC with 520 MB RAM running RedHat Linux 7.0. The d2cluster program took 6 days to organize the sequences into primary clusters. The cap3 assembly and SNP detection took a further 22 h to complete analysis. Of the 13 247 clusters produced by cap3, 3479 were found to contain one or more candidate SNP. A total of 14832 candidate polymorphisms were identified (http://www.cerealsdb.uk.net/ discover.htm). Indel size frequencies, nucleotide substitution ratios and segregation of candidate polymorphisms with haplotypes indicate that the majority of SNPs and indels identified using this approach represent true genetic variation in maize. An AutoSNP report summary. This report depicts 11 candidate SNPs, identifying their base position in the sequence alignment along with two measures of confidence in SNP validity. The Min. informative score measures the minimum number of sequences that represent a polymorphism. The cosegregation score is a measure of the number of SNPs in the alignment which share the same pattern of polymorphism between aligned sequences. The weighted cosegregation score takes account of missing data in the alignment of ESTs that may otherwise bias the cosegregation score. The key relates the aligned sequences to original GenBank sequence and also identifies the maize line (where available) derived from the GenBank annotation. The full SNP report includes the complete sequence alignment along with the above SNP summary.",SNPDiscovery,"redundancy base detection  sequence polymorphisms  express sequence tag data use autosnp 
 autosnp script  run   command line  startup  user  ask  supply fasta format input file name together   similarity cutoff  d2cluster  cap3 default value   similarity  d2cluster    cap3 program flow  dependencies initial cluster  carry   d2cluster burke    autosnp read  output table create  d2cluster  use  information  build sequence cluster file  fasta format  cluster   pass   sequence assembly program cap3 huang  madan  autosnp read  ace format output file   cap3 run  generate gap fasta format alignment file   finally pass   snp detection  cosegregation subroutines program output  primary output  autosnp   set  link html format snp report preface   index page contain statistical information relate   sequence contig assembly  candidate snpindel identification  snp report page  three components   key   sequence   alignment   summary table show  candidate snpsindels together  confidance score  iii  full vertical alignment   sequence   snps highlight figure   snp report also   hyperlink   underlie sequence alignment  fasta format  addition   main report several support file  produce  hold information    frequency distribution  cap3 sequence contig size   number  snps associate   size  sequence contig nucleotide substitution ratios  table  indel sequence  size frequency performance   maize test data  input file contain   maize ests  download  zmdb    autosnp program execute    ghz intel pentium iii     ram run redhat linux   d2cluster program take  days  organize  sequence  primary cluster  cap3 assembly  snp detection take      complete analysis     cluster produce  cap3   find  contain one   candidate snp  total   candidate polymorphisms  identify  discoverhtm indel size frequencies nucleotide substitution ratios  segregation  candidate polymorphisms  haplotypes indicate   majority  snps  indels identify use  approach represent true genetic variation  maize  autosnp report summary  report depict  candidate snps identify  base position   sequence alignment along  two measure  confidence  snp validity  min informative score measure  minimum number  sequence  represent  polymorphism  cosegregation score   measure   number  snps   alignment  share   pattern  polymorphism  align sequence  weight cosegregation score take account  miss data   alignment  ests  may otherwise bias  cosegregation score  key relate  align sequence  original genbank sequence  also identify  maize line  available derive   genbank annotation  full snp report include  complete sequence alignment along    snp summary",10
192,ATLAS-SNP2,"A SNP discovery method to assess variant allele probability from next-generation resequencing data
Bacterial data sets used in the training and validation experiments E. coli substrain K12.MG1655 and S. aureus substrain USA300_TCH1516 that were previously sequenced and finished to high accuracy using Sanger method were resequenced using the 454 platform. The reads were processed with the 454 base-caller (version Leonardo V2008B1) to produce base calls and quality metrics. The reference genome sequences were obtained from NCBI (E. coli K12.MG1655, accession no. NC_000913; S. aureus USA 300_TCH1516, accession no. NC_010079). Any identified mismatches were defined as sequencing errors. Each set of reads was also mapped to an alternative reference genome of a genetically different strain of the same species: E. coli DH10B (accession no. NC_010473) and S. aureus USA 300_FPR3757 (accession no. NC_007793), respectively. After identifying the sequencing errors by first mapping the reads to their genetically identical reference genome, the remaining mismatches were defined as the initial set of SNPs. Subsequently, to improve our SNP identification stringency, we mapped one high-quality reference genomic sequence from one strain to the high-quality reference genomic sequence from the second strain, for E. coli and S. aureus, respectively. These lists were intersected with both the initial sets of SNPs from the 454 and Illumina SNP data. Finally, we obtained 147 SNPs for E. coli data and 84 SNPs for S. aureus from both the 454 and Illumina data sets, which are in almost perfect concordance with that published before as discussed in details below. Two previous publications have identified and reported the genetic variations between the two E. coli strains and the two S. aureus strains, respectively. Durfee et al. (2008) reported 105 SNPs in genic regions (listed in Table S2 of Durfee et al. 2008) and 42 SNPs in intergenic regions, so the total number of SNPs in Durfee et al. (2008) was 147 for E. coli. (detailed genomic coordinates for the SNP loci were not provided.) For the two S. aureus strains, Highlander et al. (2007) described in the main text that there were 92 SNPs and two 4-base deletions. Their Supplemental Table 2, however, listed that six deletions, two insertions, and 83 SNPs for S. aureus—a total of 91 polymorphisms—were identified. The SNP genomic positions given by Highlander's Supplemental Table 2 used USA300-MR as the reference, whereas our SNP positions used FP3757 as the reference, so the genomic position information could not be used for comparison. We used BLAT and Cross_Match to uniquely map 98.2% of the E. coli and 95.8% of the S. aureus reads back to their respective reference genome sequences, resulting in an average coverage of ∼18× for E. coli and 31× for S. aureus (Supplemental Table S1). Slightly lower read mapping yields were achieved when the genome sequences of the related strains were used (93.1% for E. coli DH10B, and 95.8% for S. aureus USA300_FPR3757) (Supplemental Table S1). The E. coli DH10B sequence, with only 93.2% of the reference genome covered yielded a greater genetic difference between its reference and resequenced genomes. The E. coli data set was used as training data after a resampling process that produced 10,000 data points for errors and SNPs. The S. aureus data set was used as validation data as well as for tuning parameters, because of a closer genomic composition (such as GC content) to the human genome. Watson genome 454 platform sequencing data Wheeler et al. (2008) sequenced the entire genome sequence of Watson with an average read coverage of ∼7.4× using the 454 platform. We re-called all the Watson genomic sequence reads with the same version of the base-caller (Leonardo V2008B1) used in processing the bacterial sequencing reads. We also obtained the approximately 500,000 Watson genotypes determined using Affymetrix 500K genotyping microarrays (Wheeler et al. 2008) for validating the variant calls from our method. The genotypes were converted into A/C/G/T nucleotides using the Affymetrix map file and further checked against the HapMap-CEU genotypes by allele frequency matching. After filtering, 476,087 SNPs were retained for comparisons. Mapping and aligning the reads to genomic sequences A fundamental issue in read mapping is related to the presence of repeat sequences in the resequenced genome. Owing to the nature of genome assemblies, repeat sequences are occasionally collapsed into a single place in the reference genome. This process occurs in both draft and finished assemblies. As a result, a read from a repeat region in the resequenced genome can be mapped incorrectly to the reference genome, generating false-positive SNPs. A recent duplication in the resequenced genome (not found in the reference genome) can also lead to such errors. To reduce false positives due to such cases, we regard a read to be “ambiguously mapped” if it has multiple best hits, or if the mismatch rate of the best hit is larger than a predefined cutoff value (e.g., the ratio of the best hit to the second best hit exceeds 99%), which is based on the idea developed in POLYBAYES (Marth et al. 1999). Detecting duplicated reads In the 454 sequencing, some shotgun fragments share the same 5′ starting position. They can account for up to 60% of the overall NGS data obtained from the production centers. This creates a skewed coverage distribution that may subsequently bias the error model and thus substantially increases the number of false-positive SNP discoveries (data not shown). Currently, the simplistic approach is to detect the duplicates and remove all of them except the best quality read at a given position. In the future iterations, it is worth exploring whether there is any additional value to retaining some of the duplicates satisfying certain criteria, which might maintain the data integrity while maximizing high-quality coverage. Logistic regression to improve base error prediction in sequencing reads We used a logistic regression model to improve the accuracy of error estimation from each read. The model was trained on E. coli K12.MG1655 reads. We identified a priori a set of predictors based on empirical observations and results from other references (Brockman et al. 2008; Dohm et al. 2008), including raw quality score, swap (a Boolean variable), 11-base NQS 20/15 threshold (a Boolean variable), homopolymers, GC content, relative position from each end, NQS, the immediate flanking nucleotides, and the specific substitution classifications. A generalized linear model was used in the statistical training process, and a stepwise procedure was primarily used for model selection to achieve a balance between model parsimony and prediction accuracy. We chose only the variables with significant P-values. The results from our training experiments are shown in Table 1. In the current version for 454 Titanium and base-caller Leonardo V2008B1, and as shown in Equation 1, the most significant predictors in the model were The quality score of the substitution base. Whether the base is involved in “swap-base” (a phenomenon defined as that two adjacent mismatch bases invert their nucleotides relative to the reference sequence) or multi-nucleotide polymorphism (MNP) events. A Boolean variable indicating whether the NQS passes the default requirement (i.e., the quality score of the substitution base call is greater than 20, and the quality score of each of the five flanking bases on either side is greater than 15—“11-base NQS 20/15 threshold”). The relative distance of the base from the 3′-end of the read, normalized by read length. The inferred logistic regression model with overall significance is equation image We note that the new Titanium base-caller had much improved performance in dealing with homopolymers, which in previous versions caused the base-caller to overcall or undercall the number of contiguous bases from 454 data (Brockman et al. 2008). Our training results indicate that homopolymers no longer contribute significantly to increase the sequencing error probability in 454 reads. This is consistent with the vendor's feedback. The base-call error probability for a given read is equation image Bayesian framework that considers all mapped reads to assess variant allele probability for a locus We derived the locus error probability estimation as equation image for all reads i = {1, 2, …, n} with the same substitution that are mapped to a particular locus j; and derive the locus SNP probability as equation image We use Sj to stand for Pr(SNP)j, which refers to the measured signal at the locus j. The multiplication step assumes that error bases are fully stochastic and therefore arise independently of one another. This assumption, to a certain extent, may cause inaccuracy when total sequencing read coverage varies, and this inaccuracy is difficult to model with read coverage variations. We applied a Bayesian framework to try to take the read coverage variation into consideration in order to further improve our variant allele probability estimation at a given locus. Equation 5 is shown below. equation image Pr(SNP|Sj, c)j is the posterior variant allele probability at locus j when signal is Sj at a specific variant read coverage, c; Pr(Sj|SNP, c) and Pr(Sj|error, c) are inferred from the probability density distribution of Sj for SNPs and errors at a specific variant read coverage c that can be derived empirically from our E. coli training data set, as illustrated in Supplemental Figure S2; prior(SNP|c) and prior(error|c) are the prior estimations of the substitution SNP rate and the error rate when conditioned on the variant read coverage, respectively. In this paper, we used three sets of parameters (Supplemental Table S2). In particular, when there are two or more reads with the same variants, “set 1” priors were set as prior(SNP|c) = 0.9 and prior(error|c) = 0.1; “set 2” priors were prior(SNP|c) = 0.99 and prior(error|c) = 0.01; and “set 3” priors were prior(SNP|c) = 0.999 and prior(error|c) = 0.001.",SNPDiscovery," snp discovery method  assess variant allele probability  nextgeneration resequencing data
bacterial data set use   train  validation experiment  coli substrain k12mg1655   aureus substrain usa300_tch1516   previously sequence  finish  high accuracy use sanger method  resequenced use   platform  read  process    basecaller version leonardo v2008b1  produce base call  quality metrics  reference genome sequence  obtain  ncbi  coli k12mg1655 accession  nc_000913  aureus usa 300_tch1516 accession  nc_010079  identify mismatch  define  sequence errors  set  read  also map   alternative reference genome   genetically different strain    species  coli dh10b accession  nc_010473   aureus usa 300_fpr3757 accession  nc_007793 respectively  identify  sequence errors  first map  read   genetically identical reference genome  remain mismatch  define   initial set  snps subsequently  improve  snp identification stringency  map one highquality reference genomic sequence  one strain   highquality reference genomic sequence   second strain   coli   aureus respectively  list  intersect    initial set  snps     illumina snp data finally  obtain  snps   coli data   snps   aureus      illumina data set    almost perfect concordance   publish   discuss  detail  two previous publications  identify  report  genetic variations   two  coli strain   two  aureus strain respectively durfee    report  snps  genic regions list  table   durfee      snps  intergenic regions   total number  snps  durfee        coli detail genomic coordinate   snp loci   provide   two  aureus strain highlander    describe   main text     snps  two base deletions  supplemental table  however list  six deletions two insertions   snps   aureus— total   polymorphisms— identify  snp genomic position give  highlander' supplemental table  use usa300mr   reference whereas  snp position use fp3757   reference   genomic position information could   use  comparison  use blat  cross_match  uniquely map     coli      aureus read back   respective reference genome sequence result   average coverage     coli     aureus supplemental table  slightly lower read map yield  achieve   genome sequence   relate strain  use    coli dh10b     aureus usa300_fpr3757 supplemental table    coli dh10b sequence      reference genome cover yield  greater genetic difference   reference  resequenced genomes   coli data set  use  train data   resampling process  produce  data point  errors  snps   aureus data set  use  validation data  well   tune parameters    closer genomic composition    content   human genome watson genome  platform sequence data wheeler    sequence  entire genome sequence  watson   average read coverage   use   platform  recall   watson genomic sequence read    version   basecaller leonardo v2008b1 use  process  bacterial sequence read  also obtain  approximately  watson genotypes determine use affymetrix  genotyping microarrays wheeler     validate  variant call   method  genotypes  convert  acgt nucleotides use  affymetrix map file   check   hapmapceu genotypes  allele frequency match  filter  snps  retain  comparisons map  align  read  genomic sequence  fundamental issue  read map  relate   presence  repeat sequence   resequenced genome owe   nature  genome assemblies repeat sequence  occasionally collapse   single place   reference genome  process occur   draft  finish assemblies   result  read   repeat region   resequenced genome   map incorrectly   reference genome generate falsepositive snps  recent duplication   resequenced genome  find   reference genome  also lead   errors  reduce false positives due   case  regard  read   “ambiguously mapped”    multiple best hit    mismatch rate   best hit  larger   predefined cutoff value   ratio   best hit   second best hit exceed    base   idea develop  polybayes marth    detect duplicate read    sequence  shotgun fragment share   ′ start position   account       overall ngs data obtain   production center  create  skew coverage distribution  may subsequently bias  error model  thus substantially increase  number  falsepositive snp discoveries data  show currently  simplistic approach   detect  duplicate  remove    except  best quality read   give position   future iterations   worth explore whether    additional value  retain    duplicate satisfy certain criteria  might maintain  data integrity  maximize highquality coverage logistic regression  improve base error prediction  sequence read  use  logistic regression model  improve  accuracy  error estimation   read  model  train   coli k12mg1655 read  identify  priori  set  predictors base  empirical observations  result   reference brockman    dohm    include raw quality score swap  boolean variable base nqs  threshold  boolean variable homopolymers  content relative position   end nqs  immediate flank nucleotides   specific substitution classifications  generalize linear model  use   statistical train process   stepwise procedure  primarily use  model selection  achieve  balance  model parsimony  prediction accuracy  choose   variables  significant pvalues  result   train experiment  show  table    current version   titanium  basecaller leonardo v2008b1   show  equation    significant predictors   model   quality score   substitution base whether  base  involve  “swapbase”  phenomenon define   two adjacent mismatch base invert  nucleotides relative   reference sequence  multinucleotide polymorphism mnp events  boolean variable indicate whether  nqs pass  default requirement   quality score   substitution base call  greater     quality score     five flank base  either side  greater  —“base nqs  threshold”  relative distance   base   ′end   read normalize  read length  infer logistic regression model  overall significance  equation image  note   new titanium basecaller  much improve performance  deal  homopolymers   previous versions cause  basecaller  overcall  undercall  number  contiguous base   data brockman     train result indicate  homopolymers  longer contribute significantly  increase  sequence error probability   read   consistent   vendor' feedback  basecall error probability   give read  equation image bayesian framework  consider  map read  assess variant allele probability   locus  derive  locus error probability estimation  equation image   read   {  … }    substitution   map   particular locus   derive  locus snp probability  equation image  use   stand  prsnpj  refer   measure signal   locus   multiplication step assume  error base  fully stochastic  therefore arise independently  one another  assumption   certain extent may cause inaccuracy  total sequence read coverage vary   inaccuracy  difficult  model  read coverage variations  apply  bayesian framework  try  take  read coverage variation  consideration  order   improve  variant allele probability estimation   give locus equation   show  equation image prsnpsj    posterior variant allele probability  locus   signal     specific variant read coverage  prsjsnp   prsjerror   infer   probability density distribution    snps  errors   specific variant read coverage     derive empirically    coli train data set  illustrate  supplemental figure  priorsnpc  priorerrorc   prior estimations   substitution snp rate   error rate  condition   variant read coverage respectively   paper  use three set  parameters supplemental table   particular    two   read    variants “set ” priors  set  priorsnpc    priorerrorc   “set ” priors  priorsnpc    priorerrorc    “set ” priors  priorsnpc    priorerrorc  ",10
193,SNPServer,"SNPServer: a real-time SNP discovery tool
Sequence input, assembly and clustering The real-time autoSNP web server, SNPServer, acts as a web interface and wrapper for the three programs, BLAST, CAP3 and autoSNP, that make up the SNP discovery pipeline ( Figure 1 ). The complete pipeline accepts a single sequence as an input. This entry sequence is compared with a specified nucleotide sequence database using BLAST ( 13 ) to identify related sequences. The resulting sequences may then be selected for assembly with CAP3 ( 14 ) and subsequent SNP discovery using autoSNP ( 12 ). Alternatively, users may enter a list of sequences in FASTA format for assembly, or a pre-calculated sequence assembly in ACE format. Complete options for BLAST sequence comparisons, CAP3 assembly and SNP discovery may be specified at the user interface. SNP discovery SNP discovery is performed using a redundancy-based approach with a modified version of the autoSNP PERL script ( 12 , 15 ). Alignment data generated by CAP3 (or from a user submitted ACE file) are used to load the sequences in each assembly into a 2D array. Spacing characters (-) added during sequence alignment are considered as a fifth element in addition to the four nucleotides A, C, G and T. This permits the identification of insertion/deletion polymorphisms between sequences. Each row (representing a single base locus in the assembly) is assessed for differing nucleotides. Minimum redundancy scores specified by the user and associated with alignment width (the number of sequences included in the contig) determine the number of different nucleotides at a base position required for classification as a SNP. Where a SNP is recorded, an SNP score is allocated equal to the minimum number of reads that share a common polymorphism. Where several SNPs are present in an alignment, a co-segregation score is calculated for each SNP. This is measured as the frequency of haplotype specifying SNP patterns occurring in the alignment. This figure is then normalized to the number of sequences in the alignment to produce a weighted co-segregation score. HTML format files are generated to allow the user to input data, select comparison, assembly and SNP discovery parameters, and browse the SNP results ( Figure 2 ). Figure 1 An overview of components of the real-time autoSNP web server, the SNPServer. Open in new tabDownload slide An overview of components of the real-time autoSNP web server, the SNPServer. Figure 2 On entry of a sequence and specification of a sequence database for comparison ( A and B ), a BLAST job is initiated. On completion, a summary of matching sequences are displayed permitting the selection of sequences for assembly ( C ). The assembly page permits users to specify CAP3 and autoSNP parameters ( D ). A summary page provides information on the assembly and the SNP discovery, and permits users to return to the assembly page to modify parameters. The results page consists of two windows, the first provides a complete vertical alignment, highlighting SNPs ( E ) while the second lists the assembly member sequences and provides a SNP summary ( F ). Open in new tabDownload slide On entry of a sequence and specification of a sequence database for comparison ( A and B ), a BLAST job is initiated. On completion, a summary of matching sequences are displayed permitting the selection of sequences for assembly ( C ). The assembly page permits users to specify CAP3 and autoSNP parameters ( D ). A summary page provides information on the assembly and the SNP discovery, and permits users to return to the assembly page to modify parameters. The results page consists of two windows, the first provides a complete vertical alignment, highlighting SNPs ( E ) while the second lists the assembly member sequences and provides a SNP summary",SNPDiscovery,"snpserver  realtime snp discovery tool
sequence input assembly  cluster  realtime autosnp web server snpserver act   web interface  wrapper   three program blast cap3  autosnp  make   snp discovery pipeline  figure    complete pipeline accept  single sequence   input  entry sequence  compare   specify nucleotide sequence database use blast     identify relate sequence  result sequence may   select  assembly  cap3     subsequent snp discovery use autosnp    alternatively users may enter  list  sequence  fasta format  assembly   precalculated sequence assembly  ace format complete options  blast sequence comparisons cap3 assembly  snp discovery may  specify   user interface snp discovery snp discovery  perform use  redundancybased approach   modify version   autosnp perl script      alignment data generate  cap3    user submit ace file  use  load  sequence   assembly    array space character  add  sequence alignment  consider   fifth element  addition   four nucleotides       permit  identification  insertiondeletion polymorphisms  sequence  row represent  single base locus   assembly  assess  differ nucleotides minimum redundancy score specify   user  associate  alignment width  number  sequence include   contig determine  number  different nucleotides   base position require  classification   snp   snp  record  snp score  allocate equal   minimum number  read  share  common polymorphism  several snps  present   alignment  cosegregation score  calculate   snp   measure   frequency  haplotype specify snp pattern occur   alignment  figure   normalize   number  sequence   alignment  produce  weight cosegregation score html format file  generate  allow  user  input data select comparison assembly  snp discovery parameters  browse  snp result  figure   figure   overview  components   realtime autosnp web server  snpserver open  new tabdownload slide  overview  components   realtime autosnp web server  snpserver figure   entry   sequence  specification   sequence database  comparison       blast job  initiate  completion  summary  match sequence  display permit  selection  sequence  assembly     assembly page permit users  specify cap3  autosnp parameters     summary page provide information   assembly   snp discovery  permit users  return   assembly page  modify parameters  result page consist  two windows  first provide  complete vertical alignment highlight snps      second list  assembly member sequence  provide  snp summary    open  new tabdownload slide  entry   sequence  specification   sequence database  comparison       blast job  initiate  completion  summary  match sequence  display permit  selection  sequence  assembly     assembly page permit users  specify cap3  autosnp parameters     summary page provide information   assembly   snp discovery  permit users  return   assembly page  modify parameters  result page consist  two windows  first provide  complete vertical alignment highlight snps      second list  assembly member sequence  provide  snp summary",10
194,snp-search,"snp-search: simple processing, manipulation and searching of SNPs from high-throughput sequencing
snp-search is written in Ruby, a popular scripting language, and uses the Ruby ActiveRecord library that maps database tables to Ruby objects. It will run on most modern Unix-based architectures and can be installed with the command gem install snp-search. snp-search has two fundamental features: 1. Creation of a local SQLite database and schema To create the database, data is sourced from two input files: reference genome (in genbank or EMBL format) and a VCF file. snp-search requires the reference genome used in the mapping process to import the features (genes, etc) and annotations into the database and requires the VCF file to populate the database tables with the SNP information. The database schema created by snp-search is designed to facilitate construction of simple queries that will address complex biological questions. Annotations from the genbank file are populated in the annotations table which is related to the features table which contains information on the type of the feature and its nucleotide sequence. SNP positions from the VCF file are imported to the snps table which is related to the features table by the features_snps table. A SNP that has occurred in each resequenced genome is recorded as an allele in the alleles table. The strain name and description (strains table) is related to alleles via the genotype table which contains the allele_id and strain_id. Thus queries that relate strains to snps or annotations may be easily composed. These relationships are schematised in Figure 1. An external file that holds a picture, illustration, etc. Object name is 1471-2105-14-326-1.jpg Open in a separate window Figure 1 snp-search database schema. 2. Output requested objects from database Once the database has been populated, snp-search provides several filtering and output options (Figure 2). An external file that holds a picture, illustration, etc. Object name is 1471-2105-14-326-2.jpg Open in a separate window Figure 2 Snp-search machinery. Given a genbank and a VCF file, snp-search first creates the SQLite database and populates the data into the database. The user will then have the choice of various output options. Options include producing a SNP concatenated FASTA file, generating a newick-tree format file for phylogenetic analysis and a list of SNPs (depending on query) with information for every SNP (such as if the SNP is synonymous or non-synonymous). Output formats Concatenated FASTA file SNPs for each sample are collected from the database and concatenated and converted to FASTA format. Tab-delimited tabular file Information for individual SNPs are provided in tab-delimited tabular format. Newick-file format snp-search uses FastTree 2 [12] to generate a Newick file for SNP phylogeny. Filtering options Filter SNPs according to the SNP Quality A Phred-scaled score that the SNP exists at the given site (default 90). Filter SNPs according to the Genotype Quality A Phred-scaled score that the genotype is true (also known as Genotype quality score, default 30). Filter SNPs according to the AD ratio The ratio of the unfiltered count of all reads that carried that specific allele compared to other REF and ALT alleles in that site (default 0.9). Ignore SNPs from feature Ignoring particular SNP calls in features, such as phages, transposases, and insertion sequences to remove horizontally transferred DNA as a result of a recombination event. Ignore SNPs in specified range SNPs in a defined specified range will be ignored. Ignore samples Certain samples can be excluded from the SNP output. Ignore non-informative SNPs A SNP that is found in all samples is not included in the output. Analysis Unique SNPs for a set of strains Given a set of defined strains as input, snp-search queries the database for unique SNPs shared by the defined strains and outputs the results in concatenated FASTA or tabular format. SNP phylogeny Concatenated FASTA files are used to generate robust phylogenetic trees that are based on SNP differences across whole core genomes. SNP annotation snp-search processes the data in the database and outputs information on each SNP. The following are the description of this output: – Synonymous or non-synonymous SNP. This is calculated by translating the coding sequence and report if the SNP has whether caused an amino acid change or not. – Function of coding region at the SNP position. This is extracted from the annotated genbank file. – Possible pseudogene. If a stop codon is detected in the coding region (other than the end of the sequence). – Original amino acid. Provide the amino acid for the SNP region in the non-mutated sequence. – Has amino acid has changed due to the mutation? An answer of 'yes’ will be given if there are at least one amino acid changes between the original non-mutated sequence and the mutated sequence. – Is there a change in hydrophobicity of the amino acid? The answer to this question is based on the pre-defined set of hydrophobic and non-hydrophobic amino acids taken from Livingstone et al.,[13]: hydrophobic as [I,L,V,C,A,G,M,F,Y,W,H,T] and the non-hydrophobic amino acids as [K,E,Q,D,N,S,P,B]. – Is there a change in the polarity of the amino acid? The answer to this question is based on the pre-defined set of polarised and non-polarised amino acids taken from Hausman et al., [14]: polar as [R,N,D,E,Q,H,K,S,T,Y] and non-polar as [A,C,G,I,L,M,F,P,W,V]. – Is there a change in size of the amino acid? The answer to this question is based on the pre-defined set of small or non-small set of amino acids taken from Livingstone et al.,[13]: small as [V,C,A,G,D,N,S,T,P] and non-small as [I,L,M,F,Y,W,H,K,R,E,Q]. Database query SQLite database can be interrogated by user defined queries (requires some knowledge of SQL). To view, administer and query the database, one may download a SQL GUI tool or simply use the Unix version of sqlite3.",SNPDiscovery,"snpsearch simple process manipulation  search  snps  highthroughput sequencing
snpsearch  write  ruby  popular script language  use  ruby activerecord library  map database table  ruby object   run   modern unixbased architectures    instal   command gem install snpsearch snpsearch  two fundamental feature  creation   local sqlite database  schema  create  database data  source  two input file reference genome  genbank  embl format   vcf file snpsearch require  reference genome use   map process  import  feature genes etc  annotations   database  require  vcf file  populate  database table   snp information  database schema create  snpsearch  design  facilitate construction  simple query   address complex biological question annotations   genbank file  populate   annotations table   relate   feature table  contain information   type   feature   nucleotide sequence snp position   vcf file  import   snps table   relate   feature table   features_snps table  snp   occur   resequenced genome  record   allele   alleles table  strain name  description strain table  relate  alleles via  genotype table  contain  allele_id  strain_id thus query  relate strain  snps  annotations may  easily compose  relationships  schematise  figure   external file  hold  picture illustration etc object name  jpg open   separate window figure  snpsearch database schema  output request object  database   database   populate snpsearch provide several filter  output options figure   external file  hold  picture illustration etc object name  jpg open   separate window figure  snpsearch machinery give  genbank   vcf file snpsearch first create  sqlite database  populate  data   database  user     choice  various output options options include produce  snp concatenate fasta file generate  newicktree format file  phylogenetic analysis   list  snps depend  query  information  every snp     snp  synonymous  nonsynonymous output format concatenate fasta file snps   sample  collect   database  concatenate  convert  fasta format tabdelimited tabular file information  individual snps  provide  tabdelimited tabular format newickfile format snpsearch use fasttree    generate  newick file  snp phylogeny filter options filter snps accord   snp quality  phredscaled score   snp exist   give site default  filter snps accord   genotype quality  phredscaled score   genotype  true also know  genotype quality score default  filter snps accord    ratio  ratio   unfiltered count   read  carry  specific allele compare   ref  alt alleles   site default  ignore snps  feature ignore particular snp call  feature   phages transposases  insertion sequence  remove horizontally transfer dna   result   recombination event ignore snps  specify range snps   define specify range   ignore ignore sample certain sample   exclude   snp output ignore noninformative snps  snp   find   sample   include   output analysis unique snps   set  strain give  set  define strain  input snpsearch query  database  unique snps share   define strain  output  result  concatenate fasta  tabular format snp phylogeny concatenate fasta file  use  generate robust phylogenetic tree   base  snp differences across whole core genomes snp annotation snpsearch process  data   database  output information   snp  follow   description   output  synonymous  nonsynonymous snp   calculate  translate  cod sequence  report   snp  whether cause  amino acid change    function  cod region   snp position   extract   annotate genbank file  possible pseudogene   stop codon  detect   cod region    end   sequence  original amino acid provide  amino acid   snp region   nonmutated sequence   amino acid  change due   mutation  answer  'yes   give     least one amino acid change   original nonmutated sequence   mutate sequence     change  hydrophobicity   amino acid  answer   question  base   predefined set  hydrophobic  nonhydrophobic amino acids take  livingstone   hydrophobic  ilvcagmfywht   nonhydrophobic amino acids  keqdnspb     change   polarity   amino acid  answer   question  base   predefined set  polarise  nonpolarised amino acids take  hausman    polar  rndeqhksty  nonpolar  acgilmfpwv     change  size   amino acid  answer   question  base   predefined set  small  nonsmall set  amino acids take  livingstone   small  vcagdnstp  nonsmall  ilmfywhkreq database query sqlite database   interrogate  user define query require  knowledge  sql  view administer  query  database one may download  sql gui tool  simply use  unix version  sqlite3",10
195,NGS-SNP,"In-depth annotation of SNPs arising from resequencing projects using NGS-SNP
The main component of NGS-SNP is a Perl script called ‘annotate_SNPs.pl’ that accepts a SNP list as input and generates as output a SNP list with annotations added (Table 1). Information used for SNP annotation is retrieved from Ensembl (Hubbard et al., 2009), NCBI (Maglott et al., 2011) and UniProt (UniProt Consortium, 2011). Using a locally installed version of Ensembl the annotation script can process 4 million SNPs in about 2 days on a standard desktop system. Users analyzing many SNP lists, from different individuals of the same species for example, can take advantage of the script's ability to create a local database of annotation results. This database allows all the annotations and the flanking sequence for any previously processed SNPs to be obtained much more quickly. Additional components of NGS-SNP include a script for merging, filtering and sorting SNP lists as well as scripts for obtaining reference chromosome and transcript sequences from Ensembl that can be used with SNP discovery tools such as Maq. When the annotation script identifies an amino acid-changing SNP it calculates an ‘alignment score change’ value a. This process involves comparing the reference amino acid and the non-reference amino acid to each orthologue. Briefly, the amino acid encoded by the variant (i.e. non-reference) allele v is compared to each available orthologous amino acid o using a log-odds scoring matrix (BLOSUM62 by default). This provides a score s(v,o) for each of the n orthologues. Similarly, the amino acid encoded by the reference allele r is compared to the orthologues. Any set of species in Ensembl can be used as the source of orthologous sequences. The average score for the reference amino acid is subtracted from the average score for the variant amino acid (1), and the result is scaled to between –1 and 1, by dividing by the maximum possible value for the scoring matrix. A positive value indicates that the variant amino acid is more similar to the orthologues than the reference amino acid, whereas a negative value indicates that the reference amino acid is more similar to the orthologues. SNPs with large positive or negative values may be of more initial interest as candidates for further study. equation image        (1) The annotation script includes a ‘model’ option that can be used to specify a well-studied species to use as an additional annotation source. When a SNP is located near or within a gene, annotations describing the model species orthologue of the gene are obtained from Ensembl, Entrez Gene and UniProt. These annotations are used to generate values that appear in a ‘Model_Annotations’ field, in the form of key-value pairs. Examples of information provided in this field include KEGG pathway names (Kanehisa et al., 2010), the number of interacting proteins, phenotypes associated with the orthologue, the names of protein features overlapping with the SNP site in the orthologue, and phenotypes associated with mutations affecting the SNP site in the orthologue. The sample output given in Supplementary File 1 begins with the results for a contrived SNP designed to change a residue in the bovine HBB protein, to resemble a mutation responsible for sickle-cell disease in humans. The annotation script can optionally provide the genomic flanking sequence for each SNP, for use in the design of validation assays. Known SNP sites in the flanking sequence and at the SNP position can be included in the output, as lowercase IUPAC characters in the flanking, and as potentially additional alleles at the SNP site. Supplementary File 2 contains the flanking sequences provided by the annotation script (with known SNPs indicated in lowercase) for the 10 SNPs described in Supplementary File 1. Go to: 3 DISCUSSION Many existing SNP annotation tools work only for human SNPs or SNPs already present in dbSNP, or can only be used to process a few thousand SNPs at a time (Chelala et al., 2009; Johnson et al., 2008; Schmitt et al., 2010). Apart from NGS-SNP we are aware of two tools designed to annotate the very large SNP lists generated by whole-genome resequencing of humans and non-human species. ANNOVAR (Wang et al., 2010) is a command-line tool that uses information from the UCSC Genome Browser to provide annotations. SeqAnt (Shetty et al., 2010) is web-based and can be downloaded, and also relies on resources from the UCSC Genome Browser. Both can place SNPs into functional classes, describe nearby genes, and indicate which SNPs are already described in dbSNP. Neither compares affected residues to orthologous sequences, reports overlapping protein features or domains, provides gene ontology information, or provides flanking sequence. The ability to map SNP-altered residues to a protein in another species to retrieve additional information is also not supported. However, ANNOVAR and SeqAnt provide a measure of DNA conservation at the SNP site, can handle indels, and return annotations much more quickly than NGS-SNP. These features and others give each tool some unique advantages. The option to submit SNPs to SeqAnt online may be particularly appealing to some users. In summary, NGS-SNP can be used to annotate the SNP lists returned from programs such as Maq and SAMtools. SNPs are classified as synonymous, non-synonymous, 3′ -UTR, etc., regardless of whether or not they match existing SNP records. Numerous additional fields of information are provided, several of which are not available from other tools.",SNPDiscovery,"indepth annotation  snps arise  resequencing project use ngssnp
 main component  ngssnp   perl script call annotate_snpspl  accept  snp list  input  generate  output  snp list  annotations add table  information use  snp annotation  retrieve  ensembl hubbard    ncbi maglott     uniprot uniprot consortium  use  locally instal version  ensembl  annotation script  process  million snps    days   standard desktop system users analyze many snp list  different individuals    species  example  take advantage   script' ability  create  local database  annotation result  database allow   annotations   flank sequence   previously process snps   obtain much  quickly additional components  ngssnp include  script  merge filter  sort snp list  well  script  obtain reference chromosome  transcript sequence  ensembl    use  snp discovery tool   maq   annotation script identify  amino acidchanging snp  calculate  alignment score change value   process involve compare  reference amino acid   nonreference amino acid   orthologue briefly  amino acid encode   variant  nonreference allele   compare   available orthologous amino acid  use  logodds score matrix blosum62  default  provide  score svo      orthologues similarly  amino acid encode   reference allele   compare   orthologues  set  species  ensembl   use   source  orthologous sequence  average score   reference amino acid  subtract   average score   variant amino acid    result  scale       divide   maximum possible value   score matrix  positive value indicate   variant amino acid   similar   orthologues   reference amino acid whereas  negative value indicate   reference amino acid   similar   orthologues snps  large positive  negative value may    initial interest  candidates   study equation image          annotation script include  model option    use  specify  wellstudied species  use   additional annotation source   snp  locate near  within  gene annotations describe  model species orthologue   gene  obtain  ensembl entrez gene  uniprot  annotations  use  generate value  appear   model_annotations field   form  keyvalue pair examples  information provide   field include kegg pathway name kanehisa     number  interact proteins phenotypes associate   orthologue  name  protein feature overlap   snp site   orthologue  phenotypes associate  mutations affect  snp site   orthologue  sample output give  supplementary file  begin   result   contrive snp design  change  residue   bovine hbb protein  resemble  mutation responsible  sicklecell disease  humans  annotation script  optionally provide  genomic flank sequence   snp  use   design  validation assay know snp sit   flank sequence    snp position   include   output  lowercase iupac character   flank   potentially additional alleles   snp site supplementary file  contain  flank sequence provide   annotation script  know snps indicate  lowercase    snps describe  supplementary file     discussion many exist snp annotation tool work   human snps  snps already present  dbsnp     use  process   thousand snps   time chelala    johnson    schmitt    apart  ngssnp   aware  two tool design  annotate   large snp list generate  wholegenome resequencing  humans  nonhuman species annovar wang      commandline tool  use information   ucsc genome browser  provide annotations seqant shetty     webbased    download  also rely  resources   ucsc genome browser   place snps  functional class describe nearby genes  indicate  snps  already describe  dbsnp neither compare affect residues  orthologous sequence report overlap protein feature  domains provide gene ontology information  provide flank sequence  ability  map snpaltered residues   protein  another species  retrieve additional information  also  support however annovar  seqant provide  measure  dna conservation   snp site  handle indels  return annotations much  quickly  ngssnp  feature  others give  tool  unique advantage  option  submit snps  seqant online may  particularly appeal   users  summary ngssnp   use  annotate  snp list return  program   maq  samtools snps  classify  synonymous nonsynonymous ′ utr etc regardless  whether    match exist snp record numerous additional field  information  provide several     available   tool",10
196,Varscan2,"VarScan 2: somatic mutation and copy number alteration discovery in cancer by exome sequencing
Mutation detection algorithm Given pileup input for a tumor sample and matched normal control, the mutation detection algorithm performs several steps at each position. First, it determines if both samples meet the minimum coverage requirement (by default, three reads with base quality ≥20) and determines a genotype for each sample individually based upon the read bases observed. By default, a variant allele must be supported by at least two independent reads and at least 8% of all reads. If no variant allele meets the criteria, the position is called wild type (homozygous reference) in that sample. If multiple variant alleles are observed, the most-supported (by read count, and then by base quality) variant allele is chosen. Variants are called homozygous if supported by 75% or more of all reads at a position; otherwise they are called heterozygous. Positions where neither sample is determined to be variant are excluded unless the –validation flag is set to 1. Next, at positions where one or both samples have a variant, the algorithm performs a direct comparison between normal and tumor as follows. If the genotypes do not match, then their read counts are evaluated by one-tailed Fisher's exact test in a two-by-two table (see Supplemental Fig. 6), comparing the number of reference-supporting reads (outcome 1) and variant-supporting reads (outcome 2) observed in tumor (category 1) to the numbers that were observed in normal (category 2). If the resulting P-value meets the significance threshold (default 0.10), then the variant is called somatic (if the normal matches the reference) or LOH (if the normal is heterozygous). If the difference does not meet the significance threshold, the variant is called germline and processed as described below. If the genotypes match, the variant is called germline. The variant P-value is computed by one-tailed Fisher's exact test (FET) in a two-by-two table, comparing the total number of reference-supporting reads and the total number of variant supporting reads (normal and tumor values are combined) to the expected distribution for a nonvariant position due to sequencing error (0.01%). For example, the expected read distribution for a nonvariant position with 500× coverage in each sample would be 999 reference-supporting reads, and one variant-supporting read due to sequencing error. Germline, LOH, and somatic mutations are further categorized as HC or LC by the VarScan processSomatic command. By default, somatic mutations are deemed HC if the variant allele frequency is at least 10% in tumor, <5% in normal, and the FET P-value is less than 0.07. Germline variants are deemed HC if they have at least 10% variant allele frequency in both normal and tumor samples. LOH variants are deemed HC if the variant allele frequency is at least 10% in the normal sample and the FET P-value is less than 0.07. Any variant not meeting the HC criteria is deemed LC. Positions that are homozygous in normal but heterozygous in tumor (gain of heterozygosity) or where the variant allele is not the same (e.g., a SNP and an indel) are presumed to be sequencing/alignment artifacts and are discarded. CNA detection algorithm Given pileup input for a tumor sample and matched normal, the CNA detection algorithm first determines that at least one of the samples meets the minimum coverage requirement. To reduce noise from spurious differences at low coverage, the default setting for this parameter (20) is higher than that of mutation detection. Next, the algorithm computes the depth of high-quality bases (phred base quality ≥20) individually for tumor and normal samples. These depths are recorded for each consecutive position until (1) a gap in minimum coverage is encountered, (2) the end of the chromosome is reached, or (3) the ratio of tumor depth to normal depth changes significantly, as computed by Fisher's exact test. For each contiguous region, the relative copy number change (C) in the tumor is inferred as the log base 2 of the normalized depth ratio: equation image Here DT is the average tumor depth, DN is the average normal depth, IN is the number of uniquely mapped bases in the normal BAM, and IT is the number of uniquely mapped bases in the tumor BAM. The number of uniquely mapped bases is computed using SAMtools flagstat information for each BAM file, specifically as equation image where RM is the number of reads mapped, Dup is the proportion of mapped reads marked as duplicates, and L is the average read length. Raw copy number regions with chromosome, start position, stop position, and log2 value underwent CBS in the DNAcopy package (Seshan and Olshen 2010) to produce segmented calls delineated by significant change-points of at least three standard deviations (Supplemental Methods). Adjacent segments of similar copy number from the CBS algorithm were merged by an internally developed Perl script (MergeSegments), and classified by size. Events encompassing >25% of a chromosome arm were classified as large-scale; all others were considered focal events. Software implementation The VarScan 2 core software was developed in Java; the false-positive filter was implemented in Perl. Binary executables, scripts, and source code are free for noncommercial use and available at http://varscan.sourceforge.net. The false-positive filter requires the bam-readcount utility (D. Larson et al., https://github.com/genome/bam-readcount), which is written and compiled in C. Ovarian cancer data The ovarian cancer data set, including exome sequence data, SNP array data, and validated somatic mutations, was generated and published by the Cancer Genome Atlas Research Network (Cancer Genome Atlas Research Network 2011). The WGS data for the five cases utilized in the cross-platform copy number comparison will be described in a separate publication. Exome and WGS sequence data are available in BAM format at the dbGaP database (http://www.ncbi.nlm.nih.gov/gap). Identifiers for samples in this study are in Supplemental Table 1. Mutations were called in exome data for 151 tumor–normal pairs by the VarScan somatic command with the following parameters: –min-coverage 4,–min-var-freq 0.08,–p-value 0.05,–strand-filter 1–min-avg-qual 20. HC mutations were filtered to remove false positives using the criteria described in Table 1 (see Supplemental Methods). Filter-passed somatic mutations were annotated using gene structure and UCSC (Karolchik et al. 2003) annotation information, assigning each mutation to one of four tiers as previously described (Ley et al. 2008; Mardis et al. 2009). Only tier 1 mutations, which alter coding sequence (nonsynonymous, synonymous, splice site, or noncoding RNA), were reported in Figure 1 or selected for orthogonal validation. CNAs were called in exome data for 142 tumor–normal pairs (nine poor-coverage tumors were excluded) by the VarScan copynumber command with the following parameters: –min-coverage 20–min-region-size 100. Raw CNA calls underwent CBS and a subsequent merging procedure as described in Supplemental Methods. RCNA identification, annotation, and pathway analysis The CMDS algorithm (Zhang et al. 2010) was applied to identify regions of statistically significant RCNAs. For each tumor sample, the merged segmented copy number events (see Supplemental Methods) were cross-referenced with the coordinates of about 200,000 protein-coding exons to obtain the mean log2 of copy number change for the start position and stop position of each exon. CMDS was configured to run with a minimum of 20 markers (exon starts or stops), corresponding to roughly one region tested per gene. Regions meeting the significance threshold (P < 0.0001) were merged if within 100 kb of one another, yielding a set of 520 candidate RCNA regions. These were visually reviewed to identify target genes, and remove peaks encompassing six or more unrelated genes, as the target of these nonfocal events was unclear. The cytoBand.txt and refGene.txt files from the UCSC Genome Browser Database (Karolchik et al. 2003) version hg18 were used to annotate CNA events with cytogenetic band and RefSeq gene information, respectively, using a customized Perl script. Information on specific genes from the RefSeq and KEGG databases was retrieved using GeneCards (Safran et al. 2002) version 3.0. Pathway-based analysis of 582 RCNA genes was performed using KEGG and GO database information using WebGestalt Gene Set Analysis Toolkit version 2.0 (http://bioinfo.vanderbilt.edu/webgestalt/) with the default settings (hypergeometric test, BH correction, at least two genes per category).",SNPDiscovery,"varscan  somatic mutation  copy number alteration discovery  cancer  exome sequencing
mutation detection algorithm give pileup input   tumor sample  match normal control  mutation detection algorithm perform several step   position first  determine   sample meet  minimum coverage requirement  default three read  base quality ≥  determine  genotype   sample individually base upon  read base observe  default  variant allele must  support   least two independent read   least    read   variant allele meet  criteria  position  call wild type homozygous reference   sample  multiple variant alleles  observe  mostsupported  read count    base quality variant allele  choose variants  call homozygous  support       read   position otherwise   call heterozygous position  neither sample  determine   variant  exclude unless  validation flag  set   next  position  one   sample   variant  algorithm perform  direct comparison  normal  tumor  follow   genotypes   match   read count  evaluate  onetailed fisher' exact test   twobytwo table see supplemental fig  compare  number  referencesupporting read outcome   variantsupporting read outcome  observe  tumor category    number   observe  normal category    result pvalue meet  significance threshold default    variant  call somatic   normal match  reference  loh   normal  heterozygous   difference   meet  significance threshold  variant  call germline  process  describe    genotypes match  variant  call germline  variant pvalue  compute  onetailed fisher' exact test fet   twobytwo table compare  total number  referencesupporting read   total number  variant support read normal  tumor value  combine   expect distribution   nonvariant position due  sequence error   example  expect read distribution   nonvariant position   coverage   sample would   referencesupporting read  one variantsupporting read due  sequence error germline loh  somatic mutations   categorize       varscan processsomatic command  default somatic mutations  deem    variant allele frequency   least   tumor   normal   fet pvalue  less   germline variants  deem      least  variant allele frequency   normal  tumor sample loh variants  deem    variant allele frequency   least    normal sample   fet pvalue  less    variant  meet   criteria  deem  position   homozygous  normal  heterozygous  tumor gain  heterozygosity    variant allele       snp   indel  presume   sequencingalignment artifacts   discard cna detection algorithm give pileup input   tumor sample  match normal  cna detection algorithm first determine   least one   sample meet  minimum coverage requirement  reduce noise  spurious differences  low coverage  default set   parameter   higher    mutation detection next  algorithm compute  depth  highquality base phred base quality ≥ individually  tumor  normal sample  depths  record   consecutive position    gap  minimum coverage  encounter   end   chromosome  reach    ratio  tumor depth  normal depth change significantly  compute  fisher' exact test   contiguous region  relative copy number change    tumor  infer   log base    normalize depth ratio equation image     average tumor depth    average normal depth    number  uniquely map base   normal bam     number  uniquely map base   tumor bam  number  uniquely map base  compute use samtools flagstat information   bam file specifically  equation image     number  read map dup   proportion  map read mark  duplicate     average read length raw copy number regions  chromosome start position stop position  log2 value undergo cbs   dnacopy package seshan  olshen   produce segment call delineate  significant changepoints   least three standard deviations supplemental methods adjacent segment  similar copy number   cbs algorithm  merge   internally develop perl script mergesegments  classify  size events encompass    chromosome arm  classify  largescale  others  consider focal events software implementation  varscan  core software  develop  java  falsepositive filter  implement  perl binary executables script  source code  free  noncommercial use  available    falsepositive filter require  bamreadcount utility  larson      write  compile   ovarian cancer data  ovarian cancer data set include exome sequence data snp array data  validate somatic mutations  generate  publish   cancer genome atlas research network cancer genome atlas research network   wgs data   five case utilize   crossplatform copy number comparison   describe   separate publication exome  wgs sequence data  available  bam format   dbgap database  identifiers  sample   study   supplemental table  mutations  call  exome data   tumornormal pair   varscan somatic command   follow parameters mincoverage minvarfreq pvalue strandfilter minavgqual   mutations  filter  remove false positives use  criteria describe  table  see supplemental methods filterpassed somatic mutations  annotate use gene structure  ucsc karolchik    annotation information assign  mutation  one  four tiers  previously describe ley    mardis     tier  mutations  alter cod sequence nonsynonymous synonymous splice site  noncoding rna  report  figure   select  orthogonal validation cnas  call  exome data   tumornormal pair nine poorcoverage tumors  exclude   varscan copynumber command   follow parameters mincoverage minregionsize  raw cna call undergo cbs   subsequent merge procedure  describe  supplemental methods rcna identification annotation  pathway analysis  cmds algorithm zhang     apply  identify regions  statistically significant rcnas   tumor sample  merge segment copy number events see supplemental methods  crossreferenced   coordinate    proteincoding exons  obtain  mean log2  copy number change   start position  stop position   exon cmds  configure  run   minimum   markers exon start  stop correspond  roughly one region test per gene regions meet  significance threshold     merge  within    one another yield  set   candidate rcna regions   visually review  identify target genes  remove peak encompass six   unrelated genes   target   nonfocal events  unclear  cytobandtxt  refgenetxt file   ucsc genome browser database karolchik    version hg18  use  annotate cna events  cytogenetic band  refseq gene information respectively use  customize perl script information  specific genes   refseq  kegg databases  retrieve use genecards safran    version  pathwaybased analysis   rcna genes  perform use kegg   database information use webgestalt gene set analysis toolkit version     default settings hypergeometric test  correction  least two genes per category",10
197,ngs_backbone,"ngs_backbone: a pipeline for read cleaning, mapping and SNP calling using Next Generation Sequence
When the architecture of ngs_backbone was created, several characteristics were regarded as important: the use of standard file formats and third-party free software tools, modularity and extensibility, analysis reproducibility and ease of use. To facilitate interoperability with other tools, most input and output files have a standard format, such as FASTA, FASTQ, BAM, VCF and GFF, which can be produced and used by other tools. For instance, it is very easy to view the mapping and annotation obtained by loading the BAM and GFF files into a viewer, such as IGV [29]. ngs_backbone uses third-party tools of recognized quality, such as SAMtools or GATK whenever possible, in order to maintain the quality of the analyses. This approach takes its toll on the installation process, but in order to make it less complicated, we have packaged and precompiled most of these third party tools and have written a detailed step-by-step installation manual that is distributed with the tool [30]. Modularity was also an important design aim of the ngs_backbone architecture. Users demand an ever-changing set of analyses to be carried out, and these analyses have to be adjusted for every project. To meet this requirement, a collection of mapper functions focused on different tasks, such as cleaning or annotating, were created. These functions have a common interface, they all take a sequence and generate a new, modified sequence and constitute the steps of our pipelines, which are generated at runtime for every analysis. Finally, even though we are presenting ngs_backbone as a command-line tool, this is not the only way to use it. The underlying library that powers this tool is called franklin and is written in Python. This library has other capabilities that at this time are not exposed through the present command line interface, but its API is documented and easy to use, and Python programmers willing to develop their own scripts and tools on top of it are welcome to do so. Its development can be followed at the github website [31] and its license is also open (AGPL). Results and Discussion ngs_backbone pipeline algorithms This section describes the methods used internally by ngs_backbone. The third-party software cited is not supposed to be run by the user, as it will only be used internally by ngs_backbone. Only a couple of commands (backbone_create_project and backbone_analyze) will suffice to complete any analysis. A typical analysis carried out by ngs_backbone starts with a set of Sanger, 454, Illumina or SOLiD read files. The first step is the read cleaning. In this process adaptor, vector and low-quality regions are removed. The exact algorithm used for every cleaning step depends on the type of read. For instance, quality trimming in the long reads is done by lucy [32], but for the shorter reads, an internally implemented algorithm is used instead. For more details about the host of read-cleaning modules available, refer to the documentation distributed with the tool [28]. Once the cleaning is finished, quality and length distributions can be created for the raw and clean reads as a quality assessment of the cleaning process. If a reference transcriptome is unavailable, one can be assembled with the clean reads by using the MIRA assembler [5]. MIRA allows hybrid assemblies with Sanger, 454 and Illumina reads. ngs_backbone automates the preparation of a MIRA project. After running MIRA, the obtained set of contigs may be annotated with all available annotations: microsatellite, ORF, functional description, GO terms, intron location and orthologs. Once a reference transcriptome or genome is available, the reads may be mapped onto it. For the mapping, the algorithm employed also depends on the read length. Short reads are mapped by the standard bwa [6] algorithm, while the longer reads use that of BWT-SW. ngs_backbone generates a BAM file with all the mapped reads in it. The generated BAM files are processed using SAMtools [8] and Picard [33], and are merged and adapted for GATK [34] running a custom code. One frequent objective of the projects that use NGS sequences is to look for sequence variation (SNPs and small indels). To improve this process, a BAM file realignment may be done by using GATK [34] prior to the SNV calling. For the SNV calling, the reads with a mapping quality lower than 15 are not considered. The allele qualities are calculated by using the quality of the three most reliable reads (PQ1, PQ2, PQ2) using the formula PQ1 + 0.25 * (PQ2 + PQ3). This method is a slight variation of the one used by MIRA [5] to calculate the quality for a consensus position. The SNV annotation takes into account the accumulated sequence quality for every allele as well as the mapping quality for each read. A threshold is set for both parameters, and only positions with two or more high-quality alleles are considered as SNPs or indels. Thousands of SNVs are typically generated from these BAM files, so in order to be able to select the most useful ones, a set of SNV filters has been developed (Table ​(Table1).1). The code used to run the SNV filters was all custom code written for ngs_backbone. The SNVs finally obtained along with the filter information are stored in a VCF file [9]. Table 1 ngs_backbone filters for SNV selection. Description and pass conditions        Value MAF        Frequency of most frequent allele in the selected pool allele is less than        0.80 HVR        Percentage of divergence in the unigene is smaller than or equal to        4 UCR        No duplicated or fragment regions are detected by Blast        -- I30        The distance from intron/exon boundary is greater than        30 CL        The distance to ends of unigene is greater than        30 CS        The distance from neighboring SNPs is greater than        60 CEF        The SNV can be detected by endonuclease restriction        -- VK        Select the kind of marker: SNP or indel        -- GF        Frequency of most frequent allele in the selected libraries is less than        0.67 Open in a separate window Although the analysis explained is a typical one, each of the steps is in fact optional. The pipeline has several entry points and results. One could start with raw sequences and do just the cleaning, or alternatively start with the BAM file and use the tool to call the SNVs. Every analysis is independent of the others; it just takes a set of standard files as input and generates another set of standard files as output. Using the software, tomato ngs_backbone analysis To test the tool, a complete analysis of the tomato transcriptome was carried out, from the read cleaning to the experimental SNV validation. All these analyses were done using ngs_backbone. All public tomato Sanger EST reads available at the SGN and GenBank databases [35,36] with known tomato accession origins were included in this study. In addition to these Sanger sequences, 14.2 million Illumina reads obtained from a normalized cDNA library, built with an equimolar mix of floral RNA extracted from the tomato lines UC-82 and RP75/79, were added (additional file 2). After removing the low-quality regions and vector and adaptor contaminants, 9.8 million Illumina and 276,039 Sanger sequences remained. The most-represented tomato lines were Micro Tom (118,304 sequences), TA496 (104,503 sequences) and the RP75/59-UC82 mix (9.8 million sequences). ngs_backbone calculated statistics about sequence features and the cleaning process (Figure ​(Figure11). An external file that holds a picture, illustration, etc. Object name is 1471-2164-12-285-1.jpg Open in a separate window Figure 1 ngs_backbone statistical analysis. Sequence length distribution of cleaned Sanger (a) and Illumina (b) sequences. Boxplot of quality pair base lecture with respect to sequence position of Sanger (c) and Illumina (d) sequences. Alignment sequence coverage distribution of Sanger (e) and Illumina (f) sequences. The cleaned reads were mapped to the SGN tomato transcriptome [35]. 7.75 million Illumina as well as all Sanger reads were mapped, obtaining an average coverage of 4.2 for the Sanger and 8.5 for the Illumina sequences (Figure ​(Figure1).1). To improve this alignment, the realignment functionality provided by GATK [34] was applied prior to the SNV calling. The SNV annotation took into account the accumulated sequence quality for every allele as well as the mapping quality for each read. A threshold was set for both parameters, and only positions with two or more high-quality alleles were considered as SNPs or indels. All 33,306 SNVs found are reported in the VCF file (Additional File 3). Despite satisfying the quality criteria, not all SNVs seemed equally reliable. Several filters were applied to tag those most likely to be real (Table ​(Table1).1). For example, a most frequent allele frequency (MAF) filter was applied to the Illumina set because a ratio between the alleles close to 0.5 is expected in most cases when two equimolar cDNA samples are mixed. In our case, the mix corresponded to the tomato lines UC-82 and RP75/79, and the alleles present in both of them were expected to appear in the ESTs an equal number of times for most unigenes. Also, a filter that labeled the SNVs in highly variable regions (HVR) was applied to avoid unigenes with too much variation. The 23,360 SNVs that passed both filters were considered to have a higher likelihood of being real and constituted the HL set (Table ​(Table2).2). The SNV counts presented from this point on will not include the SNVs that did not pass these filters unless explicitly stated. Table 2 SNVs detected SA        IL        HL SNVs        17237        19052        23306 Indels        3389        3044        5410 SNPs        13848        16008        27896 SNVs_HVR4        16575        17005        30827 SNVs_MAF0.80        -        9903        - SNVs_HVR4_MAF0.80        16575        9640        23360 Open in a separate window SA: Sanger collection: SNVs detected with Sanger sequences. IL: Illumina collection: SNVs detected with Illumina sequences. HL: Higher likelihood collection: SNVs detected with Illumina and Sanger sequences. When using SNVs in an experimental setting, not all are equally useful and easy to use. Depending on the technique used to detect them, several SNV characteristics can ease or hinder an experiment, like closeness to an intron boundary, to another SNV or to the end of the unigene. Also, the SNVs located in unigenes that are very similar to other unigenes were tagged to avoid gene families that could make following the PCR and primer design processes difficult. This was done by applying the Unique and Continuous Region filters, I30 and CL30, available in the ngs_backbone filter collection (Table ​(Table1).1). All filters applied in order to label the SNVs as well as the results obtained are shown in Table ​Table3.3. The 6,934 SNVs that passed these filters made up the easily usable set (EU). Table 3 SNVs selected in the different collections using different ngs_backbone filters. SA        IL        HL        CO        PO SNVs        16575        9640        23360        2855        514 UCR        11312        6763        16150        1925        294 I30        16502        9619        23271        2847        507 CL30        16249        8996        22523        2722        510 EU        4360        3434        6934        860        291 CS60        6155        4765        9730        1190        98 CEF        645        480        996        129        25 Open in a separate window SA: Sanger collection: SNVs detected with Sanger sequences. IL: Illumina collection: SNVs detected with Illumina sequences. HL: Higher likelihood collection: SNVs detected with Illumina and Sanger sequences. CO: Common collection: SNVs detected in Illumina and Sanger collections. PO: Polymorphic collection: SNVs with an estimated frequency of most common allele under 0.67. EU: Easily usable SNVs set: SNVs selected using UCR I30 and CL30 filters. It is also desirable to tag the SNVs with high polymorphism. The main advantage of these highly polymorphic markers consists in their ease of use across different individuals. SNVs with a low PIC (Polymorphic Information Content) have a low likelihood of having different alleles between two randomly chosen individuals. By enriching the selection with highly polymorphic SNVs, the proportion of discriminating SNVs in any experiment dealing with a random collection of individuals is increased, thereby reducing laboratory costs. The polymorphism in a population can only be correctly inferred by having an extensive and well-genotyped sample of individuals. Since ESTs convey genotyping information from different individuals, ngs_backbone does a crude estimate of the polymorphism for each SNV by counting the number of tomato accessions in which each allele appears. The reliability of this inferred polymorphism depends on, among other parameters, the number of individuals sequenced. Taking into account only the SNVs sequenced in at least six different tomato accessions, the 514 SNVs with a frequency for the most common allele under 0.67 were included in the polymorphic (PO) set. This set was small in spite of the good sequence coverage for four of the tomato accessions, as not many sequences were available from other tomato materials. The intersection of this PO set with the easily usable one (EU) produced 291 SNVs. To augment the number of putative highly polymorphic SNVs, less stringent criteria were also applied, creating a new set with the variable SNVs in both the Illumina and in the Sanger sequences, regardless of their estimated polymorphism. 2,855 SNVs were selected, of which 860 were also present in the EU selection (Table ​(Table3).3). These SNVs were denominated common (CO), as they were polymorphic in the public EST collection as well as in the Illumina sequences. The SNVs found only to be polymorphic in the Sanger or in the Illumina collections were named SA and IL, respectively. Experimental validation of software predictions The quality of the in silico SNV calling was tested in a collection of 37 tomato accessions that included 10 commercial cultivars and 27 tomato landraces (Additional File 4). The technique used to genotype these materials was HRM PCR (High Resolution Melting PCR) [37]. To assign the melting curves to the SNV alleles, the accessions RP75/59 and UC-82, which comprise the Illumina EST set, were used as controls when possible. When no polymorphism was expected between these accessions, restriction enzyme polymorphism (also predicted by ngs_backbone) was used to differentiate the alleles. A total of 76 in silico SNVs were experimentally tested (Additional Files 2, 5). The HRM technique was able to confirm 85% of these (Table ​(Table4).4). This high success rate makes the use of the in silico-predicted SNVs possible even without any previous extensive experimental validation. Moreover, the success rate was with all probability underestimated due to the experimental technique used. HRM PCR is not able to distinguish all allele pairs, and it is quite likely that in some cases the failure to detect some of the in silico-predicted SNVs was due to a flaw in the PCR. Table 4 Statistics for assayed SNVs in the different collections. SNVs        HRM detected        % Polymorphic markers        Average frequency b        PIC c SA        14        --        21.4        0.98        0.04 IL        14        12        41.7        0.95        0.09 CO        33        28        71.4        0.85        0.22 PO        15        13        69.2        0.80        0.28 Open in a separate window a Percentage of polymorphic markers: number of polymorphic markers with respect to detected HRM markers or total markers for each set. b Average frequency of most frequent allele of all detected markers or total markers for each set. c PIC (polymorphic information index) of all detected markers or total markers for each set. This high success rate was achieved despite the low coverage employed (4.2 for Sanger and 8.5 for Illumina), although it was probably obtained at the expense of a low specificity that was not assessed in the experimental design presented. The parameters used to do the selection were even adjusted so as to tag as unreliable some SNVs that, even though they were supported by enough coverage, were in regions with high variability or that presented an allele frequency that was off balance in the equimolar RP75/59 - UC-82 Illumina sample. One of the aims of this study was to devise and test a strategy for selecting the most polymorphic SNV subset by using both the publicly available as well as the new Illumina ESTs. Although it is not possible to do an accurate PIC estimate just by using a collection of public sequences gathered from different heterogeneous projects, a rough index related to polymorphism might be calculated by counting the number of individuals in which each allele appears. Despite several confounding factors, a low PIC SNV will tend to produce very off-balance individual counts for the different alleles. The expected mean polymorphism of the SVN sets with different PICs was estimated by genotyping 37 tomato accessions. Two SNV sets were used to define the polymorphism baseline to expect. The only polymorphism-related filter applied to these sets was the requirement of having at least two different alleles in the Illumina or Sanger sequences. Once the tomato collection was genotyped, using SNVs randomly selected from these sets, we found that 3 out of 14 SNVs tested in the Sanger set and 5 out of 12 in the Illumina set were polymorphic, which is to say that the most frequent allele frequency was lower than 95%. Other SNV sets that were expected to be somewhat more polymorphic were those built by sieving the SNVs that were polymorphic in both the Sanger and the Illumina sequences (CO set) as well as those from the PO set (where sequences from at least 6 plants were available and the allele count was quite balanced). In both sets, 70% of the markers tested were polymorphic, which was clearly higher than the 21% and 42% found in the polymorphism baseline. In these sets, the polymorphic information content (PIC) was also expected to be higher than the one found in the Sanger and Illumina sets, where PIC was 0.04 and 0.08, respectively. In the CO set, the PIC was in fact higher, 0.22. Lastly, the SNVs that were expected to be most polymorphic were the ones from the PO set. In these, the sequences from at least 6 plants were available and the allele count was quite balanced. The PIC found, in this case, was 0.28, so when looking for highly polymorphic SNVs, this final strategy pays off. Unfortunately, a selection like this cannot be done directly in all non-model species with public ESTs, as in many cases almost all sequences come from just a handful of different individuals. In fact, not even in public tomato sequences is there much diversity. 81% of these public sequences came from just 2 individuals. Given the results shown, we would recommend that, when looking for SNVs, the number of individuals sequenced be taken into account.",SNPDiscovery,"ngs_backbone  pipeline  read clean map  snp call use next generation sequence
  architecture  ngs_backbone  create several characteristics  regard  important  use  standard file format  thirdparty free software tool modularity  extensibility analysis reproducibility  ease  use  facilitate interoperability   tool  input  output file   standard format   fasta fastq bam vcf  gff    produce  use   tool  instance    easy  view  map  annotation obtain  load  bam  gff file   viewer   igv  ngs_backbone use thirdparty tool  recognize quality   samtools  gatk whenever possible  order  maintain  quality   analyse  approach take  toll   installation process   order  make  less complicate   package  precompiled    third party tool   write  detail stepbystep installation manual   distribute   tool  modularity  also  important design aim   ngs_backbone architecture users demand  everchanging set  analyse   carry    analyse    adjust  every project  meet  requirement  collection  mapper function focus  different task   clean  annotate  create  function   common interface   take  sequence  generate  new modify sequence  constitute  step   pipelines   generate  runtime  every analysis finally even though   present ngs_backbone   commandline tool      way  use   underlie library  power  tool  call franklin   write  python  library   capabilities    time   expose   present command line interface   api  document  easy  use  python programmers   develop   script  tool  top    welcome     development   follow   github website    license  also open agpl result  discussion ngs_backbone pipeline algorithms  section describe  methods use internally  ngs_backbone  thirdparty software cite   suppose   run   user      use internally  ngs_backbone   couple  command backbone_create_project  backbone_analyze  suffice  complete  analysis  typical analysis carry   ngs_backbone start   set  sanger  illumina  solid read file  first step   read clean   process adaptor vector  lowquality regions  remove  exact algorithm use  every clean step depend   type  read  instance quality trim   long read    lucy     shorter read  internally implement algorithm  use instead   detail   host  readcleaning modules available refer   documentation distribute   tool    clean  finish quality  length distributions   create   raw  clean read   quality assessment   clean process   reference transcriptome  unavailable one   assemble   clean read  use  mira assembler  mira allow hybrid assemblies  sanger   illumina read ngs_backbone automate  preparation   mira project  run mira  obtain set  contigs may  annotate   available annotations microsatellite orf functional description  term intron location  orthologs   reference transcriptome  genome  available  read may  map onto    map  algorithm employ also depend   read length short read  map   standard bwa  algorithm   longer read use   bwtsw ngs_backbone generate  bam file    map read    generate bam file  process use samtools   picard    merge  adapt  gatk  run  custom code one frequent objective   project  use ngs sequence   look  sequence variation snps  small indels  improve  process  bam file realignment may    use gatk  prior   snv call   snv call  read   map quality lower     consider  allele qualities  calculate  use  quality   three  reliable read pq1 pq2 pq2 use  formula pq1   * pq2  pq3  method   slight variation   one use  mira   calculate  quality   consensus position  snv annotation take  account  accumulate sequence quality  every allele  well   map quality   read  threshold  set   parameters   position  two   highquality alleles  consider  snps  indels thousands  snvs  typically generate   bam file   order   able  select   useful ones  set  snv filter   develop table ​table1  code use  run  snv filter   custom code write  ngs_backbone  snvs finally obtain along   filter information  store   vcf file  table  ngs_backbone filter  snv selection description  pass condition        value maf        frequency   frequent allele   select pool allele  less          hvr        percentage  divergence   unigene  smaller   equal          ucr         duplicate  fragment regions  detect  blast         i30         distance  intronexon boundary  greater                   distance  end  unigene  greater                   distance  neighbor snps  greater          cef         snv   detect  endonuclease restriction                 select  kind  marker snp  indel                 frequency   frequent allele   select libraries  less          open   separate window although  analysis explain   typical one    step   fact optional  pipeline  several entry point  result one could start  raw sequence     clean  alternatively start   bam file  use  tool  call  snvs every analysis  independent   others   take  set  standard file  input  generate another set  standard file  output use  software tomato ngs_backbone analysis  test  tool  complete analysis   tomato transcriptome  carry    read clean   experimental snv validation   analyse   use ngs_backbone  public tomato sanger est read available   sgn  genbank databases   know tomato accession origins  include   study  addition   sanger sequence  million illumina read obtain   normalize cdna library build   equimolar mix  floral rna extract   tomato line   rp75  add additional file   remove  lowquality regions  vector  adaptor contaminants  million illumina   sanger sequence remain  mostrepresented tomato line  micro tom  sequence ta496  sequence   rp75uc82 mix  million sequence ngs_backbone calculate statistics  sequence feature   clean process figure ​figure11  external file  hold  picture illustration etc object name  jpg open   separate window figure  ngs_backbone statistical analysis sequence length distribution  clean sanger   illumina  sequence boxplot  quality pair base lecture  respect  sequence position  sanger   illumina  sequence alignment sequence coverage distribution  sanger   illumina  sequence  clean read  map   sgn tomato transcriptome   million illumina  well   sanger read  map obtain  average coverage     sanger     illumina sequence figure ​figure1  improve  alignment  realignment functionality provide  gatk   apply prior   snv call  snv annotation take  account  accumulate sequence quality  every allele  well   map quality   read  threshold  set   parameters   position  two   highquality alleles  consider  snps  indels   snvs find  report   vcf file additional file  despite satisfy  quality criteria   snvs seem equally reliable several filter  apply  tag   likely   real table ​table1  example   frequent allele frequency maf filter  apply   illumina set   ratio   alleles close    expect   case  two equimolar cdna sample  mix   case  mix correspond   tomato line   rp75   alleles present      expect  appear   ests  equal number  time   unigenes also  filter  label  snvs  highly variable regions hvr  apply  avoid unigenes   much variation   snvs  pass  filter  consider    higher likelihood   real  constitute   set table ​table2  snv count present   point    include  snvs    pass  filter unless explicitly state table  snvs detect                  snvs                         indels                         snps                         snvs_hvr4                         snvs_maf0                         snvs_hvr4_maf0                         open   separate window  sanger collection snvs detect  sanger sequence  illumina collection snvs detect  illumina sequence  higher likelihood collection snvs detect  illumina  sanger sequence  use snvs   experimental set    equally useful  easy  use depend   technique use  detect  several snv characteristics  ease  hinder  experiment like closeness   intron boundary  another snv    end   unigene also  snvs locate  unigenes    similar   unigenes  tag  avoid gene families  could make follow  pcr  primer design process difficult     apply  unique  continuous region filter i30  cl30 available   ngs_backbone filter collection table ​table1  filter apply  order  label  snvs  well   result obtain  show  table ​table3   snvs  pass  filter make   easily usable set  table  snvs select   different collections use different ngs_backbone filter                                  snvs                                         ucr                                         i30                                         cl30                                                                                  cs60                                         cef                                         open   separate window  sanger collection snvs detect  sanger sequence  illumina collection snvs detect  illumina sequence  higher likelihood collection snvs detect  illumina  sanger sequence  common collection snvs detect  illumina  sanger collections  polymorphic collection snvs   estimate frequency   common allele    easily usable snvs set snvs select use ucr i30  cl30 filter   also desirable  tag  snvs  high polymorphism  main advantage   highly polymorphic markers consist   ease  use across different individuals snvs   low pic polymorphic information content   low likelihood   different alleles  two randomly choose individuals  enrich  selection  highly polymorphic snvs  proportion  discriminate snvs   experiment deal   random collection  individuals  increase thereby reduce laboratory cost  polymorphism   population    correctly infer    extensive  wellgenotyped sample  individuals since ests convey genotyping information  different individuals ngs_backbone   crude estimate   polymorphism   snv  count  number  tomato accession    allele appear  reliability   infer polymorphism depend  among  parameters  number  individuals sequence take  account   snvs sequence   least six different tomato accession   snvs   frequency    common allele    include   polymorphic  set  set  small  spite   good sequence coverage  four   tomato accession   many sequence  available   tomato materials  intersection    set   easily usable one  produce  snvs  augment  number  putative highly polymorphic snvs less stringent criteria  also apply create  new set   variable snvs    illumina    sanger sequence regardless   estimate polymorphism  snvs  select     also present    selection table ​table3  snvs  denominate common     polymorphic   public est collection  well    illumina sequence  snvs find    polymorphic   sanger    illumina collections  name    respectively experimental validation  software predictions  quality    silico snv call  test   collection   tomato accession  include  commercial cultivars   tomato landraces additional file   technique use  genotype  materials  hrm pcr high resolution melt pcr   assign  melt curve   snv alleles  accession rp75    comprise  illumina est set  use  control  possible   polymorphism  expect   accession restriction enzyme polymorphism also predict  ngs_backbone  use  differentiate  alleles  total    silico snvs  experimentally test additional file    hrm technique  able  confirm    table ​table4  high success rate make  use    silicopredicted snvs possible even without  previous extensive experimental validation moreover  success rate    probability underestimate due   experimental technique use hrm pcr   able  distinguish  allele pair    quite likely    case  failure  detect     silicopredicted snvs  due   flaw   pcr table  statistics  assay snvs   different collections snvs        hrm detect         polymorphic markers        average frequency         pic                                                                                                                                                                      open   separate window  percentage  polymorphic markers number  polymorphic markers  respect  detect hrm markers  total markers   set  average frequency   frequent allele   detect markers  total markers   set  pic polymorphic information index   detect markers  total markers   set  high success rate  achieve despite  low coverage employ   sanger    illumina although   probably obtain   expense   low specificity    assess   experimental design present  parameters use    selection  even adjust    tag  unreliable  snvs  even though   support  enough coverage   regions  high variability   present  allele frequency    balance   equimolar rp75   illumina sample one   aim   study   devise  test  strategy  select   polymorphic snv subset  use   publicly available  well   new illumina ests although    possible    accurate pic estimate   use  collection  public sequence gather  different heterogeneous project  rough index relate  polymorphism might  calculate  count  number  individuals    allele appear despite several confound factor  low pic snv  tend  produce  offbalance individual count   different alleles  expect mean polymorphism   svn set  different pics  estimate  genotyping  tomato accession two snv set  use  define  polymorphism baseline  expect   polymorphismrelated filter apply   set   requirement    least two different alleles   illumina  sanger sequence   tomato collection  genotyped use snvs randomly select   set  find      snvs test   sanger set        illumina set  polymorphic    say    frequent allele frequency  lower    snv set   expect   somewhat  polymorphic   build  sieve  snvs   polymorphic    sanger   illumina sequence  set  well      set  sequence   least  plant  available   allele count  quite balance   set    markers test  polymorphic   clearly higher      find   polymorphism baseline   set  polymorphic information content pic  also expect   higher   one find   sanger  illumina set  pic     respectively    set  pic   fact higher  lastly  snvs   expect    polymorphic   ones    set    sequence   least  plant  available   allele count  quite balance  pic find   case     look  highly polymorphic snvs  final strategy pay  unfortunately  selection like  cannot   directly   nonmodel species  public ests   many case almost  sequence come    handful  different individuals  fact  even  public tomato sequence   much diversity    public sequence come    individuals give  result show  would recommend   look  snvs  number  individuals sequence  take  account",10
198,SNPhood,"SNPhood: investigate, quantify and visualise the epigenomic neighbourhood of SNPs using NGS data
SNPhood is an open-source R package (R Core Team, 2015) that is publicly available through Bioconductor (Huber et al., 2015). It builds upon several of its established packages as well as ggplot2 (Wickham, 2009) for producing publication-quality visualisations. SNPhood comprises a set of easy-to-use functions to extract, normalise, quantify and visualise read counts or enrichment over input in the local neighbourhood of ROI (e.g. SNPs) across multiple samples (e.g. individuals). It's functionalities are largely complementary to and extend current tools used for ChIP-Seq data analysis (qualitative comparison shown in Fig. 1A). For instance, in contrast to peak callers that identify regions of enriched signal, SNPhood provides functionalities to perform in-depth analyses of the binding pattern on pre-defined ROI, group them according to their signal shape profiles and, if the data are provided, test for allele-specific and genotype-dependent binding patterns. The resolution of the binding pattern can be controlled by user-defined window and bin sizes, which define the local region surrounding the ROI and the size of individual bins within the neighbourhood for which read counts are quantified separately. Users can then choose from different analysis functions: (i) detection of allelic bias across ROI, for which we implemented a procedure that identifies the most significant bin within each region controlled by an empirically determined FDR, (ii) exploration and visualisation of genotype-dependent binding patterns including generation of publication-quality figures, or unsupervised clustering-based, and (iii), optionally, genotype-dependent comparisons and grouping of the binding pattern across ROI and samples. Methodological details and all functionalities can be found in the SNPhood Vignette (http://bioconductor.org/packages/release/bioc/vignettes/SNPhood/inst/doc/IntroductionToSNPhood.html). An external file that holds a picture, illustration, etc. Object name is btw127f1p.jpg Open in a separate window Fig. 1. (A) SNPhood overview. Comparison and distinction of SNPhood with regard to commonly used tools for ChIP-Seq/RNA-Seq data. Green, yellow and red: Feature fully, partially or not supported, respectively. (B) Visualisation of an ASB region. All plots are a direct output of SNPhood. Upper panel: Overview of ASB for all SNPs within a particular genomic location on chr21 across two datasets based on an FDR threshold of 0.05. For each SNP, the most significant p-value (−log10 transformed) across 40 bins is shown. Lower panel: A detailed view around the SNP rs2822749 (vertical line) for the individual GM10847. It summarises the allelic fraction estimate and confidence intervals (top), the -log10 p-value from the binomial tests within each bin (middle) and the read counts in each bin as well as the genotypes at the SNP position for the different alleles (bottom) As input, it requires (i) a set of BAM files (e.g. from ChIP-Seq), (ii) a list of genomic positions/ROI (e.g. GWAS-SNPs), and, optionally, (iii) corresponding genotype data. If available, SNPhood also allows background normalisation (e.g. input DNA) as employed by ChIP-Seq peak callers such as MACS2 (Zhang et al., 2008). 2.2 SNPhood application example Here, we briefly illustrate some functionalities of SNPhood with a typical workflow example. For a comprehensive documentation of all functionalities, we refer the reader to the SNPhood workflow vignette: http://bioconductor.org/packages/release/bioc/vignettes/SNPhood/inst/doc/workflow.html. We start with a set of ROIs, in our case SNPs that have been identified as histone quantitative trait loci (hQTLs) within the Yoruba (YRI) population (Grubert et al., 2015), and aim to determine how many YRI hQTLs also show ASB within the Caucasian population (CEU). To do so, we employed SNPhood for H3K27ac ChIP-Seq data from two CEU individuals (Kasowski et al., 2013) around the hQTL SNPs to quantify their allelic bias. We found that 395 (33% of shared heterozygous SNPs) show significant ASB at an empirical FDR ~5.4%. To investigate some of these ASB events in more detail, we used the function plotAllelicBiasResultsOverview for a high-level overview of the allelic bias across a chromosomal region. To visualise the binding pattern for specific ROI we employed the function plotAllelicBiasResults (Fig. 1B). This revealed that the selected region harbours two ASB peaks potentially affected by the same SNP. Further analyses would involve clustering of all ROI to identify common patterns, similar to what we performed in Grubert et al., (2015). Despite the name, SNPhood is flexible and can be applied to any ROI. We believe that it will be a helpful tool to generate new biological hypotheses by integrating molecular-phenotype data in an unbiased and position-specific manner.",SNPDiscovery,"snphood investigate quantify  visualise  epigenomic neighbourhood  snps use ngs data
snphood   opensource  package  core team    publicly available  bioconductor huber     build upon several   establish package  well  ggplot2 wickham   produce publicationquality visualisations snphood comprise  set  easytouse function  extract normalise quantify  visualise read count  enrichment  input   local neighbourhood  roi  snps across multiple sample  individuals ' functionalities  largely complementary   extend current tool use  chipseq data analysis qualitative comparison show  fig   instance  contrast  peak callers  identify regions  enrich signal snphood provide functionalities  perform indepth analyse   bind pattern  predefined roi group  accord   signal shape profile    data  provide test  allelespecific  genotypedependent bind pattern  resolution   bind pattern   control  userdefined window  bin size  define  local region surround  roi   size  individual bin within  neighbourhood   read count  quantify separately users   choose  different analysis function  detection  allelic bias across roi    implement  procedure  identify   significant bin within  region control   empirically determine fdr  exploration  visualisation  genotypedependent bind pattern include generation  publicationquality figure  unsupervised clusteringbased  iii optionally genotypedependent comparisons  group   bind pattern across roi  sample methodological detail   functionalities   find   snphood vignette   external file  hold  picture illustration etc object name  btw127f1pjpg open   separate window fig   snphood overview comparison  distinction  snphood  regard  commonly use tool  chipseqrnaseq data green yellow  red feature fully partially   support respectively  visualisation   asb region  plot   direct output  snphood upper panel overview  asb   snps within  particular genomic location  chr21 across two datasets base   fdr threshold     snp   significant pvalue log10 transform across  bin  show lower panel  detail view around  snp rs2822749 vertical line   individual gm10847  summarise  allelic fraction estimate  confidence intervals top  log10 pvalue   binomial test within  bin middle   read count   bin  well   genotypes   snp position   different alleles bottom  input  require   set  bam file   chipseq   list  genomic positionsroi  gwassnps  optionally iii correspond genotype data  available snphood also allow background normalisation  input dna  employ  chipseq peak callers   macs2 zhang     snphood application example   briefly illustrate  functionalities  snphood   typical workflow example   comprehensive documentation   functionalities  refer  reader   snphood workflow vignette   start   set  rois   case snps    identify  histone quantitative trait loci hqtls within  yoruba yri population grubert     aim  determine  many yri hqtls also show asb within  caucasian population ceu     employ snphood  h3k27ac chipseq data  two ceu individuals kasowski    around  hqtl snps  quantify  allelic bias  find     share heterozygous snps show significant asb   empirical fdr ~  investigate    asb events   detail  use  function plotallelicbiasresultsoverview   highlevel overview   allelic bias across  chromosomal region  visualise  bind pattern  specific roi  employ  function plotallelicbiasresults fig   reveal   select region harbour two asb peak potentially affect    snp  analyse would involve cluster   roi  identify common pattern similar    perform  grubert    despite  name snphood  flexible    apply   roi  believe      helpful tool  generate new biological hypotheses  integrate molecularphenotype data   unbiased  positionspecific manner",10
199,Fast-GBS,"Fast-GBS: a new pipeline for the efficient and highly accurate calling of SNPs from genotyping-by-sequencing data
The Fast-GBS analysis pipeline was developed by integrating public packages with internally developed tools. The public packages include Sabre (demultiplexing) [24], Cutadapt (read trimming and cleaning) [25], BWA (read mapping) [26], SAMtools (file conversion and indexing) [27], and Platypus (post-processing of reads, haplotype construction and variant calling) [28]. Fast-GBS functions and software tools are presented in Fig. 1. Fig. 1 figure1 Schematic representation of the analytical steps in the Fast-GBS pipeline. The main steps in the analytical process are indicated in the central portion of the diagram, while the different software tools used are indicated to the left and inputs and outputs of each step to the right Full size image Creating directory structure We developed a Bash script to create the directory structure before running the Fast-GBS pipeline. This command line creates the directories for data (FASTQ files), barcodes (key file), reference genome, and results (Fast-GBS outputs). Input The input data are sequenced DNA fragments from any restriction enzyme–based GBS protocol. Fast-GBS handles raw sequencing data in FASTQ format. Preparing the parameter file The parameter file is a text file containing key information about the analysis including the path to the FASTQ files, barcodes and reference genome. It also contains information about the type of sequence (paired or single-end), the adaptor sequence and the sequencing technology. In this file we can define critical filtering options such as the minimal quality scores for reads, minimal number of reads required to call a genotype, and maximal amount of missing data allowed. Number of CPU, names of output files are also defined in this file. This file comes with the Fast-GBS pipeline. Data demultiplexing The cost efficiency of GBS is partly due to the multiplexing of samples and the resulting pooled reads will need to be demultiplexed prior to SNP calling. Fast-GBS uses Sabre [24] to demultiplex barcoded reads into separate files. It simply compares the provided barcodes with the 5′ end of each read and separates the reads into the appropriate barcode files after having clipped the barcode from the read. If a read does not have a recognized barcode, it is put into an “unknown” file. Sabre also has an option (-m) to allow mismatches within barcodes. Sabre supports gzipped input files. After demultiplexing, Sabre outputs a BC summary log file of how many reads went into each barcode file. Trimming and cleaning After demultiplexing, Fast-GBS uses Cutadapt to find and remove adapter sequences, primers, and other types of unwanted sequence from high-throughput sequencing reads [25]. Read mapping algorithms Fast-GBS uses the MEM (maximal exact matches) algorithm implemented in BWA that works by seeding alignments and then extending seeds with the Smith-Waterman (SW) algorithm using an affine gap penalty [26]. This algorithm can perform local alignment for reads of 70 bp up to 1Mbp. This algorithm can perform parallel alignment, thus markedly increasing the speed of the analysis. The ability to align reads of variable size allows the use of data obtained using different sequencing platforms (Illumina, Ion Torrent, etc). Aligned reads may be gapped to allow for Indels. Post-processing of mapped reads After initial alignment, the mapped reads are further processed by Platypus [28] in order to improve the sensitivity and specificity of variant calling. This post-processing seeks to improve the quality of mapping by performing a re-examination of poorly mapped reads and reads mapping to multiple locations. Platypus classifies poorly mapped reads in three categories: 1) reads with numerous mismatches (high level of sequencing errors), 2) reads mapping to multiple locations in the genome, and 3) any remaining linker or adaptor sequences (causing poor mapping). Variants called using such potentially incorrectly mapped reads (see next step) are highlighted using a BadReads flag. Haplotype construction and variant calling In Fast-GBS, variants are called using Platypus. Unlike alignment-based variant callers which focus on a single variant type (SNP or indel), Platypus uses multi-sample variant calling that helps to exploit information from multiple samples to call variants that may not look reliable in a single sample. This approach decreases the errors around indels and larger variants (MNPs). At first, the local assembler looks at a small window (~few kb) at a time and uses all the reads in the window to generate a colored de Bruijn graph, then using all candidate variants, it generates an exhaustive list of haplotypes. Candidate haplotypes are generated by clustering the candidate alleles across windows. Haplotype frequencies are estimated by the expectation-maximization (EM) algorithm. Then variants are called using the estimated haplotype frequencies. This approach works on the local haplotype level rather than on the level of individual variants and does well on highly divergent regions. This also decreases computational requirements. Variant and individual-level filtering Platypus was originally designed and used to detect variants in human, mouse, rat and chimpanzee samples. To optimize Platypus options in the context of the analysis of GBS-derived single-end reads, we modified several options (see [29] for details of Platypus options). Some of the filters used in Fast-GBS variant calling steps are: number of reads (NR) per locus (default = 2), mapping quality score of reads to call a variant (MQ ≥ 10), minimum base quality (default = 10), MNPs distance (minFlank = 5), and maximum missing data (MaxMD) allowed (default ≤ 80%). See Fast-GBS user manual for a full description of all filtering options. Output data The main output file of Fast-GBS is a.vcf file [30] containing detailed information on each of the variants. In addition, Fast-GBS also generates a simple text file containing only the genotypic data. The Fast-GBS log file contains the completed steps of the pipeline as it is running. In cases where an error occurs and prematurely terminates the running of the pipeline, the log file shows the step at which the analysis stopped. An analysis can be started at any point on the existing intermediate files simply by creating a log file in which the previously completed steps are listed. Fast-GBS will re-initiate the analysis starting from that point onwards.",SNPDiscovery,"fastgbs  new pipeline   efficient  highly accurate call  snps  genotypingbysequencing data
 fastgbs analysis pipeline  develop  integrate public package  internally develop tool  public package include sabre demultiplexing  cutadapt read trim  clean  bwa read map  samtools file conversion  index   platypus postprocessing  read haplotype construction  variant call  fastgbs function  software tool  present  fig  fig  figure1 schematic representation   analytical step   fastgbs pipeline  main step   analytical process  indicate   central portion   diagram   different software tool use  indicate   leave  input  output   step   right full size image create directory structure  develop  bash script  create  directory structure  run  fastgbs pipeline  command line create  directories  data fastq file barcodes key file reference genome  result fastgbs output input  input data  sequence dna fragment   restriction enzymebased gbs protocol fastgbs handle raw sequence data  fastq format prepare  parameter file  parameter file   text file contain key information   analysis include  path   fastq file barcodes  reference genome  also contain information   type  sequence pair  singleend  adaptor sequence   sequence technology   file   define critical filter options    minimal quality score  read minimal number  read require  call  genotype  maximal amount  miss data allow number  cpu name  output file  also define   file  file come   fastgbs pipeline data demultiplexing  cost efficiency  gbs  partly due   multiplexing  sample   result pool read  need   demultiplexed prior  snp call fastgbs use sabre   demultiplex barcoded read  separate file  simply compare  provide barcodes   ′ end   read  separate  read   appropriate barcode file   clip  barcode   read   read     recognize barcode   put   “unknown” file sabre also   option   allow mismatch within barcodes sabre support gzipped input file  demultiplexing sabre output   summary log file   many read go   barcode file trim  clean  demultiplexing fastgbs use cutadapt  find  remove adapter sequence primers   type  unwanted sequence  highthroughput sequence read  read map algorithms fastgbs use  mem maximal exact match algorithm implement  bwa  work  seed alignments   extend seed   smithwaterman  algorithm use  affine gap penalty   algorithm  perform local alignment  read      1mbp  algorithm  perform parallel alignment thus markedly increase  speed   analysis  ability  align read  variable size allow  use  data obtain use different sequence platforms illumina ion torrent etc align read may  gap  allow  indels postprocessing  map read  initial alignment  map read   process  platypus   order  improve  sensitivity  specificity  variant call  postprocessing seek  improve  quality  map  perform  reexamination  poorly map read  read map  multiple locations platypus classify poorly map read  three categories  read  numerous mismatch high level  sequence errors  read map  multiple locations   genome    remain linker  adaptor sequence cause poor map variants call use  potentially incorrectly map read see next step  highlight use  badreads flag haplotype construction  variant call  fastgbs variants  call use platypus unlike alignmentbased variant callers  focus   single variant type snp  indel platypus use multisample variant call  help  exploit information  multiple sample  call variants  may  look reliable   single sample  approach decrease  errors around indels  larger variants mnps  first  local assembler look   small window ~    time  use   read   window  generate  color  bruijn graph  use  candidate variants  generate  exhaustive list  haplotypes candidate haplotypes  generate  cluster  candidate alleles across windows haplotype frequencies  estimate   expectationmaximization  algorithm  variants  call use  estimate haplotype frequencies  approach work   local haplotype level rather    level  individual variants   well  highly divergent regions  also decrease computational requirements variant  individuallevel filter platypus  originally design  use  detect variants  human mouse rat  chimpanzee sample  optimize platypus options   context   analysis  gbsderived singleend read  modify several options see   detail  platypus options    filter use  fastgbs variant call step  number  read  per locus default   map quality score  read  call  variant  ≥  minimum base quality default   mnps distance minflank    maximum miss data maxmd allow default ≤  see fastgbs user manual   full description   filter options output data  main output file  fastgbs  avcf file  contain detail information     variants  addition fastgbs also generate  simple text file contain   genotypic data  fastgbs log file contain  complete step   pipeline    run  case   error occur  prematurely terminate  run   pipeline  log file show  step    analysis stop  analysis   start   point   exist intermediate file simply  create  log file    previously complete step  list fastgbs  reinitiate  analysis start   point onwards",10
200,DeepVariant,"A universal SNP and small-indel variant caller using deep neural networks
Calling genetic variants from NGS data has proven challenging because NGS reads are not only errorful (with rates from ~0.1-10%) but result from a complex error process that depends on properties of the instrument, preceding data processing tools, and the genome sequence itself1,3,4,6. State-of-the-art variant callers use a variety of statistical techniques to model these error processes and thereby accurately identify differences between the reads and the reference genome caused by real genetic variants and those arising from errors in the reads3,4,6,7. For example, the widely-used GATK uses logistic regression to model base errors, hidden Markov models to compute read likelihoods, and naive Bayes classification to identify variants, which are then filtered to remove likely false positives using a Gaussian mixture model with hand-crafted features capturing common error modes6 . These techniques allow the GATK to achieve high but still imperfect accuracy on the Illumina sequencing platform3,4. Generalizing these models to other sequencing technologies has proven difficult due to the need for manual retuning or extending these statistical models (see e.g. Ion Torrent8,9), a major problem in an area with such rapid technological progress1 . Here we describe a variant caller for NGS data that replaces the assortment of statistical modeling components with a single, deep learning model. Deep learning is a revolutionary machine learning technique applicable to a variety of domains, including image classification10 , translation11, gaming12,13, and the life sciences14–17. This toolchain, which we call DeepVariant, (Figure 1) begins by finding candidate SNPs and indels in reads aligned to the reference genome with high-sensitivity but low specificity. The deep learning model, using the Inception-v2 architecture5 , emits probabilities for each of the three diploid genotypes at a locus using a pileup image of the reference and read data around each candidate variant (Figure 1). The model is trained using labeled true genotypes, after which it is frozen and can then be applied to novel sites or samples. Throughout the following experiments, DeepVariant was trained on an independent set of samples or variants to those being evaluated. This deep learning model has no specialized knowledge about genomics or next-generation sequencing, and yet can learn to call genetic variants more accurately than state-of-the-art methods. When applied to the Platinum Genomes Project NA12878 data18 , DeepVariant produces a callset with better performance than the GATK when evaluated on the held-out chromosomes of the Genome in a Bottle ground truth set (Figure 2A). For further validation, we sequenced 35 replicates of NA12878 using a standard whole-genome sequencing protocol and called variants on 27 replicates using a GATK best-practices pipeline and DeepVariant using a model trained on the other eight replicates (see methods). Not only does DeepVariant produce more accurate results but it does so with greater consistency across a variety of quality metrics (Figure 2B). To further confirm the performance of DeepVariant, we submitted variant calls for a blinded sample, NA24385, to the Food and Drug Administration-sponsored variant calling Truth Challenge in May 2016 and won the ""highest performance"" award for SNPs by an independent team using a different evaluation methodology. Like many variant calling algorithms, GATK relies on a model that assumes read errors are independent6 . Though long-recognized as an invalid assumption2 , the true likelihood function that models multiple reads simultaneously is unknown6,19,20. Because DeepVariant presents an image of all of the reads relevant for a putative variant together, the convolutional neural network (CNN) is able to account for the complex dependence among the reads by virtue of being a universal approximator21. This manifests itself as a tight concordance between the estimated probability of error from the likelihood function and the observed error rate, as seen in Figure 2C where DeepVariant's CNN is well calibrated, strikingly more so than the GATK. That the CNN has approximated this true, but unknown, inter-dependent likelihood function is the essential technical advance enabling us to replace the hand-crafted statistical models in other approaches with a single deep learning model and still achieve such high performance in variant calling. We further explored how well DeepVariant’s CNN generalizes beyond its training data. First, a model trained with read data aligned to human genome build GRCh37 and applied to reads aligned to GRCh38 has similar performance (overall F1 = 99.45%) to one trained on GRCh38 and then applied to GRCh38 (overall F1 = 99.53%), thereby demonstrating that a model learned from one version of the human genome reference can be applied to other versions with effectively no loss in accuracy (Table S1). Second, models learned using human reads and ground truth data achieve high accuracy when applied to a mouse dataset22 (F1 = 98.29%), out-performing training on the mouse data itself (F1 = 97.84%, Table S4). This last experiment is especially demanding as not only do the species differ but nearly all of the sequencing parameters do as well: 50x 2x148bp from an Illumina TruSeq prep sequenced on a HiSeq 2500 for the human sample and 27x 2x100bp reads from a custom sequencing preparation run on an Illumina Genome Analyzer II for mouse22. Thus, DeepVariant is robust to changes in sequencing depth, preparation protocol, instrument type, genome build, and even species. The practical benefits of this capability is substantial, as DeepVariant enables resequencing projects in non-human species, which often have no ground truth data to guide their efforts22,23, to leverage the large and growing ground truth data in humans. To further assess its capabilities, we trained DeepVariant to call variants in eight datasets from Genome in a Bottle24 that span a variety of sequencing instruments and protocols, including whole genome and exome sequencing technologies, with read lengths from fifty to many thousands of basepairs (Table 1 and S6). We used the already processed BAM files to introduce additional variability as these BAMs differ in their alignment and cleaning steps. The results of this experiment all exhibit a characteristic pattern: the candidate variants have the highest sensitivity but a low PPV (mean 57.6%), which varies significantly by dataset. After retraining, all of the callsets achieve high PPVs (mean of 99.3%) while largely preserving the candidate callset sensitivity (mean loss of 2.3%). The high PPVs and low loss of sensitivity indicate that DeepVariant can learn a model that captures the technology-specific error processes in sufficient detail to separate real variation from false positives with high fidelity for many different sequencing technologies. As we already shown above that DeepVariant performs well on Illumina WGS data, we analyze here the behavior of DeepVariant on two non-Illumina WGS datasets and two exome datasets from Illumina and Ion Torrent. The SOLID and Pacific Biosciences (PacBio) WGS datasets have high error rates in the candidate callsets. SOLID (13.9% PPV for SNPs, 96.2% for indels, and 14.3% overall) has many SNP artifacts from the mapping short, color-space reads. The PacBio dataset is the opposite, with many false indels (79.8% PPV for SNPs, 1.4% for indels, and 22.1% overall) due to this technology's high indel error rate. Training DeepVariant to call variants in an exome is likely to be particularly challenging. Exomes have far fewer variants (~20-30k)25 than found in a whole-genome (~4-5M)26. The non-uniform coverage and sequencing errors from the exome capture or amplification technology also introduce many false positive variants27. For example, at 8.1% the PPV of our candidate variants for Ion Ampliseq is the lowest of all our datasets. Despite the low initial PPVs, the retrained models in DeepVariant separate errors from real variants with high accuracy in the WGS datasets (PPVs of 99.0% and 97.3% for SOLID and PacBio, respectively), though with a larger loss in sensitivity (candidates 82.5% and final 76.6% for SOLID and 93.4% and 88.5% for PacBio) than other technologies. Despite the challenges the retrained deep learning model with limited data, the exome datasets also perform strikingly well, with a small reduction in sensitivity (from 91.9% to 89.3% and 94.0% to 92.6% for Ion and TruSeq candidates and final calls) for a substantial boost in PPV (from 8.1% to 99.7% and 65.3% to 99.3% for Ion and TruSeq). The performance of DeepVariant compares favorably to those of callsets submitted to the Genome in a Bottle project site using tools developed specifically for each NGS technology and to callsets produced by the GATK or samtools (Table S7). The accuracy numbers presented here shouldn't be viewed as the maximum achievable by either the sequencing technology or DeepVariant. For consistency, we used the same model architecture, image representation, training parameters, and candidate variant criteria for each technology. Because DeepVariant achieves high PPVs for all technologies, the overall accuracy (F1), which is the harmonic mean of sensitivity and PPV, is effectively driven by the sensitivity of the candidate callset. Improvements to the data processing steps before DeepVariant and the algorithm used to identify candidate variants will likely translate into substantial improvements in overall accuracy, particularly for multi-allelic indels. Conversely, despite its effectiveness, representing variant calls as images and applying general image-classification models is certainly suboptimal, as we were unable to effectively encode all of the available information in the reads and reference into the four-channel image. The accuracy of DeepVariant will likely improve by transitioning to more expressive tensor-based28 models specialized for genomic data. Taken together, our results demonstrate that the deep learning approach employed by DeepVariant is able to learn a statistical model describing the relationship between the experimentally observed NGS reads and genetic variants in that data for potentially any sequencing technology. Technologies like DeepVariant change the problem of calling variants from a laborious process of expert-driven, technology-specific statistical modeling to a more automated process of optimizing a general model against data. With DeepVariant, creating a NGS caller for a new sequencing technology becomes a simpler matter of developing the appropriate preprocessing steps, training a deep learning model on sequencing data from samples with ground truth data, and applying this model to new, even non-human, samples. At its core, DeepVariant (1) generates candidate entities with high sensitivity but low specificity, (2) represents the experimental data about each entity in a machine-learning compatible format and then (3) applies deep learning to assign meaningful biological labels to these entities. This general framework for inferring biological entities from raw, errorful, indirect experimental data is likely applicable to many other high-throughput instruments.",SNPDiscovery," universal snp  smallindel variant caller use deep neural networks
calling genetic variants  ngs data  prove challenge  ngs read    errorful  rat  ~  result   complex error process  depend  properties   instrument precede data process tool   genome sequence itself1 stateoftheart variant callers use  variety  statistical techniques  model  error process  thereby accurately identify differences   read   reference genome cause  real genetic variants   arise  errors   reads3  example  widelyused gatk use logistic regression  model base errors hide markov model  compute read likelihoods  naive bay classification  identify variants    filter  remove likely false positives use  gaussian mixture model  handcraft feature capture common error modes6   techniques allow  gatk  achieve high  still imperfect accuracy   illumina sequence platform3 generalize  model   sequence technologies  prove difficult due   need  manual retuning  extend  statistical model see  ion torrent8  major problem   area   rapid technological progress1    describe  variant caller  ngs data  replace  assortment  statistical model components   single deep learn model deep learn   revolutionary machine learn technique applicable   variety  domains include image classification10  translation11 gaming12   life sciences14  toolchain   call deepvariant figure  begin  find candidate snps  indels  read align   reference genome  highsensitivity  low specificity  deep learn model use  inceptionv2 architecture5  emit probabilities     three diploid genotypes   locus use  pileup image   reference  read data around  candidate variant figure   model  train use label true genotypes     freeze     apply  novel sit  sample throughout  follow experiment deepvariant  train   independent set  sample  variants    evaluate  deep learn model   specialize knowledge  genomics  nextgeneration sequence  yet  learn  call genetic variants  accurately  stateoftheart methods  apply   platinum genomes project na12878 data18  deepvariant produce  callset  better performance   gatk  evaluate   heldout chromosomes   genome   bottle grind truth set figure    validation  sequence  replicate  na12878 use  standard wholegenome sequence protocol  call variants   replicate use  gatk bestpractices pipeline  deepvariant use  model train    eight replicate see methods    deepvariant produce  accurate result      greater consistency across  variety  quality metrics figure    confirm  performance  deepvariant  submit variant call   blind sample na24385   food  drug administrationsponsored variant call truth challenge  may   win  ""highest performance"" award  snps   independent team use  different evaluation methodology like many variant call algorithms gatk rely   model  assume read errors  independent6  though longrecognized   invalid assumption2   true likelihood function  model multiple read simultaneously  unknown6  deepvariant present  image     read relevant   putative variant together  convolutional neural network cnn  able  account   complex dependence among  read  virtue    universal approximator21  manifest    tight concordance   estimate probability  error   likelihood function   observe error rate  see  figure   deepvariant' cnn  well calibrate strikingly     gatk   cnn  approximate  true  unknown interdependent likelihood function   essential technical advance enable   replace  handcraft statistical model   approach   single deep learn model  still achieve  high performance  variant call   explore  well deepvariants cnn generalize beyond  train data first  model train  read data align  human genome build grch37  apply  read align  grch38  similar performance overall     one train  grch38   apply  grch38 overall    thereby demonstrate   model learn  one version   human genome reference   apply   versions  effectively  loss  accuracy table  second model learn use human read  grind truth data achieve high accuracy  apply   mouse dataset22    outperform train   mouse data     table   last experiment  especially demand      species differ  nearly    sequence parameters   well  2x148bp   illumina truseq prep sequence   hiseq    human sample   2x100bp read   custom sequence preparation run   illumina genome analyzer   mouse22 thus deepvariant  robust  change  sequence depth preparation protocol instrument type genome build  even species  practical benefit   capability  substantial  deepvariant enable resequencing project  nonhuman species  often   grind truth data  guide  efforts22  leverage  large  grow grind truth data  humans   assess  capabilities  train deepvariant  call variants  eight datasets  genome   bottle24  span  variety  sequence instrument  protocols include whole genome  exome sequence technologies  read lengths  fifty  many thousands  basepairs table     use  already process bam file  introduce additional variability   bams differ   alignment  clean step  result   experiment  exhibit  characteristic pattern  candidate variants   highest sensitivity   low ppv mean   vary significantly  dataset  retrain    callsets achieve high ppvs mean    largely preserve  candidate callset sensitivity mean loss    high ppvs  low loss  sensitivity indicate  deepvariant  learn  model  capture  technologyspecific error process  sufficient detail  separate real variation  false positives  high fidelity  many different sequence technologies   already show   deepvariant perform well  illumina wgs data  analyze   behavior  deepvariant  two nonillumina wgs datasets  two exome datasets  illumina  ion torrent  solid  pacific biosciences pacbio wgs datasets  high error rat   candidate callsets solid  ppv  snps   indels   overall  many snp artifacts   map short colorspace read  pacbio dataset   opposite  many false indels  ppv  snps   indels   overall due   technology' high indel error rate train deepvariant  call variants   exome  likely   particularly challenge exomes  far fewer variants ~  find   wholegenome ~  nonuniform coverage  sequence errors   exome capture  amplification technology also introduce many false positive variants27  example    ppv   candidate variants  ion ampliseq   lowest    datasets despite  low initial ppvs  retrain model  deepvariant separate errors  real variants  high accuracy   wgs datasets ppvs      solid  pacbio respectively though   larger loss  sensitivity candidates   final   solid      pacbio   technologies despite  challenge  retrain deep learn model  limit data  exome datasets also perform strikingly well   small reduction  sensitivity          ion  truseq candidates  final call   substantial boost  ppv          ion  truseq  performance  deepvariant compare favorably    callsets submit   genome   bottle project site use tool develop specifically   ngs technology   callsets produce   gatk  samtools table   accuracy number present  '  view   maximum achievable  either  sequence technology  deepvariant  consistency  use   model architecture image representation train parameters  candidate variant criteria   technology  deepvariant achieve high ppvs   technologies  overall accuracy     harmonic mean  sensitivity  ppv  effectively drive   sensitivity   candidate callset improvements   data process step  deepvariant   algorithm use  identify candidate variants  likely translate  substantial improvements  overall accuracy particularly  multiallelic indels conversely despite  effectiveness represent variant call  image  apply general imageclassification model  certainly suboptimal    unable  effectively encode    available information   read  reference   fourchannel image  accuracy  deepvariant  likely improve  transition   expressive tensorbased28 model specialize  genomic data take together  result demonstrate   deep learn approach employ  deepvariant  able  learn  statistical model describe  relationship   experimentally observe ngs read  genetic variants   data  potentially  sequence technology technologies like deepvariant change  problem  call variants   laborious process  expertdriven technologyspecific statistical model    automate process  optimize  general model  data  deepvariant create  ngs caller   new sequence technology become  simpler matter  develop  appropriate preprocessing step train  deep learn model  sequence data  sample  grind truth data  apply  model  new even nonhuman sample   core deepvariant  generate candidate entities  high sensitivity  low specificity  represent  experimental data   entity   machinelearning compatible format    apply deep learn  assign meaningful biological label   entities  general framework  infer biological entities  raw errorful indirect experimental data  likely applicable  many  highthroughput instrument",10
201,SNPAAmapper,"SNPAAMapper: An efficient genome-wide SNP variant analysis pipeline for next-generation sequencing data
Several initial data processing steps were used to annotate the variants. First, the short read sequences were aligned to the reference genome using BWA [4] and the SAMtools package [5] was used to call variants (SNPs and INDELs). Second, depending on the biology questions being asked, several intermediate and filter steps/parameters could be applied to select and filter these variants before the algorithms were called. The filtered Variant Call Format (VCF) file was used as the input for further processing. The overall view of the SNPAAMapper is shown in Figure 1. There are two major algorithms involved in classifying variants by regions in our variant analysis pipeline. Algorithm 1 generates a new annotation table with “coding” and other information annotated for each exon. We followed the University of California, Santa Cruz (UCSC) internal database [6] annotation coordinates (zero-based start and one-based end) to populate and/or calculate the start and end of each feature (coding region sequences or CDS, upstream or downstream sequences, untranslated regions or UTRs, intron) for each exon. Features were annotated according to their “neighboring” or deriving feature’s annotation coordinates from the UCSC. For example, if the “intronStart” was calculated from its previous exonEnd which is one-based annotation from UCSC annotation, the “intronStart” will be one-based as well. An external file that holds a picture, illustration, etc. Object name is 97320630009870F1.jpg Open in a separate window Figure 1 The SNPAAMapper analysis pipeline workflow This configure file generated by Algorithm 1 was read by Algorithm 2 to assign identified variants to the genomic locus/gene(s) and to classify the variants by regions. There are two sub-algorithms in Algorithm 2: Algorithm2-Sub1 generated a list of feature (CDS, upstream, downstream, UTRs, and intron,) annotation files only once for each genome/species. Specifically, two paired files (feature_start with feature_end and feature_start with UCSC Gene ID) for each feature and one main file (chromosome_number with feature_start) were produced as described below. All start coordinates on every chromosome for each feature’s main file were recorded and sorted for the purpose of quickly locating variant’s genomic coordinate in the genome. The “end coordinate” and associated UCSC gene IDs corresponding to every start annotation for each feature was stored in two separate files. Algorithm2-Sub2 used above annotations/files to map identified variants onto the genomic location and report annotation classes. We downloaded all human (hg19) protein-coding genes’ full coding region sequences (CDS Exons) including introns between CDSs with the genomic coordinates annotated for the beginning and end of CDSs from the UCSC genome browser’s “Gene and Gene Prediction” data track [7]. The downloaded output sequence type was set to “genomic” and we also stipulated downloaded sequences format as “Exons in upper case, everything else in lower case” for easy manipulation. We also downloaded UCSC annotation table “kgXref.txt” to convert the UCSC KnownGene ID to HUGO gene symbol for final report. The pipeline used genomic coordinate to automatically classify variants into regions: non-synonymous missense (NSM), non-synonymous nonsense (NSN), and synonymous (SYN) mutation if the hit falls into a CDS, upstream, downstream, the 5’ or 3’ UTR, and intron region. The UCSC “knownGene.txt” and “kgXref.txt” annotation files from UCSC genome browser were used to obtain the information relative to the genomic location and gene for detected variants. The final mutation effect was prioritized according to the category order listed above. SNPAAMapper Input and Output The current SNPAAMapper pipeline accepts the VCF input file in tab-delimited format. The current final output file consists of the following columns: sample number, chromosome number, variant genomic position start, gene symbol, UCSC Gene ID, variant mapping strand, amino acid position for mutation (for CDS hit), variant type (SNP or INDEL), reference amino acid (and codon) → mutant amino acid (and codon), variant class (SYN, NSM, or NSN), reference amino acid sequence, mutated amino acid sequence, hit type (CDS, Upstream, Downstream, UTRs, or Intron hit), dbSNP ID [8] if known, reference nucleotide, alternative or called nucleotide, quality, depth, allele frequency, read categories, and other useful information from variant calling results. A sample output from SNPAAMapper on a NGS dataset is shown in our additional illustration. Caveats and Future Development Our pipeline provides a convenient tool that allows detected variants to be further elucidated. This pipeline facilitates the fast downstream analysis for detected variants. We believe our pipeline will supply researchers with a convenient downstream interpretation for identified variants. In comparing to snpEff [9], an open-source, state of the art variant effect predictor tool, although their performance are similar, SNPAAMapper has the functionality of annotating regulatory variants, and the mutation effect prioritization step in SNPAAMapper also greatly simplifies downstream analysis. SNPAAMapper provides maximum flexibility and allows analysis of NGS read data generated by any short, long, single-end and paired-end protocols from different NGS sequencing platforms. Current version of SNPAAMapper processes the VCF file generated by samtools-0.1.18 or its earlier version. It is also an easy-to-use pipeline for processing VCF input data. Researchers can easily run the pipeline commands by setting their customized parameters to prioritize the SNPs for wet-lab validation. The initial pipeline was written for human data, but it can be easily modified and/or extended for other species. Also, the current pipeline only reports detailed annotation information for the SNP variant class. We’ll extend our algorithms/pipeline to handle INDELs. The pipeline is also freely available for academic users to use. Users can download all input files onto their machines and run the pipeline as long as Perl is installed.",SNPDiscovery,"snpaamapper  efficient genomewide snp variant analysis pipeline  nextgeneration sequence data
several initial data process step  use  annotate  variants first  short read sequence  align   reference genome use bwa    samtools package   use  call variants snps  indels second depend   biology question  ask several intermediate  filter stepsparameters could  apply  select  filter  variants   algorithms  call  filter variant call format vcf file  use   input   process  overall view   snpaamapper  show  figure    two major algorithms involve  classify variants  regions   variant analysis pipeline algorithm  generate  new annotation table  “coding”   information annotate   exon  follow  university  california santa cruz ucsc internal database  annotation coordinate zerobased start  onebased end  populate andor calculate  start  end   feature cod region sequence  cds upstream  downstream sequence untranslated regions  utrs intron   exon feature  annotate accord   “neighboring”  derive feature annotation coordinate   ucsc  example   “intronstart”  calculate   previous exonend   onebased annotation  ucsc annotation  “intronstart”   onebased  well  external file  hold  picture illustration etc object name  97320630009870f1jpg open   separate window figure   snpaamapper analysis pipeline workflow  configure file generate  algorithm   read  algorithm   assign identify variants   genomic locusgenes   classify  variants  regions   two subalgorithms  algorithm  algorithm2sub1 generate  list  feature cds upstream downstream utrs  intron annotation file     genomespecies specifically two pair file feature_start  feature_end  feature_start  ucsc gene    feature  one main file chromosome_number  feature_start  produce  describe   start coordinate  every chromosome   feature main file  record  sort   purpose  quickly locate variants genomic coordinate   genome  “end coordinate”  associate ucsc gene ids correspond  every start annotation   feature  store  two separate file algorithm2sub2 use  annotationsfiles  map identify variants onto  genomic location  report annotation class  download  human hg19 proteincoding genes full cod region sequence cds exons include introns  cdss   genomic coordinate annotate   begin  end  cdss   ucsc genome browsers “gene  gene prediction” data track   download output sequence type  set  “genomic”   also stipulate download sequence format  “exons  upper case everything else  lower case”  easy manipulation  also download ucsc annotation table “kgxreftxt”  convert  ucsc knowngene   hugo gene symbol  final report  pipeline use genomic coordinate  automatically classify variants  regions nonsynonymous missense nsm nonsynonymous nonsense nsn  synonymous syn mutation   hit fall   cds upstream downstream     utr  intron region  ucsc “knowngenetxt”  “kgxreftxt” annotation file  ucsc genome browser  use  obtain  information relative   genomic location  gene  detect variants  final mutation effect  prioritize accord   category order list  snpaamapper input  output  current snpaamapper pipeline accept  vcf input file  tabdelimited format  current final output file consist   follow columns sample number chromosome number variant genomic position start gene symbol ucsc gene  variant map strand amino acid position  mutation  cds hit variant type snp  indel reference amino acid  codon → mutant amino acid  codon variant class syn nsm  nsn reference amino acid sequence mutate amino acid sequence hit type cds upstream downstream utrs  intron hit dbsnp    know reference nucleotide alternative  call nucleotide quality depth allele frequency read categories   useful information  variant call result  sample output  snpaamapper   ngs dataset  show   additional illustration caveats  future development  pipeline provide  convenient tool  allow detect variants    elucidate  pipeline facilitate  fast downstream analysis  detect variants  believe  pipeline  supply researchers   convenient downstream interpretation  identify variants  compare  snpeff   opensource state   art variant effect predictor tool although  performance  similar snpaamapper   functionality  annotate regulatory variants   mutation effect prioritization step  snpaamapper also greatly simplify downstream analysis snpaamapper provide maximum flexibility  allow analysis  ngs read data generate   short long singleend  pairedend protocols  different ngs sequence platforms current version  snpaamapper process  vcf file generate  samtools   earlier version   also  easytouse pipeline  process vcf input data researchers  easily run  pipeline command  set  customize parameters  prioritize  snps  wetlab validation  initial pipeline  write  human data     easily modify andor extend   species also  current pipeline  report detail annotation information   snp variant class well extend  algorithmspipeline  handle indels  pipeline  also freely available  academic users  use users  download  input file onto  machine  run  pipeline  long  perl  instal",10
202,SNP-ML,"Machine learning as an effective method for identifying true SNPs in polyploid plants
Data sets The re-sequencing data set was created using 21 tetraploid A. hypogaea genotypes described in Clevenger et al. (2017) [8] and deposited publically at ncbi.nlm.nih.gov (Bio Project PRJNA340877 and Bio Samples SAMN05721179 to SAMN05721198). The RNA-seq data set has information from nine tetraploid peanut genotypes described in Clevenger et al. (2015, 2016a, 2016b) [18-20]. Validated true and false-positive SNP sets were based on testing the Arachis Affymetrix array with 384 peanut genotypes [8]. Mapping parameters were extracted from the vcf files used for the original design of the array. All positions of SNPs and surrounding sequence are based on the A. duranensis and A. ipaensis v1 pseudomolecules [https://peanutbase.org/, 1] Creating and testing a new Affymetrix array based on SNP-ML A new affymttrix array was designed containing 28,218 SNPs which were extracted by SNP-ML using peanut real data re-sequencing model of neural network and tree bagger (S1 Table). The previously described 21 genotypes alongside 8 more genotypes and 103 minicore peanut lines [8] were assayed on the array and all 28,218 SNP-ML-derived markers were manually curated for polymorphism. A total of 21,112 markers were validated as polymorphic between genotypes (75%). Creating and testing the machine learning models The data sets were prepared by R statistical software, e.g. extracting the attributes, randomly created training and testing sets and preparing fasta files for SNP flanking segments. Various toolboxes in MATLAB R2015b (the University of Georgia campus-wide site licensing agreement) were used for different purposes. Bioinformatics Toolbox was used for calculating the thermodynamic parameters, molecular weights and GC contents, Statistics and Machine Learning Toolbox was used for creating and testing the different models of supervised machine learning and Graphics functions were used for producing the bar plots and ROC (Receiver Operating Characteristic) graphs. The specific arguments of the different machine learning models are given in S2 Table. SNP-ML construction We built paired (neural network and TB) specific trainer models for the two data types, WGS re-sequencing and RNA-seq. The models were built and stored in four files by a python script. In addition, three C++ classes were built, vcf.h, csv_write.h and csv.h, to process vcf and csv files. The SNP-ML main steps are illustrated in Fig 1. It uses C++ class vcf.h to extract the eight selected attributes from the input file, which is a vcf file of the output of SNP calling by mpileup of SAMtools, either directly or after primary filtration by SWEEP. The output is saved using the C++ class csv_write.h into a csv file, which is read by a python script to be applied to one pair of stored models (two files, one for neural network and the other for TB) depending on the data type. The two score sets are saved to a csv file, which is read by C++ class csv.h. The scores are filtered by passing only SNPs that have a value higher than the cutoff of neural network, which can be selected by the user (the default is 0.5), and occurred in the two score sets (shared SNPs in the output of the neural network and TB score file), in case the user selects that option. The scores are stored in csv files and the corresponding SNPs are stored in a vcf file. Fig 1: Download figureOpen in new tab Fig 1: SNP-ML/SNP-MLer infrastructure. To extend the program applications, a second tool was designed, designated SNP-Mler (pronounced ‘snip miller’) to allow users to create predictors that are suitable for interested species/experimental conditions. SNP-MLer uses reading/writing approach as described above, it takes validated true-positive and false-positive vcf files as input and generates predictor model as outputs. Both tools, SNP-ML and SNP-MLer allow the user to skip or select some of the eight attributes, and to apply new user defined attributes as csv files. SNP-ML requirements The script was written by C++ and python 2.7.1 (S1 file). C++ was used for processing the data, input, output and filtering. The binary file was created by GCC 4.1.2 that was run on Red Hat 4.1.2-55 linux system. Python was used for creating the neural network and bagging machine learning models and applying the prediction using them. Different python packages were used for these purposes, i.e. numpy-1.11.0 (SciPy.org), scipy-0.17.1 (SciPy.org), pandas-0.18.1 (pandas.pydata.org), python-dateutil-2.0 (pypi.python.org), pytz-2016.4 (pypi.python.org), scikit-learn-0.17.1 (scikit-learn.org) and pyrenn 0.1 (pyrenn.readthedocs.io). Creating and testing models using simulated data The pseudo molecule assembly AD1_BGI of cotton [http://www.cottongen.org/,21], the pseudomolecule assembly of the 3B chromosome of wheat [22], the contigs of TGACv1 wheat genome [https://plants.ensembl.org/index.html], the pseudomolecule assembly of Fragaria vesca Genome v1.1 [http://www.rosaceae.org/, 23], and the contigs of F. nipponica Genome v1.0 (FNI_r1.1), F. nubicola Genome v1.0 (FNU_r1.1) and F. orientalis Genome v1.0 (FOR_r1.1) [http://www.rosaceae.org/,24] were downloaded. 10,000 random loci were assigned in Chromosomes Aradu.A01, At_chr1, 3B and LG1, of peanut, cotton, wheat and strawberry, respectively. The loci were randomly mutated five times to form five synthetic genotypes using ART tool [25]. HiSeq 125 bp paired end sequences with different depths, 10x to 50x, were generated. The fastq produced files were mapped using BWA 0.7.10 [26] with default parameters to synthetic references as follows: a synthetic tetraploid reference containing Aradu.A01 and Araip.B01 chromosomes for peanut, a synthetic tetraploid reference containing At_chr1 and Dt_chr1 for cotton, a synthetic hexaploid reference containing 3B chromosome and the contigs of A and D genomes for wheat, and a synthetic octoploid reference containing LG1 chromosome and the contigs of FNI, FNU and FOR genomes for strawberry. SNPs were called using samtools mpileup 1.2 and bcftools 1.2.1 with default parameters without filtration. The SNP calling was carried out twice for every species. SNPs between two genotypes were called in the first instance and SNPs among the five genotypes were called in the second. For each species, the SNPs located among the 10,000 loci were extracted in a separate vcf file, and considered to be True-positive (TP) SNPs. Any others identified by the program were extracted in another vcf file, and considered to be False-positive (FP) SNPs. Seventy percent of each one were randomly selected, and combined to be used as training sets, and the remaining 30% were used as testing sets for Neural Network models using Matlab R2015b (the University of Georgia campus-wide site licensing agreement). Testing simulated data against the real data: For peanut, 21 synthetic genotypes with 10X depth were generated and SNPs were called in four batches (three with five and one with six genotypes). The simulated data were used to train the model to mimic the conditions of the real data. All sets of the TP and FP simulated data were used to train the models, to increase the strength, and the testing sets of the real data were re-applied to these simulated models. The generation of synthetic genotypes and carrying out the machine learning (training and testing) were applied as described above.",SNPDiscovery,"machine learn   effective method  identify true snps  polyploid plants
data set  resequencing data set  create use  tetraploid  hypogaea genotypes describe  clevenger      deposit publically  ncbinlmnihgov bio project prjna340877  bio sample samn05721179  samn05721198  rnaseq data set  information  nine tetraploid peanut genotypes describe  clevenger       validate true  falsepositive snp set  base  test  arachis affymetrix array   peanut genotypes  map parameters  extract   vcf file use   original design   array  position  snps  surround sequence  base    duranensis   ipaensis  pseudomolecules   create  test  new affymetrix array base  snpml  new affymttrix array  design contain  snps   extract  snpml use peanut real data resequencing model  neural network  tree bagger  table  previously describe  genotypes alongside   genotypes   minicore peanut line   assay   array    snpmlderived markers  manually curated  polymorphism  total   markers  validate  polymorphic  genotypes  create  test  machine learn model  data set  prepare   statistical software  extract  attribute randomly create train  test set  prepare fasta file  snp flank segment various toolboxes  matlab r2015b  university  georgia campuswide site license agreement  use  different purpose bioinformatics toolbox  use  calculate  thermodynamic parameters molecular weight   content statistics  machine learn toolbox  use  create  test  different model  supervise machine learn  graphics function  use  produce  bar plot  roc receiver operate characteristic graph  specific arguments   different machine learn model  give   table snpml construction  build pair neural network   specific trainer model   two data type wgs resequencing  rnaseq  model  build  store  four file   python script  addition three  class  build vcfh csv_writeh  csvh  process vcf  csv file  snpml main step  illustrate  fig   use  class vcfh  extract  eight select attribute   input file    vcf file   output  snp call  mpileup  samtools either directly   primary filtration  sweep  output  save use   class csv_writeh   csv file   read   python script   apply  one pair  store model two file one  neural network      depend   data type  two score set  save   csv file   read   class csvh  score  filter  pass  snps    value higher   cutoff  neural network    select   user  default    occur   two score set share snps   output   neural network   score file  case  user select  option  score  store  csv file   correspond snps  store   vcf file fig  download figureopen  new tab fig  snpmlsnpmler infrastructure  extend  program applications  second tool  design designate snpmler pronounce snip miller  allow users  create predictors   suitable  interest speciesexperimental condition snpmler use readingwriting approach  describe   take validate truepositive  falsepositive vcf file  input  generate predictor model  output  tool snpml  snpmler allow  user  skip  select    eight attribute   apply new user define attribute  csv file snpml requirements  script  write    python   file   use  process  data input output  filter  binary file  create  gcc    run  red hat  linux system python  use  create  neural network  bag machine learn model  apply  prediction use  different python package  use   purpose  numpy scipyorg scipy scipyorg pandas pandaspydataorg pythondateutil pypipythonorg pytz pypipythonorg scikitlearn scikitlearnorg  pyrenn  pyrennreadthedocsio create  test model use simulate data  pseudo molecule assembly ad1_bgi  cotton   pseudomolecule assembly    chromosome  wheat   contigs  tgacv1 wheat genome   pseudomolecule assembly  fragaria vesca genome      contigs   nipponica genome  fni_r1  nubicola genome  fnu_r1   orientalis genome  for_r1   download  random loci  assign  chromosomes aradua01 at_chr1   lg1  peanut cotton wheat  strawberry respectively  loci  randomly mutate five time  form five synthetic genotypes use art tool  hiseq   pair end sequence  different depths     generate  fastq produce file  map use bwa    default parameters  synthetic reference  follow  synthetic tetraploid reference contain aradua01  araipb01 chromosomes  peanut  synthetic tetraploid reference contain at_chr1  dt_chr1  cotton  synthetic hexaploid reference contain  chromosome   contigs     genomes  wheat   synthetic octoploid reference contain lg1 chromosome   contigs  fni fnu   genomes  strawberry snps  call use samtools mpileup   bcftools   default parameters without filtration  snp call  carry  twice  every species snps  two genotypes  call   first instance  snps among  five genotypes  call   second   species  snps locate among   loci  extract   separate vcf file  consider   truepositive  snps  others identify   program  extract  another vcf file  consider   falsepositive  snps seventy percent   one  randomly select  combine   use  train set   remain   use  test set  neural network model use matlab r2015b  university  georgia campuswide site license agreement test simulate data   real data  peanut  synthetic genotypes   depth  generate  snps  call  four batch three  five  one  six genotypes  simulate data  use  train  model  mimic  condition   real data  set      simulate data  use  train  model  increase  strength   test set   real data  reapplied   simulate model  generation  synthetic genotypes  carry   machine learn train  test  apply  describe ",10
203,Bambino,"Bambino: a variant detector and alignment viewer for next-generation sequencing data in the SAM/BAM format
Bambino's assembly viewer displays alignments from one or more BAM files against a reference sequence either loaded from a file (FASTA, UCSC .2bit and .nib formats are supported), or generated from the underlying reads. Nucleotides are displayed color-shaded based on quality values, in a style similar to the consed program (Gordon et al., 1998). Display of various SAM alignment tags is supported, including soft and hard clipping and spliced alignments. Padding characters are added to the reference sequence and alignments as necessary to provide complete visualization of insertions and short tandem repeats. The viewer generates a summary histogram of non-reference allele frequencies for both tumor and normal samples, providing a quick impression of whether potential SNP sites are homozygous or heterozygous, germline or somatic. Another panel displays a bird's eye view of the wider region, showing normalized depth of coverage and exon positions. The viewer also displays dbSNP entries and NCBI RefSeq protein translations retrieved from a configurable MySQL UCSC genome annotation database (Rhead et al., 2009) and can predict whether a given variant alters protein coding. 3 VARIANT DETECTION Bambino includes a variant detector, which can identify single nucleotide variants (SNVs), insertions and deletions directly from one or more BAM files. SAM-specific features include the ability to specify a minimum read mapping quality, mate-pair read consistency checks and SAM tag filtering. The latter feature allows the user to leverage even custom SAM tags: for example, if the BAM data were generated using BWA, filters using the X0 or XT tags could ensure variant calling was performed exclusively with uniquely mapped reads. Minimum read quality, depth of coverage and allele frequency are also configurable. Each variant is assigned a Bayesian quality score (Buetow et al., 1999) based on the conversion of associated SAM reads' phred-scaled (Ewing et al., 1998) quality scores into probability-of-error values. This score is most helpful for evaluating calls in low-coverage regions. A variety of low-level options and settings are available for configuration by the user, increasing transparency and making it easier to adapt the detector to different use cases, for example, the analysis of assemblies of long Sanger-based reads aligned with BWA's ‘bwasw’ command. Variant detection may be configured and run from the command line or interactively from within the assembly viewer. Various techniques are used to avoid false positive variation calls, several of which focus on ambiguously mapped or mismapped reads. A given read may be rejected altogether for variant calling if it contains more than a maximum number of mismatches to the reference sequence for one of two sequence quality thresholds (the default settings permit a maximum of three high-quality mismatches and six low-quality mismatches). Mismatches of extremely low quality (q ≤ 2) may be optionally ignored to accommodate Illumina's reserved usage of these values. A mismapped read filter tracks high-quality mismatches in these rejected reads, disqualifying candidate variants elsewhere if their alleles appear too frequently in the mismatch set. This prevents false positive calls based on reads which even partially overlap problematic regions. Another filter discounts reference mismatches near read termini occurring within regions deleted from other reads, considering them possible indel alignment errors. Read mate-pair consistency checks are also performed, excluding overlapping reads from variant calling if their base calls disagree. Table 1. Summary of results: validation of novel variants, and detection of variants confirmed by other groups Dataset        Samples        Variants        Detected        Validation rate (%) Validation of novel SNPs in liver cancer        3        55        50        90.9 TCGA-validated variants found via next-gen sequencing        440        1739        1704        97.9 TCGA-validated variants found via SNP6        7        1 728 968        1 717 830        99.3 Open in a separate window The variant detector can pool data from multiple BAM files, facilitating analysis of tumor/normal pairs, or data from multiple runs or platforms. Each file may be annotated as tumor or normal. Detailed counts of reads supporting each variant are provided, broken down by tumor/normal status, allele and strand. This adds an additional level of granularity beyond that provided by pileup-format files, and can be used to determine whether detected variants are homozygous or heterozygous, germline or somatic. As measures of supporting read diversity, counts of unique clone names observed for each variant are provided, as well as a summary flag indicating whether each variant was observed bidirectionally. An optional read-level report provides extended detail about participating reads and their SAM annotations. For BAM files containing regions of extremely high-read coverage (e.g. liver albumin), an optional limiter may be employed to restrict memory usage during processing. The variant detector uses a streaming model to manage memory usage, which along with the limiter feature makes it capable of analyzing even very large whole-genome datasets. Memory consumption is dependent on the limiter settings used; the program generally runs well using 1 or 2 GB RAM.",Visualization,"bambino  variant detector  alignment viewer  nextgeneration sequence data   sambam format
bambino' assembly viewer display alignments  one   bam file   reference sequence either load   file fasta ucsc 2bit  nib format  support  generate   underlie read nucleotides  display colorshaded base  quality value   style similar   con program gordon    display  various sam alignment tag  support include soft  hard clip  splice alignments pad character  add   reference sequence  alignments  necessary  provide complete visualization  insertions  short tandem repeat  viewer generate  summary histogram  nonreference allele frequencies   tumor  normal sample provide  quick impression  whether potential snp sit  homozygous  heterozygous germline  somatic another panel display  bird' eye view   wider region show normalize depth  coverage  exon position  viewer also display dbsnp entries  ncbi refseq protein translations retrieve   configurable mysql ucsc genome annotation database rhead      predict whether  give variant alter protein cod  variant detection bambino include  variant detector   identify single nucleotide variants snvs insertions  deletions directly  one   bam file samspecific feature include  ability  specify  minimum read map quality matepair read consistency check  sam tag filter  latter feature allow  user  leverage even custom sam tag  example   bam data  generate use bwa filter use     tag could ensure variant call  perform exclusively  uniquely map read minimum read quality depth  coverage  allele frequency  also configurable  variant  assign  bayesian quality score buetow    base   conversion  associate sam reads' phredscaled ewing    quality score  probabilityoferror value  score   helpful  evaluate call  lowcoverage regions  variety  lowlevel options  settings  available  configuration   user increase transparency  make  easier  adapt  detector  different use case  example  analysis  assemblies  long sangerbased read align  bwa' bwasw command variant detection may  configure  run   command line  interactively  within  assembly viewer various techniques  use  avoid false positive variation call several   focus  ambiguously map  mismapped read  give read may  reject altogether  variant call   contain    maximum number  mismatch   reference sequence  one  two sequence quality thresholds  default settings permit  maximum  three highquality mismatch  six lowquality mismatch mismatch  extremely low quality  ≤  may  optionally ignore  accommodate illumina' reserve usage   value  mismapped read filter track highquality mismatch   reject read disqualify candidate variants elsewhere   alleles appear  frequently   mismatch set  prevent false positive call base  read  even partially overlap problematic regions another filter discount reference mismatch near read termini occur within regions delete   read consider  possible indel alignment errors read matepair consistency check  also perform exclude overlap read  variant call   base call disagree table  summary  result validation  novel variants  detection  variants confirm   group dataset        sample        variants        detect        validation rate  validation  novel snps  liver cancer                                 tcgavalidated variants find via nextgen sequence                                 tcgavalidated variants find via snp6                                     open   separate window  variant detector  pool data  multiple bam file facilitate analysis  tumornormal pair  data  multiple run  platforms  file may  annotate  tumor  normal detail count  read support  variant  provide break   tumornormal status allele  strand  add  additional level  granularity beyond  provide  pileupformat file    use  determine whether detect variants  homozygous  heterozygous germline  somatic  measure  support read diversity count  unique clone name observe   variant  provide  well   summary flag indicate whether  variant  observe bidirectionally  optional readlevel report provide extend detail  participate read   sam annotations  bam file contain regions  extremely highread coverage  liver albumin  optional limiter may  employ  restrict memory usage  process  variant detector use  stream model  manage memory usage  along   limiter feature make  capable  analyze even  large wholegenome datasets memory consumption  dependent   limiter settings use  program generally run well use     ram",11
204,consed,"Consed: a graphical editor for next-generation sequencing
Consed’s graphical editor (Gordon, 2003; Gordon et al., 1998), the centerpiece of the consed package, has a rich and flexible set of editing and analysis features whose full functionality requires memory-intensive data structures. We have improved its resource management to reasonably handle up to a few million reads (e.g. a 1.7 million-read dataset requires 2.7 GB RAM and 1.5 min for start-up on a desk station), sufficient for most bacterial genomes. To deal with larger assemblies, we implemented a two-step approach in which a limited-feature viewer, bamScape, that readily handles several billion reads, is used to identify regions of interest and then to launch consed’s graphical editor (with full editing and analysis capability) on read sets extracted from these regions. BamScape takes as input a reference sequence and a BAM (Li et al., 2009) file of reads aligned to it. Resource requirements are modest, e.g. ~300 MB RAM and 10 s to start up and display an 800 KB region from a BAM file of 2 billion human reads, and <5 s to jump to other locations. The Reads vs Reference Window (Supplementary Fig. S1) displays read depth, depth of inconsistent mate pairs (i.e. those with anomalous relative orientation or map location) and read-reference discrepancy rate. Potential ‘problem sites’ (misassemblies or sequence variants) are found using user-defined thresholds for these variables (Supplementary Fig. S2). Problem sites are added to an interactive list (i.e. such that clicking on a list item scrolls the window to that location). At any region of interest, the user can click to bring up consed’s graphical editor (taking e.g. ~12 s for a ~20 KB region of coverage depth ~25×), examine the read data in greater detail and, if desired, edit the reference sequence or the assembly in these regions. (As consed’s graphical editor is restricted to datasets of a few million reads, this method can currently fix misassemblies of regions at most this size.) Edits to various locations in the reference may be made in one or several editing sessions, and consed can create a new version of the reference that reflects all edits. This can then be used with a read alignment program to create a new BAM alignment file. Tracks [as in the UCSC Genome Browser (Kent et al., 2002)] can be shown in the Aligned Reads Window (Supplementary Fig. S3) either as a graph, as genes with indications of untranslated regions, introns and the translated amino acid sequence or as bars with grayscale indicating quantitative data (e.g. conservation scores). A tag is a label attached to a region of a read or reference sequence (Gordon et al., 1998). We have expanded the tag feature to allow comments, user-defined tag types and user-specified tag fields that may contain numbers, text or references to other tags. Consed has flexible tag-search capabilities and generates interactive lists of the tags that are found. Consed detects putative SNPs and indel polymorphisms and calculates genotype qualities using the method of Li et al. (2008), producing an interactive list that allows putative variants to be viewed along with their supporting read data. A VCF format (Danecek et al., 2011) report can also be generated in batch. The consed graphical editor provides several capabilities (alternative to those in bamScape) for misassembly detection and correction. The Assembly View Window (Supplementary Fig. S4) (Nielsen et al., 2010) gives a close-up view of potentially misassembled regions showing the order and orientation of contigs in scaffolds, read depth, clusters of inconsistent mate pairs and sequence similarity. The Highly Discrepant Positions Window (Supplementary Fig. S5) shows an interactive list of locations where multiple reads disagree with the consensus or reference sequence. An interactive list of high- (or low-) depth coverage regions can be generated. The user can select a read to be placed near the top of the Aligned Reads Window (Supplementary Fig. S3) and determine, by visually comparing it to other reads at informative sites, which reads belong with it and which do not; the latter can then be moved to another location. Reads can be removed from the contig by right-clicking on read names, by clicking on inconsistent mate pairs in Assembly View, by highlighting read names (see later in the text) and requesting that highlighted reads be removed or by supplying a list of reads to a batch program. A group of reads can be removed together into a single contig that preserves alignments, removed individually into separate contigs for each read, or deleted entirely from the assembly. Optionally, mates of reads can also be removed. The removed reads can either be added to a different location using the join feature (Gordon et al., 2001) or reassembled by clicking ‘miniassembly’. Contig joins can be made in any of three ways: manually, using sequence similarity as shown in Assembly View or as found with the search-for-string feature, by clicking to display the alignment in the Compare Contigs Window (Supplementary Fig. S6, bottom) and then clicking ‘join’; in semiautomated fashion using an interactive list of potential joins (Supplementary Fig. S6, top) generated in batch by consed’s autoreport program; or in fully automated batch mode accepting all joins recommended by autoreport. False joins can be corrected using the tear function (Gordon et al., 2001), which now allows the user to sort the reads into two new contigs while looking at base discrepancies. Consed can pick PCR or walking primers for closing gaps between contigs (either under manual control or in batch mode). Once new reads have been generated, a button click or a script running in batch adds them to the assembly by running cross_match to find the best gapped alignment of each read against the existing consensus sequence, incorporating the read at the aligned location. Optionally, reads can be targeted to an approximate location. An interactive list of newly added reads is displayed. A batch feature corrects and extends the contig sequences where appropriate. Consensus bases and base qualities (Ewing and Green, 1998) can optionally be recalculated following changes to the assembly. The interactive list ‘questionable consensus bases’ indicates potential errors in the consensus sequence (identified using a choice of three different algorithms involving read frequency, quality and strand) for user review and editing. The consensus can be trimmed. Reads can be grouped by highlighting their names and then operated on as a group. Highlighting can be done in various ways (e.g. by clicking on read names or by specifying a sequence at a particular location). All reads at a particular reference position can be edited at once to have the same base. All read bases to the left (or right) of a particular position can be changed to X’s (indicating vector). Additional new features are described in Supplemental information.",Visualization,"con  graphical editor  nextgeneration sequencing
conseds graphical editor gordon  gordon     centerpiece   con package   rich  flexible set  edit  analysis feature whose full functionality require memoryintensive data structure   improve  resource management  reasonably handle     million read    millionread dataset require   ram   min  startup   desk station sufficient   bacterial genomes  deal  larger assemblies  implement  twostep approach    limitedfeature viewer bamscape  readily handle several billion read  use  identify regions  interest    launch con graphical editor  full edit  analysis capability  read set extract   regions bamscape take  input  reference sequence   bam     file  read align   resource requirements  modest  ~  ram     start   display    region   bam file   billion human read     jump   locations  read  reference window supplementary fig  display read depth depth  inconsistent mate pair    anomalous relative orientation  map location  readreference discrepancy rate potential problem sit misassemblies  sequence variants  find use userdefined thresholds   variables supplementary fig  problem sit  add   interactive list    click   list item scroll  window   location   region  interest  user  click  bring  con graphical editor take  ~    ~  region  coverage depth ~ examine  read data  greater detail   desire edit  reference sequence   assembly   regions  con graphical editor  restrict  datasets    million read  method  currently fix misassemblies  regions    size edit  various locations   reference may  make  one  several edit sessions  con  create  new version   reference  reflect  edit     use   read alignment program  create  new bam alignment file track    ucsc genome browser kent      show   align read window supplementary fig  either   graph  genes  indications  untranslated regions introns   translate amino acid sequence   bar  grayscale indicate quantitative data  conservation score  tag   label attach   region   read  reference sequence gordon      expand  tag feature  allow comment userdefined tag type  userspecified tag field  may contain number text  reference   tag con  flexible tagsearch capabilities  generate interactive list   tag   find con detect putative snps  indel polymorphisms  calculate genotype qualities use  method      produce  interactive list  allow putative variants   view along   support read data  vcf format danecek    report  also  generate  batch  con graphical editor provide several capabilities alternative    bamscape  misassembly detection  correction  assembly view window supplementary fig  nielsen    give  closeup view  potentially misassembled regions show  order  orientation  contigs  scaffold read depth cluster  inconsistent mate pair  sequence similarity  highly discrepant position window supplementary fig  show  interactive list  locations  multiple read disagree   consensus  reference sequence  interactive list  high  low depth coverage regions   generate  user  select  read   place near  top   align read window supplementary fig   determine  visually compare    read  informative sit  read belong        latter    move  another location read   remove   contig  rightclicking  read name  click  inconsistent mate pair  assembly view  highlight read name see later   text  request  highlight read  remove   supply  list  read   batch program  group  read   remove together   single contig  preserve alignments remove individually  separate contigs   read  delete entirely   assembly optionally mat  read  also  remove  remove read  either  add   different location use  join feature gordon     reassemble  click miniassembly contig join   make    three ways manually use sequence similarity  show  assembly view   find   searchforstring feature  click  display  alignment   compare contigs window supplementary fig  bottom   click join  semiautomated fashion use  interactive list  potential join supplementary fig  top generate  batch  con autoreport program   fully automate batch mode accept  join recommend  autoreport false join   correct use  tear function gordon      allow  user  sort  read  two new contigs  look  base discrepancies con  pick pcr  walk primers  close gap  contigs either  manual control   batch mode  new read   generate  button click   script run  batch add    assembly  run cross_match  find  best gap alignment   read   exist consensus sequence incorporate  read   align location optionally read   target   approximate location  interactive list  newly add read  display  batch feature correct  extend  contig sequence  appropriate consensus base  base qualities ewing  green   optionally  recalculate follow change   assembly  interactive list questionable consensus base indicate potential errors   consensus sequence identify use  choice  three different algorithms involve read frequency quality  strand  user review  edit  consensus   trim read   group  highlight  name   operate    group highlight     various ways   click  read name   specify  sequence   particular location  read   particular reference position   edit       base  read base   leave  right   particular position   change   indicate vector additional new feature  describe  supplemental information",11
205,Tablet,"Tablet—next generation sequence assembly visualization
Tablet is written in Java and is compatible with any Java-enabled system with a runtime level of ≥1.6. We provide installable versions that include everything required to run the application, including a suitable Java runtime. The installers are available for Windows, Mac OS X, Linux and Solaris, in both 32- and 64-bit versions. Once installed and running, Tablet will also monitor our server for new versions and will prompt, download and update quickly and easily whenever a new release is available, along with redirecting the user to a web page describing the new features that have been added. A prime requisite during development of Tablet has been computing efficiency and speed. The two main approaches to handling assembly data in viewers are either memory-based, where all the data are loaded into memory, or disk-cached, where the data reside on disk with only the currently visible segment of the dataset held in memory. Memory-based applications are faster for viewing and navigation (after an initial delay while loading the data) and can provide whole dataset overviews and statistical summaries, but the size of dataset they can handle is limited by the amount of available memory. In contrast, cache-based applications can display views from much larger datasets using a minimum of memory, but access to the data can be orders of magnitude slower (which then affects navigation and rendering), and the feature sets available are often limited. With Tablet, we have chosen a hybrid solution that provides us with advantages from both approaches. We hold a ‘skeleton’ layout of the reads in memory, with data on each read limited to just an internal ID, its position against the consensus or reference sequence and its length. The nucleotide data itself (efficiently compressed so it can be read as quickly as possible), along with other supplementary information—such as the read's name and its orientation—is held in an indexed disk-cache and is only accessed (via the read's ID) when required. Tablet also allocates memory on a per-contig basis, including information for features such as how to pack the data for display, coverage calculations, padded-to-unpadded mappings, etc. These data are calculated and stored before each contig is rendered and discarded again after display. This approach allows us to provide maximum functionality—instant access to any portion of the data; extremely fast and high-quality rendering; entire dataset overviews—yet memory usage is kept relatively low. Comparing data indexing/loading times and memory consumption across a range of tools for an assembly file containing ∼2.9 million Illumina Solexa reads of length 51, we found that the cache-based viewers (Maqview, MapView, tview) were fairly constant in memory usage (between 35 MB and 70 MB while viewing), with indexing times varying from 10 s to 50 s, although memory consumption during indexing did peak as high as 350 MB with MapView. For the memory-based viewers, we compared Hawkeye (5500 MB; 107 s), Consed (2600 MB; 73 s) and EagleView (2450 MB; 98 s). Tablet, being a hybrid, loads the data in 25 s, and uses just 175 MB of memory.",Visualization,"tablet—next generation sequence assembly visualization
tablet  write  java   compatible   javaenabled system   runtime level  ≥  provide installable versions  include everything require  run  application include  suitable java runtime  installers  available  windows mac   linux  solaris     bite versions  instal  run tablet  also monitor  server  new versions   prompt download  update quickly  easily whenever  new release  available along  redirect  user   web page describe  new feature    add  prime requisite  development  tablet   compute efficiency  speed  two main approach  handle assembly data  viewers  either memorybased    data  load  memory  diskcached   data reside  disk    currently visible segment   dataset hold  memory memorybased applications  faster  view  navigation   initial delay  load  data   provide whole dataset overviews  statistical summaries   size  dataset   handle  limit   amount  available memory  contrast cachebased applications  display view  much larger datasets use  minimum  memory  access   data   order  magnitude slower   affect navigation  render   feature set available  often limit  tablet   choose  hybrid solution  provide   advantage   approach  hold  skeleton layout   read  memory  data   read limit    internal   position   consensus  reference sequence   length  nucleotide data  efficiently compress     read  quickly  possible along   supplementary information—   read' name   orientation— hold   index diskcache    access via  read'   require tablet also allocate memory   percontig basis include information  feature     pack  data  display coverage calculations paddedtounpadded mappings etc  data  calculate  store   contig  render  discard   display  approach allow   provide maximum functionality—instant access   portion   data extremely fast  highquality render entire dataset overviews—yet memory usage  keep relatively low compare data indexingloading time  memory consumption across  range  tool   assembly file contain  million illumina solexa read  length   find   cachebased viewers maqview mapview tview  fairly constant  memory usage        view  index time vary       although memory consumption  index  peak  high     mapview   memorybased viewers  compare hawkeye     con      eagleview     tablet   hybrid load  data     use     memory",11
206,IGV,"Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration
IGV is a desktop application written in the Java programming language and runs on all major platforms (Windows, Mac and Linux). Below, we describe in more detail some components of the IGV implementation, including our data-tiling approach for supporting large data sets and IGVs support for different categories of file formats. We also provide a high-level overview of IGVs software architecture. Data tiling A primary design goal of IGV is to support interactive exploration of large-scale genomic data sets on standard desktop computers. This poses a difficult challenge as NGS and recent array-based technologies can generate data sets from gigabytes to terabytes in size. Simply loading these entire data sets into memory is not a viable option. In addition, researchers search for meaningful events at many different genomic resolution scales, from whole genome to individual base pairs. The problem is analogous to that faced by interactive geographical mapping tools, which provide views of very large geographical databases over many resolution scales. Tools such as Google Maps solve this problem by precomputing images representing sections of maps at multiple resolution scales, and providing fast access to the images as needed to construct a view. We considered such an approach for IGV, based on precomputed images of genomic data. However, millions of images would be required to support all resolution scales for a large genome, thus making image management difficult without introducing the requirement of a database. Furthermore, the representation of the data would be fixed when the images are computed, making it difficult to provide interactive graphing options. Consequently, we adopted a different approach that is based on precomputing summarizations of data at multiple resolution scales, with rendering of the data deferred to runtime. We refer to this as ‘data tiling’, to distinguish it from ‘image tiling’. IGVs data tiling implementation is built on a pyramidal data structure that can be described as follows. For each resolution scale (‘zoom level’), the genome is divided into tiles that correspond to the region viewable on the screen of a typical user display. The first zoom level consists of a single tile that covers the entire genome. The next zoom level contains a single tile for each chromosome. The number of tiles then increases by a factor of 2 for each level, so the next zoom level consists of two tiles per chromosome, then four, etc. Each tile is subdivided into ‘bins’, with the width of a bin chosen to correspond to the approximate genomic width represented by a screen pixel at that resolution scale. The value of each bin is calculated from the underlying genomic data with a summary statistic, such as ‘mean’, ‘median’ or ‘maximum’. By organizing data in this way, tile sizes for each zoom level are constant and small, containing only the data needed to render the view at the resolution supported by the screen display. Hence, a single tile at the lowest resolution, which spans the entire genome, has the same memory footprint as a tile at a high-resolution zoom level, which might span only a few kilobases. As the user moves across the genome and through zoom levels, IGV only retrieves the tiles required to support the current view and discards tiles no longer in view to free memory. This method supports browsing very large data sets at all resolution scales with minimal memory requirements. For large genomes, precomputing tiles for all zoom levels would be inordinately expensive with respect to disk space. For example, the human genome requires approximately 23 zoom levels, or on the order of 223 tiles, to cover the whole genome to base pair resolution. In practice, IGV uses a hybrid approach; combining precomputed lower-level zoom levels with high-resolution tiles computed on the fly. This is possible as the high-resolution tiles cover relatively small portions of the genome. The number of precomputed zoom levels required to achieve good performance varies by data density and genome size. In our experience, seven levels give acceptable performance for even the highest density human genome data. File formats To support the multiresolution data model described earlier, we developed a corresponding file format. The ‘tiled data format’, or TDF, stores the pyramidal data tile structure and provides fast access to individual tiles. TDF files can be created using the auxiliary package ‘igvtools’. We note however that IGV does not require conversion to TDF before data can be loaded. In fact, IGV supports a variety of genomic file formats, which can be divided into three categories: (i) nonindexed, (ii) indexed and (iii) multiresolution formats: Nonindexed formats include flat file formats such as GFF [11], BED [12] and WIG [13]. Files in these formats must be read in their entirety and are only suitable for relatively small data sets. Indexed formats include BAM and Goby [14] for sequence alignments. Additionally, many tab-delimited feature formats can be converted to an indexed file using Tabix [15] or ‘igvtools’. Indexed formats provide rapid and efficient access to subsets of the data for display, but only when zoomed in to a sufficiently small genomic region. Zooming out requires ever-larger portions of the file to be loaded. Thus, indexed formats can efficiently support views only for a limited range of resolution scales. This range depends on the genomic density of the underlying data and can span tens of kilobases for NGS alignments, hundreds of megabases for typical variant (SNP) files, or whole chromosomes for sparse feature files. IGV uses heuristics to determine a suitable upper limit on the genomic range that can be loaded quickly with a reasonable memory footprint. If zoomed out beyond this limit, the data are not loaded. Multiresolution formats, such as our TDF described earlier and the bigWig and bigBed formats [16], include both an index for the raw data, and precomputed indexed summary data for lower resolution (zoomed out) scales. Multiresolution formats can efficiently support views at any resolution scale. Software architecture The IGV software structure is designed around a core set of interfaces and extendable classes. These components can be separated into three conceptual layers as illustrated in Figure 1: (i) a top-level application layer, (ii) a data layer and (iii) a stream layer. These are described in more detail below: The application layer includes the main IGV window and user interface elements, along with controllers for user interaction events. It also contains representations of genomic features and data. IGV displays these in horizontal rows known as ‘tracks‘. Tracks are displayed in a data panel, which is implemented as a class derived from Java Swing components. The data panel is responsible for the coordination of track layout and rendering, and managing mouse events. It handles certain globally shared mouse actions, such as zooming and panning, and delegates other events to the objects representing tracks. Tracks are responsible for handling these events, as well as requesting features as needed from the data layer and drawing these features on the panel. Most track implementations delegate the drawing task to a renderer object. Renderers are designed to be pluggable, and can be swapped at runtime, for example to switch graph types in response to a menu action. The data layer reads and parses the different genomic file formats and supplies the application layer with data tiles on demand. It also implements caching of tiles, for improved efficiency if a previously visited genomic region is requested again. The stream layer is responsible for supporting random access to sections of files accessed by any of the protocols supported by IGV, i.e. local file, HTTP, HTTPS or FTP. Random file access is necessary to take advantage of the indexed and multiresolution file formats. For local files, this is straightforward using Java’s RandomAccessFile class, or alternatively positionable file channels. Remote files presented a challenge, as there are no Java built-in functions or libraries that support this access pattern. Initially, we solved this problem using a web service. However, this approach was not ideal, as users who wished to host IGV files were also required to install and run the web service on their systems. Consequently, we designed and implemented a set of classes to provide a uniform interface for random file access for all the protocols. IGVs implementation for the HTTP protocol uses byte range requests from the standard HTTP specification. For the FTP protocol, IGV uses the mechanism for restarting downloads that is supported by most FTP servers via the ‘REST’ command. An external file that holds a picture, illustration, etc. Object name is bbs017f1.jpg Open in a separate window Figure 1: IGV class diagram, illustrating the IGV software structure. FEATURES IGV is a desktop application for the visualization and interactive exploration of genomic data in the context of a reference genome. A key characteristic of IGV is its focus on the integrative nature of genomic studies. It allows investigators to flexibly visualize many different types of data together—and importantly also integrate these data with the display of sample attribute information such as clinical and phenotypic information. To support interactive exploration of data, IGV provides direct manipulation navigation in the style of Google Maps. For instance, you click and drag to pan the view across the genome and double-click on a region to zoom in for a more detailed view. It supports real-time interaction at all scales of genome resolution, from whole genome to base pairs, even for very large data sets. The Broad IGV data server hosts many genome annotation files and data sets from a variety of public sources (including from TCGA, 1000 Genomes Project, ENCODE Project [17] and others). However, the primary emphasis is on supporting biomedical researchers who wish to load, visualize and explore their own data sets aligned to the selected reference genome. Researchers can also make their data sets available to others for view in IGV, sharing them with colleagues or the community at large.",Visualization,"integrative genomics viewer igv highperformance genomics data visualization  exploration
igv   desktop application write   java program language  run   major platforms windows mac  linux   describe   detail  components   igv implementation include  datatiling approach  support large data set  igvs support  different categories  file format  also provide  highlevel overview  igvs software architecture data tile  primary design goal  igv   support interactive exploration  largescale genomic data set  standard desktop computers  pose  difficult challenge  ngs  recent arraybased technologies  generate data set  gigabytes  terabytes  size simply load  entire data set  memory    viable option  addition researchers search  meaningful events  many different genomic resolution scale  whole genome  individual base pair  problem  analogous   face  interactive geographical map tool  provide view   large geographical databases  many resolution scale tool   google map solve  problem  precomputing image represent section  map  multiple resolution scale  provide fast access   image  need  construct  view  consider   approach  igv base  precomputed image  genomic data however millions  image would  require  support  resolution scale   large genome thus make image management difficult without introduce  requirement   database furthermore  representation   data would  fix   image  compute make  difficult  provide interactive graph options consequently  adopt  different approach   base  precomputing summarizations  data  multiple resolution scale  render   data defer  runtime  refer    data tile  distinguish   image tile igvs data tile implementation  build   pyramidal data structure    describe  follow   resolution scale zoom level  genome  divide  tile  correspond   region viewable   screen   typical user display  first zoom level consist   single tile  cover  entire genome  next zoom level contain  single tile   chromosome  number  tile  increase   factor     level   next zoom level consist  two tile per chromosome  four etc  tile  subdivide  bin   width   bin choose  correspond   approximate genomic width represent   screen pixel   resolution scale  value   bin  calculate   underlie genomic data   summary statistic   mean median  maximum  organize data   way tile size   zoom level  constant  small contain   data need  render  view   resolution support   screen display hence  single tile   lowest resolution  span  entire genome    memory footprint   tile   highresolution zoom level  might span    kilobases   user move across  genome   zoom level igv  retrieve  tile require  support  current view  discard tile  longer  view  free memory  method support browse  large data set   resolution scale  minimal memory requirements  large genomes precomputing tile   zoom level would  inordinately expensive  respect  disk space  example  human genome require approximately  zoom level    order   tile  cover  whole genome  base pair resolution  practice igv use  hybrid approach combine precomputed lowerlevel zoom level  highresolution tile compute   fly   possible   highresolution tile cover relatively small portion   genome  number  precomputed zoom level require  achieve good performance vary  data density  genome size   experience seven level give acceptable performance  even  highest density human genome data file format  support  multiresolution data model describe earlier  develop  correspond file format  tile data format  tdf store  pyramidal data tile structure  provide fast access  individual tile tdf file   create use  auxiliary package igvtools  note however  igv   require conversion  tdf  data   load  fact igv support  variety  genomic file format    divide  three categories  nonindexed  index  iii multiresolution format nonindexed format include flat file format   gff  bed   wig  file   format must  read   entirety    suitable  relatively small data set index format include bam  goby   sequence alignments additionally many tabdelimited feature format   convert   index file use tabix   igvtools index format provide rapid  efficient access  subsets   data  display    zoom    sufficiently small genomic region zoom  require everlarger portion   file   load thus index format  efficiently support view    limit range  resolution scale  range depend   genomic density   underlie data   span tens  kilobases  ngs alignments hundreds  megabases  typical variant snp file  whole chromosomes  sparse feature file igv use heuristics  determine  suitable upper limit   genomic range    load quickly   reasonable memory footprint  zoom  beyond  limit  data   load multiresolution format    tdf describe earlier   bigwig  bigbed format  include   index   raw data  precomputed index summary data  lower resolution zoom  scale multiresolution format  efficiently support view   resolution scale software architecture  igv software structure  design around  core set  interfaces  extendable class  components   separate  three conceptual layer  illustrate  figure    toplevel application layer   data layer  iii  stream layer   describe   detail   application layer include  main igv window  user interface elements along  controllers  user interaction events  also contain representations  genomic feature  data igv display   horizontal row know  track track  display   data panel   implement   class derive  java swing components  data panel  responsible   coordination  track layout  render  manage mouse events  handle certain globally share mouse action   zoom  pan  delegate  events   object represent track track  responsible  handle  events  well  request feature  need   data layer  draw  feature   panel  track implementations delegate  draw task   renderer object renderers  design   pluggable    swap  runtime  example  switch graph type  response   menu action  data layer read  parse  different genomic file format  supply  application layer  data tile  demand  also implement cache  tile  improve efficiency   previously visit genomic region  request   stream layer  responsible  support random access  section  file access     protocols support  igv  local file http https  ftp random file access  necessary  take advantage   index  multiresolution file format  local file   straightforward use javas randomaccessfile class  alternatively positionable file channel remote file present  challenge     java builtin function  libraries  support  access pattern initially  solve  problem use  web service however  approach   ideal  users  wish  host igv file  also require  install  run  web service   systems consequently  design  implement  set  class  provide  uniform interface  random file access    protocols igvs implementation   http protocol use byte range request   standard http specification   ftp protocol igv use  mechanism  restart download   support   ftp servers via  rest command  external file  hold  picture illustration etc object name  bbs017f1jpg open   separate window figure  igv class diagram illustrate  igv software structure feature igv   desktop application   visualization  interactive exploration  genomic data   context   reference genome  key characteristic  igv   focus   integrative nature  genomic study  allow investigators  flexibly visualize many different type  data together— importantly also integrate  data   display  sample attribute information   clinical  phenotypic information  support interactive exploration  data igv provide direct manipulation navigation   style  google map  instance  click  drag  pan  view across  genome  doubleclick   region  zoom     detail view  support realtime interaction   scale  genome resolution  whole genome  base pair even   large data set  broad igv data server host many genome annotation file  data set   variety  public source include  tcga  genomes project encode project   others however  primary emphasis   support biomedical researchers  wish  load visualize  explore   data set align   select reference genome researchers  also make  data set available  others  view  igv share   colleagues   community  large",11
207,MagicViewer,"MagicViewer: integrated solution for next-generation sequencing data visualization and genetic variation detection and annotation
Different tools often use their own defined format for the data input (e.g. XML format was used as input for NGSView and MVF format for MapView, etc.), which shows the weakness of the compatibility and leads to laborious efforts to convert various formats. This contradiction is especially prominent when processing huge mass of data obtained from high-throughput sequencing. Most recently, a generic alignment (SAM) format has been developed for storing aligned short reads in a flexible style with compacted size (12). Hence, to be compatible with such a powerful format, MagicViewer employed the SAM format to enable an easy conversion of various input file formats, including PSL, MAQ, Bowtie, SOAP and ZOOM. When starting with a new project, MagicViewer requires a reference genome sequence in fasta format, a sorted bam file containing the aligned short reads obtained from SAMtools (12) and an optional reference genome annotation file in GFF format. MagicViewer can save intermediate results as a log file, thereby facilitating an easier manipulation of project for later reuse. Taking existing archive into account, MagicViewer introduces a conspicuously new feature of workspace where users can load their most frequently used resources for quick access. Through such a convenient way, users can easily load, browse, further update and modify their previous results, instead of reconstructing a new project. Go to: ALIGNMENT VISUALIZATION MagicViewer, written in the Java programming language, provides a user-friendly interface and can be performed in a standalone, operating system-independent manner (Figure 1). Large-scale short reads mapped onto the reference genome are optimally placed in multiple lines with compact arrangement and can be visualized intuitively. To get a better graph view, users can acquire scrollable thumbnail image through zooming in and out. Theoretically, the short reads image can be zoomed to any resolution, from whole chromosome to individual bases at any desired level. When the mouse hovers on a specific read, auxiliary information will be shown in a tooltip, such as reads ID, location, base quality, read length and orientation. The sequencing depth distribution of mapped reads can be visualized on the top of graphical representation of short reads alignments. In addition, MagicViewer provides extensive flexibility to change the appearance of the displayed short read alignment and sequencing depth. Users can change font and colors in many different combinations, such as nucleotide and background color. Such a color or format setting function is not trivial, because users usually need a better display when exploring SNPs from hundreds of fold coverage of short read alignments. An external file that holds a picture, illustration, etc. Object name is gkq302f1.jpg Figure 1. The workflow and screenshots of MagicViewer. Go to: GENETIC VARIATION DETECTION, FILTRATION AND ANNOTATION Next-generation sequencing technologies have been widely used for effective, easy and in-depth investigation of genetic variation, including SNPs and InDels (insertion/deletions), to a better understanding of human health (13). To satisfy these requirements, beyond a sophisticated short read visualization tool, MagicViewer is devoted to serve as a comprehensive workflow for genetic variation detection, filtration and annotation (Figure 1). In order to efficiently identify genetic variation between large-scale short reads and reference genomes, the Genome Analysis Toolkit (GATK, http://www.broadinstitute.org/gsa/wiki/index.php/The_Genome_Analysis_Toolkit) is incorporated. The GATK is a structured software library designed to enable rapid development of efficient and robust analysis tools for next-generation sequencing data. The MagicViewer user interface allows users to change many of the parameters, such as heterozygosity, confidence threshold and max coverage. The output of genetic variation calling is organized in a variable call format (VCF, http://www.broadinstitute.org/gsa/wiki/index.php/VCF_Validator), which is the standard variant calling file format used by the 1000 Genomes Project. Meanwhile, the identified genetic variations will be displayed at the top of the main window of MagicViewer for easily interpreting the results. For candidate SNPs, MagicViewer provides a number of versatile display and filtering options for users to remove low confidence SNPs. Such options include thresholds for coverage, quality, variant frequency and number of reads. Another predominate feature of MagicViewer is that it can be used to link the detected genetic variations to the annotation information of the reference genome. Users need to provide such information in a general feature format (GFF format, http://www.sanger.ac.uk/Software/formats/GFF) before launching a new project. MagicViewer also provides a variety of genetic variation analysis functions, including SNP category selection, result organization and visualization. Users can select a subset of different SNPs categories through custom setting to achieve an extensive annotation, including intergenic, intron, missense, nonsense, readthrough, splice site, synonymous, 3′- and 5′-UTR. The graphical display of output can be customized using supplied options to define view mode, arrow mode, read height, color, vertical space, track display height and background color. In many genetic variation projects, Sanger sequencing is usually necessary for the verification of the detected genetic variations. Therefore, MagicViewer has also provided a facility to help users design primers for specific genomic region flanking the SNP site in a batch mode by the implement of Primer3 (14). To fulfill this function, MagicViewer allows users to adjust a number of important parameters, including primer length, Tm, GC content, product Tm and the number of primers. Go to: CASE STUDIES To determine the effectiveness of MagicViewer, we simulated 50 million (75-bp Illumina paired-end reads) with ∼25-fold coverage and 0.001% divergence from human chromosome 8 using the MAQ program (15). As a result, MagicViewer identified a total of 138 604 SNPs with an accuracy of 97.23% in comparison with the original simulated SNPs. By further observation of the undetected SNPs, we found that the majority of them (98.65%) located in repeats or low coverage regions. In real data sets, MagicViewer was applied to five pooled human exon samples, which were obtained using the NimbleGen 2.1M human exome array and Illumina Genome Analyzer IIx instrument (data not shown). Originally, a total of approximate 56.41 million single-end 75-bp reads were obtained with a size of 3462 MB, among which over 46.12 million reads (35% reads were on targeted exon regions) could be mapped onto the reference genome using the SOAP program with the default setting (data not shown). Among the five pooled samples, MagicViewer identified a number of 28 328 SNPs in targeted exon regions based on default settings, among which homozygous mutations account for 5.53% of the total SNPs and the remaining parts are heterozygous alleles. Functional annotation of these SNPs indicated that synonymous mutations accounted for approximate 18.7%, and the other kinds of mutations were as follows: non-synoymous (31.5%), nonsense (45.2%) and readthrough (4.3%). To experimentally evaluate the robustness of MagicViewer, 80 SNPs were randomly selected for validation using Sequenom’s MassARRAY system (data not shown), and 77 of them were confirmed, revealing the accuracy of MagicViewer in identifying genetic variants.",Visualization,"magicviewer integrate solution  nextgeneration sequence data visualization  genetic variation detection  annotation
different tool often use   define format   data input  xml format  use  input  ngsview  mvf format  mapview etc  show  weakness   compatibility  lead  laborious efforts  convert various format  contradiction  especially prominent  process huge mass  data obtain  highthroughput sequence  recently  generic alignment sam format   develop  store align short read   flexible style  compact size  hence   compatible    powerful format magicviewer employ  sam format  enable  easy conversion  various input file format include psl maq bowtie soap  zoom  start   new project magicviewer require  reference genome sequence  fasta format  sort bam file contain  align short read obtain  samtools    optional reference genome annotation file  gff format magicviewer  save intermediate result   log file thereby facilitate  easier manipulation  project  later reuse take exist archive  account magicviewer introduce  conspicuously new feature  workspace  users  load   frequently use resources  quick access    convenient way users  easily load browse  update  modify  previous result instead  reconstruct  new project   alignment visualization magicviewer write   java program language provide  userfriendly interface    perform   standalone operate systemindependent manner figure  largescale short read map onto  reference genome  optimally place  multiple line  compact arrangement    visualize intuitively  get  better graph view users  acquire scrollable thumbnail image  zoom    theoretically  short read image   zoom   resolution  whole chromosome  individual base   desire level   mouse hover   specific read auxiliary information   show   tooltip   read  location base quality read length  orientation  sequence depth distribution  map read   visualize   top  graphical representation  short read alignments  addition magicviewer provide extensive flexibility  change  appearance   display short read alignment  sequence depth users  change font  color  many different combinations   nucleotide  background color   color  format set function   trivial  users usually need  better display  explore snps  hundreds  fold coverage  short read alignments  external file  hold  picture illustration etc object name  gkq302f1jpg figure   workflow  screenshots  magicviewer   genetic variation detection filtration  annotation nextgeneration sequence technologies   widely use  effective easy  indepth investigation  genetic variation include snps  indels insertiondeletions   better understand  human health   satisfy  requirements beyond  sophisticate short read visualization tool magicviewer  devote  serve   comprehensive workflow  genetic variation detection filtration  annotation figure   order  efficiently identify genetic variation  largescale short read  reference genomes  genome analysis toolkit gatk   incorporate  gatk   structure software library design  enable rapid development  efficient  robust analysis tool  nextgeneration sequence data  magicviewer user interface allow users  change many   parameters   heterozygosity confidence threshold  max coverage  output  genetic variation call  organize   variable call format vcf     standard variant call file format use    genomes project meanwhile  identify genetic variations   display   top   main window  magicviewer  easily interpret  result  candidate snps magicviewer provide  number  versatile display  filter options  users  remove low confidence snps  options include thresholds  coverage quality variant frequency  number  read another predominate feature  magicviewer      use  link  detect genetic variations   annotation information   reference genome users need  provide  information   general feature format gff format   launch  new project magicviewer also provide  variety  genetic variation analysis function include snp category selection result organization  visualization users  select  subset  different snps categories  custom set  achieve  extensive annotation include intergenic intron missense nonsense readthrough splice site synonymous ′  ′utr  graphical display  output   customize use supply options  define view mode arrow mode read height color vertical space track display height  background color  many genetic variation project sanger sequence  usually necessary   verification   detect genetic variations therefore magicviewer  also provide  facility  help users design primers  specific genomic region flank  snp site   batch mode   implement  primer3   fulfill  function magicviewer allow users  adjust  number  important parameters include primer length   content product    number  primers   case study  determine  effectiveness  magicviewer  simulate  million  illumina pairedend read  fold coverage   divergence  human chromosome  use  maq program    result magicviewer identify  total    snps   accuracy    comparison   original simulate snps   observation   undetected snps  find   majority    locate  repeat  low coverage regions  real data set magicviewer  apply  five pool human exon sample   obtain use  nimblegen  human exome array  illumina genome analyzer iix instrument data  show originally  total  approximate  million singleend  read  obtain   size    among    million read  read   target exon regions could  map onto  reference genome use  soap program   default set data  show among  five pool sample magicviewer identify  number    snps  target exon regions base  default settings among  homozygous mutations account     total snps   remain part  heterozygous alleles functional annotation   snps indicate  synonymous mutations account  approximate     kinds  mutations   follow nonsynoymous  nonsense   readthrough   experimentally evaluate  robustness  magicviewer  snps  randomly select  validation use sequenoms massarray system data  show      confirm reveal  accuracy  magicviewer  identify genetic variants",11
208,EagleView,"EagleView: a genome assembly viewer for next-generation sequencing technologies
Efficiency test We tested the efficiency of three tools, consed (ver. 16.0), Hawkeye (ver. 2.0.4), and EagleView (ver. 1.6), on a 64-bit Linux server with 24-GB memory. The 64-bit version of each tool was used for this test. The genome assembly file used for this test is a reference-based genome assembly of E. coli K-12 genome by Illumina sequencing technology from our collaborators at the Washington University Genome Sequencing Center (WUGSC). The assembly contains a reference genome of length 4,661,217 bases and 6,872,388 Illumina 32-base reads. This data set was selected because the assembly could be loaded on our 24-GB memory Linux server by all three programs. In the test, both consed and EagleView loaded the assembly file in the ACE format, while Hawkeye loaded the assembly file in its native bank format converted from the ACE assembly file. The CPU time and memory usage for each tool were measured after it loaded and displayed Contig view. Two larger testing assemblies were subsets of the whole-genome resequencing study of C. elegans of which the primary sequencing data were also from WUGSC (Hillier et al. 2008). The two larger assemblies contain 14,562,818, and 19,566,095 Illumina 32-base reads, respectively. All assembly files are available at the EagleView Web site. Data file formats EagleView reads a genome assembly file in the standard ACE format, a tag-based format commonly used by genome assembly programs (a detailed description of the ACE format is available at http://bozeman.mbt.washington.edu/consed/consed.html). EagleView uses three optional, auxiliary data files: READS, EGL, and MAP files (see Table 3). The READS and EGL files are paired together for storing base qualities and technology-specific trace signals of sequence reads. The READS file contains all read data while the EGL file is just the indexes of the contig start locations in the corresponding READS file. EagleView automatically loads base quality and trace information, if both the READS and the EGL files are present in the same directory as the ACE assembly file. The MAP file is for storing location mapping information of genome features, such as genes, exons, or SNPs. If present, the MAP file is also loaded automatically. All three optional files are in tab-delimited text formats (detailed format descriptions are provided in the EagleView documentation). Table 3. EagleView data files An external file that holds a picture, illustration, etc. Object name is 1538tbl3.jpg Files are identified by the file extension. Utility tools EagleView comes with three data conversion tools to prepare the optional data files. EagleIndexFasta converts FASTA files containing base quality and read trace information to the corresponding READS and EGL files. EagleIndexSff and EagleIndexSffM, both specific to 454 reads, extract base quality and flow signal information from the 454 binary SFF files and convert into the READ and EGL formats. EagleIndexSff converts from a single SFF file; EagleIndexSffM can convert from multiple SFF files. Detailed usage is described at EagleView’s documentation. Go to:",Visualization,"eagleview  genome assembly viewer  nextgeneration sequence technologies
efficiency test  test  efficiency  three tool con ver  hawkeye ver   eagleview ver    bite linux server   memory  bite version   tool  use   test  genome assembly file use   test   referencebased genome assembly   coli  genome  illumina sequence technology   collaborators   washington university genome sequence center wugsc  assembly contain  reference genome  length  base   illumina base read  data set  select   assembly could  load    memory linux server   three program   test  con  eagleview load  assembly file   ace format  hawkeye load  assembly file   native bank format convert   ace assembly file  cpu time  memory usage   tool  measure   load  display contig view two larger test assemblies  subsets   wholegenome resequencing study   elegans    primary sequence data  also  wugsc hillier     two larger assemblies contain    illumina base read respectively  assembly file  available   eagleview web site data file format eagleview read  genome assembly file   standard ace format  tagbased format commonly use  genome assembly program  detail description   ace format  available   eagleview use three optional auxiliary data file read egl  map file see table   read  egl file  pair together  store base qualities  technologyspecific trace signal  sequence read  read file contain  read data   egl file    index   contig start locations   correspond read file eagleview automatically load base quality  trace information    read   egl file  present    directory   ace assembly file  map file   store location map information  genome feature   genes exons  snps  present  map file  also load automatically  three optional file   tabdelimited text format detail format descriptions  provide   eagleview documentation table  eagleview data file  external file  hold  picture illustration etc object name  1538tbl3jpg file  identify   file extension utility tool eagleview come  three data conversion tool  prepare  optional data file eagleindexfasta convert fasta file contain base quality  read trace information   correspond read  egl file eagleindexsff  eagleindexsffm  specific   read extract base quality  flow signal information    binary sff file  convert   read  egl format eagleindexsff convert   single sff file eagleindexsffm  convert  multiple sff file detail usage  describe  eagleviews documentation  ",11
209,Icarus,"Icarus: visualizer for de novo assembly evaluation
Icarus pipeline consists of the following steps (see Supplementary Fig. 1): Running QUAST: aligning contigs to the reference (if available), gene finding, detecting assembly errors; Post-processing: detecting similarities between the assemblies; Creating JavaScript-based web pages. Icarus outputs two types of interactive HTML files: contig alignment viewer and contig size viewer. Each viewer contains at least two panes: an assembly overview with all contigs shown at full scale, and a pane where users can zoom into any region of interest. Both panes display all assemblies in several tracks next to each other. The viewers support standard genome browser functionality: navigating, dragging, zooming, clicking on elements for detailed information such as contig name, type and length. Specific features of each viewer are described below. Examples of Icarus output for assemblies of B.impatiens, S.aureus and CAMI (http://cami-challenge.org) metagenomic dataset are demonstrated in the Supplementary Material and on our website. 2.1 Contig alignment viewer This type of viewer is available only if a reference genome is provided. If the genome consists of large chromosomes ( ≥50 Mb), each sequence is displayed in a separate viewer. This is also true for multiple reference genomes (see Mikheenko et al., 2016). An example of the viewer is given in Figure 1. Fig. 1. An example of the contig alignment viewer for ABySS (Simpson et al., 2009), SPAdes (Bankevich et al., 2012) and Velvet (Zerbino and Birney, 2008) assemblies of S.aureus single-cell dataset from Chitsaz et al. (2011). The top grey panel shows Icarus controls for moving and zooming, and it also includes checkboxes for showing and hiding all types of detected misassemblies. The right grey panel presents details of the selected block. In this example, the highlighted block is a fragment of the misassembled contig NODE_5, which consists of three misassembled blocks. Users can expand detailed information on these blocks on this panel, or switch to any of them in the detailed view pane. The main viewer section is divided into four panels. From top to bottom: detailed assembly view, detailed read coverage (minimized to ‘Show read coverage’ button here), assembly overview and read coverage overview. Most of the contigs are correct (colored green and blue), but the assembly overview panel clearly shows that there are two regions where all three assemblers generate erroneous contigs (misassemblies, colored red and orange). One of these regions is depicted on the detailed assembly view. The orange color of ABySS and SPAdes misassembled blocks means that they have similar mappings to the reference. Worth noting that Velvet also made a misassembly at the same position, but its contig is shorter Open in new tabDownload slide An example of the contig alignment viewer for ABySS (Simpson et al., 2009), SPAdes (Bankevich et al., 2012) and Velvet (Zerbino and Birney, 2008) assemblies of S.aureus single-cell dataset from Chitsaz et al. (2011). The top grey panel shows Icarus controls for moving and zooming, and it also includes checkboxes for showing and hiding all types of detected misassemblies. The right grey panel presents details of the selected block. In this example, the highlighted block is a fragment of the misassembled contig NODE_5, which consists of three misassembled blocks. Users can expand detailed information on these blocks on this panel, or switch to any of them in the detailed view pane. The main viewer section is divided into four panels. From top to bottom: detailed assembly view, detailed read coverage (minimized to ‘Show read coverage’ button here), assembly overview and read coverage overview. Most of the contigs are correct (colored green and blue), but the assembly overview panel clearly shows that there are two regions where all three assemblers generate erroneous contigs (misassemblies, colored red and orange). One of these regions is depicted on the detailed assembly view. The orange color of ABySS and SPAdes misassembled blocks means that they have similar mappings to the reference. Worth noting that Velvet also made a misassembly at the same position, but its contig is shorter The contig alignment viewer places contigs according to their mapping to the reference genome produced by Nucmer aligner (Kurtz et al., 2004). The color scheme is designed to differentiate between correct contigs (green or blue) and contigs with assembly errors (misassemblies, red or orange). Misassembled contigs are broken into correctly aligned blocks, so users can easily identify and switch between blocks of the same erroneous contig. The side of the block with the misassembly event is highlighted. Icarus supports all types of misassembly events detected by QUAST (relocations, inversions, etc). Users can show and hide all blocks containing specific types of misassemblies. If several assemblies are provided, Icarus identifies all contigs that are similar in most of the assemblies and colors them in blue (for correct contigs) or orange (for misassembled ones). This feature helps researchers and assembly algorithms developers to see analogies between various assembly approaches. Details of Icarus similarity identification algorithm and examples of its performance are presented in the Supplementary Material. The viewer can additionally visualize genes, operons and read coverage distribution along the genome using two additional tracks. The annotation track helps to understand which assembly contains more functional elements, and whether any of them are not covered by any assemblies at all. The coverage distribution track helps to monitor behaviour of assembly algorithms in regions of extremely high or low coverage. 2.2 Contig size viewer This type of viewer shows contigs sorted by size in descending order. This ordering is suitable for comparing the largest contigs (which are the most interesting ones in most genomic studies). The viewer also labels the commonly used assembly quality statistics, N50 and N75. If the approximate genome length for the organism is known, NG50 and NG75 are also labelled (see Gurevich et al. (2013) for details). If the full reference genome is provided, the viewer shares the coloring scheme with the contig alignment viewer in order to highlight correctly mapped contigs (green blocks), unaligned contigs (grey blocks) and misassembled contigs (red blocks). Misassembly breakpoints are marked with dashed lines. All blocks are linked to their representations in the alignment viewer, allowing to quickly navigate between the viewers.",Visualization,"icarus visualizer   novo assembly evaluation
icarus pipeline consist   follow step see supplementary fig  run quast align contigs   reference  available gene find detect assembly errors postprocessing detect similarities   assemblies create javascriptbased web page icarus output two type  interactive html file contig alignment viewer  contig size viewer  viewer contain  least two pan  assembly overview   contigs show  full scale   pane  users  zoom   region  interest  pan display  assemblies  several track next     viewers support standard genome browser functionality navigate drag zoom click  elements  detail information   contig name type  length specific feature   viewer  describe  examples  icarus output  assemblies  bimpatiens saureus  cami  metagenomic dataset  demonstrate   supplementary material    website  contig alignment viewer  type  viewer  available    reference genome  provide   genome consist  large chromosomes  ≥   sequence  display   separate viewer   also true  multiple reference genomes see mikheenko     example   viewer  give  figure  fig   example   contig alignment viewer  aby simpson    spade bankevich     velvet zerbino  birney  assemblies  saureus singlecell dataset  chitsaz     top grey panel show icarus control  move  zoom   also include checkboxes  show  hide  type  detect misassemblies  right grey panel present detail   select block   example  highlight block   fragment   misassembled contig node_5  consist  three misassembled block users  expand detail information   block   panel  switch       detail view pane  main viewer section  divide  four panel  top  bottom detail assembly view detail read coverage minimize  show read coverage button  assembly overview  read coverage overview    contigs  correct color green  blue   assembly overview panel clearly show    two regions   three assemblers generate erroneous contigs misassemblies color red  orange one   regions  depict   detail assembly view  orange color  aby  spade misassembled block mean    similar mappings   reference worth note  velvet also make  misassembly    position   contig  shorter open  new tabdownload slide  example   contig alignment viewer  aby simpson    spade bankevich     velvet zerbino  birney  assemblies  saureus singlecell dataset  chitsaz     top grey panel show icarus control  move  zoom   also include checkboxes  show  hide  type  detect misassemblies  right grey panel present detail   select block   example  highlight block   fragment   misassembled contig node_5  consist  three misassembled block users  expand detail information   block   panel  switch       detail view pane  main viewer section  divide  four panel  top  bottom detail assembly view detail read coverage minimize  show read coverage button  assembly overview  read coverage overview    contigs  correct color green  blue   assembly overview panel clearly show    two regions   three assemblers generate erroneous contigs misassemblies color red  orange one   regions  depict   detail assembly view  orange color  aby  spade misassembled block mean    similar mappings   reference worth note  velvet also make  misassembly    position   contig  shorter  contig alignment viewer place contigs accord   map   reference genome produce  nucmer aligner kurtz     color scheme  design  differentiate  correct contigs green  blue  contigs  assembly errors misassemblies red  orange misassembled contigs  break  correctly align block  users  easily identify  switch  block    erroneous contig  side   block   misassembly event  highlight icarus support  type  misassembly events detect  quast relocations inversions etc users  show  hide  block contain specific type  misassemblies  several assemblies  provide icarus identify  contigs   similar     assemblies  color   blue  correct contigs  orange  misassembled ones  feature help researchers  assembly algorithms developers  see analogies  various assembly approach detail  icarus similarity identification algorithm  examples   performance  present   supplementary material  viewer  additionally visualize genes operons  read coverage distribution along  genome use two additional track  annotation track help  understand  assembly contain  functional elements  whether      cover   assemblies    coverage distribution track help  monitor behaviour  assembly algorithms  regions  extremely high  low coverage  contig size viewer  type  viewer show contigs sort  size  descend order  order  suitable  compare  largest contigs     interest ones   genomic study  viewer also label  commonly use assembly quality statistics n50  n75   approximate genome length   organism  know ng50  ng75  also label see gurevich     detail   full reference genome  provide  viewer share  color scheme   contig alignment viewer  order  highlight correctly map contigs green block unaligned contigs grey block  misassembled contigs red block misassembly breakpoints  mark  dash line  block  link   representations   alignment viewer allow  quickly navigate   viewers",11
210,MapView,"MapView: visualization of short reads alignment on a desktop computer 
MapView format (MVF) is a novel file format designed for fast and memory efficiency visualization of huge amount of short reads alignment data. The MVF (Supplementary Material) consists of four sections: file header, data, index and statistics information. The MVF binary file, combined with effective compressing and indexing of the alignments, will enable reduction in disk usage and fast retrieval of alignments in a specified region. 2.2 Loading and navigation algorithm MapView is a disk-based viewer and it only loads a tiny portion of the MVF file into memory. Specifically, MapView loads current displayed page and six neighbor pages. This fractional loading and neighbor caching algorithm leads to a small burden on memory resources while not compromising the speed. The MVF file offset of alignment data is indexed by reference position. To quickly find alignments associated with a specified region, MapView uses the index to locate the offset address of short reads mapped on the specified region and then retrieve the alignments data. This indexed navigation algorithm enables the users to quickly jump to different regions.",Visualization,"mapview visualization  short read alignment   desktop computer 
mapview format mvf   novel file format design  fast  memory efficiency visualization  huge amount  short read alignment data  mvf supplementary material consist  four section file header data index  statistics information  mvf binary file combine  effective compress  index   alignments  enable reduction  disk usage  fast retrieval  alignments   specify region  load  navigation algorithm mapview   diskbased viewer    load  tiny portion   mvf file  memory specifically mapview load current display page  six neighbor page  fractional load  neighbor cache algorithm lead   small burden  memory resources   compromise  speed  mvf file offset  alignment data  index  reference position  quickly find alignments associate   specify region mapview use  index  locate  offset address  short read map   specify region   retrieve  alignments data  index navigation algorithm enable  users  quickly jump  different regions",11
211,MetaSee,"MetaSee: an interactive and extendable visualization toolbox for metagenomic sample analysis and comparison
MetaSee is implemented based on all of the taxonomical and functional information that could be retrieved from metagenomic samples, and takes advantage of modern computer visualization technology, including HTML5 canvas, JavaScript, SVG and modern web browsers. The only requirement for viewing the result of MetaSee is an updated web browser, and the results can be viewed (online or off-line) on almost all operating systems (OS) with Graphical User Interface (GUI). 1 High-performance Computational Backbone Visualization tools are particularly powerful when used in combination with high-throughput automated analysis software (e.g., Parallel-META [27], [28]). Features, such as easy-to-use, cross OS platform and open source of MetaSee, make it easy to build this visualization tool in high-throughput automated analysis pipelines. In this work, we have used Parallel-META [27], [28] to analyze the metagenomic data, and the interactive visualization effects were built based on these results. 2 The Core Visualization Engine The core visualization engine is composed of multiple viewing components: the viewing components include (not exclusive of each other): overall framework (Figure 2(A)), MetaSee visualization panel (Figure 2(B)), Global view (Figure 2(C)), Taxa view (Figure 2(D)), Phylogenetic view (Figure 2(E)), Phylogenetic file (Figure 2(F)), Link out annotations (Figure 2(G)) and Sample view (Figure 2(H)). These components are capable of providing GUI for visualization of uploaded files, and aim to answer questions regarding the relative abundance of taxa across multiple levels of the hierarchy for multiple samples simultaneously. An external file that holds a picture, illustration, etc. Object name is pone.0048998.g002.jpg Open in a separate window Figure 2 Overview of the components of MetaSee. Each pie stands for an element of MetaSee, and directed arrow stand for a front-end link from one component to another component. (1) Framework (Figure 3) An external file that holds a picture, illustration, etc. Object name is pone.0048998.g003.jpg Figure 3 Overview of the visualization result of MetaSee and the Framework. (A) Left side bar for navigation, (B) Main window is the working area for visualization. The framework includes the left sidebar ( Figure 3(A) ) and the main window ( Figure 3(B) ). The left sidebar is the navigation bar for visualization results, which can be flipped on and off. The main window is the working area and all the views will be displayed in this area. (2) MetaSee visualization panel (Figure 2(B)) The MetaSee visualization panel is the main interactive operation panel of MetaSee, which is designed for interactive analysis of the structure of metagenome. This panel is a pie chart, and when a sector (representing a taxa) of it is selected, the area will be highlighted and turn to the right. The right side bar of MetaSee visualization panel will display the detailed information of this node and links to other views. The lengths of layers of these charts indicate which part of this dataset was classified more precisely. And the color of each sector indicates the abundance of this sector (taxa) (red color indicates more abundant taxa). (3) Global view (Figure 2(C)) For each sample, a Global view is a hierarchical tree that contains every taxa and their proportion in the sample. Two or more samples can be shown in a single Global view, with each node composed of a bar-plot showing the relative abundance of different samples at those taxa. Thus, Global view shows the whole picture of all samples being compared. In Global view, all the taxonomy units at the same level are in the same rank, so it is easy to find which part of the input dataset was enriched (classified with more details). The heights of each pillar stand for the relative abundance of each sample at this taxonomy unit. The detail information of a certain taxonomy unit is linked from small bar chart to their Taxa view (a pair of pie-charts and a pair of bar-charts) with relative abundance, absolutely abundance and legend. In Global view each color indicates a sample (as indicated in figure legend), and it is convenient to find the difference among multiple samples at the global level of a certain taxonomy unit. (4) Taxa view (Figure 2(D)) For one or a set of samples, the Taxa view focuses on the detail information of one node (taxa) in Global view, a taxonomical hierarchy tree structure (by clicking the bar-plot for that node). This detailed information includes the abundance information at the specific taxa, which is useful for comparing different samples for specific taxa. It can be shown in either pie-chart or bar-chart format. (5) Phylogenetic view (Figure 2(E)) For each sample, a Phylogenetic view is an unweighted phylogenetic tree. It elucidates the evolutionary relationship of all microbes in a microbiome community. (6) Phylogenetic tree file (Figure 2(F)) Unweighted phylogenetic tree file is presented in Newick format. It can also be imported into other phylogenetic tree visualization tool (e.g. Phylogenetic tree Maker (http://www.metasee.org/visualizationlab/phylogenetictrees.jsp) ). (7) Sample view (Figure 2(H)) For each sample, the taxonomical community structure is represented in a dynamic multi-layer pie-chart, so that each taxon’s (at each level) proportion can be vividly seen by interactively zoomed-in or zoomed-out. Moreover, pie-charts for multi-samples can be smoothly shifted from one to another for comparison of structure and proportion. The sample view is implemented by the Krona software [14]. The Sample view can also be viewed directly or linked from the Global view (by clicking the legend box at the up-right corner). (8) Link-out annotation (Figure 2(G)) Each of the taxa or function could be linked-out to their annotation from external sources from MetaSee visualization panel, Global view (by clicking the name of that node), Sample view or Taxa view. Here we use the taxonomy browser database of NCBI [29] as external link-out annotation source, which would facilitate digging the detailed information of a certain taxa and speeding up the manual analysis process. 3 Multi-sample Comparisons Metagenomic data are often generated at discrete points across multiple locations or times. MetaSee is able to store the data from multiple samples in a single framework. Individual samples may then be stepped through. Thus, it makes the comparison among samples coming from different time points or conditions easy (Figure 4). In Global view (Figure 4(A)), the bars with the same color come from the same sample, and the height of each pillar represents the relative abundance of corresponding samples at corresponding nodes (taxa). Taxa view (Figure 4(B) and Figure 4(C)) includes a pie-chart and a bar-chart for each node, and both pie chart and bar chart have two graphs to represent the relative abundance and absolute number, respectively. In addition to these visualization functions, to provide high quality graph for publication purpose, all the graphs produced by MetaSee are vector graphs. An external file that holds a picture, illustration, etc. Object name is pone.0048998.g004.jpg Open in a separate window Figure 4 Comparison across multiple samples. (A) Global view, (B) Taxa view with pie-chart format, (C) Taxa view with bar-chart format. 4 The Front-end Interactive Analysis Interface The front-end interface mainly serves for a set of real metagenome projects based on MetaSee visualization system. Two areas may need this metagenomic visualization system: dentistry and field experimental studies. For dentists, this system would help them for quick diagnosis by using the novel samples that have been collected as queries to search in the database of known samples of microbial communities. This has been proven to be workable for dentists so far [11]. For field study experts such as those on ocean expeditions or doing soil sample testing, this tool would help them to analyze their data quickly and get illustrative results easily. For these two areas, we have designed two interfaces, “Digital mouth” and “Metagenome global survey”, as examples of front-end. 5 Open-source Portals for Plug-in Development Open-source portals were designed to extend the usability of the MetaSee visualization system. Firstly, community structure files in many formats can be imported into MetaSee. As XML is easy to expand, it was selected as the default format. Yet, during run time, community structure files in many formats could be stored in random access memory (RAM) as double linked trees, by an independent component for tree building. Based on this design model, it is very easy to develop other APIs for new input file formats. As examples, we developed APIs for importing output files from parallel-META [27], [28], MEGAN [16] and MG-RAST [22]. And other APIs for input data manipulations are under development. Secondly, the work flow of MetaSee could build a tree structure and then output this tree to a variety of views. Therefore, adding new APIs for other views (such as back-to-back sample views) or modifying existing views would be facilitated. Thirdly, the search function of the MetaSee toolbox (http://www.metasee.org/visualizationlab/search/) provided a portal for searching any metagenomic samples against a metagenome database. Right now only samples from “Metagenome global survey” could be searched against a pre-built database of annotated metagenomic samples (just to show-case its functions). Yet this open-source portal could facilitate the re-development of search functions to search any user-specified metagenomic sample against any metagenome database. Finally, we have established a repository (http://www.metasee.org/tools.jsp and http://www.metasee.org/laboratory.jsp) to provide more viewing options and more viewing services. Other APIs, such as those for online tools and database connections, are under development. 6 Online Web Services and Resources The online version (http://www.metasee.org) of MetaSee accepts files of many formats and when a file is uploaded, a GUI will be produced. Users can analyze the resulting dynamic graph online and also download it. Additionally, high quality vector graphs could be used for publication purpose. Additionally, stand-alone MetaSee application could be downloaded as a virtual machine, which was developed in Java, and can run almost on all OS using both GUI (Figure 5) and command line. As MetaSee is multi-threaded, it can accept a very large dataset. The output result is a set of HTML pages with high-resolution figures. An external file that holds a picture, illustration, etc. Object name is pone.0048998.g005.jpg Figure 5 The GUI of standalone version of MetaSee. Firstly, select the format of input data with the drop-down list. Secondly, click the “input file” to select input file, multiple files can be accepted, but these files should be in uniform format. Thirdly, press the “output folder” button to assign the output path. Finally, press “submit” button to run MetaSee. We released our source codes and development documents. With these documents, example source codes, sample data and our discussion group (http://groups.google.com/group/metasee), developer can develop new APIs of MetaSee for their purposes, and we would like to accept the codes contributed by other developers. Go to:",Visualization,"metasee  interactive  extendable visualization toolbox  metagenomic sample analysis  comparison
metasee  implement base     taxonomical  functional information  could  retrieve  metagenomic sample  take advantage  modern computer visualization technology include html5 canvas javascript svg  modern web browsers   requirement  view  result  metasee   update web browser   result   view online  offline  almost  operate systems   graphical user interface gui  highperformance computational backbone visualization tool  particularly powerful  use  combination  highthroughput automate analysis software  parallelmeta   feature   easytouse cross  platform  open source  metasee make  easy  build  visualization tool  highthroughput automate analysis pipelines   work   use parallelmeta    analyze  metagenomic data   interactive visualization effect  build base   result   core visualization engine  core visualization engine  compose  multiple view components  view components include  exclusive    overall framework figure  metasee visualization panel figure  global view figure  taxa view figure  phylogenetic view figure  phylogenetic file figure  link  annotations figure   sample view figure   components  capable  provide gui  visualization  upload file  aim  answer question regard  relative abundance  taxa across multiple level   hierarchy  multiple sample simultaneously  external file  hold  picture illustration etc object name  poneg002jpg open   separate window figure  overview   components  metasee  pie stand   element  metasee  direct arrow stand   frontend link  one component  another component  framework figure   external file  hold  picture illustration etc object name  poneg003jpg figure  overview   visualization result  metasee   framework  leave side bar  navigation  main window   work area  visualization  framework include  leave sidebar  figure     main window  figure    leave sidebar   navigation bar  visualization result    flip     main window   work area    view   display   area  metasee visualization panel figure   metasee visualization panel   main interactive operation panel  metasee   design  interactive analysis   structure  metagenome  panel   pie chart    sector represent  taxa    select  area   highlight  turn   right  right side bar  metasee visualization panel  display  detail information   node  link   view  lengths  layer   chart indicate  part   dataset  classify  precisely   color   sector indicate  abundance   sector taxa red color indicate  abundant taxa  global view figure    sample  global view   hierarchical tree  contain every taxa   proportion   sample two   sample   show   single global view   node compose   barplot show  relative abundance  different sample   taxa thus global view show  whole picture   sample  compare  global view   taxonomy units    level     rank    easy  find  part   input dataset  enrich classify   detail  heights   pillar stand   relative abundance   sample   taxonomy unit  detail information   certain taxonomy unit  link  small bar chart   taxa view  pair  piecharts   pair  barcharts  relative abundance absolutely abundance  legend  global view  color indicate  sample  indicate  figure legend    convenient  find  difference among multiple sample   global level   certain taxonomy unit  taxa view figure   one   set  sample  taxa view focus   detail information  one node taxa  global view  taxonomical hierarchy tree structure  click  barplot   node  detail information include  abundance information   specific taxa   useful  compare different sample  specific taxa    show  either piechart  barchart format  phylogenetic view figure    sample  phylogenetic view   unweighted phylogenetic tree  elucidate  evolutionary relationship   microbes   microbiome community  phylogenetic tree file figure  unweighted phylogenetic tree file  present  newick format   also  import   phylogenetic tree visualization tool  phylogenetic tree maker    sample view figure    sample  taxonomical community structure  represent   dynamic multilayer piechart    taxons   level proportion   vividly see  interactively zoomedin  zoomedout moreover piecharts  multisamples   smoothly shift  one  another  comparison  structure  proportion  sample view  implement   krona software   sample view  also  view directly  link   global view  click  legend box   upright corner  linkout annotation figure     taxa  function could  linkedout   annotation  external source  metasee visualization panel global view  click  name   node sample view  taxa view   use  taxonomy browser database  ncbi   external linkout annotation source  would facilitate dig  detail information   certain taxa  speed   manual analysis process  multisample comparisons metagenomic data  often generate  discrete point across multiple locations  time metasee  able  store  data  multiple sample   single framework individual sample may   step  thus  make  comparison among sample come  different time point  condition easy figure   global view figure   bar    color come    sample   height   pillar represent  relative abundance  correspond sample  correspond nod taxa taxa view figure   figure  include  piechart   barchart   node   pie chart  bar chart  two graph  represent  relative abundance  absolute number respectively  addition   visualization function  provide high quality graph  publication purpose   graph produce  metasee  vector graph  external file  hold  picture illustration etc object name  poneg004jpg open   separate window figure  comparison across multiple sample  global view  taxa view  piechart format  taxa view  barchart format   frontend interactive analysis interface  frontend interface mainly serve   set  real metagenome project base  metasee visualization system two areas may need  metagenomic visualization system dentistry  field experimental study  dentists  system would help   quick diagnosis  use  novel sample    collect  query  search   database  know sample  microbial communities    prove   workable  dentists  far   field study experts     ocean expeditions   soil sample test  tool would help   analyze  data quickly  get illustrative result easily   two areas   design two interfaces “digital mouth”  “metagenome global survey”  examples  frontend  opensource portals  plugin development opensource portals  design  extend  usability   metasee visualization system firstly community structure file  many format   import  metasee  xml  easy  expand   select   default format yet  run time community structure file  many format could  store  random access memory ram  double link tree   independent component  tree build base   design model    easy  develop  apis  new input file format  examples  develop apis  import output file  parallelmeta   megan   mgrast    apis  input data manipulations   development secondly  work flow  metasee could build  tree structure   output  tree   variety  view therefore add new apis   view   backtoback sample view  modify exist view would  facilitate thirdly  search function   metasee toolbox  provide  portal  search  metagenomic sample   metagenome database right   sample  “metagenome global survey” could  search   prebuilt database  annotate metagenomic sample   showcase  function yet  opensource portal could facilitate  redevelopment  search function  search  userspecified metagenomic sample   metagenome database finally   establish  repository     provide  view options   view service  apis     online tool  database connections   development  online web service  resources  online version   metasee accept file  many format    file  upload  gui   produce users  analyze  result dynamic graph online  also download  additionally high quality vector graph could  use  publication purpose additionally standalone metasee application could  download   virtual machine   develop  java   run almost    use  gui figure   command line  metasee  multithreaded   accept   large dataset  output result   set  html page  highresolution figure  external file  hold  picture illustration etc object name  poneg005jpg figure   gui  standalone version  metasee firstly select  format  input data   dropdown list secondly click  “input file”  select input file multiple file   accept   file    uniform format thirdly press  “output folder” button  assign  output path finally press “submit” button  run metasee  release  source cod  development document   document example source cod sample data   discussion group  developer  develop new apis  metasee   purpose   would like  accept  cod contribute   developers  ",11
212,Strainer,"Strainer: software for analysis of population variation in community genomic datasets
Strainer is built around an interactive display of community genomic data and provides a suite of automated and manual tools to explore, quantify, and visualize the patterns of variation in sampled populations. Strainer uses the BioJava [16] programming framework to read and write a number of different file formats including FASTA, BLAST output, and GenBank. Data Preparation Strainer displays sequence reads relative to a user defined reference sequence. The reference can be a fully assembled chromosome or genome, a contig from an assembler such as Phrap [17,18], or the genome of a related organism. Reference sequences can be input as either FASTA [19] or GenBank [20] formatted files. The latter format allows for gene annotations to be included. Read alignments to the reference sequence can be imported into the strainer XML format from two sources. First, a contig and all aligned reads can be read directly from an ACE file produced by Phrap. Alternatively, the blastn procedural query in BLAST can be used to align reads to the reference sequence [21]. When working with BLAST alignments, reads may align to multiple places on a reference sequence. To validate read placements, alignments are compared to corresponding mate-pair alignments. Mate-pairs are left and right end reads of a cloned DNA fragment. Reads are typically at least 700 base pairs (bp) in length, completely sequenced fragments will be obtained only from very small insert clone libraries. The size of the unsequenced region in typical libraries is not precisely known, but is constrained by the average clone size (3,000 to 5,000 bp for small insert libraries and ~40 kb for fosmid libraries). Strainer allows flexibility in mate pair placement because gene insertions in a subset of strains result in larger than expected mate pair separation in strains lacking the insert. Strainer finds alignments that place paired reads within a window of separation supplied by the user. If no such alignments are found for a pair of reads, the best individual alignments are then chosen. Yellow outlines are applied to reads that could not be placed within the user specified range of their mate-pair (the range of average clone sizes). As a result, regions in which gene order is not constant across the population are marked by yellow reads (Figure 1A). Visualization Figure 1A shows an image of the Strainer interface. The black bar along the top of the window represents the reference sequence and the overlaid red frame indicates the scope of the current field of view. If a GenBank file is used, the genes are displayed as dark grey arrows immediately below this region. Bars linked with thin horizontal lines, representing aligned mate-paired reads, are displayed below. Pointed tips on bars indicate the sequencing direction and thus point to the expected placement of the paired read. Using the toolbar, the user can zoom in or out and pan left or right to explore the read alignments. When the zoom level is high enough (when a single base is at least as wide as a pixel), colored ticks appear on each read bar to indicate, base-by-base, where the read sequence differs from the reference sequence (Figure 1B,C). By default, reads are sorted by length or by identity with the reference sequence, but can be arrayed based on read length. Clicking on the gene symbol reveals the gene position relative to the aligned reads and displays gene information in the box at the bottom of the read display. Coloring There are three options for coloring read bars. The default option is to color regions within the reads to indicate locations where there are disagreements with the reference sequence. Each column of pixels is assigned one of two user-defined colors based on the percent sequence identity for a small window of bases centered at the corresponding position on the read. Both the window size for averaging and the threshold at which coloring occurs are user-defined. Regions without sequence information – e.g., the line between reads in a mate pair – are colored according to the overall sequence identity between the read and the reference. Alternatively, a single read can be shaded to an extent that depends on the percent sequence identity between that read and the reference sequence. Finally, a single solid color can be used for all reads, regardless of its identity with the reference sequence. Quality Data Strainer can read in confidence values assigned to base calls (e.g., Phred scores) from a FASTA formatted quality file generated by Phred/Phrap. Base call differences with scores below a user-defined confidence level will be grayed. The user can then set a threshold confidence level below which base call differences will be deemphasized (colored gray). Strainer allows the user to specify whether unknown bases (N's or low quality bases) should be ignored when calculating sequence divergences. Read Groupings and Apparent Recombinant Reads A major goal is to group reads with similar sequences so as to reconstruct variant gene sequences. In the manual strain reconstruction mode, the user clicks on reads to select them (selected reads are highlighted in blue) and then uses the ""Make Strain"" button to bring all the selected reads into a single strain fragment indicated by a surrounding colored rectangle. Similarly, strain groups can be joined. Strain fragments are given random colors by default, or can be colored using the same 3 methods available for reads. This choice is independent of the read coloring method chosen. Grouping into strain fragments will often highlight reads that are divergent over only a portion of their length. This may be due to insertion of sequence (e.g., a transposon in only a subset of individuals) or to homologous recombination with another sequence type. Recombinants can be recognized most easily as chimeras of two variant sequences. The user can flag such reads as recombinants (the program will outline them in red) and assign the mate pairs to the most relevant strains. Automatic Generation of Read Groups Strainer includes an algorithm to reconstruct all possible variant sequences by considering all legitimate linkage pathways. Reads with overlapping sequence patterns are linked to form more extended strain sequence types so long as overlap exceeds a user-specified threshold. In cases where alternative variant paths are possible, such as when overlapping sequence subsequently diverges into two or more paths (Fig. 2), both potential paths are generated. The algorithm can be executed on any chosen gene, for every gene in the annotation, or for any user defined segment of the genome. The variants can be output as a FASTA list of nucleotide or amino acid sequences. Strainer can also use this algorithm to automatically group reads in the display into strain types. Since variants are determined by exploring all the possible ways to link reads, a single read can be associated with multiple variant sequences. In instances where reads can be assigned to multiple groups, reads are placed into the largest group. Groups generated automatically can be manually curated to resolve complicated regions, such as conflicting read placements due to recombination. Group Sequences and Read Lists Strainer allows the user to select a series of reads or strain fragments and export the composite sequence to a FASTA file as either nucleotides or amino acids. Regions containing gaps in sequence coverage can either be filled in from the reference sequence or marked with N's (or X's for amino acid sequences). In addition, nucleotide or amino acid sequences of all strain variant groups for all genes can be output. Also, a list of reads contained in strain fragments can be output to a text file for use in other applications. Editing the Consensus Sequence Assembly of closely related sequences can generate a composite sequence that is a mosaic of strain types and is not actually found in the environment. The composite may also take on the sequence of a less abundant variant. Therefore, it is important to have the ability to alter the composite sequence after strain analysis. Strainer can alter the reference sequence to match a selected strain, read, or single base pair. The updated reference sequence can be exported to a FASTA file for ORF searches or other applications. Obtaining Strainer Strainer was developed in Java to enable seamless execution in almost any computing environment. It is available for download as a self-contained application (see below). Additionally, the source code and more detailed documentation are available online. A programming interface (API) is provided and described in the online documentation (see below) to allow custom algorithms (such as new clustering methods) to be implemented, if desired. ",Visualization,"strainer software  analysis  population variation  community genomic datasets
strainer  build around  interactive display  community genomic data  provide  suite  automate  manual tool  explore quantify  visualize  pattern  variation  sample populations strainer use  biojava  program framework  read  write  number  different file format include fasta blast output  genbank data preparation strainer display sequence read relative   user define reference sequence  reference    fully assemble chromosome  genome  contig   assembler   phrap    genome   relate organism reference sequence   input  either fasta   genbank  format file  latter format allow  gene annotations   include read alignments   reference sequence   import   strainer xml format  two source first  contig   align read   read directly   ace file produce  phrap alternatively  blastn procedural query  blast   use  align read   reference sequence   work  blast alignments read may align  multiple place   reference sequence  validate read placements alignments  compare  correspond matepair alignments matepairs  leave  right end read   clone dna fragment read  typically  least  base pair   length completely sequence fragment   obtain    small insert clone libraries  size   unsequenced region  typical libraries   precisely know   constrain   average clone size      small insert libraries  ~   fosmid libraries strainer allow flexibility  mate pair placement  gene insertions   subset  strain result  larger  expect mate pair separation  strain lack  insert strainer find alignments  place pair read within  window  separation supply   user    alignments  find   pair  read  best individual alignments   choose yellow outline  apply  read  could   place within  user specify range   matepair  range  average clone size   result regions   gene order   constant across  population  mark  yellow read figure  visualization figure  show  image   strainer interface  black bar along  top   window represent  reference sequence   overlay red frame indicate  scope   current field  view   genbank file  use  genes  display  dark grey arrows immediately   region bar link  thin horizontal line represent align matepaired read  display  point tip  bar indicate  sequence direction  thus point   expect placement   pair read use  toolbar  user  zoom     pan leave  right  explore  read alignments   zoom level  high enough   single base   least  wide   pixel color tick appear   read bar  indicate basebybase   read sequence differ   reference sequence figure 1bc  default read  sort  length   identity   reference sequence    array base  read length click   gene symbol reveal  gene position relative   align read  display gene information   box   bottom   read display color   three options  color read bar  default option   color regions within  read  indicate locations    disagreements   reference sequence  column  pixels  assign one  two userdefined color base   percent sequence identity   small window  base center   correspond position   read   window size  average   threshold   color occur  userdefined regions without sequence information    line  read   mate pair   color accord   overall sequence identity   read   reference alternatively  single read   shade   extent  depend   percent sequence identity   read   reference sequence finally  single solid color   use   read regardless   identity   reference sequence quality data strainer  read  confidence value assign  base call  phred score   fasta format quality file generate  phredphrap base call differences  score   userdefined confidence level   gray  user   set  threshold confidence level   base call differences   deemphasized color gray strainer allow  user  specify whether unknown base '  low quality base   ignore  calculate sequence divergences read group  apparent recombinant read  major goal   group read  similar sequence    reconstruct variant gene sequence   manual strain reconstruction mode  user click  read  select  select read  highlight  blue   use  ""make strain"" button  bring   select read   single strain fragment indicate   surround color rectangle similarly strain group   join strain fragment  give random color  default    color use    methods available  read  choice  independent   read color method choose group  strain fragment  often highlight read   divergent    portion   length  may  due  insertion  sequence   transposon    subset  individuals   homologous recombination  another sequence type recombinants   recognize  easily  chimeras  two variant sequence  user  flag  read  recombinants  program  outline   red  assign  mate pair    relevant strain automatic generation  read group strainer include  algorithm  reconstruct  possible variant sequence  consider  legitimate linkage pathways read  overlap sequence pattern  link  form  extend strain sequence type  long  overlap exceed  userspecified threshold  case  alternative variant paths  possible    overlap sequence subsequently diverge  two   paths fig   potential paths  generate  algorithm   execute   choose gene  every gene   annotation    user define segment   genome  variants   output   fasta list  nucleotide  amino acid sequence strainer  also use  algorithm  automatically group read   display  strain type since variants  determine  explore   possible ways  link read  single read   associate  multiple variant sequence  instance  read   assign  multiple group read  place   largest group group generate automatically   manually curated  resolve complicate regions   conflict read placements due  recombination group sequence  read list strainer allow  user  select  series  read  strain fragment  export  composite sequence   fasta file  either nucleotides  amino acids regions contain gap  sequence coverage  either  fill    reference sequence  mark  '  '  amino acid sequence  addition nucleotide  amino acid sequence   strain variant group   genes   output also  list  read contain  strain fragment   output   text file  use   applications edit  consensus sequence assembly  closely relate sequence  generate  composite sequence    mosaic  strain type    actually find   environment  composite may also take   sequence   less abundant variant therefore   important    ability  alter  composite sequence  strain analysis strainer  alter  reference sequence  match  select strain read  single base pair  update reference sequence   export   fasta file  orf search   applications obtain strainer strainer  develop  java  enable seamless execution  almost  compute environment   available  download   selfcontained application see  additionally  source code   detail documentation  available online  program interface api  provide  describe   online documentation see   allow custom algorithms   new cluster methods   implement  desire ",11
213,Krona,"Interactive metagenomic visualization in a Web browser
Architecture Thanks to technologies such as HTML5 and JavaScript, modern Web browsers are capable of rendering fully featured, graphical user interfaces for both Web sites and local applications. Krona's architecture takes a hybrid approach in which data are stored locally, but the interface code is hosted on the Internet. This allows each Krona chart to be contained in a single file, making them easy to view, share, and integrate with existing websites. The only requirements for viewing are an Internet connection and a recent version of any major web browser (though local charts that do not require an Internet connection can also be created and viewed with a Krona installation). Modularity is achieved by embedding XML chart data in an XHTML document that links to an external JavaScript implementation of the interface (Figure 1). When a web browser renders the XHTML document, the JavaScript loads chart data from the embedded XML and renders the chart to an HTML5 canvas tag. Hosting the JavaScript on the Internet avoids installation requirements and allows seamless, automatic updating as Krona evolves. To allow Krona to be used for a wide variety of applications, utilities for creating Krona charts are separated from the viewing engine. A package of these, called KronaTools, comprises Perl scripts for importing data from several popular bioinformatics tools and generic file types. Figure 1 figure1 The Krona architecture. XML within an XHTML document is used to store chart data within a web page. XML tag nesting is used to describe the hierarchy, while attributes are used to store magnitude and other information about each node. Krona displays these attributes as HTML elements, allowing hyperlinks to supplemental pages for each node. These could be either pages created with the Krona chart, such as BLAST results, or existing web pages, such as NCBI taxonomy pages for the taxonomy IDs of the nodes. The Krona interface JavaScript is linked into the chart either via the Web or locally. Full size image Hierarchical classifications can be directly imported from the RDP Classifier, Phymm/PhymmBL, MG-RAST (both taxonomic and functional), or the web-based bioinformatics platform Galaxy [17]. Sequences can also be taxonomically classified from BLAST results downloaded from NCBI [9, 10] or the METAREP metagenomic repository [4]. Classification of raw BLAST results is performed by finding the lowest common ancestor of the highest scoring alignments (an approach similar to that of MEGAN), and data are mapped to a taxonomy tree automatically downloaded and indexed from the NCBI taxonomy database [18]. When importing classifications from RDP and PhymmBL a color gradient can be used to represent the average reported confidence of assignments to each node. For MG-RAST, METAREP, and raw BLAST results, the nodes can be colored by average log of e-value or average percent identity. Also, since Phymm/PhymmBL and BLAST classifications can be performed either on reads or assembled contigs, the scripts for importing from these tools allow the optional specification of magnitudes for each classified sequence. A script is also provided to generate magnitudes based on reads per contig from assemblies in the common ACE file format. Other types of classifications can be imported from basic text files or an Excel template detailing lineage and magnitude. Finally, an XML file can be imported to gain complete control over the chart, including custom attributes and colors for each node. Since node attributes can contain HTML and hyperlinks, XML import allows Krona to be deployed as a custom data browsing and extraction platform in addition to a visualization tool. Visual design The Krona display resembles a pie chart, in that it subdivides separate classes into sectors, but with an embedded hierarchy. Each sector is overlaid with smaller sectors representing its children, which are squeezed toward the outside of the chart to give the parent room for labeling. This does not cause distortion because, as in a pie chart, magnitudes are represented by the angle of each sector rather than the area. For example, Figure 2 shows an oceanic metagenome [19] imported from METAREP. The taxon ""Gammaproteobacteria"" is selected, and the angle of the highlighted sector indicates the relative magnitude of the node (in this case 110,467 classified sequencing reads, as shown in the upper right corner). The sector also surrounds smaller sectors, which represent constituents of Gammaproteobacteria. In this case, the sum of the constituent angles equals the angle of the parent, indicating that no assignments were made directly to Gammaproteobacteria. If assignments had been made to this internal node, its angular sweep would be wider than the sum of its children's, clearly showing both the summary and the assigned amount in relation to each other. Figure 2 figure2 The Krona RSF display. The bacterioplankton metagenome from a vertical profiling of the North Pacific Subtropical Gyre [19] was imported from METAREP and displayed using Krona. Taxonomy nodes are shown as nested sectors arranged from the top level of the hierarchy at the center and progressing outward. Navigational controls are at the top left, and details of the selected node are at the top right. The chart is zoomed to place the domain ""Bacteria"" at the root and the taxon ""Gammaproteobacteria"" is shown selected. An interactive version of this chart is available on the Krona website. Full size image A common criticism of RSF displays is the difficulty of comparing similarly sized nodes. To make comparisons easier, Krona sorts nodes by decreasing magnitude with respect to their siblings. In addition, the nodes can be colored using a novel algorithm that works with the sorting to visually emphasize both hierarchy and quantity. This algorithm, which is enabled by default, uses the hue-saturation-lightness (HSL) color model to allow procedural coloring that can adapt to different datasets. First, the hue spectrum is divided among the immediate children of the current root node. Each of these children in turn subdivides its hue range among its children using their magnitudes as weights. Coloring each sorted node by the minimum of its hue range causes recursive inheritance of node hue by the largest child of each generation. The result is visual consistency for lineages that are quantitatively skewed toward particular branches. To distinguish each generation without disrupting this consistency, the lightness aspect of the HSL model is increased with relative hierarchical depth, with saturation remaining constant. Spatial efficiency Metagenomic hierarchies can easily become too complex for all nodes to be discernibly apportioned and labeled on a computer screen. Although Krona ameliorates this problem with interactive zooming, it also offers several modifications to RSF displays that maximize the amount of information contained in each view. First, radix-tree compression is used to collapse linear subgraphs in the hierarchy, simplifying the chart without removing quantitative relationships. Linear subgraphs, which represent multiple ranks of the same classification, occur when taxonomic classifications for a sample are mapped onto a full taxonomy tree. For example, if Homo sapiens were the only representative species of the class Mammalia, it would typically be redundantly classified under Primates, Hominids, and other ranks. To allow such classifications to be viewed, collapsing can be dynamically toggled, with animation depicting the transition. For additional simplification of complex trees, the taxonomy can be pruned to summarize the data at a specified depth. Figure 2, for example, shows an NCBI taxonomy summarized at a maximum depth of 6 levels and with linear subgraphs collapsed. Second, since deeper taxonomical levels are often the most interesting (e.g. genus and species classifications), Krona allows significant quantities at these levels to be viewed in direct relation to the root of the hierarchy. This is accomplished by dynamically reducing the labeling area of intermediate classifications, removing their labels if necessary. Compression is increased moving outward from the center to ensure that the highest levels of the current view can also be labeled. The intermediate levels that have been compressed can always be seen more clearly by zooming. Finally, Krona's labeling algorithms greatly increase textual information density compared to other RSF implementations. Space is used efficiently by orienting leaf node labels along radii and internal node labels along tangents. Internal labels use step-wise positioning and collision-based shortening to display as much text as possible while avoiding overlaps. Polar-coordinate zooming Because radial space-filling displays recursively subdivide angles, the shapes of the nodes approach rectangles as hierarchical depth increases and as node magnitudes decrease. Thus, zooming small nodes by simply scaling the entire figure in Cartesian coordinate space would result in a loss of the angular aspect that makes RSF displays intuitive and space-efficient. To increase the capacity of the displays without causing this problem, Krona uses a polar coordinate space for zooming. This is accomplished by increasing the angular sweep and radius of the zooming node until it occupies the same circle as the original overview. The angular sweeps of surrounding nodes are decreased simultaneously, creating an animated ""fisheye"" effect. This animation ensures user cognition of the change in context, and the final zoomed view retains the entire capacity as the original. Zooming can then be repeated for any node with children, providing informative views of even the deepest levels of a complex hierarchy. Zooming out to traverse up the hierarchy can be accomplished similarly by clicking ancestral nodes, which are shown in the center of the plot and as summary pie charts next to the plot. This triggers the reverse of the fisheye animation, compressing the current node to reveal its position in the new, broader context. Multi-dimensional data To visualize secondary attributes in addition to magnitude, individual nodes in Krona may be colored by variable. For categorical variables, users may define the color of every node in the XML. For quantitative variables, a gradient may be defined that will color each node by value. An example of this is shown in Figure 3, where each node is colored by a quantitative red-green gradient representing classification confidence. Figure 3 figure3 Coloring by classification confidence. Human gut sample MH0072 from the MetaHIT project [23] was classified using PhymmBL and displayed using Krona. Abundance can be simultaneously visualized with an accessory attribute by linking it to hue. In this example, hue is used to display classification confidence as reported by PhymmBL. The average confidence value for each node is colored from low (red) to high (green), distinguishing uncertain from certain classifications. An interactive version of this chart is available on the Krona website. Full size image Additionally, metagenomic data are often generated at discrete points across multiple locations or times. Krona is able to store the data from multiple samples in a single document. Individual samples may then be stepped through, at any zoom level, using the navigation interface at the top left. For example, in Figure 2 Krona is displaying one of seven depth samples from the oceanic water column. Advancing through these samples progresses through samples at greater and greater depths. The transition between samples is animated using a polar ""tween"" effect, emphasizing the difference between samples. The result of this style of navigation is a series of moving pictures, where the taxa dynamically grow and shrink from sample to sample-in this case as sampling descends the water column. This approach is eye-catching for a few samples, but direct comparison between many samples simultaneously is difficult with radial charts. Analysis across many samples is better left to traditional heatmap and differential barchart visualizations.",Visualization,"interactive metagenomic visualization   web browser
architecture thank  technologies   html5  javascript modern web browsers  capable  render fully feature graphical user interfaces   web sit  local applications krona' architecture take  hybrid approach   data  store locally   interface code  host   internet  allow  krona chart   contain   single file make  easy  view share  integrate  exist websites   requirements  view   internet connection   recent version   major web browser though local chart    require  internet connection  also  create  view   krona installation modularity  achieve  embed xml chart data   xhtml document  link   external javascript implementation   interface figure    web browser render  xhtml document  javascript load chart data   embed xml  render  chart   html5 canvas tag host  javascript   internet avoid installation requirements  allow seamless automatic update  krona evolve  allow krona   use   wide variety  applications utilities  create krona chart  separate   view engine  package   call kronatools comprise perl script  import data  several popular bioinformatics tool  generic file type figure  figure1  krona architecture xml within  xhtml document  use  store chart data within  web page xml tag nest  use  describe  hierarchy  attribute  use  store magnitude   information   node krona display  attribute  html elements allow hyperlinks  supplemental page   node  could  either page create   krona chart   blast result  exist web page   ncbi taxonomy page   taxonomy ids   nod  krona interface javascript  link   chart either via  web  locally full size image hierarchical classifications   directly import   rdp classifier phymmphymmbl mgrast  taxonomic  functional   webbased bioinformatics platform galaxy  sequence  also  taxonomically classify  blast result download  ncbi     metarep metagenomic repository  classification  raw blast result  perform  find  lowest common ancestor   highest score alignments  approach similar    megan  data  map   taxonomy tree automatically download  index   ncbi taxonomy database   import classifications  rdp  phymmbl  color gradient   use  represent  average report confidence  assignments   node  mgrast metarep  raw blast result  nod   color  average log  evalue  average percent identity also since phymmphymmbl  blast classifications   perform either  read  assemble contigs  script  import   tool allow  optional specification  magnitudes   classify sequence  script  also provide  generate magnitudes base  read per contig  assemblies   common ace file format  type  classifications   import  basic text file   excel template detail lineage  magnitude finally  xml file   import  gain complete control   chart include custom attribute  color   node since node attribute  contain html  hyperlinks xml import allow krona   deploy   custom data browse  extraction platform  addition   visualization tool visual design  krona display resemble  pie chart    subdivide separate class  sectors    embed hierarchy  sector  overlay  smaller sectors represent  children   squeeze toward  outside   chart  give  parent room  label    cause distortion     pie chart magnitudes  represent   angle   sector rather   area  example figure  show  oceanic metagenome  import  metarep  taxon ""gammaproteobacteria""  select   angle   highlight sector indicate  relative magnitude   node   case  classify sequence read  show   upper right corner  sector also surround smaller sectors  represent constituents  gammaproteobacteria   case  sum   constituent angle equal  angle   parent indicate   assignments  make directly  gammaproteobacteria  assignments   make   internal node  angular sweep would  wider   sum   children' clearly show   summary   assign amount  relation    figure  figure2  krona rsf display  bacterioplankton metagenome   vertical profile   north pacific subtropical gyre   import  metarep  display use krona taxonomy nod  show  nest sectors arrange   top level   hierarchy   center  progress outward navigational control    top leave  detail   select node    top right  chart  zoom  place  domain ""bacteria""   root   taxon ""gammaproteobacteria""  show select  interactive version   chart  available   krona website full size image  common criticism  rsf display   difficulty  compare similarly size nod  make comparisons easier krona sort nod  decrease magnitude  respect   siblings  addition  nod   color use  novel algorithm  work   sort  visually emphasize  hierarchy  quantity  algorithm   enable  default use  huesaturationlightness hsl color model  allow procedural color   adapt  different datasets first  hue spectrum  divide among  immediate children   current root node    children  turn subdivide  hue range among  children use  magnitudes  weight color  sort node   minimum   hue range cause recursive inheritance  node hue   largest child   generation  result  visual consistency  lineages   quantitatively skew toward particular branch  distinguish  generation without disrupt  consistency  lightness aspect   hsl model  increase  relative hierarchical depth  saturation remain constant spatial efficiency metagenomic hierarchies  easily become  complex   nod   discernibly apportion  label   computer screen although krona ameliorate  problem  interactive zoom  also offer several modifications  rsf display  maximize  amount  information contain   view first radixtree compression  use  collapse linear subgraphs   hierarchy simplify  chart without remove quantitative relationships linear subgraphs  represent multiple rank    classification occur  taxonomic classifications   sample  map onto  full taxonomy tree  example  homo sapiens    representative species   class mammalia  would typically  redundantly classify  primates hominids   rank  allow  classifications   view collapse   dynamically toggle  animation depict  transition  additional simplification  complex tree  taxonomy   prune  summarize  data   specify depth figure   example show  ncbi taxonomy summarize   maximum depth   level   linear subgraphs collapse second since deeper taxonomical level  often   interest  genus  species classifications krona allow significant quantities   level   view  direct relation   root   hierarchy   accomplish  dynamically reduce  label area  intermediate classifications remove  label  necessary compression  increase move outward   center  ensure   highest level   current view  also  label  intermediate level    compress  always  see  clearly  zoom finally krona' label algorithms greatly increase textual information density compare   rsf implementations space  use efficiently  orient leaf node label along radii  internal node label along tangents internal label use stepwise position  collisionbased shorten  display  much text  possible  avoid overlap polarcoordinate zoom  radial spacefilling display recursively subdivide angle  shape   nod approach rectangles  hierarchical depth increase   node magnitudes decrease thus zoom small nod  simply scale  entire figure  cartesian coordinate space would result   loss   angular aspect  make rsf display intuitive  spaceefficient  increase  capacity   display without cause  problem krona use  polar coordinate space  zoom   accomplish  increase  angular sweep  radius   zoom node   occupy   circle   original overview  angular sweep  surround nod  decrease simultaneously create  animate ""fisheye"" effect  animation ensure user cognition   change  context   final zoom view retain  entire capacity   original zoom    repeat   node  children provide informative view  even  deepest level   complex hierarchy zoom   traverse   hierarchy   accomplish similarly  click ancestral nod   show   center   plot   summary pie chart next   plot  trigger  reverse   fisheye animation compress  current node  reveal  position   new broader context multidimensional data  visualize secondary attribute  addition  magnitude individual nod  krona may  color  variable  categorical variables users may define  color  every node   xml  quantitative variables  gradient may  define   color  node  value  example    show  figure    node  color   quantitative redgreen gradient represent classification confidence figure  figure3 color  classification confidence human gut sample mh0072   metahit project   classify use phymmbl  display use krona abundance   simultaneously visualize   accessory attribute  link   hue   example hue  use  display classification confidence  report  phymmbl  average confidence value   node  color  low red  high green distinguish uncertain  certain classifications  interactive version   chart  available   krona website full size image additionally metagenomic data  often generate  discrete point across multiple locations  time krona  able  store  data  multiple sample   single document individual sample may   step    zoom level use  navigation interface   top leave  example  figure  krona  display one  seven depth sample   oceanic water column advance   sample progress  sample  greater  greater depths  transition  sample  animate use  polar ""tween"" effect emphasize  difference  sample  result   style  navigation   series  move picture   taxa dynamically grow  shrink  sample  samplein  case  sample descend  water column  approach  eyecatching    sample  direct comparison  many sample simultaneously  difficult  radial chart analysis across many sample  better leave  traditional heatmap  differential barchart visualizations",11
214,iTOL,"Interactive Tree Of Life (iTOL): an online tool for phylogenetic tree display and annotation 
iTOL's functions provide an easy way to manipulate and create customized graphical representations of phylogenetic trees in the standard ‘New Hampshire’ or Newick format. In addition to the standard tree representation, trees can be displayed in a circular (radial) mode (Fig. 1), which is particularly useful for the visualization of mid-sized trees (up to several thousand leaves). Circular trees can be rotated and displayed in user-defined arc sizes. Fig. 1 iTOL's user interface and example trees. The interface with a tree of life (Ciccarelli et al., 2006), annotated with genome sizes (blue bars), is shown. Several branches are collapsed and displayed as triangles. Popup windows give detailed information on branches/leaves, such as bootstrap values and taxonomical categories. (a) A 200 species tree displayed in a 180° arc with a stacked bar dataset. (b) A protein kinases tree displayed without branch length information and annotated with predicted site counts for various families. Open in new tabDownload slide iTOL's user interface and example trees. The interface with a tree of life (Ciccarelli et al., 2006), annotated with genome sizes (blue bars), is shown. Several branches are collapsed and displayed as triangles. Popup windows give detailed information on branches/leaves, such as bootstrap values and taxonomical categories. (a) A 200 species tree displayed in a 180° arc with a stacked bar dataset. (b) A protein kinases tree displayed without branch length information and annotated with predicted site counts for various families. Several functions are available, which allow users to customize their tree displays in various ways. Branches can be pruned or collapsed and any node can be used to re-root the tree. Colors can be assigned to various leaf groups, simplifying the navigation around the tree. For trees whose leaf IDs are based on NCBI Taxonomy (Wheeler et al., 2006), iTOL can automatically determine taxonomic classes of all internal nodes and assign proper scientific names to the leafs. Internal node labels are displayed in popup windows, which appear when mouse pointer is positioned above the corresponding node (Fig. 1). 2.1 Pruning and collapsing branches Pruning is a process of selecting one or several branches from the original tree and creating a new, smaller tree. Sub-branches and individual leaves can be easily added or removed. Branches whose detailed structure is not needed can be collapsed. Collapsed branches are displayed as triangles. Total branch lengths to the closest and the farthest leaf are used to calculate the lengths of the triangle's sides. 2.2 Displaying external data on a tree iTOL can display several types of data directly on the tree (Fig. 1). Up to five datasets can be uploaded with each tree. Datasets are contained in plain text files with each line corresponding to one leaf in the tree. Supported dataset types are: binary data, simple bars and stacked bars (multiple values associated with each leaf). iTOL is the first visualization tool that supports the display of horizontal gene transfers (HGTs) annotated directly in the original Newick tree file. HGT information is encoded in the IDs of tree nodes. Therefore, in addition to standard leaf IDs, the tree must have unique IDs assigned to all internal nodes. Detailed explanation with example trees and datasets is available in the iTOL online help pages. 2.3 Exporting trees to other formats Each tree display in iTOL (either default or customized) can be exported to several graphical formats, both bitmap and vector based. Currently supported formats are Scalable Vector Graphics (svg), Portable Network Graphics (png), Encapsulated Postscript (eps), Postscript (ps) and Portable Document Format (pdf). In addition, pruned trees can be exported as Newick plain text files. 3 IMPLEMENTATION iTOL is a WWW-based tool, accessible using any modern web browser. It is coded in Shockwave Flash and JavaScript. The Flash object, which displays the tree and provides interactivity, is created dynamically by the server using a set of Perl scripts and the Ming library (Author Webpage). Flash plug-in version 7 or higher is required for full functionality. Tree export is done through an intermediary svg file, which is converted to other formats using the Inkscape graphics package (Author Webpage). User-uploaded trees and data are stored locally on the server and available for at least one year after original submission.",Visualization,"interactive tree  life itol  online tool  phylogenetic tree display  annotation 
itol' function provide  easy way  manipulate  create customize graphical representations  phylogenetic tree   standard new hampshire  newick format  addition   standard tree representation tree   display   circular radial mode fig    particularly useful   visualization  midsized tree   several thousand leave circular tree   rotate  display  userdefined arc size fig  itol' user interface  example tree  interface   tree  life ciccarelli    annotate  genome size blue bar  show several branch  collapse  display  triangles popup windows give detail information  branchesleaves   bootstrap value  taxonomical categories    species tree display   ° arc   stack bar dataset   protein kinases tree display without branch length information  annotate  predict site count  various families open  new tabdownload slide itol' user interface  example tree  interface   tree  life ciccarelli    annotate  genome size blue bar  show several branch  collapse  display  triangles popup windows give detail information  branchesleaves   bootstrap value  taxonomical categories    species tree display   ° arc   stack bar dataset   protein kinases tree display without branch length information  annotate  predict site count  various families several function  available  allow users  customize  tree display  various ways branch   prune  collapse   node   use  reroot  tree color   assign  various leaf group simplify  navigation around  tree  tree whose leaf ids  base  ncbi taxonomy wheeler    itol  automatically determine taxonomic class   internal nod  assign proper scientific name   leaf internal node label  display  popup windows  appear  mouse pointer  position   correspond node fig   prune  collapse branch prune   process  select one  several branch   original tree  create  new smaller tree subbranches  individual leave   easily add  remove branch whose detail structure   need   collapse collapse branch  display  triangles total branch lengths   closest   farthest leaf  use  calculate  lengths   triangle' side  display external data   tree itol  display several type  data directly   tree fig    five datasets   upload   tree datasets  contain  plain text file   line correspond  one leaf   tree support dataset type  binary data simple bar  stack bar multiple value associate   leaf itol   first visualization tool  support  display  horizontal gene transfer hgts annotate directly   original newick tree file hgt information  encode   ids  tree nod therefore  addition  standard leaf ids  tree must  unique ids assign   internal nod detail explanation  example tree  datasets  available   itol online help page  export tree   format  tree display  itol either default  customize   export  several graphical format  bitmap  vector base currently support format  scalable vector graphics svg portable network graphics png encapsulate postscript eps postscript   portable document format pdf  addition prune tree   export  newick plain text file  implementation itol   wwwbased tool accessible use  modern web browser   cod  shockwave flash  javascript  flash object  display  tree  provide interactivity  create dynamically   server use  set  perl script   ming library author webpage flash plugin version   higher  require  full functionality tree export     intermediary svg file   convert   format use  inkscape graphics package author webpage useruploaded tree  data  store locally   server  available   least one year  original submission",11
215,REAPR,"REAPR: a universal tool for genome assembly evaluation
Read mapping The read mapper SMALT [21] was used in all examples to map sequencing reads to assemblies. The entire command lines used are given in Additional file 1, but we note that the -x option was always used, so that each read in a mate pair was independently mapped thereby avoiding the false placement of a read near to its mate, instead of elsewhere with a better alignment. The -r option was also always used to randomly place reads which map repetitively, to prevent all repetitive regions of the reference sequence from having zero read coverage. After mapping, duplicate read-pairs were marked using the MarkDuplicates function of Picard version 1.47 [22]. REAPR pipeline The assembly analysis algorithm was implemented in a tool called REAPR: 'recognition of errors in assembly using paired reads'. The pipeline is simple to run, requiring as input an assembly in FASTA format and read pairs in FASTQ format. Alternatively, the user can map the reads to the assembly and provide a BAM file [23]. The steps in the pipeline are outlined in Figure ​Figure11 and described below (see Additional file 1 for full details of each stage). Initially, input to the REAPR pipeline must be generated, starting with the unique and perfectly aligned read coverage of a high quality set of paired reads. For small genomes (<100 MB), this is calculated using the extremely fast but high memory tool SNP-o-matic [24]. For large genomes, the coverage is extracted from a BAM file of reads mapped using SMALT. This perfect and unique mapping information, together with a BAM file of the larger insert size reads mapped to the genome, is used as input to the REAPR pipeline. REAPR version 1.0.11 was used in all cases, with the default parameters. The pipeline begins with a pre-processing step that estimates various statistics, such as average fragment length and depth of coverage, using a sample of the genome. In particular, GC bias is accounted for by calculating the expected fragment coverage at any given value of GC content. This correction to the fragment coverage is applied in subsequent stages of the pipeline. The method used is to take a LOWESS line through a scatter plot of fragment coverage versus GC content (see Additional file 1, Figure S3d). The next stage calculates statistics at each base of the assembly, using the information in the input BAM file and the perfect and uniquely mapped read depth. These statistics are used to call errors in the assembly and to score each base of the assembly. We shall use 'inner fragment' to mean the inner mate pair distance or, equivalently, a fragment without including the reads (see Additional file 1 Figure S2a). The metrics calculated are read depth and type of read coverage, inner fragment coverage, error in inner fragment coverage (corrected for GC content), FCD error and amount of soft clipping. The metrics are explained in more detail below and in Additional file 1. Recall that the FCD error at each base of an assembly is taken to be the area between the observed and ideal fragment coverage distributions (see Figure ​Figure1c).1c). It is normalized for both fragment depth and mean insert size so that results are comparable for data from different libraries. A correction is made for the presence of the nearest gap, if it lies within one insert size of the base of interest (see Additional file 1). If a base has zero fragment coverage then this metric cannot be used and the assumption is that the assembly is incorrect. The exception to this is where a gap has length longer than half the average insert size, in which case it is impossible to determine if this scaffolding is correct and therefore no further analysis is performed. In addition to the absolute count of read coverage, the type of read coverage is considered. At each base, and for each strand, the proportion of reads of the following types is calculated: proper read pairs, defined to be in the correct orientation and insert size, which should be in the majority if the genome is correct; orphaned reads, whereby a read's mate is either unmapped or mapped to a different chromosome; reads with the correct orientation but wrong insert size; and read pairs with an incorrect orientation. Most read mapping tools are capable of soft-clipping reads, where most of a read is aligned to the genome, but a few bases at either end of the read do not match. In this case the read is still reported as mapped, but the mismatching bases are not considered as part of the alignment and designated as soft-clipped (Additional file 1, Figure S2c). At each base, the number of alignments is counted that start or end at that base due to a soft-clipped read. In order to call assembly errors from a given metric, a minimum window length is considered and appropriate minimum and maximum values. Any region of length no smaller than the window length and with at least 80% of the bases falling outside the acceptable range is reported. For example, a collapsed repeat is called if the relative error in fragment coverage is at least two for 80% of the bases in a stretch of at least 100bp. The default choice of parameter for each metric is described in the Additional file 1. In the actual implementation, the user can choose all parameters. As described earlier, each base scores one if it is covered by at least five perfect and uniquely mapped reads, and the FCD error is acceptable. If either of these tests fail, then the score is set to the number of tests that pass (considering all per-base metrics) scaled from zero to one, that is, a base scores zero if every test fails. The FCD error cutoff is chosen by sampling windows from the genome, then for each window the cutoff in FCD error needed to call that window as an error is calculated. In other words, for each window we find the value c such that 80% of the values in that window are greater than c. The proportion of failed windows as a function of cutoff value is plotted (Figure ​(Figure2).2). The cutoff value for the FCD error is chosen to be the first value found, working from largest to smallest, such that the magnitude of the first and second derivatives (normalized to have a maximum magnitude of 1) of the plot are both at least 0.05. REAPR output REAPR reports assembly errors and warnings in a GFF file, compatible with most genome viewers such as Artemis [25]. Regions with a high FCD error or low fragment coverage are reported as an error, whereas regions that fail any other tests are output as warnings for manual inspection. A summary spreadsheet is produced containing error counts, broken down in to each type of error, for each contig and for the whole assembly. REAPR also produces a new assembly based on the error calls by breaking the genome wherever an error is called over a gap. Error regions within contigs are replaced with Ns, enabling them to be accurately reassembled locally by a gap closing tool [26,27]. A second run of REAPR can be performed after gap closing to verify any new sequenced added to the assembly. REAPR also generates plot files, compatible with Artemis, of all the statistics examined at each base for easy visualisation (see Additional file 1, Figure S7 for an example). De novo assemblies The de novo assemblies of S. aureus and P. falciparum were produced using similar methods (see Additional file 1 for full details). Short insert Illumina reads were assembled using Velvet [28] version 1.2.03. These assemblies were scaffolded iteratively with SSPACE [29] version 2 using the short insert reads, followed by further rounds of scaffolding with larger insert reads, where available. Assembly analysis Manual comparison between the de novo assemblies and reference genomes of S. aureus and P. falciparum were performed using ACT [20]. BLAST hits between the sequences were generated for viewing in ACT using blastall version 2.2.15 with the settings -p blastn -W 25 -F T -m 8 -e 1e-20. When counting scaffolding error calls in S. aureus, the Velvet assembly was found to contain three problematic regions, with many gaps and errors due to repetitive sequences. Each of these regions was counted as one scaffolding error for the purpose of calculating REAPR's performance at error calling. The read sets used for P. falciparum assemblies were Illumina 500bp insert, Illumina 3 kb insert and 454 8 kb insert reads. The short insert Illumina reads were used to generate perfect and uniquely mapped read depth, and also to call collapsed repeats. All other errors were identified using the 454 reads. Perfectly mapped and unique read depth was generated for the C. elegans genome (WS228) using three Illumina lanes combined and the larger insert size dataset comprised four combined Illumina lanes. Prior to mapping the latter reads, inner adaptor sequences were removed using in-house scripts based on SSAHA2 [30], retaining read pairs where each mate of the pair had a length of at least 35bp. PCR primers were designed to amplify the top 20 FCD error regions using AcePrimer 1.3 [31]. High coverage Illumina data [32] were used to analyse the human and mouse reference genomes. For each organism, the dataset comprised short insert data and more than one 2-3 kb insert 'jumping' library. The short insert data were used to compute the perfect and uniquely mapped read depth and the 2-3kb libraries were combined to obtain enough coverage for analysis with REAPR.",AssemblyEvaluation,"reapr  universal tool  genome assembly evaluation
read map  read mapper smalt   use   examples  map sequence read  assemblies  entire command line use  give  additional file    note    option  always use    read   mate pair  independently map thereby avoid  false placement   read near   mate instead  elsewhere   better alignment   option  also always use  randomly place read  map repetitively  prevent  repetitive regions   reference sequence   zero read coverage  map duplicate readpairs  mark use  markduplicates function  picard version   reapr pipeline  assembly analysis algorithm  implement   tool call reapr 'recognition  errors  assembly use pair reads'  pipeline  simple  run require  input  assembly  fasta format  read pair  fastq format alternatively  user  map  read   assembly  provide  bam file   step   pipeline  outline  figure ​figure11  describe  see additional file   full detail   stage initially input   reapr pipeline must  generate start   unique  perfectly align read coverage   high quality set  pair read  small genomes     calculate use  extremely fast  high memory tool snpomatic   large genomes  coverage  extract   bam file  read map use smalt  perfect  unique map information together   bam file   larger insert size read map   genome  use  input   reapr pipeline reapr version   use   case   default parameters  pipeline begin   preprocessing step  estimate various statistics   average fragment length  depth  coverage use  sample   genome  particular  bias  account   calculate  expect fragment coverage   give value   content  correction   fragment coverage  apply  subsequent stag   pipeline  method use   take  low line   scatter plot  fragment coverage versus  content see additional file  figure s3d  next stage calculate statistics   base   assembly use  information   input bam file   perfect  uniquely map read depth  statistics  use  call errors   assembly   score  base   assembly  shall use 'inner fragment'  mean  inner mate pair distance  equivalently  fragment without include  read see additional file  figure s2a  metrics calculate  read depth  type  read coverage inner fragment coverage error  inner fragment coverage correct   content fcd error  amount  soft clip  metrics  explain   detail    additional file  recall   fcd error   base   assembly  take    area   observe  ideal fragment coverage distributions see figure ​figure1c1c   normalize   fragment depth  mean insert size   result  comparable  data  different libraries  correction  make   presence   nearest gap   lie within one insert size   base  interest see additional file    base  zero fragment coverage   metric cannot  use   assumption    assembly  incorrect  exception      gap  length longer  half  average insert size   case   impossible  determine   scaffold  correct  therefore   analysis  perform  addition   absolute count  read coverage  type  read coverage  consider   base    strand  proportion  read   follow type  calculate proper read pair define     correct orientation  insert size      majority   genome  correct orphan read whereby  read' mate  either unmapped  map   different chromosome read   correct orientation  wrong insert size  read pair   incorrect orientation  read map tool  capable  softclipping read     read  align   genome    base  either end   read   match   case  read  still report  map   mismatch base   consider  part   alignment  designate  softclipped additional file  figure s2c   base  number  alignments  count  start  end   base due   softclipped read  order  call assembly errors   give metric  minimum window length  consider  appropriate minimum  maximum value  region  length  smaller   window length    least    base fall outside  acceptable range  report  example  collapse repeat  call   relative error  fragment coverage   least two     base   stretch   least 100bp  default choice  parameter   metric  describe   additional file    actual implementation  user  choose  parameters  describe earlier  base score one    cover   least five perfect  uniquely map read   fcd error  acceptable  either   test fail   score  set   number  test  pass consider  perbase metrics scale  zero  one    base score zero  every test fail  fcd error cutoff  choose  sample windows   genome    window  cutoff  fcd error need  call  window   error  calculate   word   window  find  value       value   window  greater    proportion  fail windows   function  cutoff value  plot figure ​figure2  cutoff value   fcd error  choose    first value find work  largest  smallest    magnitude   first  second derivatives normalize    maximum magnitude     plot    least  reapr output reapr report assembly errors  warn   gff file compatible   genome viewers   artemis  regions   high fcd error  low fragment coverage  report   error whereas regions  fail   test  output  warn  manual inspection  summary spreadsheet  produce contain error count break     type  error   contig    whole assembly reapr also produce  new assembly base   error call  break  genome wherever  error  call   gap error regions within contigs  replace   enable    accurately reassemble locally   gap close tool   second run  reapr   perform  gap close  verify  new sequence add   assembly reapr also generate plot file compatible  artemis    statistics examine   base  easy visualisation see additional file  figure    example  novo assemblies   novo assemblies   aureus   falciparum  produce use similar methods see additional file   full detail short insert illumina read  assemble use velvet  version   assemblies  scaffold iteratively  sspace  version  use  short insert read follow   round  scaffold  larger insert read  available assembly analysis manual comparison    novo assemblies  reference genomes   aureus   falciparum  perform use act  blast hit   sequence  generate  view  act use blastall version    settings  blastn          count scaffold error call   aureus  velvet assembly  find  contain three problematic regions  many gap  errors due  repetitive sequence    regions  count  one scaffold error   purpose  calculate reapr' performance  error call  read set use   falciparum assemblies  illumina 500bp insert illumina   insert     insert read  short insert illumina read  use  generate perfect  uniquely map read depth  also  call collapse repeat   errors  identify use   read perfectly map  unique read depth  generate    elegans genome ws228 use three illumina lanes combine   larger insert size dataset comprise four combine illumina lanes prior  map  latter read inner adaptor sequence  remove use inhouse script base  ssaha2  retain read pair   mate   pair   length   least 35bp pcr primers  design  amplify  top  fcd error regions use aceprimer   high coverage illumina data   use  analyse  human  mouse reference genomes   organism  dataset comprise short insert data    one   insert 'jumping' library  short insert data  use  compute  perfect  uniquely map read depth   3kb libraries  combine  obtain enough coverage  analysis  reapr",12
216,QUAST-LG,"Versatile genome assembly evaluation with QUAST-LG
Upper bound assembly construction We construct upper bound assembly based on a reference genome and a given set of reads. At first, the construction algorithm maps all reads to the reference genome and detects zero-coverage regions (Fig. 1a). We use Minimap2 (Li, 2017) for aligning long error-prone reads (PacBio and Nanopore) and BWA-MEM (Li, 2013) for short Illumina reads (paired-ends and mate-pairs). Fig. 1. Upper bound assembly construction. (a) All available reads (brown for long reads, orange for mate-pairs, and yellow for paired-ends) are mapped to the reference (gray) to compute zero-coverage genomic regions. Repeat sequences (red) are detected using repeat finder software. Non-repetitive covered fragments are reported as upper bound contigs. (b) The overlaps between the contigs (green), and either long or mate-pair reads are detected, and contigs are further joined to form upper bound scaffolds. (c) The gaps between adjacent contigs within a scaffold are filled either with reference sequence (for covered regions) or with stretches of N nucleotides (for coverage gaps). Unresolved repeats are added as separate sequences Open in new tabDownload slide Upper bound assembly construction. (a) All available reads (brown for long reads, orange for mate-pairs, and yellow for paired-ends) are mapped to the reference (gray) to compute zero-coverage genomic regions. Repeat sequences (red) are detected using repeat finder software. Non-repetitive covered fragments are reported as upper bound contigs. (b) The overlaps between the contigs (green), and either long or mate-pair reads are detected, and contigs are further joined to form upper bound scaffolds. (c) The gaps between adjacent contigs within a scaffold are filled either with reference sequence (for covered regions) or with stretches of N nucleotides (for coverage gaps). Unresolved repeats are added as separate sequences Further on, the lightweight Red (Girgis, 2015) de novo repeat finder is used to mark long genomic repeats in the reference (Fig. 1a). We call a repeat long if its length exceeds the median insert size of a paired-end library (when several paired-end libraries are available, the maximum median value is used). Among the detected repeated sequences, we select only those that occur at least twice in remote parts of the genome. Such long repeats cause ambiguities during the assembly, which may be resolved only by long reads or mate-pairs. Other long regions marked by Red appear to be short tandem repeats having multiple copies at the same genomic loci. To the best of our knowledge, such tandem repeats do not cause ambiguities and can be approximately resolved by the assemblers without using long-range information [e.g. using de Bruijn graph topology (Miller et al., 2010)]. Splitting the reference sequence by coverage gaps and long repeats results in a set of unique genomic fragments referred to as upper bound contigs, that however do not reflect the best possible assembly of the entire dataset. To achieve a more realistic upper bound, we detect the contigs that are connected by long reads or mate-pairs and further join them into upper bound scaffolds if the number of connections exceeds a small threshold n (Fig. 1b). In this study we used n = 1 for long reads and n = 2 for mate-pairs. We say that a long read connects two contigs if it simply overlaps with both contigs. A pair of reads connects contigs if the left read overlaps with the first contig and the right read overlaps with the second contig. During this analysis we ignore read pairs that map inconsistently or with abnormal insert size (in the first or the last decile). To enable efficient overlap detection between reads and upper bound contigs, we sort all reads according to their mapping positions. Thus, the scaffold construction algorithm requires O(NlogN) time for read sorting and O(N) time for finding overlaps, where N is the total number of long and mate-pair reads used for scaffolding. Once upper bound contigs are joined into scaffolds, the gaps between adjacent contigs are filled with the corresponding genomic sequences from the reference genome, or—in case of coverage gaps—with stretches of N’s (Fig. 1c). Remaining unresolved repeats are added to the final upper bound assembly as separate sequences. 2.2 Adaption of conventional metrics for large genomes The key characteristics of the assembly quality are the assembly completeness (what fraction of the genome is assembled), correctness (how many errors the assembly contains) and contiguity (how many fragments the assembly consists of and how long they are). Both completeness and correctness can be accurately measured by QUAST-LG only when a high-quality reference genome is available. Some contiguity statistics, such as the well-known N50 metric, do not require a reference. However, when an estimate of the genome size is known, their more suitable analogues can be computed, namely NG50. If a reference sequence is available, we provide even more relevant insight by computing NGA50-like statistics (Gurevich et al., 2013), the contiguity measures based on error-free aligned assembly fragments rather than the initial contigs/scaffolds. The alignment against the reference genome appears to be the most time consuming step in the assembly evaluation, especially for large genomes. To address this bottleneck, we replaced an accurate and slow NUCmer aligner [from MUMmer v3.23 package (Kurtz et al., 2004)] used in original QUAST with a faster Minimap2 aligner (Li, 2017). The recently released MUMmer 4 package (Marcais et al., 2018) was also outperformed by Minimap2 in our benchmark experiments, albeit the speed increase in this case was not as substantial as the Minimap2’s improvement over the previous MUMmer version. We have thoroughly chosen Minimap2 options in order to maintain the alignment speed-accuracy ratio for different scenarios. In standard mode QUAST-LG runs alignment with the parameters enabling accuracy comparable with NUCmer which is suitable for small genomes. In ‘‐‐large’ mode QUAST-LG configures Minimap2 to achieve adequate running times for large and complex inputs. The assembly correctness is usually characterized by the number of large assembly errors, so-called misassemblies. Gurevich et al. (2013) define a misassembly breakpoint as a position in an assembled contig where the flanking sequences align to opposite strands (inversion), or to different chromosomes (translocation), or the inconsistency size δ (a gap or an overlap) between the alignments of the left and right flanking sequences on the reference is more than a predefined breakpoint threshold X (relocation). The alignments on the same strand of the same chromosome and having δ<X are considered as small errors and classified as local misassembly. Eukaryotic genomes usually contain a lot of transposable elements (TEs) which may cause discrepancies between the reference genome and the genome actually being assembled. These short variations result in a huge number of false positive misassemblies if computed according to the definition given above. To distinguish between true misassemblies and the ones caused by TEs, QUAST-LG performs an additional check of each relocation and inversion to identify possible TEs (Fig. 2). The identification procedure depends on the size of the breakpoint threshold X which optimal value should slightly exceed the length of the largest TE in the genome (the processing of tandem TE insertions and deletions is out of scope of this paper). The optimal value thus depends on the subject organism and we allow users to set it manually. For the sake of consistency, we used the same X = 7 kb in all benchmark experiments in this study (see Supplementary Methods for details on the value choice). This is also the default value in QUAST-LG in contrast to regular QUAST which uses X = 1 kb. Fig. 2. Detection of discrepancies caused by TEs. On each subfigure, we plot the reference genome R (top), the contig C (bottom), their matching fragments (blue and green bars for the positions in C and R, respectively) and locations of TEs (violet bars) causing discrepancies in the mapping. The inconsistencies in the alignments are shown by arrows and δ characters. (a) TE is present in R and missing in C. Since δ here is equal to the TE’s length, a specifically chosen breakpoint threshold X transforms classification of this discrepancy from a relocation to a local misassembly (X>δ). (b) TE is located inside C but its position in R is significantly away from the rest of C mappings and could also be located on the opposite strand. Original QUAST would treat this situation as two misassembly breakpoints (relocations or inversions) because δ1 and δ2 are usually much higher than X. In contrast, QUAST-LG classifies such pattern as possible TE since it computes δ=δ2−δ1, that is again equal to the TE’s length and could be prevailed by appropriate X. (c) TE is the first or the last alignment fragment in C, while its location on R is large distance δ away from the neighboring C fragment. QUAST-LG cannot reliably distinguish this situation from a real relocation/inversion: it would need to be able to recognize TE based on its genomic sequence, which is out of scope of this paper Open in new tabDownload slide Detection of discrepancies caused by TEs. On each subfigure, we plot the reference genome R (top), the contig C (bottom), their matching fragments (blue and green bars for the positions in C and R, respectively) and locations of TEs (violet bars) causing discrepancies in the mapping. The inconsistencies in the alignments are shown by arrows and δ characters. (a) TE is present in R and missing in C. Since δ here is equal to the TE’s length, a specifically chosen breakpoint threshold X transforms classification of this discrepancy from a relocation to a local misassembly (⁠X>δ⁠). (b) TE is located inside C but its position in R is significantly away from the rest of C mappings and could also be located on the opposite strand. Original QUAST would treat this situation as two misassembly breakpoints (relocations or inversions) because δ1 and δ2 are usually much higher than X. In contrast, QUAST-LG classifies such pattern as possible TE since it computes δ=δ2−δ1⁠, that is again equal to the TE’s length and could be prevailed by appropriate X. (c) TE is the first or the last alignment fragment in C, while its location on R is large distance δ away from the neighboring C fragment. QUAST-LG cannot reliably distinguish this situation from a real relocation/inversion: it would need to be able to recognize TE based on its genomic sequence, which is out of scope of this paper 2.3 Best set of alignments selection Long contigs are rarely mapped to the reference perfectly as a single unambiguous alignment. An alignment software typically reports multiple alignment fragments from different locations of the genome. This may happen due to the presence of genomic repeats and TEs in the reference genome and in some cases because of algorithmic issues in the assembly or/and alignment software. QUAST-LG attempts to accurately assess each contig and select a set of non-overlapping alignments that maximizes the total alignment score, which is defined as a function of the alignment lengths, mapping qualities and side by side inconsistencies (misassemblies). This problem is known as the collinear chaining problem (Myers and Miller, 1995) and it is usually solved by sequence aligners for a low-level chaining, that is joining short matching seeds into larger alignment fragments. For example, MUMmer (Kurtz et al., 2004) combines maximal unique matches and Minimap2 (Li, 2017) chains minimizers (Roberts et al., 2004). Here we implement a dynamic programming algorithm called BestSetSelection for a high-level chaining, that is combining alignment fragments (see Supplementary Methods). Our algorithm is conceptually similar to delta-filter utility from MUMmer package (Kurtz et al., 2004) but our approach includes a comprehensive set of penalties for various misassembly events. This feature allows BestSetSelection to correctly resolve many complex sets of alignments, which are typical for eukaryotic assemblies, and produce a more accurate chaining than delta-filter in our benchmark experiments. BestSetSelection is a quadratic algorithm with respect to the number of fragment alignments per contig which is usually fine since this number is generally small (up to 100). However, this may cause a significant slowdown in case of large genomes evaluation when the number of alignments may reach dozens of thousands in some contigs. Although there are chaining algorithms with sub-quadratic time complexity (Abouelhoda and Ohlebusch, 2005), they are not applicable to our gap cost function and associated with a large constant. Instead, we have implemented a simple heuristic that always finds the best alignment set or one of the several sets that maximize the score (Supplementary Methods). And even though the heuristic idea does not guarantee the speed up in theory, it significantly dropped the running time of all six benchmark dataset evaluations. 2.4 K-mer-based quality metrics As shown above, the presence of many TEs and other specific features of eukaryotic genomes significantly complicates assembly evaluation. Although QUAST-LG adjustment of the conventional completeness and correctness measures improve the assessment, it may still not be good enough to form the complete picture of eukaryote assembly quality. Here we propose to assess assemblies using a completely different strategy inspired by the evaluation procedures in Putnam et al. (2016) and Chapman et al. (2016) and generalized for an arbitrary genome analysis in QUAST-LG. This strategy is based on the analysis of unique k-mers (non-repeated sequences of length k) both in the reference genome and in the assembly. If k value is sufficiently large (QUAST-LG uses 101-mers by default), unique k-mers appear to be widespread across the genome. For instance, the fruit fly genome contains 122 millions unique 101-mers out of 137 millions total 101-mers. The existence and the positions of such k-mers in the assembly describe its completeness and correctness. We use KMC 3 (Kokot et al., 2017) to detect all unique k-mers in the reference genome. The percentage of these k-mers detected in the assembly is reported as its k-mer completeness. Compared to the genome fraction completeness measure, the k-mer-based value accounts for per-base quality of an assembly which is usually highly important for the downstream analysis such as genome annotation. The benchmarking below shows that assemblies with a very similar genome fraction may have completely different k-mer completeness due to a high mismatch and indel error rates. The k-mer-based correctness is calculated based on a small uniformly distributed subset of all unique k-mers in order to speed up the computation. We select the subset in a way that any two k-mers from the subset are at least 1 kb apart from each other in the reference genome R. The subset is provided to KMC and it identifies contigs having at least two k-mers. The contig position of each detected k-mer is examined and we refer to a consecutive list of k-mers k1,k2,…,kn (where n≥2⁠) in a contig C as a marker if for any i∈[1,n−1] the distances between ki and ki+1 in C and R are equivalent within a small error (5% of the distance in R by default). We further process contigs having at least two markers to check whether the relative positions of adjacent markers mj and mj+1 correlate with their locations in R. QUAST-LG reports a k-mer-based translocation breakpoint if mj and mj+1 are originated from different chromosomes and a k-mer-based relocation if the markers are from the same chromosome but the inconsistency between their positions in C and R is larger than a predefined threshold (we use 100 kb threshold by default). We further refer to k-mer-based translocations and relocations as k-mer-based misjoins to exclude confusion with regular QUAST misassemblies. K-mer-based misjoins are essentially similar to the regular misassembly metrics, except that they are focused on the most critical assembly errors. The key benefit of these measures is in their tolerance to inconsistencies caused by TEs, since TEs mostly correspond to genomic repeats and thus lack unique k-mers. For example, k-mer-based relocations can successfully resolve the situations when a contig starts or ends with a TE which cause an ambiguity in the regular misassembly detection algorithm (Fig. 2c). 2.5 Evaluation without a reference genome In most real assembly projects a reference genome sequence is not available and the assembly quality assessment must rely on other sources of information. The primary purpose of QUAST-LG is the reference-based analysis but we also include a few de novo eukaryote-targeted completeness measures to make our tool useful in a wider set of applications. QUAST-LG is supplied with GeneMark-ES (Lomsadze et al., 2005) software for de novo gene prediction in eukaryotic genomes. However, despite the relevance of the gene finding in assessing downstream analysis perspectives its heuristic nature may result in a misleading output in some experiments. For instance, an assembly may contain multiple copies of one gene which will be reported several times. To counter this, we additionally use BUSCO (Simao et al., 2015) to find the number of assembled conserved genes that are present nearly universally in eukaryotes in a single copy. To demonstrate how BUSCO completeness correlates with more accurate reference-based quality metrics, we added its computation in all our benchmark experiments. Note that reference-free correctness metrics are out of scope of QUAST-LG and we recommend using specialized de novo evaluation tools for this scenario. For instance, REAPR (Hunt et al., 2013) identifies likely assembly and scaffolding errors based on paired reads mapping. Another example is KAT (Mapleson et al., 2017) that compares the k-mer spectrum of the assembly to the k-mer spectrum of the reads, which is quite useful in identifying missing sequence, collapsed repeats and expanded sequences in the assembly",AssemblyEvaluation,"versatile genome assembly evaluation  quastlg
upper bind assembly construction  construct upper bind assembly base   reference genome   give set  read  first  construction algorithm map  read   reference genome  detect zerocoverage regions fig   use minimap2    align long errorprone read pacbio  nanopore  bwamem    short illumina read pairedends  matepairs fig  upper bind assembly construction   available read brown  long read orange  matepairs  yellow  pairedends  map   reference gray  compute zerocoverage genomic regions repeat sequence red  detect use repeat finder software nonrepetitive cover fragment  report  upper bind contigs   overlap   contigs green  either long  matepair read  detect  contigs   join  form upper bind scaffold   gap  adjacent contigs within  scaffold  fill either  reference sequence  cover regions   stretch   nucleotides  coverage gap unresolved repeat  add  separate sequence open  new tabdownload slide upper bind assembly construction   available read brown  long read orange  matepairs  yellow  pairedends  map   reference gray  compute zerocoverage genomic regions repeat sequence red  detect use repeat finder software nonrepetitive cover fragment  report  upper bind contigs   overlap   contigs green  either long  matepair read  detect  contigs   join  form upper bind scaffold   gap  adjacent contigs within  scaffold  fill either  reference sequence  cover regions   stretch   nucleotides  coverage gap unresolved repeat  add  separate sequence    lightweight red girgis   novo repeat finder  use  mark long genomic repeat   reference fig   call  repeat long   length exceed  median insert size   pairedend library  several pairedend libraries  available  maximum median value  use among  detect repeat sequence  select    occur  least twice  remote part   genome  long repeat cause ambiguities   assembly  may  resolve   long read  matepairs  long regions mark  red appear   short tandem repeat  multiple copy    genomic loci   best   knowledge  tandem repeat   cause ambiguities    approximately resolve   assemblers without use longrange information  use  bruijn graph topology miller    split  reference sequence  coverage gap  long repeat result   set  unique genomic fragment refer   upper bind contigs  however   reflect  best possible assembly   entire dataset  achieve   realistic upper bind  detect  contigs   connect  long read  matepairs   join   upper bind scaffold   number  connections exceed  small threshold  fig    study  use     long read      matepairs  say   long read connect two contigs   simply overlap   contigs  pair  read connect contigs   leave read overlap   first contig   right read overlap   second contig   analysis  ignore read pair  map inconsistently   abnormal insert size   first   last decile  enable efficient overlap detection  read  upper bind contigs  sort  read accord   map position thus  scaffold construction algorithm require onlogn time  read sort   time  find overlap     total number  long  matepair read use  scaffold  upper bind contigs  join  scaffold  gap  adjacent contigs  fill   correspond genomic sequence   reference genome — case  coverage gaps— stretch   fig  remain unresolved repeat  add   final upper bind assembly  separate sequence  adaption  conventional metrics  large genomes  key characteristics   assembly quality   assembly completeness  fraction   genome  assemble correctness  many errors  assembly contain  contiguity  many fragment  assembly consist    long    completeness  correctness   accurately measure  quastlg    highquality reference genome  available  contiguity statistics    wellknown n50 metric   require  reference however   estimate   genome size  know   suitable analogues   compute namely ng50   reference sequence  available  provide even  relevant insight  compute nga50like statistics gurevich     contiguity measure base  errorfree align assembly fragment rather   initial contigsscaffolds  alignment   reference genome appear     time consume step   assembly evaluation especially  large genomes  address  bottleneck  replace  accurate  slow nucmer aligner  mummer  package kurtz    use  original quast   faster minimap2 aligner    recently release mummer  package marcais     also outperform  minimap2   benchmark experiment albeit  speed increase   case    substantial   minimap2s improvement   previous mummer version   thoroughly choose minimap2 options  order  maintain  alignment speedaccuracy ratio  different scenarios  standard mode quastlg run alignment   parameters enable accuracy comparable  nucmer   suitable  small genomes  ‐‐large mode quastlg configure minimap2  achieve adequate run time  large  complex input  assembly correctness  usually characterize   number  large assembly errors socalled misassemblies gurevich    define  misassembly breakpoint   position   assemble contig   flank sequence align  opposite strand inversion   different chromosomes translocation   inconsistency size   gap   overlap   alignments   leave  right flank sequence   reference     predefined breakpoint threshold  relocation  alignments    strand    chromosome     consider  small errors  classify  local misassembly eukaryotic genomes usually contain  lot  transposable elements tes  may cause discrepancies   reference genome   genome actually  assemble  short variations result   huge number  false positive misassemblies  compute accord   definition give   distinguish  true misassemblies   ones cause  tes quastlg perform  additional check   relocation  inversion  identify possible tes fig   identification procedure depend   size   breakpoint threshold   optimal value  slightly exceed  length   largest    genome  process  tandem  insertions  deletions    scope   paper  optimal value thus depend   subject organism   allow users  set  manually   sake  consistency  use         benchmark experiment   study see supplementary methods  detail   value choice   also  default value  quastlg  contrast  regular quast  use     fig  detection  discrepancies cause  tes   subfigure  plot  reference genome  top  contig  bottom  match fragment blue  green bar   position     respectively  locations  tes violet bar cause discrepancies   map  inconsistencies   alignments  show  arrows   character    present    miss   since    equal   tes length  specifically choose breakpoint threshold  transform classification   discrepancy   relocation   local misassembly     locate inside    position    significantly away   rest   mappings  could also  locate   opposite strand original quast would treat  situation  two misassembly breakpoints relocations  inversions      usually much higher    contrast quastlg classify  pattern  possible  since  compute δδ2δ1    equal   tes length  could  prevail  appropriate      first   last alignment fragment     location    large distance  away   neighbor  fragment quastlg cannot reliably distinguish  situation   real relocationinversion  would need   able  recognize  base   genomic sequence     scope   paper open  new tabdownload slide detection  discrepancies cause  tes   subfigure  plot  reference genome  top  contig  bottom  match fragment blue  green bar   position     respectively  locations  tes violet bar cause discrepancies   map  inconsistencies   alignments  show  arrows   character    present    miss   since    equal   tes length  specifically choose breakpoint threshold  transform classification   discrepancy   relocation   local misassembly ⁠⁠    locate inside    position    significantly away   rest   mappings  could also  locate   opposite strand original quast would treat  situation  two misassembly breakpoints relocations  inversions      usually much higher    contrast quastlg classify  pattern  possible  since  compute δδ2δ1⁠    equal   tes length  could  prevail  appropriate      first   last alignment fragment     location    large distance  away   neighbor  fragment quastlg cannot reliably distinguish  situation   real relocationinversion  would need   able  recognize  base   genomic sequence     scope   paper  best set  alignments selection long contigs  rarely map   reference perfectly   single unambiguous alignment  alignment software typically report multiple alignment fragment  different locations   genome  may happen due   presence  genomic repeat  tes   reference genome    case   algorithmic issue   assembly orand alignment software quastlg attempt  accurately assess  contig  select  set  nonoverlapping alignments  maximize  total alignment score   define   function   alignment lengths map qualities  side  side inconsistencies misassemblies  problem  know   collinear chain problem myers  miller     usually solve  sequence aligners   lowlevel chain   join short match seed  larger alignment fragment  example mummer kurtz    combine maximal unique match  minimap2   chain minimizers roberts      implement  dynamic program algorithm call bestsetselection   highlevel chain   combine alignment fragment see supplementary methods  algorithm  conceptually similar  deltafilter utility  mummer package kurtz      approach include  comprehensive set  penalties  various misassembly events  feature allow bestsetselection  correctly resolve many complex set  alignments   typical  eukaryotic assemblies  produce   accurate chain  deltafilter   benchmark experiment bestsetselection   quadratic algorithm  respect   number  fragment alignments per contig   usually fine since  number  generally small    however  may cause  significant slowdown  case  large genomes evaluation   number  alignments may reach dozens  thousands   contigs although   chain algorithms  subquadratic time complexity abouelhoda  ohlebusch     applicable   gap cost function  associate   large constant instead   implement  simple heuristic  always find  best alignment set  one   several set  maximize  score supplementary methods  even though  heuristic idea   guarantee  speed   theory  significantly drop  run time   six benchmark dataset evaluations  kmerbased quality metrics  show   presence  many tes   specific feature  eukaryotic genomes significantly complicate assembly evaluation although quastlg adjustment   conventional completeness  correctness measure improve  assessment  may still   good enough  form  complete picture  eukaryote assembly quality   propose  assess assemblies use  completely different strategy inspire   evaluation procedures  putnam     chapman     generalize   arbitrary genome analysis  quastlg  strategy  base   analysis  unique kmers nonrepeated sequence  length     reference genome    assembly   value  sufficiently large quastlg use mers  default unique kmers appear   widespread across  genome  instance  fruit fly genome contain  millions unique mers    millions total mers  existence   position   kmers   assembly describe  completeness  correctness  use kmc  kokot     detect  unique kmers   reference genome  percentage   kmers detect   assembly  report   kmer completeness compare   genome fraction completeness measure  kmerbased value account  perbase quality   assembly   usually highly important   downstream analysis   genome annotation  benchmarking  show  assemblies    similar genome fraction may  completely different kmer completeness due   high mismatch  indel error rat  kmerbased correctness  calculate base   small uniformly distribute subset   unique kmers  order  speed   computation  select  subset   way   two kmers   subset   least   apart      reference genome   subset  provide  kmc   identify contigs   least two kmers  contig position   detect kmer  examine   refer   consecutive list  kmers k1k2…  ≥⁠   contig    marker    ∈  distance          equivalent within  small error    distance    default   process contigs   least two markers  check whether  relative position  adjacent markers    correlate   locations   quastlg report  kmerbased translocation breakpoint      originate  different chromosomes   kmerbased relocation   markers     chromosome   inconsistency   position      larger   predefined threshold  use   threshold  default   refer  kmerbased translocations  relocations  kmerbased misjoins  exclude confusion  regular quast misassemblies kmerbased misjoins  essentially similar   regular misassembly metrics except    focus    critical assembly errors  key benefit   measure    tolerance  inconsistencies cause  tes since tes mostly correspond  genomic repeat  thus lack unique kmers  example kmerbased relocations  successfully resolve  situations   contig start  end     cause  ambiguity   regular misassembly detection algorithm fig   evaluation without  reference genome   real assembly project  reference genome sequence   available   assembly quality assessment must rely   source  information  primary purpose  quastlg   referencebased analysis   also include    novo eukaryotetargeted completeness measure  make  tool useful   wider set  applications quastlg  supply  genemarkes lomsadze    software   novo gene prediction  eukaryotic genomes however despite  relevance   gene find  assess downstream analysis perspectives  heuristic nature may result   mislead output   experiment  instance  assembly may contain multiple copy  one gene    report several time  counter   additionally use busco simao     find  number  assemble conserve genes   present nearly universally  eukaryotes   single copy  demonstrate  busco completeness correlate   accurate referencebased quality metrics  add  computation    benchmark experiment note  referencefree correctness metrics    scope  quastlg   recommend use specialize  novo evaluation tool   scenario  instance reapr hunt    identify likely assembly  scaffold errors base  pair read map another example  kat mapleson     compare  kmer spectrum   assembly   kmer spectrum   read   quite useful  identify miss sequence collapse repeat  expand sequence   assembly",12
217,ALE,"ALE: a generic assembly likelihood evaluation framework for assessing the accuracy of genome and metagenome assemblies
The ALE score and the likelihood of an assembly The ALE framework is founded on a statistical model that describes two probabilities: a Bayesian prior probability distribution forumla describing the likelihood of an assembly forumla without any read information and a probability forumla describing the likelihood of a set of reads, forumla⁠, being generated from an assembly, forumla⁠. The prior forumla can be computed using the k-mer distribution of the assembly, whereas the likelihood forumla is calculated from information about read quality, agreement between the mapped reads and the proposed assembly, mate pair orientation, insert length (paired-end reads) and sequencing depth. A detailed description of the likelihood and prior probability is given in the following. The ALE score, except for a proportionality constant that depends on the reads but not on the assembly, is the logarithm of the probability that the assembly is correct, forumla⁠. According to Bayes’ rule, this probability is formula (1) where forumla is a proportionality constant ensuring forumla is a probability distribution. As is typical in large-scale applications of Bayesian statistics, computing forumla exactly is intractable. The ALE score is computed by replacing forumla with an approximation described in the Supplementary Materials, and then taking the logarithm of the resulting approximation to forumla⁠. The ALE score can be used to compare two different assemblies of the same genome, forumla and forumla⁠. Call forumla⁠, the ALE score of the first assembly, and forumla⁠, the ALE score of the second, both generated from the same set of reads forumla⁠. The difference of these scores is then given by the equation formula (2) The assembly with the higher ALE score is also the one with the larger probability of being correct. Moreover, the difference between two assemblies’ ALE scores describes their relative probabilities of correctness. Below, we refer to the ALE score more precisely as the total ALE score, to differentiate it from the sub-scores (described later in the text) used to construct it. Although the ALE score can be reported as a standalone value, this is made possible only to facilitate comparisons with other assemblies of the same genome. We emphasize that the ALE score is a comparative measure and should not be used to judge the quality of a single assembly in isolation, as errors in estimating forumla may cause a large difference between the ALE score and forumla⁠. We also emphasize that the ALE score should only be used to compare different assemblies of the same genome, for which the ALE scores have been calculated using the same set of reads. Figure 1 shows the pipeline used to compute the total ALE score. Given a set of reads and a proposed assembly, ALE first takes as input the alignments of the reads onto the assembly in the form of a SAM or BAM file (Li et al., 2009), which can be produced by a third-party alignment algorithm such as bowtie (Langmead et al., 2009) or bwa (Li et al., 2009). ALE then determines the probabilistic placement of each read and a corresponding placement sub-score for each mapped base, which describes how well the read agrees with the assembly. In the case of paired-end reads, ALE also calculates an insert sub-score for all mapped bases of the assembly from the read pair, which describes how well the distance between the mapped reads matches the distribution of lengths that we would expect from the sequencing library. This insert sub-score is similar to the compression-expansion (CE) statistic of Zimin et al. (2008) with details given in the Supplementary Materials. ALE also calculates a depth sub-score, which measures the evenness of the sequencing depth accounting for the GC bias prevalent in some NGS techniques. The placement, insert and depth sub-scores together determine forumla⁠. Independently, with only the assembly and not the reads, ALE calculates the k-mer sub-score and the prior forumla⁠. Each sub-score is calculated for each scaffold or contig within an assembly independently, allowing for genome variations commonly found in metagenomes because each contig/scaffold is likely from a different species with a different k-mer profile. The four sub-scores are then combined to form the total ALE score. The constituent calculations in this pipeline are described in the Supplementary Material. Fig. 1. The components of the total ALE score. ALE takes a proposed assembly and an alignment of reads as input. Four scores, the k-mer, placement, depth and insert sub-scores are computed using the model described in Section 2. From the four scores, a total ALE score is calculated and reported as a text file (.ale), and the text file can be used for input into the supplied plotter to generate a PDF file for visualization Open in new tabDownload slide The components of the total ALE score. ALE takes a proposed assembly and an alignment of reads as input. Four scores, the k-mer, placement, depth and insert sub-scores are computed using the model described in Section 2. From the four scores, a total ALE score is calculated and reported as a text file (.ale), and the text file can be used for input into the supplied plotter to generate a PDF file for visualization The contributions to these four sub-scores are reported by ALE as a function of position within the assembly and can be visualized with the included plotting package or exported to genome viewers including the Integrative Genomics Viewer (Nicol et al., 2009) and the UCSC genome browser (Kent et al., 2002). 2.2 Details of the probabilistic ingredients of the ALE score We now describe the four sub-scores (placement, insert, depth and k-mer) and the role they play within the ALE framework. The first three of these sub-scores appear in the likelihood forumla⁠. ALE computes forumla using a probabilistic model for the way in which reads are generated from an assembly during the whole genome shotgun sequencing process. This model makes independence assumptions that decomposes this probability into a product of three terms, formula (3) Each term is a separate sub-score and is explained later in the text in detail. 2.2.1 Placement quantifies how well the sequence of the reads agrees with the assembly. Assuming that every paired read forumla is generated independently from the assembly, the probability of a set of reads forumla given an assembly forumla is forumlaforumla⁠, where forumla is itself the product of two independent probability distributions, forumla⁠. Here, forumla describes how well the read matches the subsection of the assembly to which it maps, and forumla describes whether the mate pairs have an orientation that is consistent with the library. We now describe in detail how these two probabilities are computed, beginning with forumla⁠. Assuming that each base forumla of the read is correctly called by the sequencer independently with a probability equal to the base’s quality score forumla⁠, we have forumla⁠, where forumla when the base forumla correctly matches the assembly and forumla when it does not. This expression follows from our modeling assumption that all four possible errors that the sequencer could have reported (three different substitutions and deletion) are equally likely when the read does not match the sequence. If the assembly has an unknown base (denoted ‘N’), we set forumla⁠, modeling the lack of information about the correct base at that location. If an ambiguity code is reported by the sequencer, then the aforementioned expression is modified to account for the distribution over the possible bases encoded by that code. Each read may only be ‘placed’ at a single position in the assembly. If the aligner placed a particular read at more than one position, we choose one position at random, weighting by forumla⁠. This allows for repeat regions to be properly represented with the correct number of reads in expectation. The orientation likelihood, forumla⁠, is calculated by first counting the number of times that each orientation occurs in each library from the mapping information. The likelihood forumla is then the empirical frequency of the observed orientation of the read forumla in the library to which forumla belongs. (This likelihood can also be overridden with user-specified values.) We also derive per-base placement sub-scores at each position in the assembly. The placement sub-score at a particular position is the geometric mean forumla⁠, where the product is over all reads forumla covering the given position, and forumla is the number of such reads. 2.2.2 Insert describes how well the mate pairs’ insert lengths match those we would expect and is computed as forumla⁠. The insert likelihood, forumla⁠, is determined by first observing all insert lengths from all mappings of all reads and calculating the population mean, forumla⁠, and variance, forumla⁠, of these lengths (the mean and variance can also be set by the user, if they are known). This step only needs to be done once. Once completed, we calculate the insert likelihood for each read pair forumla by assuming that the observed insert length forumla is distributed normally with this mean and variance, forumla As with the placement sub-score, we calculate the insert sub-score at a position as the geometric mean of the forumla of all reads forumla covering that position. This can identify areas of constriction or expansion within a proposed assembly. The insert sub-score is similar to the CE statistics of Zimin et al. (2008), as we now show. To describe the similarity, we first write the insert sub-score as forumlaforumla where the products and sums over forumla are over all reads covering a given position, and forumla is the number of such reads. We now use the fact that forumla⁠, where forumla is the sample mean of the implied insert lengths and forumla is the sample variance. The CE statistic from Zimin et al. (2008) is forumla⁠, which implies forumla⁠. As forumla grows large, the CE statistic is asymptotically normal, owing to the central limit theorem. The log of the insert sub-score can then be written forumla⁠. The insert sub-score is decreasing in forumla⁠, and the term involving the CE statistic dominates when forumla is large. Thus, when forumla is large, the positions with lowest insert sub-score are those positions whose CE statistic is furthest from forumla⁠. This, flagging those regions with low insert-sub score is similar to the rule recommended in Zimin et al. (2008) of flagging those regions with forumla larger than a fixed cutoff value. 2.2.3 Depth describes how well the depth at each location agrees with the depth that we would expect, given the GC content at that location. It is the product of a depth sub-score over all positions in the assembly, forumla⁠, where forumla is the depth at position forumla⁠. The depth forumla is ideally Poisson-distributed (Lander and Waterman, 1988). However, most next-generation sequencers and library preparation techniques can bias GC-rich areas of a genome (Aird et al., 2011). This bias affects the observed depth in specific areas. We model the depths as Poisson distributed about a mean drawn from an independent Gamma distribution centered at the expected depth for that position, given its GC content. This models our uncertainty about the mean of the Poisson distribution, arising from the dependence of the expected depth on more than just the GC content at that position, including ‘hard stops’, and the GC content at nearby positions. It results in an infinite mixture of Poissons that is equivalent to a Negative Binomial distribution. We first calculate for each of the following 100 ranges of GC content, 0–1, 1–2, forumla⁠, 99–100%, the average observed depth over positions in the assembly whose GC content (calculated as the GC content within an average read length) is within this range. Let forumla be the average observed depth for the GC content range in which forumla falls, where forumla is the GC content percentage averaged across all reads that map (in the placement step) to that position. If any forumla falls below a minimum value of forumla⁠, we use this minimum value instead. This discounts regions of exceptionally low average depth. Then, at any given position forumla⁠, the depth sub-score is forumlaforumlaforumla(Xi)),1/2). 2.2.4 k-mer describes the likelihood of the assembly forumla⁠, in the absence of any read information. Within this prior probability distribution, we encode the belief that within a single genome, each k-mer (a permutation of k base pairs, where k is a fixed user defined number initially set to 4) has a unique k-mer frequency. The forumla dimensional vector giving this frequency for each k-mer is conserved across a genome and can help determine if two different genomes have been mistakenly combined (Teeling et al., 2004; Woyke et al., 2006). Let forumla be the set of all possible unique k-mers, so forumla⁠, and for each forumla in forumla let forumla be the number of times this k-mer appears in a contig in the assembly. Then, the frequency forumla of a particular k-mer forumla within a contig is forumla⁠. The k-mer score is the product of this frequency over each k-mer appearing in each contig of the assembly forumla⁠, which can be written as forumla⁠. This is equivalent to assuming each k-mer in the assembly is drawn independently from a common multinomial distribution with probabilities empirically estimated from the assembly. This prior distribution does not account for horizontal gene transfer, e.g. from phages, and thus may inappropriately flag such regions as being misassembled. The k-mer sub-score of a base at any given position in the assembly is the geometric average of forumla of all k-mers that cover that position. In calculating this average, the very first base in the genome only has one contributing k-mer, the second has two, up to k contributing k-mers after forumla bases.",AssemblyEvaluation,"ale  generic assembly likelihood evaluation framework  assess  accuracy  genome  metagenome assemblies
 ale score   likelihood   assembly  ale framework  found   statistical model  describe two probabilities  bayesian prior probability distribution forumla describe  likelihood   assembly forumla without  read information   probability forumla describe  likelihood   set  read forumla⁠  generate   assembly forumla⁠  prior forumla   compute use  kmer distribution   assembly whereas  likelihood forumla  calculate  information  read quality agreement   map read   propose assembly mate pair orientation insert length pairedend read  sequence depth  detail description   likelihood  prior probability  give   follow  ale score except   proportionality constant  depend   read     assembly   logarithm   probability   assembly  correct forumla⁠ accord  bay rule  probability  formula   forumla   proportionality constant ensure forumla   probability distribution   typical  largescale applications  bayesian statistics compute forumla exactly  intractable  ale score  compute  replace forumla   approximation describe   supplementary materials   take  logarithm   result approximation  forumla⁠  ale score   use  compare two different assemblies    genome forumla  forumla⁠ call forumla⁠  ale score   first assembly  forumla⁠  ale score   second  generate    set  read forumla⁠  difference   score   give   equation formula   assembly   higher ale score  also  one   larger probability   correct moreover  difference  two assemblies ale score describe  relative probabilities  correctness   refer   ale score  precisely   total ale score  differentiate    subscores describe later   text use  construct  although  ale score   report   standalone value   make possible   facilitate comparisons   assemblies    genome  emphasize   ale score   comparative measure     use  judge  quality   single assembly  isolation  errors  estimate forumla may cause  large difference   ale score  forumla⁠  also emphasize   ale score    use  compare different assemblies    genome    ale score   calculate use   set  read figure  show  pipeline use  compute  total ale score give  set  read   propose assembly ale first take  input  alignments   read onto  assembly   form   sam  bam file        produce   thirdparty alignment algorithm   bowtie langmead     bwa     ale  determine  probabilistic placement   read   correspond placement subscore   map base  describe  well  read agree   assembly   case  pairedend read ale also calculate  insert subscore   map base   assembly   read pair  describe  well  distance   map read match  distribution  lengths   would expect   sequence library  insert subscore  similar   compressionexpansion  statistic  zimin     detail give   supplementary materials ale also calculate  depth subscore  measure  evenness   sequence depth account    bias prevalent   ngs techniques  placement insert  depth subscores together determine forumla⁠ independently    assembly    read ale calculate  kmer subscore   prior forumla⁠  subscore  calculate   scaffold  contig within  assembly independently allow  genome variations commonly find  metagenomes   contigscaffold  likely   different species   different kmer profile  four subscores   combine  form  total ale score  constituent calculations   pipeline  describe   supplementary material fig   components   total ale score ale take  propose assembly   alignment  read  input four score  kmer placement depth  insert subscores  compute use  model describe  section    four score  total ale score  calculate  report   text file ale   text file   use  input   supply plotter  generate  pdf file  visualization open  new tabdownload slide  components   total ale score ale take  propose assembly   alignment  read  input four score  kmer placement depth  insert subscores  compute use  model describe  section    four score  total ale score  calculate  report   text file ale   text file   use  input   supply plotter  generate  pdf file  visualization  contributions   four subscores  report  ale   function  position within  assembly    visualize   include plot package  export  genome viewers include  integrative genomics viewer nicol      ucsc genome browser kent     detail   probabilistic ingredients   ale score   describe  four subscores placement insert depth  kmer   role  play within  ale framework  first three   subscores appear   likelihood forumla⁠ ale compute forumla use  probabilistic model   way   read  generate   assembly   whole genome shotgun sequence process  model make independence assumptions  decompose  probability   product  three term formula   term   separate subscore   explain later   text  detail  placement quantify  well  sequence   read agree   assembly assume  every pair read forumla  generate independently   assembly  probability   set  read forumla give  assembly forumla  forumlaforumla⁠  forumla    product  two independent probability distributions forumla⁠  forumla describe  well  read match  subsection   assembly    map  forumla describe whether  mate pair   orientation   consistent   library   describe  detail   two probabilities  compute begin  forumla⁠ assume   base forumla   read  correctly call   sequencer independently   probability equal   base quality score forumla⁠   forumla⁠  forumla   base forumla correctly match  assembly  forumla      expression follow   model assumption   four possible errors   sequencer could  report three different substitutions  deletion  equally likely   read   match  sequence   assembly   unknown base denote   set forumla⁠ model  lack  information   correct base   location   ambiguity code  report   sequencer   aforementioned expression  modify  account   distribution   possible base encode   code  read may   place   single position   assembly   aligner place  particular read    one position  choose one position  random weight  forumla⁠  allow  repeat regions   properly represent   correct number  read  expectation  orientation likelihood forumla⁠  calculate  first count  number  time   orientation occur   library   map information  likelihood forumla    empirical frequency   observe orientation   read forumla   library   forumla belong  likelihood  also  override  userspecified value  also derive perbase placement subscores   position   assembly  placement subscore   particular position   geometric mean forumla⁠   product    read forumla cover  give position  forumla   number   read  insert describe  well  mate pair insert lengths match   would expect   compute  forumla⁠  insert likelihood forumla⁠  determine  first observe  insert lengths   mappings   read  calculate  population mean forumla⁠  variance forumla⁠   lengths  mean  variance  also  set   user    know  step  need      complete  calculate  insert likelihood   read pair forumla  assume   observe insert length forumla  distribute normally   mean  variance forumla    placement subscore  calculate  insert subscore   position   geometric mean   forumla   read forumla cover  position   identify areas  constriction  expansion within  propose assembly  insert subscore  similar    statistics  zimin       show  describe  similarity  first write  insert subscore  forumlaforumla   products  sum  forumla    read cover  give position  forumla   number   read   use  fact  forumla⁠  forumla   sample mean   imply insert lengths  forumla   sample variance   statistic  zimin     forumla⁠  imply forumla⁠  forumla grow large   statistic  asymptotically normal owe   central limit theorem  log   insert subscore    write forumla⁠  insert subscore  decrease  forumla⁠   term involve   statistic dominate  forumla  large thus  forumla  large  position  lowest insert subscore   position whose  statistic  furthest  forumla⁠  flag  regions  low insertsub score  similar   rule recommend  zimin     flag  regions  forumla larger   fix cutoff value  depth describe  well  depth   location agree   depth   would expect give   content   location    product   depth subscore   position   assembly forumla⁠  forumla   depth  position forumla⁠  depth forumla  ideally poissondistributed lander  waterman  however  nextgeneration sequencers  library preparation techniques  bias gcrich areas   genome aird     bias affect  observe depth  specific areas  model  depths  poisson distribute   mean draw   independent gamma distribution center   expect depth   position give   content  model  uncertainty   mean   poisson distribution arise   dependence   expect depth       content   position include hard stop    content  nearby position  result   infinite mixture  poissons   equivalent   negative binomial distribution  first calculate     follow  range   content   forumla⁠   average observe depth  position   assembly whose  content calculate    content within  average read length  within  range let forumla   average observe depth    content range   forumla fall  forumla    content percentage average across  read  map   placement step   position   forumla fall   minimum value  forumla⁠  use  minimum value instead  discount regions  exceptionally low average depth    give position forumla⁠  depth subscore  forumlaforumlaforumlaxi  kmer describe  likelihood   assembly forumla⁠   absence   read information within  prior probability distribution  encode  belief  within  single genome  kmer  permutation   base pair     fix user define number initially set     unique kmer frequency  forumla dimensional vector give  frequency   kmer  conserve across  genome   help determine  two different genomes   mistakenly combine teeling    woyke    let forumla   set   possible unique kmers  forumla⁠    forumla  forumla let forumla   number  time  kmer appear   contig   assembly   frequency forumla   particular kmer forumla within  contig  forumla⁠  kmer score   product   frequency   kmer appear   contig   assembly forumla⁠    write  forumla⁠   equivalent  assume  kmer   assembly  draw independently   common multinomial distribution  probabilities empirically estimate   assembly  prior distribution   account  horizontal gene transfer   phages  thus may inappropriately flag  regions   misassembled  kmer subscore   base   give position   assembly   geometric average  forumla   kmers  cover  position  calculate  average   first base   genome   one contribute kmer  second  two    contribute kmers  forumla base",12
218,CGAL,"CGAL: computing genome assembly likelihoods
Mapping reads The first step in computing the likelihood is mapping reads to the assembly. A number of tools are available for this such as Bowtie [31,32], MAQ [33], BWA [34] and BFAST [35]. Our present implementation can use either BFAST or Bowtie 2 for mapping reads as they support mapping with indels and report multiple alignments in a way that gives all the required information without accessing the assembly sequence. But any tool that reports multiple alignments of reads and allows for insertions and deletions can be used with some minor modifications. However, existing tools do not usually map all reads, and for the likelihood computation it is necessary to assign probabilities to reads that are unmapped. We found that mapping tools were unable to map a large fraction of reads in our experiments. One option is to assign probabilities to these reads, assuming that they could have been generated from any site, using the number and types of errors not handled by the mapping tool. But it is then often the case that unmapped reads are deemed more probable than mapped ones, which we believe is anomalous. Furthermore, in our analyses we determined that the resulting probabilities were inaccurate (results not shown). Therefore, we chose to directly align the reads not mapped by BFAST or Bowtie 2 using an adaptation of the Smith-Waterman algorithm. We adapted the striped implementation of the Smith-Waterman algorithm by Farrar [36]. This step is time consuming, so we align only a random subset of reads with the number specified by the user and approximate probabilities using these. Learning distributions To compute the likelihood from mapped reads, we need to learn the distribution of fragment lengths, their distribution across the genome and error characteristics. Since they differ with library preparation methods and sequencing instruments, we chose to learn these from sequencing data generated in the experiment. We do this by mapping reads to the assembly and using reads that map uniquely. However, this can be easily extended to take into account all reads by using the expectation-maximization (EM) algorithm at the expense of more iterations. We explain each distribution in more detail below. Fragment length distribution The distribution of fragment lengths depends on the method used for size selection and may not be approximated well by common distributions [37]. So, we use the empirical distribution. Distribution of fragments along genome In our implementation, we assume that fragments are distributed uniformly across the genome. We leave incorporating sequencing bias as future work. Error model In the error model used at present, we have made the assumption that sequencing errors are independent of one another. We learn an error rate for each position in the read since error rates are known to be different across positions in reads [38]. We also learn separate error rates for each type of base and substitution type. Although errors are known to depend on sequence context [38], we have ignored them for the sake of simplicity. To account for varying indel rates across positions in reads, we learn an insertion rate and a deletion rate for each position in the read. Since short indels are more likely than longer ones, we also count the number of insertions and deletions by length. Implementation As mentioned earlier, we use BFAST or Bowtie 2 to map reads to assemblies. The parameters are set so that they report all alignments of a read found. The remaining code for computing likelihood is written in C++ and it consists of three parts: convert: This converts the output generated by BFAST or Bowtie 2 to an internal format. It also separates reads with no end or one end mapped and reads with ends mapped to different scaffolds if needed. Separating this module also allow us to support other mapping tools by writing a conversion routine. align: To align the reads not mapped by the mapping tool, we adapted the striped implementation of the Smith-Waterman algorithm by Farrar [36]. As this step is time consuming, we align a random subset of reads with the number determined by the user. This step is multithreaded to speed up the process. CGAL: This learns the fragment length distribution and parameters for the error model using uniquely mapped reads and then uses these to compute the likelihood value. Assembling genomes To assemble reads, we varied the k-mer length used to construct the de Bruijn graph to obtain different assemblies for each assembly tool. For other parameters, the default values or values suggested in manuals were used. Data analysis Likelihoods were computed by running CGAL with the default parameters and aligning between 300 and 1,000 randomly chosen reads not mapped by the mapping tool used. The running time for CGAL was approximately 1/3 of the time taken to map reads using Bowtie 2. To compute the difference between an assembly and the reference, we aligned the assembly to the reference using NUCmer [28]. The difference refers to the number of bases in the reference that are either not covered by the assembly or differ in the reference and assembly. Contigs were generated by splitting scaffolds at sites with 25 or more N's (character representing any base).",AssemblyEvaluation,"cgal compute genome assembly likelihoods
mapping read  first step  compute  likelihood  map read   assembly  number  tool  available     bowtie  maq  bwa   bfast   present implementation  use either bfast  bowtie   map read   support map  indels  report multiple alignments   way  give   require information without access  assembly sequence   tool  report multiple alignments  read  allow  insertions  deletions   use   minor modifications however exist tool   usually map  read    likelihood computation   necessary  assign probabilities  read   unmapped  find  map tool  unable  map  large fraction  read   experiment one option   assign probabilities   read assume   could   generate   site use  number  type  errors  handle   map tool     often  case  unmapped read  deem  probable  map ones   believe  anomalous furthermore   analyse  determine   result probabilities  inaccurate result  show therefore  choose  directly align  read  map  bfast  bowtie  use  adaptation   smithwaterman algorithm  adapt  strip implementation   smithwaterman algorithm  farrar   step  time consume   align   random subset  read   number specify   user  approximate probabilities use  learn distributions  compute  likelihood  map read  need  learn  distribution  fragment lengths  distribution across  genome  error characteristics since  differ  library preparation methods  sequence instrument  choose  learn   sequence data generate   experiment     map read   assembly  use read  map uniquely however    easily extend  take  account  read  use  expectationmaximization  algorithm   expense   iterations  explain  distribution   detail  fragment length distribution  distribution  fragment lengths depend   method use  size selection  may   approximate well  common distributions    use  empirical distribution distribution  fragment along genome   implementation  assume  fragment  distribute uniformly across  genome  leave incorporate sequence bias  future work error model   error model use  present   make  assumption  sequence errors  independent  one another  learn  error rate   position   read since error rat  know   different across position  read   also learn separate error rat   type  base  substitution type although errors  know  depend  sequence context    ignore    sake  simplicity  account  vary indel rat across position  read  learn  insertion rate   deletion rate   position   read since short indels   likely  longer ones  also count  number  insertions  deletions  length implementation  mention earlier  use bfast  bowtie   map read  assemblies  parameters  set    report  alignments   read find  remain code  compute likelihood  write     consist  three part convert  convert  output generate  bfast  bowtie    internal format  also separate read   end  one end map  read  end map  different scaffold  need separate  module also allow   support  map tool  write  conversion routine align  align  read  map   map tool  adapt  strip implementation   smithwaterman algorithm  farrar    step  time consume  align  random subset  read   number determine   user  step  multithreaded  speed   process cgal  learn  fragment length distribution  parameters   error model use uniquely map read   use   compute  likelihood value assemble genomes  assemble read  vary  kmer length use  construct   bruijn graph  obtain different assemblies   assembly tool   parameters  default value  value suggest  manuals  use data analysis likelihoods  compute  run cgal   default parameters  align     randomly choose read  map   map tool use  run time  cgal  approximately    time take  map read use bowtie   compute  difference   assembly   reference  align  assembly   reference use nucmer   difference refer   number  base   reference   either  cover   assembly  differ   reference  assembly contigs  generate  split scaffold  sit     ' character represent  base",12
219,Baa. pl,"Baa.pl: A tool to evaluate de novo genome assemblies with RNA transcripts
Baa.pl requires that a set of RNA transcripts are first aligned to the genome using the freely available program BLAT [9]. The output of this program, as well as the FASTA file of the RNA 3 transcripts used as input to BLAT, are the only required inputs to baa.pl. The output consists of four measures, explained in Table 1. The ratio of transcripts with a BLAT entry (Table 1a) is determined by dividing the number of transcripts with a BLAT alignment by the total number of transcripts. The algorithm groups each set of sub-alignments in the BLAT file by query transcript. A score for each BLAT sub-alignment is determined using the algorithm implemented by James Kent (the author of BLAT) in the UCSC Genome Browser [10]. Starting with the highest scoring sub-alignment, each position in the query transcript that aligns is marked as “covered” using an array the size of the query sequence. Each successive sub-alignment (sorted by decreasing score) is then considered. If a sub-alignment contributes more than X positions (X by default = 5) to the coverage of the query, then the subalignment is considered to have contributed to the coverage. If the target sequence of the contributing sub-alignment has not previously contributed to the “coverage,” then the number of target sequences (e.g., contigs) contributing to the alignment is incremented. These calculations are used to determine the number of transcripts mapping to a single target sequence (Table 1b) and the average number of target sequences per mapped transcript (Table 1c). The array of covered positions is then analyzed and if more than Y consecutive positions (Y by default = 5) are not covered, these positions count against the total coverage of the query. This calculation is used to determine total percent coverage of all transcript nucleotides (Table 1d). Baa.pl is fast compared to other assembly assessment methods. The time of the analysis is determined by how long it takes to align a set of transcripts to a genome with BLAT. Aligning transcripts to an assembly is decidedly faster than aligning all sequencing reads to an alignment or predicting highly conserved genes. The running of baa.pl itself on a BLAT output file usually takes less than 10 seconds with a reasonably sized data set and a modern computer. To illustrate the effectiveness of baa.pl, I produced several semi-random permuted versions of human chromosome 22, then aligned mRNAs of 42 genes located on the same chromosome. Permutations to chromosome 22 included breaking it into fragments at random positions and deleting random chunks of sequence. Position 0 on the X axis of both Figure 1 and Figure 2 shows that in a perfect case scenario (i.e., every gene aligns almost perfectly to the target sequence or sequences) the metrics reflect the ideal alignments of the transcripts to the genome. To test the sensitivity of baa.pl to contiguity, I generated versions of chromosome 22 that had been divided at random positions into fragmented sequences at 1 kb frequencies between 1,000 and 4 10,000. The level of fragmentation is inversely analogous to the level of contiguity in a genome assembly. These results (Figure 1) show that two metrics in the form of the “number of transcripts mapping to a single sequence” and the “average number of sequences per mapped mRNA” diverge from 0 in a manner proportional to the number of breaks introduced. These results show that these two metrics are appropriate for assessing levels of contiguity. The other metrics in these breakage analyses remain mostly unchanged. To test the sensitivity of baa.pl to missing sequence data, I introduced random sequence deletions of 5,000 nucleotides in random positions of chromosome 22 at 1 kb frequencies between 1,000 and 10,000. The number of introduced deletions is analogous to the amount of missing data in a genome assembly. In the case of deletions (Table 2c), the metrics “total percent coverage of all mRNA nucleotides” and “ratio of mRNAs with a BLAT entry” tend to decrease as more sequences are deleted. These results show that these two metrics are appropriate for assessing levels of completeness. The other metrics in these breakage analyses remain mostly unchanged. Discussion Genome sequencing has become a routine task in biology. Generating high-quality genome assemblies, however, has remained difficult. One of the major difficulties of this process is that assembly algorithms and parameter settings for these algorithms perform differently depending on the nature of the genome and the sequencing [1,11]. For this reason, it is important to run multiple assemblers with multiple sets of parameters and to integrate robust assessment methods into any genome-sequencing pipeline. Existing tools offer metrics that can be used to compare assemblies. Methods that use alignments of sequencing reads used in constructing the assemblies are powerful, but can be misleading if the underlying assumption of uniform coverage of sequencing reads is violated. This assumption is most certainly violated in cases that use DNA amplification—a technique critical for sequencing the genomes of microorganisms. Methods that predict highly conserved genes in an assembly and compare the counts of these genes between assemblies avoid these issues, and are invaluable. However, these methods consider only a few hundred genes and therefore offer limited resolution. The use of baa.pl to evaluate alignments of RNA transcripts offers several advantages over these methods. RNA transcripts are independently generated and, as such, are not affected by any artifacts introduced during the genomic sequencing used to generate the alignment. In addition, most eukaryotes contain tens of thousands of genes and current RNA sequencing methods recover 5 the vast majority of these transcripts. These numbers offer a much higher resolution than a small set of highly conserved genes does. The metrics from baa.pl offer the added advantages of providing a rough assessment of the extent to which a particular genome assembly encapsulates the entire transcriptome of a species, as well as a measure of how often transcripts occur on a single sequence (an important measure in determining the extent to which gene predictions will be effective). Lastly, an analysis involving baa.pl is very straightforward and considerably faster than other assessment algorithms. Baa.pl uses evidence from alignments of RNA transcript data to a genome assembly to generate metrics that can then be compared across multiple assemblies, each generated using different programs and/or different sets of parameters. The metrics are intuitive and offer clues as to the nature of differences between assemblies (e.g., contiguity vs. completeness). The program has no requirements outside of BLAT and Perl, and includes a straightforward installation. The program’s speed, ease-of-use, and intuitive nature make it an ideal tool for genome assembly assessment.",AssemblyEvaluation,"baapl  tool  evaluate  novo genome assemblies  rna transcripts
baapl require   set  rna transcripts  first align   genome use  freely available program blat   output   program  well   fasta file   rna  transcripts use  input  blat    require input  baapl  output consist  four measure explain  table   ratio  transcripts   blat entry table   determine  divide  number  transcripts   blat alignment   total number  transcripts  algorithm group  set  subalignments   blat file  query transcript  score   blat subalignment  determine use  algorithm implement  jam kent  author  blat   ucsc genome browser  start   highest score subalignment  position   query transcript  align  mark  “covered” use  array  size   query sequence  successive subalignment sort  decrease score   consider   subalignment contribute    position   default     coverage   query   subalignment  consider   contribute   coverage   target sequence   contribute subalignment   previously contribute   “coverage”   number  target sequence  contigs contribute   alignment  incremented  calculations  use  determine  number  transcripts map   single target sequence table    average number  target sequence per map transcript table   array  cover position   analyze      consecutive position   default     cover  position count   total coverage   query  calculation  use  determine total percent coverage   transcript nucleotides table  baapl  fast compare   assembly assessment methods  time   analysis  determine   long  take  align  set  transcripts   genome  blat align transcripts   assembly  decidedly faster  align  sequence read   alignment  predict highly conserve genes  run  baapl    blat output file usually take less   second   reasonably size data set   modern computer  illustrate  effectiveness  baapl  produce several semirandom permute versions  human chromosome   align mrnas   genes locate    chromosome permutations  chromosome  include break   fragment  random position  delete random chunk  sequence position     axis   figure   figure  show    perfect case scenario  every gene align almost perfectly   target sequence  sequence  metrics reflect  ideal alignments   transcripts   genome  test  sensitivity  baapl  contiguity  generate versions  chromosome     divide  random position  fragment sequence    frequencies       level  fragmentation  inversely analogous   level  contiguity   genome assembly  result figure  show  two metrics   form   “number  transcripts map   single sequence”   “average number  sequence per map mrna” diverge     manner proportional   number  break introduce  result show   two metrics  appropriate  assess level  contiguity   metrics   breakage analyse remain mostly unchanged  test  sensitivity  baapl  miss sequence data  introduce random sequence deletions   nucleotides  random position  chromosome     frequencies      number  introduce deletions  analogous   amount  miss data   genome assembly   case  deletions table   metrics “total percent coverage   mrna nucleotides”  “ratio  mrnas   blat entry” tend  decrease   sequence  delete  result show   two metrics  appropriate  assess level  completeness   metrics   breakage analyse remain mostly unchanged discussion genome sequence  become  routine task  biology generate highquality genome assemblies however  remain difficult one   major difficulties   process   assembly algorithms  parameter settings   algorithms perform differently depend   nature   genome   sequence    reason   important  run multiple assemblers  multiple set  parameters   integrate robust assessment methods   genomesequencing pipeline exist tool offer metrics    use  compare assemblies methods  use alignments  sequence read use  construct  assemblies  powerful    mislead   underlie assumption  uniform coverage  sequence read  violate  assumption   certainly violate  case  use dna amplification— technique critical  sequence  genomes  microorganisms methods  predict highly conserve genes   assembly  compare  count   genes  assemblies avoid  issue   invaluable however  methods consider    hundred genes  therefore offer limit resolution  use  baapl  evaluate alignments  rna transcripts offer several advantage   methods rna transcripts  independently generate      affect   artifacts introduce   genomic sequence use  generate  alignment  addition  eukaryotes contain tens  thousands  genes  current rna sequence methods recover   vast majority   transcripts  number offer  much higher resolution   small set  highly conserve genes   metrics  baapl offer  add advantage  provide  rough assessment   extent    particular genome assembly encapsulate  entire transcriptome   species  well   measure   often transcripts occur   single sequence  important measure  determine  extent   gene predictions   effective lastly  analysis involve baapl   straightforward  considerably faster   assessment algorithms baapl use evidence  alignments  rna transcript data   genome assembly  generate metrics     compare across multiple assemblies  generate use different program andor different set  parameters  metrics  intuitive  offer clue    nature  differences  assemblies  contiguity  completeness  program   requirements outside  blat  perl  include  straightforward installation  program speed easeofuse  intuitive nature make   ideal tool  genome assembly assessment",12
220,QUAST,"QUAST: quality assessment tool for genome assemblies
Metrics QUAST aggregates methods and quality metrics from existing software, such as Plantagora, GAGE, GeneMark.hmm (Lukashin and Borodovsky 1998) and GlimmerHMM (Majoros et al., 2004), and it extends these with new metrics. For example, the well-known N50 statistic can be artificially increased by concatenating contigs, at the expense of increasing the number of misassemblies; QUAST introduces a new statistic, NA50, to counter this. QUAST uses the Nucmer aligner from MUMmer v3.23 (Kurtz et al., 2004) to align assemblies to a reference genome and evaluate metrics depending on alignments. QUAST also computes metrics that are useful for assessing assemblies of previously unsequenced species, whereas most other assembly assessment software require a reference genome. We will split the metrics evaluated by QUAST into several groups. Most have been used in previous studies, but some are new to QUAST. 2.1.1 Contig sizes The following metrics (except for NGx) can be evaluated with or without a reference genome. We also provide filtered versions of them, restricted to contigs of length above a specified minimum size, to exclude short contigs that may not be of much use. No. of contigs: The total number of contigs in the assembly. Largest contig: The length of the largest contig in the assembly. Total length: The total number of bases in the assembly. Nx (where An external file that holds a picture, illustration, etc. Object name is btt086i1.jpg): The largest contig length, L, such that using contigs of length An external file that holds a picture, illustration, etc. Object name is btt086i2.jpg accounts for at least x% of the bases of the assembly. NGx, Genome Nx: The contig length such that using equal or longer length contigs produces x% of the length of the reference genome, rather than x% of the assembly length. 2.1.2 Misassemblies and structural variations The following metrics describe structural errors in the contigs. QUAST can evaluate them only with respect to a known reference genome. If the reference genome exactly matches the dataset being assembled, differences may be attributed to misassemblies by the software or to sequencing errors, such as chimeric reads. Sometimes one uses a reference genome that is related to but different than the dataset being sequenced. In this case, the differences may still be misassemblies, but they may also be true structural variations, such as rearrangements, large indels, different repeat copy numbers and so forth. No. of misassemblies: The number of misassemblies, using Plantagora’s definition. Plantagora defines a misassembly breakpoint as a position in the assembled contigs where the left flanking sequence aligns over 1 kb away from the right flanking sequence on the reference, or they overlap by >1 kb, or the flanking sequences align on opposite strands or different chromosomes. QUAST also generates a report with the number of misassemblies because of each of these reasons. See the Supplementary Methods for details. No. of misassembled contigs: The number of contigs that contain misassembly breakpoints. Misassembled contigs length: The total number of bases contained in all contigs that have one or more misassemblies. No. of unaligned contigs: The number of contigs that have no alignment to the reference sequence. No. of ambiguously mapped contigs: The number of contigs that have high-scoring reference alignments of equal quality in multiple locations on the reference genome. In addition to these summary statistics, QUAST also generates reports with detailed information about each contig, including whether the contig is unaligned, ambiguously mapped, misassembled or correct. 2.1.3 Genome representation and its functional elements This section lists metrics evaluating genome representation in contigs and the number of assembled functional elements, such as genes and operons. Most of these require a reference genome. Genome fraction (%): The total number of aligned bases in the reference, divided by the genome size. A base in the reference genome is counted as aligned if at least one contig has at least one alignment to this base. Contigs from repeat regions may map to multiple places, and thus may be counted multiple times in this quantity. Duplication ratio: The total number of aligned bases in the assembly (i.e. total length minus unaligned contigs length), divided by the total number of aligned bases in the reference [see the genome fraction (%) metric]. If the assembly contains many contigs that cover the same regions of the reference, its duplication ratio may be much >1. This may occur due to overestimating repeat multiplicities and due to small overlaps between contigs, among other reasons. GC (%): The total number of G and C nucleotides in the assembly, divided by the total length of the assembly. This metric can be computed without a reference genome. No. of mismatches per 100 kb: The average number of mismatches per 100 000 aligned bases. QUAST also generates a more detailed report with the coordinates of mismatches. This metric does not distinguish between single-nucleotide polymorphisms, which are true differences in the assembled genome versus the reference genome, and single-nucleotide errors, which are due to errors in reads or errors in the assembly algorithm. No. of indels per 100 kb: The average number of single nucleotide insertions or deletions per 100 000 aligned bases. A detailed report with coordinates of indels for all assemblies is also available. No. of genes: The number of genes in the assembly (complete and partial), based on a user-provided annotated list of gene positions in the reference genome. A gene is partially covered if the assembly contains at least 100 bp of the gene but not the whole gene. QUAST also reports a list of the completely covered genes for each assembly. If an annotated list of gene positions is not available, this metric cannot be computed, but the number of predicted genes (see later in the text) can be used instead. No. of operons: Complete and partial operons are counted in a similar fashion to genes, using a user-provided annotated list of operon positions in the reference genome. No. of predicted genes: The number of genes in the assembly predicted by QUAST’s gene-finding module, which is based on GeneMark.hmm (Lukashin and Borodovsky 1998) for prokaryotes and GlimmerHMM (Majoros et al., 2004) for eukaryotes. The GeneMark.hmm authors have kindly allowed use of their software inside QUAST, and GlimmerHMM is an open-source tool. If the user provides a reference genome with an annotated list of genes, we use the number of genes statistic instead. Otherwise, QUAST counts the number of genes annotated by GeneMark.hmm or GlimmerHMM and then filters them to count only those with lengths above one or more specified minimum thresholds. 2.1.4 Variations of N50 based on aligned blocks The following metrics in QUAST are new, but they have similarities with GAGE’s ‘corrected Nx’ (Salzberg et al., 2011), Assemblathon’s ‘contig path Nx over alignment graph’ (Earl et al., 2011) and the ‘normalized N50’ (Makinen et al., 2012) metric. Here, we give short descriptions for these metrics. See the Supplementary Methods for more detailed information. NAx (A stands for aligned; x ranges from 0–100): This is a combination of the well-known Nx metric and Plantagora’s number of misassemblies metric. It is computed in two steps. First, we break the contigs into aligned blocks. If a contig has misassembly breakpoints (per the previous definition from Plantagora), it is broken into multiple blocks at these breakpoints. Additionally, if there are unaligned regions within a contig, these regions are removed, and the contig is split into blocks. Next, we compute the ordinary Nx statistic on these blocks instead of on the original contigs. NGAx: We break contigs into aligned blocks as described for NAx, and then we compute the NGx statistic (instead of Nx) on these blocks. Both the NAx and NGAx metrics require a reference genome. If the reference genome is different than the sample being assembled, some breakpoints and indels may represent true structural differences. 2.2 Visualization QUAST presents a number of statistics in graphical form and supports SVG, PNG and PDF formats. Sample plots are presented in the Supplementary Material. These plots are divided into several groups: Nx-like plots: These show the trends of Nx, NGx, NAx or NGAx metrics as x varies. This is more informative than just using N50. Cumulative plots: Contigs are ordered from largest to smallest (in number of bases) for all the types of cumulative plots considered. The cumulative length plot shows the number of bases in the first x contigs, as x varies from zero to the number of contigs. The cumulative number of complete genes and cumulative number of complete operons plots are computed similarly. GC content plots: These show the distribution of GC content in the contigs. The x value shows the per cent of GC (from 0 to 100). The y value shows the number of non-overlapping 100 bp windows whose GC content is x. This distribution is often Gaussian (Bohlin et al., 2010); however, if there are contaminants with a different GC content, there will often be a superposition of multiple Gaussians. Contig alignment plots (Fig. 1): These show alignment of contigs to the reference genome and the positions of misassemblies in these contigs. Colour coding indicates when block boundaries are shared by multiple assemblies, and to show misassembled blocks. An optional track shows the read coverage along the reference genome. An external file that holds a picture, illustration, etc. Object name is btt086f1p.jpg Fig. 1. Alignment of single-cell E.coli assemblies to the reference genome. On all tracks, the x-axis is genome position. Top track: Read coverage on a logarithmic scale. The red curve shows coverage binned in 1000 bp windows. Blue positions on the x-axis have zero coverage, even if their bin has some coverage. Coverage is highly non-uniform, ranging from 0 to near 10 000. All other tracks: Comparison of positions of aligned contigs. Contigs that align correctly are coloured blue if the boundaries agree (within 2000 bp on each side) in at least half of the assemblies, and green otherwise. Contigs with misassemblies are broken into blocks and coloured orange if the boundaries agree in at least half of the assemblies, and red otherwise. Contigs are staggered vertically and are shown in different shades of their colour to distinguish the separate contigs, including small ones QUAST also makes comparative histograms of several metrics: the number of complete genes, the number of complete operons and the genome fraction (%). Histograms of other metrics can be added as well. 2.3 Comparing assemblers In this study, we evaluated several of the leading genome assemblers on three datasets: Escherichia coli (a single-cell sample), Homo sapiens chromosome 14 and Bombus impatiens (the bumble bee, which at publication time does not have a finished assembly). The E.coli dataset and some of its assemblies are taken from Chitsaz et al. (2011). The SPAdes and IDBA-UD assemblies are new. All assemblies of H.sapiens and B.impatiens and both datasets are taken from Salzberg et al. (2011). In this article, we present some of QUAST’s comparison statistics and a sample plot comparing E.coli assemblies. See Supplementary Figures S3–S29 and Supplementary Tables S2–S8 for more plots and extended tables for E.coli and for comparisons of assemblers on the other two datasets. 2.3.1 Comparison of E.coli assemblies The reference genome is E.coli str. K-12 substr. MG1655 (Blattner et al., 1997), available at the NCBI website. Gene annotations were taken from http://www.ecogene.org/. We include several well-known assemblers designed for cultured bacterial datasets: EULER-SR (Pevzner et al., 2001), Velvet (Zerbino and Birney, 2008), and SOAPdenovo (Li et al., 2010). We also include several recently introduced assemblers that have been adapted or designed from scratch to handle single-cell datasets: Velvet-SC and EULER + Velvet-SC (Chitsaz et al., 2011), our assembler, SPAdes (Bankevich et al., 2012) and IDBA-UD (Peng et al., 2012). Table 1 shows that SPAdes and IDBA-UD have the best results in almost all metrics. IDBA-UD assembled the largest contig (224 018 bp) and has the smallest number of contigs (283), but SPAdes has a larger NGA50 than IDBA-UD (99 913 versus 90 607 bp) and assembled a higher percentage of the genome (96.99 versus 95.90%). SPAdes also assembled the highest number of complete genes (4071 of 4324), with IDBA-UD a close second (4030). However, both SPAdes and IDBA-UD have more misassemblies than the three Velvet-based assemblers. Table 1. Comparison of assemblies of a single-cell sample of E.coli (for contigs An external file that holds a picture, illustration, etc. Object name is btt086i3.jpg bp) Assembler        No. of contigs        NGA50 (bp)        Largest (bp)        Total (bp)        Genome fraction (%)        No. of misassemblies        No. of complete genes EULER-SR        610        26 580        140 518        4 306 898        86.54        19        3442 E+V-SC        396        32 051        132 865        4 555 721        93.58        2        3816 IDBA-UD        283        90 607        224 018        4 734 432        95.90        9        4030 SOAPdenovo        817        16 606        87 533        4 183 037        81.36        6        3060 SPAdes        532        99 913        211 020        4 975 641        96.99        11        4071 Velvet        310        22 648        132 865        3 517 182        75.53        2        3121 Velvet-SC        617        19 791        121 367        4 556 809        93.31        2        3662 The best value for each column is indicated in bold. Figure 1 shows how the contigs align to the reference genome and reveals high similarity between some of the assemblies. E+V-SC, Velvet and Velvet-SC generated assemblies with dozens of similar contigs; this is natural because all of these assemblers are modifications of Velvet. The top track shows the read coverage along the genome. Velvet was not able to assemble low-coverage regions of the genome, whereas the assemblers designed for single-cell datasets (Velvet-SC, E+V-SC, SPAdes and IDBA-UD) did much better, although, of course, none of them can assemble the regions that literally have zero coverage.",AssemblyEvaluation,"quast quality assessment tool  genome assemblies
metrics quast aggregate methods  quality metrics  exist software   plantagora gage genemarkhmm lukashin  borodovsky   glimmerhmm majoros      extend   new metrics  example  wellknown n50 statistic   artificially increase  concatenate contigs   expense  increase  number  misassemblies quast introduce  new statistic na50  counter  quast use  nucmer aligner  mummer  kurtz     align assemblies   reference genome  evaluate metrics depend  alignments quast also compute metrics   useful  assess assemblies  previously unsequenced species whereas   assembly assessment software require  reference genome   split  metrics evaluate  quast  several group    use  previous study    new  quast  contig size  follow metrics except  ngx   evaluate   without  reference genome  also provide filter versions   restrict  contigs  length   specify minimum size  exclude short contigs  may    much use   contigs  total number  contigs   assembly largest contig  length   largest contig   assembly total length  total number  base   assembly    external file  hold  picture illustration etc object name  btt086i1jpg  largest contig length    use contigs  length  external file  hold  picture illustration etc object name  btt086i2jpg account   least    base   assembly ngx genome   contig length   use equal  longer length contigs produce    length   reference genome rather     assembly length  misassemblies  structural variations  follow metrics describe structural errors   contigs quast  evaluate    respect   know reference genome   reference genome exactly match  dataset  assemble differences may  attribute  misassemblies   software   sequence errors   chimeric read sometimes one use  reference genome   relate   different   dataset  sequence   case  differences may still  misassemblies   may also  true structural variations   rearrangements large indels different repeat copy number   forth   misassemblies  number  misassemblies use plantagoras definition plantagora define  misassembly breakpoint   position   assemble contigs   leave flank sequence align    away   right flank sequence   reference   overlap      flank sequence align  opposite strand  different chromosomes quast also generate  report   number  misassemblies      reason see  supplementary methods  detail   misassembled contigs  number  contigs  contain misassembly breakpoints misassembled contigs length  total number  base contain   contigs   one   misassemblies   unaligned contigs  number  contigs    alignment   reference sequence   ambiguously map contigs  number  contigs   highscoring reference alignments  equal quality  multiple locations   reference genome  addition   summary statistics quast also generate report  detail information   contig include whether  contig  unaligned ambiguously map misassembled  correct  genome representation   functional elements  section list metrics evaluate genome representation  contigs   number  assemble functional elements   genes  operons    require  reference genome genome fraction   total number  align base   reference divide   genome size  base   reference genome  count  align   least one contig   least one alignment   base contigs  repeat regions may map  multiple place  thus may  count multiple time   quantity duplication ratio  total number  align base   assembly  total length minus unaligned contigs length divide   total number  align base   reference see  genome fraction  metric   assembly contain many contigs  cover   regions   reference  duplication ratio may  much   may occur due  overestimate repeat multiplicities  due  small overlap  contigs among  reason    total number     nucleotides   assembly divide   total length   assembly  metric   compute without  reference genome   mismatch per    average number  mismatch per   align base quast also generate   detail report   coordinate  mismatch  metric   distinguish  singlenucleotide polymorphisms   true differences   assemble genome versus  reference genome  singlenucleotide errors   due  errors  read  errors   assembly algorithm   indels per    average number  single nucleotide insertions  deletions per   align base  detail report  coordinate  indels   assemblies  also available   genes  number  genes   assembly complete  partial base   userprovided annotate list  gene position   reference genome  gene  partially cover   assembly contain  least     gene    whole gene quast also report  list   completely cover genes   assembly   annotate list  gene position   available  metric cannot  compute   number  predict genes see later   text   use instead   operons complete  partial operons  count   similar fashion  genes use  userprovided annotate list  operon position   reference genome   predict genes  number  genes   assembly predict  quasts genefinding module   base  genemarkhmm lukashin  borodovsky   prokaryotes  glimmerhmm majoros     eukaryotes  genemarkhmm author  kindly allow use   software inside quast  glimmerhmm   opensource tool   user provide  reference genome   annotate list  genes  use  number  genes statistic instead otherwise quast count  number  genes annotate  genemarkhmm  glimmerhmm   filter   count    lengths  one   specify minimum thresholds  variations  n50 base  align block  follow metrics  quast  new    similarities  gag correct  salzberg    assemblathons contig path   alignment graph earl      normalize n50 makinen    metric   give short descriptions   metrics see  supplementary methods   detail information nax  stand  align  range      combination   wellknown  metric  plantagoras number  misassemblies metric   compute  two step first  break  contigs  align block   contig  misassembly breakpoints per  previous definition  plantagora   break  multiple block   breakpoints additionally    unaligned regions within  contig  regions  remove   contig  split  block next  compute  ordinary  statistic   block instead    original contigs ngax  break contigs  align block  describe  nax    compute  ngx statistic instead     block   nax  ngax metrics require  reference genome   reference genome  different   sample  assemble  breakpoints  indels may represent true structural differences  visualization quast present  number  statistics  graphical form  support svg png  pdf format sample plot  present   supplementary material  plot  divide  several group nxlike plot  show  trend   ngx nax  ngax metrics   vary    informative   use n50 cumulative plot contigs  order  largest  smallest  number  base    type  cumulative plot consider  cumulative length plot show  number  base   first  contigs   vary  zero   number  contigs  cumulative number  complete genes  cumulative number  complete operons plot  compute similarly  content plot  show  distribution   content   contigs   value show  per cent         value show  number  nonoverlapping   windows whose  content    distribution  often gaussian bohlin    however    contaminants   different  content   often   superposition  multiple gaussians contig alignment plot fig   show alignment  contigs   reference genome   position  misassemblies   contigs colour cod indicate  block boundaries  share  multiple assemblies   show misassembled block  optional track show  read coverage along  reference genome  external file  hold  picture illustration etc object name  btt086f1pjpg fig  alignment  singlecell ecoli assemblies   reference genome   track  xaxis  genome position top track read coverage   logarithmic scale  red curve show coverage bin    windows blue position   xaxis  zero coverage even   bin   coverage coverage  highly nonuniform range    near     track comparison  position  align contigs contigs  align correctly  colour blue   boundaries agree within     side   least half   assemblies  green otherwise contigs  misassemblies  break  block  colour orange   boundaries agree   least half   assemblies  red otherwise contigs  stagger vertically   show  different shade   colour  distinguish  separate contigs include small ones quast also make comparative histograms  several metrics  number  complete genes  number  complete operons   genome fraction  histograms   metrics   add  well  compare assemblers   study  evaluate several   lead genome assemblers  three datasets escherichia coli  singlecell sample homo sapiens chromosome   bombus impatiens  bumble bee   publication time     finish assembly  ecoli dataset     assemblies  take  chitsaz     spade  idbaud assemblies  new  assemblies  hsapiens  bimpatiens   datasets  take  salzberg      article  present   quasts comparison statistics   sample plot compare ecoli assemblies see supplementary figure s3s29  supplementary table s2s8   plot  extend table  ecoli   comparisons  assemblers    two datasets  comparison  ecoli assemblies  reference genome  ecoli str  substr mg1655 blattner    available   ncbi website gene annotations  take    include several wellknown assemblers design  culture bacterial datasets eulersr pevzner    velvet zerbino  birney   soapdenovo      also include several recently introduce assemblers    adapt  design  scratch  handle singlecell datasets velvetsc  euler  velvetsc chitsaz     assembler spade bankevich     idbaud peng    table  show  spade  idbaud   best result  almost  metrics idbaud assemble  largest contig       smallest number  contigs   spade   larger nga50  idbaud   versus     assemble  higher percentage   genome  versus  spade also assemble  highest number  complete genes     idbaud  close second  however  spade  idbaud   misassemblies   three velvetbased assemblers table  comparison  assemblies   singlecell sample  ecoli  contigs  external file  hold  picture illustration etc object name  btt086i3jpg  assembler          contigs        nga50         largest         total         genome fraction           misassemblies          complete genes eulersr                                                             evsc                                                             idbaud                                                             soapdenovo                                                             spade                                                             velvet                                                             velvetsc                                                              best value   column  indicate  bold figure  show   contigs align   reference genome  reveal high similarity     assemblies evsc velvet  velvetsc generate assemblies  dozens  similar contigs   natural     assemblers  modifications  velvet  top track show  read coverage along  genome velvet   able  assemble lowcoverage regions   genome whereas  assemblers design  singlecell datasets velvetsc evsc spade  idbaud  much better although  course none    assemble  regions  literally  zero coverage",12
221,QUAST-LG,"Versatile genome assembly evaluation with QUAST-LG
Upper bound assembly construction We construct upper bound assembly based on a reference genome and a given set of reads. At first, the construction algorithm maps all reads to the reference genome and detects zero-coverage regions (Fig. 1a). We use Minimap2 (Li, 2017) for aligning long error-prone reads (PacBio and Nanopore) and BWA-MEM (Li, 2013) for short Illumina reads (paired-ends and mate-pairs). An external file that holds a picture, illustration, etc. Object name is bty266f1.jpg Fig. 1. Upper bound assembly construction. (a) All available reads (brown for long reads, orange for mate-pairs, and yellow for paired-ends) are mapped to the reference (gray) to compute zero-coverage genomic regions. Repeat sequences (red) are detected using repeat finder software. Non-repetitive covered fragments are reported as upper bound contigs. (b) The overlaps between the contigs (green), and either long or mate-pair reads are detected, and contigs are further joined to form upper bound scaffolds. (c) The gaps between adjacent contigs within a scaffold are filled either with reference sequence (for covered regions) or with stretches of N nucleotides (for coverage gaps). Unresolved repeats are added as separate sequences Further on, the lightweight Red (Girgis, 2015) de novo repeat finder is used to mark long genomic repeats in the reference (Fig. 1a). We call a repeat long if its length exceeds the median insert size of a paired-end library (when several paired-end libraries are available, the maximum median value is used). Among the detected repeated sequences, we select only those that occur at least twice in remote parts of the genome. Such long repeats cause ambiguities during the assembly, which may be resolved only by long reads or mate-pairs. Other long regions marked by Red appear to be short tandem repeats having multiple copies at the same genomic loci. To the best of our knowledge, such tandem repeats do not cause ambiguities and can be approximately resolved by the assemblers without using long-range information [e.g. using de Bruijn graph topology (Miller et al., 2010)]. Splitting the reference sequence by coverage gaps and long repeats results in a set of unique genomic fragments referred to as upper bound contigs, that however do not reflect the best possible assembly of the entire dataset. To achieve a more realistic upper bound, we detect the contigs that are connected by long reads or mate-pairs and further join them into upper bound scaffolds if the number of connections exceeds a small threshold n (Fig. 1b). In this study we used n = 1 for long reads and n = 2 for mate-pairs. We say that a long read connects two contigs if it simply overlaps with both contigs. A pair of reads connects contigs if the left read overlaps with the first contig and the right read overlaps with the second contig. During this analysis we ignore read pairs that map inconsistently or with abnormal insert size (in the first or the last decile). To enable efficient overlap detection between reads and upper bound contigs, we sort all reads according to their mapping positions. Thus, the scaffold construction algorithm requires O(N log ⁡N) time for read sorting and O(N) time for finding overlaps, where N is the total number of long and mate-pair reads used for scaffolding. Once upper bound contigs are joined into scaffolds, the gaps between adjacent contigs are filled with the corresponding genomic sequences from the reference genome, or—in case of coverage gaps—with stretches of N’s (Fig. 1c). Remaining unresolved repeats are added to the final upper bound assembly as separate sequences. 2.2 Adaption of conventional metrics for large genomes The key characteristics of the assembly quality are the assembly completeness (what fraction of the genome is assembled), correctness (how many errors the assembly contains) and contiguity (how many fragments the assembly consists of and how long they are). Both completeness and correctness can be accurately measured by QUAST-LG only when a high-quality reference genome is available. Some contiguity statistics, such as the well-known N50 metric, do not require a reference. However, when an estimate of the genome size is known, their more suitable analogues can be computed, namely NG50. If a reference sequence is available, we provide even more relevant insight by computing NGA50-like statistics (Gurevich et al., 2013), the contiguity measures based on error-free aligned assembly fragments rather than the initial contigs/scaffolds. The alignment against the reference genome appears to be the most time consuming step in the assembly evaluation, especially for large genomes. To address this bottleneck, we replaced an accurate and slow NUCmer aligner [from MUMmer v3.23 package (Kurtz et al., 2004)] used in original QUAST with a faster Minimap2 aligner (Li, 2017). The recently released MUMmer 4 package (Marcais et al., 2018) was also outperformed by Minimap2 in our benchmark experiments, albeit the speed increase in this case was not as substantial as the Minimap2’s improvement over the previous MUMmer version. We have thoroughly chosen Minimap2 options in order to maintain the alignment speed-accuracy ratio for different scenarios. In standard mode QUAST-LG runs alignment with the parameters enabling accuracy comparable with NUCmer which is suitable for small genomes. In ‘‐‐large’ mode QUAST-LG configures Minimap2 to achieve adequate running times for large and complex inputs. The assembly correctness is usually characterized by the number of large assembly errors, so-called misassemblies. Gurevich et al. (2013) define a misassembly breakpoint as a position in an assembled contig where the flanking sequences align to opposite strands (inversion), or to different chromosomes (translocation), or the inconsistency size δ (a gap or an overlap) between the alignments of the left and right flanking sequences on the reference is more than a predefined breakpoint threshold X (relocation). The alignments on the same strand of the same chromosome and having δ < X are considered as small errors and classified as local misassembly. Eukaryotic genomes usually contain a lot of transposable elements (TEs) which may cause discrepancies between the reference genome and the genome actually being assembled. These short variations result in a huge number of false positive misassemblies if computed according to the definition given above. To distinguish between true misassemblies and the ones caused by TEs, QUAST-LG performs an additional check of each relocation and inversion to identify possible TEs (Fig. 2). The identification procedure depends on the size of the breakpoint threshold X which optimal value should slightly exceed the length of the largest TE in the genome (the processing of tandem TE insertions and deletions is out of scope of this paper). The optimal value thus depends on the subject organism and we allow users to set it manually. For the sake of consistency, we used the same X = 7 kb in all benchmark experiments in this study (see Supplementary Methods for details on the value choice). This is also the default value in QUAST-LG in contrast to regular QUAST which uses X = 1 kb. An external file that holds a picture, illustration, etc. Object name is bty266f2.jpg Fig. 2. Detection of discrepancies caused by TEs. On each subfigure, we plot the reference genome R (top), the contig C (bottom), their matching fragments (blue and green bars for the positions in C and R, respectively) and locations of TEs (violet bars) causing discrepancies in the mapping. The inconsistencies in the alignments are shown by arrows and δ characters. (a) TE is present in R and missing in C. Since δ here is equal to the TE’s length, a specifically chosen breakpoint threshold X transforms classification of this discrepancy from a relocation to a local misassembly (X > δ). (b) TE is located inside C but its position in R is significantly away from the rest of C mappings and could also be located on the opposite strand. Original QUAST would treat this situation as two misassembly breakpoints (relocations or inversions) because δ1 and δ2 are usually much higher than X. In contrast, QUAST-LG classifies such pattern as possible TE since it computes δ = δ2 − δ1, that is again equal to the TE’s length and could be prevailed by appropriate X. (c) TE is the first or the last alignment fragment in C, while its location on R is large distance δ away from the neighboring C fragment. QUAST-LG cannot reliably distinguish this situation from a real relocation/inversion: it would need to be able to recognize TE based on its genomic sequence, which is out of scope of this paper 2.3 Best set of alignments selection Long contigs are rarely mapped to the reference perfectly as a single unambiguous alignment. An alignment software typically reports multiple alignment fragments from different locations of the genome. This may happen due to the presence of genomic repeats and TEs in the reference genome and in some cases because of algorithmic issues in the assembly or/and alignment software. QUAST-LG attempts to accurately assess each contig and select a set of non-overlapping alignments that maximizes the total alignment score, which is defined as a function of the alignment lengths, mapping qualities and side by side inconsistencies (misassemblies). This problem is known as the collinear chaining problem (Myers and Miller, 1995) and it is usually solved by sequence aligners for a low-level chaining, that is joining short matching seeds into larger alignment fragments. For example, MUMmer (Kurtz et al., 2004) combines maximal unique matches and Minimap2 (Li, 2017) chains minimizers (Roberts et al., 2004). Here we implement a dynamic programming algorithm called BestSetSelection for a high-level chaining, that is combining alignment fragments (see Supplementary Methods). Our algorithm is conceptually similar to delta-filter utility from MUMmer package (Kurtz et al., 2004) but our approach includes a comprehensive set of penalties for various misassembly events. This feature allows BestSetSelection to correctly resolve many complex sets of alignments, which are typical for eukaryotic assemblies, and produce a more accurate chaining than delta-filter in our benchmark experiments. BestSetSelection is a quadratic algorithm with respect to the number of fragment alignments per contig which is usually fine since this number is generally small (up to 100). However, this may cause a significant slowdown in case of large genomes evaluation when the number of alignments may reach dozens of thousands in some contigs. Although there are chaining algorithms with sub-quadratic time complexity (Abouelhoda and Ohlebusch, 2005), they are not applicable to our gap cost function and associated with a large constant. Instead, we have implemented a simple heuristic that always finds the best alignment set or one of the several sets that maximize the score (Supplementary Methods). And even though the heuristic idea does not guarantee the speed up in theory, it significantly dropped the running time of all six benchmark dataset evaluations. 2.4 K-mer-based quality metrics As shown above, the presence of many TEs and other specific features of eukaryotic genomes significantly complicates assembly evaluation. Although QUAST-LG adjustment of the conventional completeness and correctness measures improve the assessment, it may still not be good enough to form the complete picture of eukaryote assembly quality. Here we propose to assess assemblies using a completely different strategy inspired by the evaluation procedures in Putnam et al. (2016) and Chapman et al. (2016) and generalized for an arbitrary genome analysis in QUAST-LG. This strategy is based on the analysis of unique k-mers (non-repeated sequences of length k) both in the reference genome and in the assembly. If k value is sufficiently large (QUAST-LG uses 101-mers by default), unique k-mers appear to be widespread across the genome. For instance, the fruit fly genome contains 122 millions unique 101-mers out of 137 millions total 101-mers. The existence and the positions of such k-mers in the assembly describe its completeness and correctness. We use KMC 3 (Kokot et al., 2017) to detect all unique k-mers in the reference genome. The percentage of these k-mers detected in the assembly is reported as its k-mer completeness. Compared to the genome fraction completeness measure, the k-mer-based value accounts for per-base quality of an assembly which is usually highly important for the downstream analysis such as genome annotation. The benchmarking below shows that assemblies with a very similar genome fraction may have completely different k-mer completeness due to a high mismatch and indel error rates. The k-mer-based correctness is calculated based on a small uniformly distributed subset of all unique k-mers in order to speed up the computation. We select the subset in a way that any two k-mers from the subset are at least 1 kb apart from each other in the reference genome R. The subset is provided to KMC and it identifies contigs having at least two k-mers. The contig position of each detected k-mer is examined and we refer to a consecutive list of k-mers k1, k2, …, kn (where n ≥ 2) in a contig C as a marker if for any i ∈ [1, n − 1] the distances between ki and ki+1 in C and R are equivalent within a small error (5% of the distance in R by default). We further process contigs having at least two markers to check whether the relative positions of adjacent markers mj and mj+1 correlate with their locations in R. QUAST-LG reports a k-mer-based translocation breakpoint if mj and mj+1 are originated from different chromosomes and a k-mer-based relocation if the markers are from the same chromosome but the inconsistency between their positions in C and R is larger than a predefined threshold (we use 100 kb threshold by default). We further refer to k-mer-based translocations and relocations as k-mer-based misjoins to exclude confusion with regular QUAST misassemblies. K-mer-based misjoins are essentially similar to the regular misassembly metrics, except that they are focused on the most critical assembly errors. The key benefit of these measures is in their tolerance to inconsistencies caused by TEs, since TEs mostly correspond to genomic repeats and thus lack unique k-mers. For example, k-mer-based relocations can successfully resolve the situations when a contig starts or ends with a TE which cause an ambiguity in the regular misassembly detection algorithm (Fig. 2c). 2.5 Evaluation without a reference genome In most real assembly projects a reference genome sequence is not available and the assembly quality assessment must rely on other sources of information. The primary purpose of QUAST-LG is the reference-based analysis but we also include a few de novo eukaryote-targeted completeness measures to make our tool useful in a wider set of applications. QUAST-LG is supplied with GeneMark-ES (Lomsadze et al., 2005) software for de novo gene prediction in eukaryotic genomes. However, despite the relevance of the gene finding in assessing downstream analysis perspectives its heuristic nature may result in a misleading output in some experiments. For instance, an assembly may contain multiple copies of one gene which will be reported several times. To counter this, we additionally use BUSCO (Simao et al., 2015) to find the number of assembled conserved genes that are present nearly universally in eukaryotes in a single copy. To demonstrate how BUSCO completeness correlates with more accurate reference-based quality metrics, we added its computation in all our benchmark experiments. Note that reference-free correctness metrics are out of scope of QUAST-LG and we recommend using specialized de novo evaluation tools for this scenario. For instance, REAPR (Hunt et al., 2013) identifies likely assembly and scaffolding errors based on paired reads mapping. Another example is KAT (Mapleson et al., 2017) that compares the k-mer spectrum of the assembly to the k-mer spectrum of the reads, which is quite useful in identifying missing sequence, collapsed repeats and expanded sequences in the assembly. Go to:",AssemblyEvaluation,"versatile genome assembly evaluation  quastlg
upper bind assembly construction  construct upper bind assembly base   reference genome   give set  read  first  construction algorithm map  read   reference genome  detect zerocoverage regions fig   use minimap2    align long errorprone read pacbio  nanopore  bwamem    short illumina read pairedends  matepairs  external file  hold  picture illustration etc object name  bty266f1jpg fig  upper bind assembly construction   available read brown  long read orange  matepairs  yellow  pairedends  map   reference gray  compute zerocoverage genomic regions repeat sequence red  detect use repeat finder software nonrepetitive cover fragment  report  upper bind contigs   overlap   contigs green  either long  matepair read  detect  contigs   join  form upper bind scaffold   gap  adjacent contigs within  scaffold  fill either  reference sequence  cover regions   stretch   nucleotides  coverage gap unresolved repeat  add  separate sequence    lightweight red girgis   novo repeat finder  use  mark long genomic repeat   reference fig   call  repeat long   length exceed  median insert size   pairedend library  several pairedend libraries  available  maximum median value  use among  detect repeat sequence  select    occur  least twice  remote part   genome  long repeat cause ambiguities   assembly  may  resolve   long read  matepairs  long regions mark  red appear   short tandem repeat  multiple copy    genomic loci   best   knowledge  tandem repeat   cause ambiguities    approximately resolve   assemblers without use longrange information  use  bruijn graph topology miller    split  reference sequence  coverage gap  long repeat result   set  unique genomic fragment refer   upper bind contigs  however   reflect  best possible assembly   entire dataset  achieve   realistic upper bind  detect  contigs   connect  long read  matepairs   join   upper bind scaffold   number  connections exceed  small threshold  fig    study  use     long read      matepairs  say   long read connect two contigs   simply overlap   contigs  pair  read connect contigs   leave read overlap   first contig   right read overlap   second contig   analysis  ignore read pair  map inconsistently   abnormal insert size   first   last decile  enable efficient overlap detection  read  upper bind contigs  sort  read accord   map position thus  scaffold construction algorithm require  log ⁡ time  read sort   time  find overlap     total number  long  matepair read use  scaffold  upper bind contigs  join  scaffold  gap  adjacent contigs  fill   correspond genomic sequence   reference genome — case  coverage gaps— stretch   fig  remain unresolved repeat  add   final upper bind assembly  separate sequence  adaption  conventional metrics  large genomes  key characteristics   assembly quality   assembly completeness  fraction   genome  assemble correctness  many errors  assembly contain  contiguity  many fragment  assembly consist    long    completeness  correctness   accurately measure  quastlg    highquality reference genome  available  contiguity statistics    wellknown n50 metric   require  reference however   estimate   genome size  know   suitable analogues   compute namely ng50   reference sequence  available  provide even  relevant insight  compute nga50like statistics gurevich     contiguity measure base  errorfree align assembly fragment rather   initial contigsscaffolds  alignment   reference genome appear     time consume step   assembly evaluation especially  large genomes  address  bottleneck  replace  accurate  slow nucmer aligner  mummer  package kurtz    use  original quast   faster minimap2 aligner    recently release mummer  package marcais     also outperform  minimap2   benchmark experiment albeit  speed increase   case    substantial   minimap2s improvement   previous mummer version   thoroughly choose minimap2 options  order  maintain  alignment speedaccuracy ratio  different scenarios  standard mode quastlg run alignment   parameters enable accuracy comparable  nucmer   suitable  small genomes  ‐‐large mode quastlg configure minimap2  achieve adequate run time  large  complex input  assembly correctness  usually characterize   number  large assembly errors socalled misassemblies gurevich    define  misassembly breakpoint   position   assemble contig   flank sequence align  opposite strand inversion   different chromosomes translocation   inconsistency size   gap   overlap   alignments   leave  right flank sequence   reference     predefined breakpoint threshold  relocation  alignments    strand    chromosome       consider  small errors  classify  local misassembly eukaryotic genomes usually contain  lot  transposable elements tes  may cause discrepancies   reference genome   genome actually  assemble  short variations result   huge number  false positive misassemblies  compute accord   definition give   distinguish  true misassemblies   ones cause  tes quastlg perform  additional check   relocation  inversion  identify possible tes fig   identification procedure depend   size   breakpoint threshold   optimal value  slightly exceed  length   largest    genome  process  tandem  insertions  deletions    scope   paper  optimal value thus depend   subject organism   allow users  set  manually   sake  consistency  use         benchmark experiment   study see supplementary methods  detail   value choice   also  default value  quastlg  contrast  regular quast  use      external file  hold  picture illustration etc object name  bty266f2jpg fig  detection  discrepancies cause  tes   subfigure  plot  reference genome  top  contig  bottom  match fragment blue  green bar   position     respectively  locations  tes violet bar cause discrepancies   map  inconsistencies   alignments  show  arrows   character    present    miss   since    equal   tes length  specifically choose breakpoint threshold  transform classification   discrepancy   relocation   local misassembly       locate inside    position    significantly away   rest   mappings  could also  locate   opposite strand original quast would treat  situation  two misassembly breakpoints relocations  inversions      usually much higher    contrast quastlg classify  pattern  possible  since  compute         equal   tes length  could  prevail  appropriate      first   last alignment fragment     location    large distance  away   neighbor  fragment quastlg cannot reliably distinguish  situation   real relocationinversion  would need   able  recognize  base   genomic sequence     scope   paper  best set  alignments selection long contigs  rarely map   reference perfectly   single unambiguous alignment  alignment software typically report multiple alignment fragment  different locations   genome  may happen due   presence  genomic repeat  tes   reference genome    case   algorithmic issue   assembly orand alignment software quastlg attempt  accurately assess  contig  select  set  nonoverlapping alignments  maximize  total alignment score   define   function   alignment lengths map qualities  side  side inconsistencies misassemblies  problem  know   collinear chain problem myers  miller     usually solve  sequence aligners   lowlevel chain   join short match seed  larger alignment fragment  example mummer kurtz    combine maximal unique match  minimap2   chain minimizers roberts      implement  dynamic program algorithm call bestsetselection   highlevel chain   combine alignment fragment see supplementary methods  algorithm  conceptually similar  deltafilter utility  mummer package kurtz      approach include  comprehensive set  penalties  various misassembly events  feature allow bestsetselection  correctly resolve many complex set  alignments   typical  eukaryotic assemblies  produce   accurate chain  deltafilter   benchmark experiment bestsetselection   quadratic algorithm  respect   number  fragment alignments per contig   usually fine since  number  generally small    however  may cause  significant slowdown  case  large genomes evaluation   number  alignments may reach dozens  thousands   contigs although   chain algorithms  subquadratic time complexity abouelhoda  ohlebusch     applicable   gap cost function  associate   large constant instead   implement  simple heuristic  always find  best alignment set  one   several set  maximize  score supplementary methods  even though  heuristic idea   guarantee  speed   theory  significantly drop  run time   six benchmark dataset evaluations  kmerbased quality metrics  show   presence  many tes   specific feature  eukaryotic genomes significantly complicate assembly evaluation although quastlg adjustment   conventional completeness  correctness measure improve  assessment  may still   good enough  form  complete picture  eukaryote assembly quality   propose  assess assemblies use  completely different strategy inspire   evaluation procedures  putnam     chapman     generalize   arbitrary genome analysis  quastlg  strategy  base   analysis  unique kmers nonrepeated sequence  length     reference genome    assembly   value  sufficiently large quastlg use mers  default unique kmers appear   widespread across  genome  instance  fruit fly genome contain  millions unique mers    millions total mers  existence   position   kmers   assembly describe  completeness  correctness  use kmc  kokot     detect  unique kmers   reference genome  percentage   kmers detect   assembly  report   kmer completeness compare   genome fraction completeness measure  kmerbased value account  perbase quality   assembly   usually highly important   downstream analysis   genome annotation  benchmarking  show  assemblies    similar genome fraction may  completely different kmer completeness due   high mismatch  indel error rat  kmerbased correctness  calculate base   small uniformly distribute subset   unique kmers  order  speed   computation  select  subset   way   two kmers   subset   least   apart      reference genome   subset  provide  kmc   identify contigs   least two kmers  contig position   detect kmer  examine   refer   consecutive list  kmers   …    ≥    contig    marker     ∈      distance          equivalent within  small error    distance    default   process contigs   least two markers  check whether  relative position  adjacent markers    correlate   locations   quastlg report  kmerbased translocation breakpoint      originate  different chromosomes   kmerbased relocation   markers     chromosome   inconsistency   position      larger   predefined threshold  use   threshold  default   refer  kmerbased translocations  relocations  kmerbased misjoins  exclude confusion  regular quast misassemblies kmerbased misjoins  essentially similar   regular misassembly metrics except    focus    critical assembly errors  key benefit   measure    tolerance  inconsistencies cause  tes since tes mostly correspond  genomic repeat  thus lack unique kmers  example kmerbased relocations  successfully resolve  situations   contig start  end     cause  ambiguity   regular misassembly detection algorithm fig   evaluation without  reference genome   real assembly project  reference genome sequence   available   assembly quality assessment must rely   source  information  primary purpose  quastlg   referencebased analysis   also include    novo eukaryotetargeted completeness measure  make  tool useful   wider set  applications quastlg  supply  genemarkes lomsadze    software   novo gene prediction  eukaryotic genomes however despite  relevance   gene find  assess downstream analysis perspectives  heuristic nature may result   mislead output   experiment  instance  assembly may contain multiple copy  one gene    report several time  counter   additionally use busco simao     find  number  assemble conserve genes   present nearly universally  eukaryotes   single copy  demonstrate  busco completeness correlate   accurate referencebased quality metrics  add  computation    benchmark experiment note  referencefree correctness metrics    scope  quastlg   recommend use specialize  novo evaluation tool   scenario  instance reapr hunt    identify likely assembly  scaffold errors base  pair read map another example  kat mapleson     compare  kmer spectrum   assembly   kmer spectrum   read   quite useful  identify miss sequence collapse repeat  expand sequence   assembly  ",12
222,BUSCO,"BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs
Genomics data acquisition continues to accelerate, however, the short lengths of sequencing reads make their assembly into full-length chromosomes extremely challenging. To gauge potential limitations and implement improvements it is thus important to assess the quality of the resulting data. Proposed measures (Clark et al., 2013; Gurevich et al., 2013; Hunt et al., 2013; Simpson, 2014) reflect methodologies, e.g. per-base error rates, insert size distributions; or genome biases, e.g. k-mer distributions; or fragment (contig) length distributions, e.g. N50, which summarizes assembly contiguity in a single number: half the genome is assembled on contigs of length N50 or longer. However, such measures do not assess assembly completeness in terms of gene content: an important consideration that also affects data interpretation and helps to guide improved assembly and annotation strategies. With the growing number of available sequenced genomes, knowledge of their gene content is consolidating and can be used to develop an evolutionary measure of genome completeness. Here, we revisit the idea of using known genes to measure genome assembly and annotation completeness (Mende et al., 2013; Parra et al., 2009), by introducing a citable notation for well-defined measures, compiling the comprehensive datasets to support such assessments, and offering these as an off-the-shelf software. As proposed previously (Waterhouse et al., 2013), Benchmarking Universal Single-Copy Orthologs (BUSCO) are ideal for such quantifications of completeness, as the expectations for these genes to be found in a genome and to be found only in single-copy are evolutionarily sound. We used our OrthoDB database of orthologs (www.orthodb.org) to define BUSCO sets for six major phylogenetic clades. Sampling hundreds of genomes, orthologous groups with single-copy orthologs in >90% of species were selected. Importantly, this threshold accommodates the fact that even well-conserved genes can be lost in some lineages, as well as allowing for incomplete gene annotations and rare gene duplications. Subsequent filtering, e.g. on sequence uniqueness and conservation levels [see Supplementary Online Material (SOM) for details], resulted in BUSCO sets representing 3023 genes for vertebrates, 2675 for arthropods, 843 for metazoans, 1438 for fungi and 429 for eukaryotes. We also adopted 40 universal marker genes proposed for the assessment of prokaryotic genomes (Mende et al., 2013). The clades spanning many phyla offer comprehensive coverage of the tree of life, while the more narrowly defined clades provide a much greater resolution with much larger BUSCO sets. These are applicable not only to the assessment of genome assemblies, but also to annotated gene sets, as well as assembled transcriptomes (Fig. 1). Additionally, as near-universal single-copy markers, the recovered genes are ideal for species phylogeny reconstructions. Fig. 1. BUSCO assessment workflow and relative run-times Open in new tabDownload slide BUSCO assessment workflow and relative run-times We propose intuitive metrics to describe genome, gene set or transcriptome completeness in BUSCO notation - C:complete [D:duplicated], F:fragmented, M:missing, n:number of genes used (Fig. 1). The recovered genes are classified as ‘complete’ when their lengths are within two standard deviations of the BUSCO group mean length (i.e. within ∼95% expectation, Supplementary Fig. S1). ‘Complete’ genes found with more than one copy are classified as ‘duplicated’. These should be rare, as BUSCOs are evolving under single-copy control (Waterhouse et al., 2011), and the recovery of many duplicates may therefore indicate erroneous assembly of haplotypes. Genes only partially recovered are classified as ‘fragmented’, and genes not recovered are classified as ‘missing’. Finally, the ‘number of genes used’ indicates the resolution and hence is informative of the confidence of these assessments. Using HMMER 3 (Eddy, 2011) hidden Markov model (HMM) profiles from amino acid alignments, the core of the analysis workflow (Fig. 1) assesses whether BUSCO gene matches are orthologous or not (i.e. satisfy BUSCO group-specific bitscore cut-offs; detailed in SOM), and classifies positive matches as complete or fragmented. This core analysis is the same for assessing genomes, transcriptomes or gene sets. However, additional analyses are required to first annotate genes from transcriptomes and genomes. The simple longest open reading frame approach performs well for transcriptomes. For genomes, gene annotation is performed with Augustus (Keller et al., 2011), guided by amino acid BUSCO group block-profiles, on genomic loci detected with tBLASTn searches using BUSCO group consensus sequences (detailed in SOM). Although this gene prediction approach may have its limitations and biases, they are consistent across different species, making for fair comparisons. Conveniently, the thousands of confident BUSCO gene models provide an excellent gene predictor training set for use as part of genome annotation pipelines. Table 1 reports BUSCO notation assessments of five diverse species for both their genome assemblies and their annotated gene sets. Assessing 70 genomes, 163 gene sets, and 96 transcriptomes revealed substantial variability of completeness (Supplementary Table S1). Poor correlation with scaffold N50 (Supplementary Fig. S2) highlights how completeness provides important complementary information for quality assessment. Nevertheless, the fact that some genome assemblies appear less complete than their corresponding gene sets (e.g. H. sapiensTable 1) reveals limitations of the BUSCO gene prediction step. On the other hand, a reversal of this trend (e.g. A. nidulansTable 1) suggests that the annotated gene set may be missing some BUSCO gene matches that are in fact present in the genome. Thus, it should be noted that while BUSCO assessments aim to robustly estimate completeness of the datasets, technical limitations (particularly gene prediction) may inflate proportions of ‘fragmented’ and ‘missing’ BUSCOs, especially for large genomes. More ‘missing’ BUSCOs may also be reported for species that are highly derived with respect to the assessment clade—even with high-quality genomes (e.g. C. elegansTable 1)—reflecting the organism’s evolutionary history rather than an incomplete assembly. Table 1. Assessment of fruitfly (D. mela,), nematode worm (C. eleg,), human (H. sapi,), owl limpet (L. giga,), and fungus (A. nidu,) genome assemblies (upper row) and gene sets (lower row) in BUSCO notation (C:complete [D:duplicated], F:fragmented, M:missing, n: gene number) graphic Open in new tab Comparing genome to gene set completeness of 40 species using a 250-BUSCO eukaryotic subset reveals generally consistent assessments across highly divergent lineages from fungi to human (Fig. 2). Employing the 248 genes of the Core Eukaryotic Gene Mapping Approach (CEGMA) (Parra et al., 2007) in a like-for-like comparison (i.e. implementing gene set assessments using CEGMA HMMs, see SOM for details) appears somewhat less consistent (Fig. 2, BUSCO linear regression is closer to the diagonal). Additionally, in comparable 250-BUSCO and 248-CEGMA assessments BUSCO run-times are substantially faster, ∼2x for small genomes and ∼10x for large genomes, but of course the higher resolutions achievable with the thousands of vertebrate, arthropod and fungal BUSCO sets do require longer run-times (Supplementary Table S2). Run-times are generally proportional to the size of the BUSCO set used and the sizes of the genomes being assessed, e.g. on 4 CPU cores with up to 8 GB memory: the 180 Mbp fruit fly genome ran for 3.2–7.6 h, while the 3381 Mbp human genome ran for 13–29 h with 843 metazoan and 2675 arthropod or 3023 vertebrate BUSCOs, respectively (Supplementary Table S2). Fig. 2. BUSCOs (eukaryotic subset) and CEGMA CEGs recovered from 40 representative genome assemblies and their respective gene sets. Inset: number of genes in each BUSCO set and the CEGMA CEGs Open in new tabDownload slide BUSCOs (eukaryotic subset) and CEGMA CEGs recovered from 40 representative genome assemblies and their respective gene sets. Inset: number of genes in each BUSCO set and the CEGMA CEGs BUSCO quality assessments provide high-resolution quantifications citeable in the simple C[D],F,M,n notation for genomes, gene sets and transcriptomes. This facilitates informative comparisons, e.g. of newly sequenced draft genome assemblies to those of gold-standard models, or to quantify iterative improvements to assemblies or annotations. BUSCO assessments therefore offer intuitive metrics, based on evolutionarily informed expectations of gene content from hundreds of species, to gauge completeness of rapidly accumulating genomic data and satisfy an Iberian’s quest for quality—‘Busco calidad/qualidade’.",AssemblyEvaluation,"busco assess genome assembly  annotation completeness  singlecopy orthologs
genomics data acquisition continue  accelerate however  short lengths  sequence read make  assembly  fulllength chromosomes extremely challenge  gauge potential limitations  implement improvements   thus important  assess  quality   result data propose measure clark    gurevich    hunt    simpson  reflect methodologies  perbase error rat insert size distributions  genome bias  kmer distributions  fragment contig length distributions  n50  summarize assembly contiguity   single number half  genome  assemble  contigs  length n50  longer however  measure   assess assembly completeness  term  gene content  important consideration  also affect data interpretation  help  guide improve assembly  annotation strategies   grow number  available sequence genomes knowledge   gene content  consolidate    use  develop  evolutionary measure  genome completeness   revisit  idea  use know genes  measure genome assembly  annotation completeness mende    parra     introduce  citable notation  welldefined measure compile  comprehensive datasets  support  assessments  offer    offtheshelf software  propose previously waterhouse    benchmarking universal singlecopy orthologs busco  ideal   quantifications  completeness   expectations   genes   find   genome    find   singlecopy  evolutionarily sound  use  orthodb database  orthologs wwworthodborg  define busco set  six major phylogenetic clades sample hundreds  genomes orthologous group  singlecopy orthologs    species  select importantly  threshold accommodate  fact  even wellconserved genes   lose   lineages  well  allow  incomplete gene annotations  rare gene duplications subsequent filter   sequence uniqueness  conservation level see supplementary online material som  detail result  busco set represent  genes  vertebrates   arthropods   metazoans   fungi    eukaryotes  also adopt  universal marker genes propose   assessment  prokaryotic genomes mende     clades span many phyla offer comprehensive coverage   tree  life    narrowly define clades provide  much greater resolution  much larger busco set   applicable     assessment  genome assemblies  also  annotate gene set  well  assemble transcriptomes fig  additionally  nearuniversal singlecopy markers  recover genes  ideal  species phylogeny reconstructions fig  busco assessment workflow  relative runtimes open  new tabdownload slide busco assessment workflow  relative runtimes  propose intuitive metrics  describe genome gene set  transcriptome completeness  busco notation  ccomplete dduplicated ffragmented mmissing nnumber  genes use fig   recover genes  classify  complete   lengths  within two standard deviations   busco group mean length  within  expectation supplementary fig  complete genes find    one copy  classify  duplicate    rare  buscos  evolve  singlecopy control waterhouse      recovery  many duplicate may therefore indicate erroneous assembly  haplotypes genes  partially recover  classify  fragment  genes  recover  classify  miss finally  number  genes use indicate  resolution  hence  informative   confidence   assessments use hmmer  eddy  hide markov model hmm profile  amino acid alignments  core   analysis workflow fig  assess whether busco gene match  orthologous    satisfy busco groupspecific bitscore cutoffs detail  som  classify positive match  complete  fragment  core analysis     assess genomes transcriptomes  gene set however additional analyse  require  first annotate genes  transcriptomes  genomes  simple longest open read frame approach perform well  transcriptomes  genomes gene annotation  perform  augustus keller    guide  amino acid busco group blockprofiles  genomic loci detect  tblastn search use busco group consensus sequence detail  som although  gene prediction approach may   limitations  bias   consistent across different species make  fair comparisons conveniently  thousands  confident busco gene model provide  excellent gene predictor train set  use  part  genome annotation pipelines table  report busco notation assessments  five diverse species    genome assemblies   annotate gene set assess  genomes  gene set   transcriptomes reveal substantial variability  completeness supplementary table  poor correlation  scaffold n50 supplementary fig  highlight  completeness provide important complementary information  quality assessment nevertheless  fact   genome assemblies appear less complete   correspond gene set   sapienstable  reveal limitations   busco gene prediction step    hand  reversal   trend   nidulanstable  suggest   annotate gene set may  miss  busco gene match    fact present   genome thus    note   busco assessments aim  robustly estimate completeness   datasets technical limitations particularly gene prediction may inflate proportion  fragment  miss buscos especially  large genomes  miss buscos may also  report  species   highly derive  respect   assessment clade—even  highquality genomes   eleganstable —reflecting  organisms evolutionary history rather   incomplete assembly table  assessment  fruitfly  mela nematode worm  eleg human  sapi owl limpet  giga  fungus  nidu genome assemblies upper row  gene set lower row  busco notation ccomplete dduplicated ffragmented mmissing  gene number graphic open  new tab compare genome  gene set completeness   species use  busco eukaryotic subset reveal generally consistent assessments across highly divergent lineages  fungi  human fig  employ   genes   core eukaryotic gene map approach cegma parra      likeforlike comparison  implement gene set assessments use cegma hmms see som  detail appear somewhat less consistent fig  busco linear regression  closer   diagonal additionally  comparable busco  cegma assessments busco runtimes  substantially faster   small genomes    large genomes   course  higher resolutions achievable   thousands  vertebrate arthropod  fungal busco set  require longer runtimes supplementary table  runtimes  generally proportional   size   busco set use   size   genomes  assess    cpu core      memory   mbp fruit fly genome run       mbp human genome run      metazoan   arthropod   vertebrate buscos respectively supplementary table  fig  buscos eukaryotic subset  cegma cegs recover   representative genome assemblies   respective gene set inset number  genes   busco set   cegma cegs open  new tabdownload slide buscos eukaryotic subset  cegma cegs recover   representative genome assemblies   respective gene set inset number  genes   busco set   cegma cegs busco quality assessments provide highresolution quantifications citeable   simple cdfmn notation  genomes gene set  transcriptomes  facilitate informative comparisons   newly sequence draft genome assemblies    goldstandard model   quantify iterative improvements  assemblies  annotations busco assessments therefore offer intuitive metrics base  evolutionarily inform expectations  gene content  hundreds  species  gauge completeness  rapidly accumulate genomic data  satisfy  iberians quest  quality—busco calidadqualidade",12
223,dnAQET,"dnAQET: a framework to compute a consolidated metric for benchmarking quality of de novo assemblies
The dnAQET framework comprises of two main steps: (i) aligning assembled scaffolds (contigs) to a trusted reference genome and then (ii) calculating quality scores for the scaffolds and the whole assembly. For the alignment step, dnAQET provides two separate alignment tools for users to choose. The first one is the Nucmer pipeline, from the MUMmer4 package [23], whose predecessor in MUMmer3 package [24] is a very widely used general purpose alignment tool to map long DNA sequences. The more recently released MUMmer4 package contains a much faster and memory efficient Nucmer version that can handle large genomes. The most recent version of the Nucmer is used in dnAQET. The second option for alignment that dnAQET offers is the Minimap2 aligner [25], which is also a very fast pairwise aligner for nucleotide sequences. To enhance the computational performance, the alignment process is broken down into three sub-steps: (a) partitioning of the reference genome and the assembly files into smaller chunks, (b) aligning each partition of the assembly against each partition of the reference genome in parallel, and finally (c) filtering the redundant and overlapping alignments for each scaffold. After filtering, the remaining alignments are used to compute the quality scores. Scalable alignment of scaffolds to a reference genome The dnAQET handles alignment of scaffolds to a reference genome in a parallelized manner by partitioning the assembly file into multiple approximately-equal sized files and the reference genome into multiple reference files, each containing a single chromosome (Additional file 1: Figure S1), enabling the method to be scalable for handling assemblies of large genomes. The total number of partition files of an assembly can be determined by the user, but its default value is set to one. The tool distributes the scaffolds to the user specified number of files in such a way that the total number of base pairs contained in each file would be similar across the partitions. For partitioning the genome, dnAQET distributes the chromosomes of the reference genome into multiple files so that each assembly partition can be aligned to a single chromosome independently on a High-Performance Computing (HPC) or a multi-threaded computing environment. Computing quality scores for scaffolds The dnAQET parses the alignment results for each scaffold and filters out the redundant and ambiguous alignments to obtain the longest consistent matches between the scaffold and the reference. For this purpose, we adapted the underlying algorithm of delta-filter utility in the MUMmer package and implemented the same approach in dnAQET. When the alignment step is completed either using Nucmer or Minimap2, the alignment results are scanned using appropriate parsers specifically designed to parse data in the corresponding alignment format. They are then converted into an internal alignment format to be filtered using our filtering algorithm. Note that the filtering step is independent of the chosen alignment tool and is applied to all alignment results no matter what tool is used to generate them. We then compute a quality score of an individual contig using the set of best alignments based on the total number of aligned base pairs in the scaffold (reward), the total number of misassembly determined in the scaffold (penalty), and the length of the scaffold (length scaling coefficient). The reward of a scaffold is supposed to be directly proportional to the total number of bases that are aligned to the reference genome by the alignment tool. Based on this assumption, dnAQET assigns a reward value, denoted by R(s), to a scaffold s that is equivalent to the ratio of the total number of aligned bases to the total number of bases in s without the scaffolding gaps. 𝑅(𝑠)=𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑎𝑙𝑖𝑔𝑛𝑒𝑑 𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠 𝑖𝑛 𝑠𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠 𝑖𝑛 𝑠−𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠 𝑖𝑛 𝑡ℎ𝑒 𝑠𝑐𝑎𝑓𝑓𝑜𝑙𝑑𝑖𝑛𝑔 𝑔𝑎𝑝𝑠 In the above formulation, the total number of ambiguous base pairs, which are detected to be scaffolding gaps, are subtracted from the total size of the scaffold (see the denominator in the above formula). This avoids improper reduction of the reward of a scaffold. The penalty assigned to a scaffold is directly related to the misassembly, which is basically summarized as the inconsistencies between the flanking alignments of a scaffold and the reference. In dnAQET, we consider three types of misassembly, which is also consistent with the types and definitions of the misassembly reported by previous literature [21]. These misassembly types (Fig. 1) are described in detail below. Fig. 1 figure1 Illustration of misassembly types. The green and blue bars indicate two flanking sequences of a scaffold and the white bars represent regions in a reference genome. The relocations with a distance (a) and an overlap (b) are marked with white and yellow bars under the reference bars, respectively. Translocation is denoted by two chromosomes (c). The arrows depict the directions of strands with red to show the inversion (d) Full size image Relocation, a type of misassembly in a scaffold, can happen in two cases: two consecutive sequence segments of the scaffold are aligned on the same chromosome with a separation distance of more than t base pairs (Fig. 1a), or they are aligned on the same chromosome with an overlap of more than t base pairs (Fig. 1b). The relocation threshold t is 1000 base pairs by default and can be adjusted by the user. A distance or overlap between the alignments of the flanking sequences smaller than t is not considered as a relocation. Translocation is a type of misassembly that is observed when the two flanking sequences of a scaffold are aligned to two different chromosomes in the reference genome (Fig. 1c). Inversion is a type of misassembly that occurs in cases where the two flanking sequences of a scaffold are aligned in the opposite strands of the same chromosome (Fig. 1d). In theory, one would expect a perfect alignment of each reference chromosome (scaffold) back to itself. However, due to gaps (ambiguous sequences), repetitive sequences in the reference genome and limitations of alignment algorithms, it is still possible to observe misalignment between two identical long fragment sequences (> 1 MB), which is considered as artifact of MUMmer and MiniMap2. Thus, dnAQET considers the fact that some artifacts should be expected even though a scaffold is assembled perfectly, and these artifacts should be correlated to the length of assembled scaffold, i.e. the longer a scaffold is, the more artifacts should be expected. When dnAQET processes a scaffold for computing the penalty, it needs to decide how the misassembly detected in this scaffold compares to the artifact expected from a scaffold of this size (l) with a given relocation threshold (t). The dnAQET uses a regression model to find the expected artifact given a scaffold with size, l, and a relocation threshold, t, which is used to decide the relocation type of misassembly. For each reference chromosome, dnAQET first randomly creates a set of artificial contigs/scaffolds, which cover the whole chromosome with 1X depth of coverage. Then these contigs, along with the original reference chromosome sequence, are aligned back to the whole reference genome. At last, the total number of misassembly is computed for each contig/scaffold and for each relocation threshold t in the range [100, 10,000] with a 100 base pair increments. The computed artifact is fit to the following model: 𝜀=𝛼𝑐𝑙+𝛽𝑐𝑡+𝑘𝑐 via least-squares regression, where ε is the artifact; αc and βc are model parameters and kc is the intercept for the model obtained for chromosome c. To determine the model parameters and the intercept, dnAQET fits the observed artifact in each of the chromosomes (scaffolds) of the given reference genome in the above model. Additional file 2: Tables S1–S5 provide the computed values for these coefficients for the chromosomes of the latest builds of human, chimpanzee, mouse, rat and zebrafish genome assemblies from University of California, Santa Cruz (UCSC) Genome Browser web site [26]. After all model parameters for each chromosome in the reference are identified, dnAQET computes the expected misassembly, εs, for a scaffold s as follows: 𝜀𝑠={𝛼𝑐𝑙𝑠+𝛽𝑐𝑡𝑢+𝑘𝑐𝑖𝑓 (𝛼𝑐𝑙𝑠+𝛽𝑐𝑡𝑢+𝑘𝑐)>00𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 where ls denotes the length of the scaffold, tu is the relocation threshold set by the user, αc, βc and kc are the model coefficients computed for chromosome c, which contains most of the alignments for scaffold s. Finally, the penalty assigned to scaffold s by dnAQET, denoted by P(s), is computed as follows: 𝑃(𝑠)={log100 (𝑚𝑠−𝜀𝑠)𝑖𝑓 (𝑚𝑠−𝜀𝑠)>1 0𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 where ms represents the total number of misassembly that dnAQET detects in the scaffold s. When the difference between the observed misassembly (ms) and the expected artifact (εs) for this scaffold with the specified relocation threshold is less than or equal to one, there is no penalty assigned to this scaffold. Otherwise, the logarithmic value of this difference in the base of 100 is used as the penalty to scaffold s. The length of a scaffold is an important indication of assembly quality. A well-assembled and high-quality scaffold should be equal to or longer than the shortest chromosome of the trusted reference genome, against which the quality is computed. The length of the shortest chromosome of reference genome G, is called length scaling factor of G and denoted by θG. It is used as a benchmarking value to assess the quality of a scaffold of the de novo assembly. Thus, dnAQET incorporates a coefficient called length scaling coefficient of a scaffold s in its quality score calculation formulation, which is denoted by L(s). This coefficient is computed with respect to the shortest reference chromosome length (θG) using the following equation: 𝐿(𝑠)=⎧⎩⎨⎪⎪1𝑖𝑓 𝑙𝑠≥θ𝐺−1log10(𝑙𝑠θ𝐺)−1 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 When the scaffold is larger than or equal to the shortest chromosome of the reference, the length scaling coefficient is set to 1. This guarantees that the contigs or scaffolds longer than the shortest chromosome are not punished. For a scaffold shorter than the shortest chromosome, the coefficient value increases as the scaffold gets longer, finally reaching one when ls is equal to θG. Instead of taking a simple ratio of the length of the scaffold to the length of the shortest chromosome, dnAQET uses the above function not to penalize the small to medium sized scaffolds very harshly. After reward, penalty and length scaling coefficient are calculated, dnAQET integrates them in an overall quality score for scaffold s, Q(s), using below equation. 𝑄(𝑠)=𝐿(𝑠)𝑅(𝑠)1+𝑃(𝑠) In case dnAQET identifies no misassembly in a scaffold or the misassembly is negligible, the quality score is basically equal to the alignment ratio of this scaffold multiplied with its length scaling coefficient. On the other hand, the quality of a scaffold decreases with the increasing number of misassembly detected in the scaffold. Computing quality score for assembly After quality scores for individual scaffolds (or contigs) are calculated, dnAQET computes a quality score for the whole de novo assembly using the individual quality scores and the redundancy observed in the assembly. There are three factors that dnAQET considers for quality score computation: (a) distribution of the quality scores of scaffolds of an assembly, (b) the reference genome coverage provided by the scaffolds at different quality thresholds, and (c) the redundancy of the scaffolds in the assembly. The dnAQET framework uses the quality score distribution of the scaffolds of a de novo assembly, as a component of the final quality score. To utilize this information, the ratio of the total number of base pairs of the scaffolds having quality scores higher than certain quality thresholds to the whole assembly size is first plotted at the corresponding quality thresholds (from 0 to 1 with 0.01 increments) as a curve (see Additional file 1: Figure S2A for an example). The area under this curve is then calculated, which is denoted by ΔA, where A represents the assembly. In the ideal case where all scaffolds of an assembly have a perfect quality score of 1, the area under this curve is equal to one (i.e., ΔA = 1). As it is not always possible to obtain perfectly scored scaffolds in practice, the curve may reach the maximum ratio of 1 at a lower quality threshold in most cases. In such cases, ΔA would be much lower than one. In this respect, ΔA value directly reflects the quality distribution of the scaffolds of a de novo assembly. Like the cumulative scaffold quality distribution graph, we plot the cumulative reference genome coverages of the scaffolds that meet certain quality thresholds. This curve presents us crucial information from two perspectives: 1) how much reference genome coverage can be reached with the scaffolds/contigs of the assembly and 2) how the reference coverage changes with quality distribution of the scaffolds of this assembly. An example of cumulative reference genome coverage graph is given in Additional file 1: Figure S2B. The dnAQET framework uses the area under the cumulative genome coverage ratio curve, which is denoted by ΩA for the assembly A, as another component of the overall quality score function to evaluate the assembly. In case the whole reference genome is fully covered by perfect scaffolds (i.e., scaffolds with a quality score of 1) of the de novo assembly, the ΩA is equal to 1. When the coverage is less than 100% or a high coverage is achieved with low quality scaffolds, dnAQET would reflect these less than ideal cases to the overall quality score of the assembly, by assigning a much lower value to ΩA. In summary, we incorporate ΩA to our quality score scheme to distinguish two assemblies, say A1 and A2, where both assemblies cover the same amount of the reference genome with different quality scaffolds. In such a case, the one with high quality scaffolds will have a larger area under the cumulative genome coverage curve, hence a higher ΩA value. Thus, this component is incorporated into the final quality score formulation and is used to distinguish A1 and A2 in terms of their quality. The last component of dnAQET’s assembly quality scoring scheme is the inverted redundancy identified in an assembly. The redundancy in an assembly is defined as the unnecessarily repeatedly assembled sequence in the de novo assembly when it is compared to the reference genome. An example is given in Additional file 1: Figure S3 where two scaffolds, s1 and s2, are aligned to a reference genome with alignments λ1 and λ2 respectively. The total covered reference genome is denoted with Φ. The overlap between these alignments is denoted with o, which is shared by λ1 and λ2 redundantly. Then the total inverted redundancy ratio of a de novo assembly, A, with set of scaffolds, A = {s1, s2,…, si} is denoted by ΠA and computed as: Π𝐴=Φ𝐴∑𝑠𝑖∈𝐴𝜆𝑖 where λi represents the total aligned base pairs in scaffold i and ΦA represents the total covered reference genome size by all scaffolds of assembly A. The inverted redundancy ratio is always a value between 0 and 1 for a de novo assembly. In the optimal case, where there are no redundant alignments, the inverted redundancy would be equal to 1. As this ratio gets smaller, the redundancy of an assembly increases, which makes the assembly less desirable. Therefore, it is essential to include such a component, which measures redundancy of an assembly, in our quality score computation. For this reason, we consider the inverse of the redundancy value of an assembly A denoted by ΠA and incorporate it into our formula. The dnAQET framework computes the final quality score for an assembly A using below equation. 𝑄(𝐴)=Δ𝐴Ω𝐴Π𝐴‾‾‾‾‾‾‾‾‾√3 The final quality score is the geometric mean of the area under the cumulative distribution curve of quality scores of scaffolds, the area under the cumulative curve of genome coverage and the inverted redundancy of the de novo assembly A. These three components represent three aspects of a desirable assembly: (i) scaffolds should have high individual quality scores, (ii) reference genome should be mostly covered with high quality scaffolds, and (iii) redundancy in an assembly should be minimal. Note that all these three values are defined to be a real value in [0, 1], which guarantees that the quality value obtained by taking the geometric mean would always be a real value in the same interval. Performance evaluation of dnAQET To evaluate the performance of dnAQET’s quality scores and compare them with the currently well-established metrics, three types of data were used: (i) in-silico scaffold data generated from the latest human reference genome build (i.e., hg38) with various genome coverages, numbers of misassemblies per scaffold and mean scaffold lengths, (ii) various builds of whole genome assemblies of five different organisms (human, mouse, rat, chimpanzee and zebrafish) obtained from UCSC Genome Browser website [26] and (iii) six de novo assemblies for the sample with National Institute of Standards and Technology (NIST) ID HG002 and Coriell ID NA24385 from the Genome in a Bottle (GIAB) project [27]. For in-silico performance analysis, we generated four types of synthetic assembly datasets from human reference build hg38, where each of these datasets were designed to analyze a different aspect of the quality computation process as follows: (i) Mean scaffold length dataset: We generated scaffolds with randomly chosen coordinates covering hg38 with 1X depth of coverage. The lengths of the scaffolds were drawn from normal distributions with mean lengths of 104, 105, 106, 107 base pairs and standard deviations of 10, 102, 103 and 104, respectively. (ii) Misassembly dataset: We randomly created assemblies that contain scaffolds with 0, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 misassemblies per scaffold from hg38. The lengths of these scaffolds were drawn from a normal distribution with a mean of 107 and standard deviation of 104. Each assembly had a total genome coverage of 1X. (iii) Coverage dataset: Assemblies that cover 0.1X up to 1X (with increments of 0.1X) of the hg38 build were created randomly. We didn’t induce any artificial misassemblies into the scaffolds. The lengths of these scaffolds were drawn from a normal distribution with a mean and standard deviation of 107. (iv) Redundancy dataset: We created assemblies that covered the hg38 once (1X), twice (2X), three times (3X), four times (4X) and finally five times (5X). There were no artificially induced misassemblies and the scaffold lengths were normally distributed with a mean value of 107 and standard deviation of 104. Note that for all the above described synthetic datasets, we created five different assemblies for each value of the parameter that was to be analyzed. For testing dnAQET on human reference data, we used fifteen builds of reference genome assemblies, more specifically hg4, hg5, hg6, hg7, hg8, hg10, hg11, hg12, hg13, hg15, hg16, hg17, hg18, hg19 and hg38. The dnAQET was used to evaluate each of these genome builds against the hg38 build. Since hg38 is the most recent reference genome, it is expected to be the best among all these builds. Note that some of the reference builds (such as hg1, hg2, hg3, hg9, hg14) are missing from our study due to the unavailability to download in UCSC Genome Browser data repository. Similarly, there were ten available builds (mm1 to mm10) of mouse reference genome, six builds (rn1 to rn6) of rat reference genome, six builds (panTro1 to panTro6) of chimpanzee reference genome and nine builds (danRer1, danRer2, danRer3, danRer4, danRer5, danRer6, danRer7, danRer10 and danRer11) of zebrafish genome. We used the latest builds mm10, rn6, panTro6 and danRer11 as the references for mouse, rat, chimpanzee and zebrafish, respectively, against which the rest of the builds were evaluated by dnAQET. We calculated the quality scores for each chromosome of the builds and for the whole reference assemblies. Two restrictions were imposed on the datasets: (i) only the primary chromosomes of these builds were included in our analysis for the sake of simplicity and (ii) to fairly evaluate human, mouse and rat genome builds, we had to exclude Y chromosome from the analysis due to the lack of availability of this chromosome for some of the earlier builds. When using different builds of reference genomes to evaluate dnAQET’s scores, our hypothesis is that the quality scores of the chromosomes of a more recent build of the reference genome of an organism should be better than that of an older build of the reference genome of the same organism. Moreover, the overall quality score from dnAQET for the older builds should be lower than those for the later builds for all the organisms used in this work. To test dnAQET on real genome assemblies, we used two de novo assemblies from GIAB data repository for sample NA24385. The first assembly was created using Celera assembler [15] and is available on GIAB ftp site [28]. The second assembly was generated using Falcon [16] and is available on GIAB ftp site [29]. The other four assemblies were generated in-house using the MECAT assembler [14] on the PacBio data with four different depth of coverages namely 5X, 25X, 50X and 70X. Note that the PacBio data was produced by the GIAB consortium and was freely available on GIAB ftp site [30]. Quality analysis of synthetic data We first analyzed how dnAQET’s quality score changed as the mean scaffold length of the assemblies increased using the first synthetic dataset. As demonstrated in Fig. 2a, the quality scores of the assemblies increased as the average size of the scaffolds got longer. Initially, the quality scores of the assemblies with mean scaffold sizes of 10,000 base pairs concentrated around 0.35 and quality values monotonically increased for assemblies with larger scaffolds, finally becoming slightly larger than 0.7 for the assemblies with an average of 10 million base pair scaffolds. In this sense, the dnAQET’s quality score is concordant with the general assumption that an assembly with larger scaffolds should be better in quality than an assembly with shorter scaffolds given that both assemblies have same total length. Fig. 2 figure2 The dnAQET quality scores for synthetic datasets. The y-axis indicates the quality scores that dnAQET computed for the synthetic datasets with varying mean scaffold length (a), number of misassemblies per scaffold (b), genome coverage (c) and redundancy (d) Full size image Using our second dataset, we investigated the effect of the increasing number of misassemblies in the dnAQET’s quality score formulation. As shown in Fig. 2b, the quality scores of the assemblies with no artificially induced misassembly were slightly larger than 0.7 but with the introduction of 10 misassemblies per scaffold into the assembly, we observed the quality scores dropped down to 0.55. This trend of decrease in quality scores continued with the increasing misassembly numbers, and finally quality score settled down at 0.38 for the assemblies with 1000 misassemblies per scaffold. This result is concordant with the expectation that an assembly with lower number of misassemblies should have a higher quality than the one with more misassemblies. In this respect, our quality score formulation correctly characterizes the effect of misassemblies on the quality of an assembly. In Fig. 2c, we present the effect of increasing genome coverage in dnAQET’s quality score. As expected, the quality of an assembly with higher genome coverage was consistently higher than that of an assembly with lower coverage. The highest quality scores were obtained for assemblies with 1X coverage whereas the lowest quality scores were around 0.3 for 0.1X coverage assemblies. This result demonstrated that dnAQET’s quality score behaved as expected for changing genome coverage values. Finally, we investigated the effect of the redundancy on our quality score formulation as presented in Fig. 2d. We computed the quality scores for datasets with 1X, 2X, 3X, 4X and 5X genome coverages, where all coverages more than 1X corresponded to redundant assemblies. As presented in Fig. 2d, the quality scores of the assemblies decreased as their redundancy increased. This result is concordant with the hypothesis that assemblies with unnecessary, repeated scaffolds should be in lower quality than concise assemblies. Overall, these results we obtained using synthetic data reflects the capability of dnAQET’s quality score to capture and combine different aspects of quality evaluation into a single formula. Quality of reference genome builds of five organisms The quality score distribution of chromosomes for different builds of reference genomes for the five organisms are given in Fig. 3. The segregation of reference genome builds is clear for all five organisms in terms of the quality scores of chromosomes calculated by dnAQET. For example, the quality score distributions of human reference genome builds (Fig. 3a) revealed that approximately 65% of the chromosomes in the earlier versions of human reference genome (i.e., builds hg4 to hg6) had quality scores in the range of 0.2 to 0.3. In contrast, the quality scores of the chromosomes of the newer builds hg7 to hg12 were improved to the range of 0.3 to 0.4 that included more than 48% of the chromosomes in the worst case. The shift of the quality scores towards the higher quality bins continued to reach the point where more than 50% of the chromosomes of hg13 had quality scores between 0.4 and 0.5. Majority of the chromosomes in builds hg15 and hg16 had quality scores in the range of 0.5 to 0.7 whereas the majority of the hg17, hg18 and hg19 chromosomes had quality scores between 0.6 and 0.8. Finally, the percentage of the high-quality chromosomes (i.e., chromosomes with quality scores higher than 0.8) reached to 95% for build hg38. Fig. 3 figure3 Heatmaps of quality scores of chromosomes and scaffolds for the reference genome builds of five organisms and the de novo assemblies of NA24385. The x-axis depicts the builds or assemblies. The y-axis indicates the ten quality scores bins between 0 and 1. The color given in the legend shows the percentage of the chromosomes and scaffolds that fall in the quality bin indicated by the y-axis. The quality score heatmaps are for human (a), mouse (b), rat (c), chimpanzee (d) and zebrafish (e) reference chromosomes from specified builds and the scaffolds of de novo assemblies for sample NA24385 (f) from six different assemblies. Note that there are 10 quality interval bins, dividing the [0, 1] range into nine equal sized left-closed, right-open intervals and a single closed interval, which is [0.9, 1] Full size image Similarly, we found that there was a clear segregation between the quality scores of chromosomes of the earlier and the most recent versions of the mouse reference genomes as presented in Fig. 3b. In the earlier versions, such as the builds mm2 to mm6, the quality scores of chromosomes were concentrated in the [0.3, 0.4) interval. However, majority of the chromosomes of build mm7 had quality scores in the [0.4, 0.5) bin. On the other hand, the ratio of medium to high quality chromosomes (i.e., chromosomes with a quality higher than 0.5 and less than 0.8) gradually increased for builds mm8 and mm9. Finally, more than 85% of the chromosomes of the latest build mm10 had quality scores higher than 0.9. Overall, for the five organisms, the chromosome quality scores from dnAQET were higher for more recent builds. These results were concordant with our hypothesis and evidently demonstrated that dnAQET could be used to fairly evaluate quality of scaffolds and contigs. Additionally, we present the individual quality scores of the chromosomes for each genome build of the considered organisms in Additional file 1: Figure S4. The final quality scores computed by dnAQET for the reference genome builds for the five organisms and the de novo assemblies of NA24385 were plotted in Fig. 4. The quality scores of the more recent versions of the genome builds were consistently higher than the older builds except one case. The quality score of the zebrafish reference genome build danRer6 had a slightly smaller quality score than the older build danRer5. Examining the quality scores for human reference genome builds (Fig. 4a), we observed that the quality scores almost monotonically increased from 0.401 for hg4 up to 0.575 for hg13. Then the quality score of hg15 jumped up to 0.676 and the quality scores stabilized at around 0.7 for all more recent builds up to hg19, while keeping a slow increase rate. Finally, the quality score reached 0.98 for hg38. The dramatic quality shift between hg13 and hg15, was concurrent with the introduction of the first finished human genome assembly, dated April 2003 [31]. These results clearly showed that the later versions of the human reference genome builds had higher quality scores from dnAQET, which was very consistent with the expectation that the build quality increased with each newly introduced build. Fig. 4 figure4 Quality scores for reference genome builds and de novo assemblies of NA24385. The x-axis depicts the builds or assemblies. The y-axis indicates quality scores that dnAQET computed for the reference genome builds of human (a), mouse (b), rat (c), chimpanzee (d), zebrafish (e) and the de novo assemblies for NA24385 (f). The yellow bars show the final quality scores Q. The lines give the area under the cumulative scaffold quality scores distribution curve Δ (blue), the area under the cumulative genome coverage curve Ω (light brown), and the inverted duplication ratio Π (grey) Full size image It is interesting to note that dnAQET gave a quality score of 0.98 to hg38 instead of a perfect score of 1, since hg38 was scored against itself. It was not possible to compute a perfect score for hg38 even when it was compared back to itself due to the ambiguous base pairs it contains (5% of the genome) and the misassembly observed due to the repetitive sequences (which may have caused misalignment). Since dnAQET took the artifacts due to alignment tools into consideration, it computed a nearly perfect score for hg38 assembly. It is also important to note that the quality scores of the latest builds for rat, chimpanzee and zebrafish genomes were remarkably higher than those of their predecessor builds. The striking quality shift between the rat builds rn5 and rn6 could be attributed to the introduction of additional PacBio data with 10X coverage into the assembly process [32]. Similarly, the zebrafish reference build danRer11 had a significantly higher quality score than danRer10 due to additional assemblies WGS29 (CAAK00000000.1) and WGS32 (CZQB00000000.1) added where necessary, to fill the gaps [33]. Finally, the quality score for the recently introduced chimpanzee genome build panTro6 was much higher than that of panTro5 because of the high depth of coverage (124X) data used for the assembly and the three-stage progressive assembly methodology which incorporated data from multiple platforms [34]. Since there was no other tool in the literature that could report a single quality score to unify multiple measures for a de novo assembly, we could not directly compare the quality scores that dnAQET computed for these assembly builds with other tools. Consequently, we decided to compare the rankings of the dnAQET quality scores with the rankings from QUAST-LG metrics for these builds. QUAST-LG could provide 37 quality metrics to compare multiple assemblies [22]. Except benchmarking universal single-copy orthologs (BUSCO) completeness and k-mer-based completeness (both metrics require the reads used to generate the assemblies), the remaining 35 QUAST-LG quality metrics were calculated for almost all the reference genome builds (except metrics such as NA75, NG75, NGA50, NGA75 etc., which could not be computed for some of the earlier builds of zebrafish and mouse). To be consistent in our comparison across multiple organisms, metrics that were not reported for the reference builds of all organisms were discarded, resulting 13 metrics that were reported for all reference genome builds by QUAST-LG and provided a clear ranking of the builds without any ties (see Additional file 2: Tables S6–S10 for detail). We next computed Pearson correlation coefficients between the rankings provided by each of these quality metrics and the rankings of these builds (the hypothesis for determining the ranking of the builds: the more recent a build is, the better is its quality, and thus a higher rank it has). The result given in Table 1 shows that the rankings of the dnAQET quality scores always had the highest Pearson correlation coefficients and were consistent with the inherent rankings dictated by the hypothesis. These results demonstrated that the dnAQET quality scores reflected the quality of these genome builds better than any of the other metrics due to its sophisticated design to combine different aspects of an assembly into a single value. Table 1 The Pearson correlation coefficients between the rankings of quality scores yielded from dnAQET and QUAST-LG and the rankings of the reference genome builds by hypothesis Full size table Although the genome fraction (%) from QUAST-LG achieved the same performance as the dnAQET quality score in terms of the rankings of genome builds for human, mouse, rat and chimpanzee, dnAQET quality score outperformed the genome fraction (%) metric in ranking of the builds of zebrafish genome. Like the genome fraction (%) metric, any single metric from QUAST-LG (Table 1) is not able to reliably rank the genome builds compared to dnAQET quality score and thus is not suitable for assessing quality of de novo assemblies as a whole. These metrics focus only on one side of the assemblies and evaluate them by examining from only a single perspective. For instance, the genome fraction (%) just reported how much of the reference genome was covered by a de novo assembly without considering the quality of the individual scaffolds. For the reference genome builds, the total genome fraction (%) increased almost every time for a more recent build, but this may not be the case for other de novo assemblies. For example, one assembly might have a slightly higher genome fraction (%) but lower N50 (or NA50) value and higher number of misassemblies than another assembly. On another case, the genome fraction (%) of multiple assemblies may indicate a reverse ranking of the assemblies when they are ranked with respect to another metric. Another example that demonstrated the inconsistency of these metrics was the poor performance of metrics such as number of indels per 100kbp, number of mismatches per 100kbp, NA50, LA50 when they were used to rank the chimpanzee genome builds. Although these metrics’ rankings had high Pearson correlation coefficients when used on human, mouse, rat and zebrafish data, they considerably failed in correctly ranking chimpanzee genome builds. Therefore, assessing the quality of an assembly using one metric from QUAST-LG could not provide sufficient information about its overall quality. We see a clear need for a quality metric that unifies these multiple crucial metrics into a single quality value that can be used to reliably assess the quality of an assembly. This need was met by the meticulously designed quality score formulation of dnAQET that united multiple metrics into a single metric. Quality of six NA24385 assemblies The quality score distributions of the contigs of the six assemblies were plotted in in Fig. 3f. None of the six assemblies had any contigs with a quality score higher than 0.8. Both MECAT assemblies generated with 50X and 70X coverage data had contigs with quality scores higher than 0.7 but the percentage of these contigs was only 0.1%. From this perspective, none of the real data assemblies had an outstanding performance in terms of the contig quality scores. It is interesting to note that the best performing assemblies were generated by MECAT on the data of 70X, 50X and 25X coverages. More than 85% of the contigs in these assemblies had quality scores higher than 0.2. In contrast with MECAT, Falcon and Celera assemblies only had 44 and 53% of contigs with quality score higher than 0.2, respectively. The overall quality scores computed by dnAQET for the six de novo assemblies were presented in Fig. 4f. The assemblies generated by MECAT assembler with high coverage data (50X and 70X) had the best quality score 0.6. The Celera assembly also had a quality score of 0.58, which is slightly lower than the scores of the leading assemblies. The assembly generated by MECAT using 25X coverage had a quality score of 0.55 and followed by the assembly from Falcon with quality score of 0.54. The lowest quality score of 0.37 was obtained from MECAT assembly created with 5X coverage data, which was not surprising because it was generated with the very low coverage data. We also evaluated these assemblies using QUAST-LG. The computed quality metrics were presented in Additional file 2: Table S11. As mentioned before, some of these metrics produced multiple rankings of these assemblies that contradict with each other. For instance, using NA50 metric, these assemblies were ranked as MECAT70X, MECAT50X, Falcon, Celera, MECAT25X and MECAT5X in the decreasing order (i.e., higher the NA50, better the assembly). However, the ranking was completely changed to Celera, Falcon, MECAT50X, MECAT70X, MECAT25X and finally MECAT5X when genome fraction (%) was used as the ranking metric in the decreasing order. When number of misassemblies metric was used to order them, the ranking was MECAT5X, MECAT25X, MECAT70X, MECAT50X, Celera and Falcon, in an increasing number of misassemblies (i.e., lower the misassembly, better the assembly). Thus, it was evident from these results that there was a need to consolidate these metrics into a single value, as done by dnAQET, to fairly evaluate the overall quality of these assemblies. Since there was no inherent ranking of these real data assemblies available to compare the rankings of QUAST-LG metrics with that of dnAQET scores, we used an approach proposed in [18] to infer a reliable ranking that would be considered closest to the ground truth. In that study, the authors calculated the z-scores for each metric for all assemblies in consideration, then summed these scores and finally ranked the assemblies based on the summed z-scores. This was called the z-score-based ranking. To obtain the ground truth ranking for our assemblies, we applied a similar approach to 24 metrics, which gave a clear ranking of the assemblies without a tie and were commonly reported for all assemblies, computed by QUAST-LG. We calculated the z-score for each metric and summed the z-scores for each assembly as presented in Additional file 2: Table S11. According to the z-score-based ranking, the ranking of these assemblies was MECAT70X, MECAT50X, MECAT25X, Celera, Falcon and MECAT5X in decreasing order (i.e., higher the z-score, better the assembly). The Pearson correlation coefficient between dnAQET quality score ranking of these assemblies and the z-score-based ranking was 0.94 whereas the QUAST-LG metrics, namely Largest contig, N50, NG50, N75, L50, LG50, L75, Largest alignment, NGA50 and NA75 provided the second-high correlation coefficient value, which was 0.77. The results indicated that dnAQET’s quality score outperformed the other metrics based on the z-score-based ranking. Runtime and peak memory usage We compared the runtime and memory performances of dnAQET with those of QUAST-LG using the human reference genome builds and the six de novo assemblies of sample NA24385. All the benchmarking tests were done on a server with 125.5 gigabytes of memory running 24 cores of Intel Xeon E5–2680 v3 2.50GHz CPUs. To be able to fairly evaluate the performance of dnAQET against QUAST-LG, we partitioned neither the reference genome nor the input genome builds to be evaluated. Both dnAQET and QUAST-LG were run using 12 threads on a single server and Minimap2 was used as the alignment tool. The runtime and peak memory usage of dnAQET and QUAST-LG for the human reference genome builds and the de novo assemblies are presented in Fig. 5. Clearly, the runtime of dnAQET was very stable across all the human reference genome builds and it took 1184 s at the worst case for dnAQET to process a human reference genome build. On the other hand, QUAST-LG performed poorly for the earlier builds (Fig. 5a). For example, QUAST-LG took approximately 21.8 h to finish processing build hg4. QUAST-LG performed better on the more recent builds and its runtime was decreased to approximately 2500 s for the most recent builds. For all six NA24385 assemblies, dnAQET was faster than QUAST-LG to process the assemblies (Fig. 5b). It took 440 s for dnAQET to process the Celera assembly, which was the slowest performance of dnAQET for these assemblies. On the other hand, QUAST-LG’s best runtime was 1686 s for MECAT5X assembly. Fig. 5 figure5 Runtime and peak memory usage. The dnAQET (blue bars) and QUAST-LG (orange bars) runtimes to process the human reference genome builds (a) and the six assemblies of NA24385 (b) were plotted as the bars (left y-axis). The peak memory usages (in gigabytes) were plotted in the lines (right y-axis) for dnAQET (gray lines) and QUAST-LG (yellow lines) Full size image In terms of the peak memory usage, both tools used more than 35 gigabytes of memory when analyzing the human reference builds. The dnAQET’s memory usage increased from hg8 to hg10, reaching to 36.5 gigabytes on average, and stabilizing at this number for the later builds. However, QUAST-LG had a steady peak memory usage of 35 gigabytes for builds hg4 to hg13. But starting with hg15, the peak memory usage went up, reaching 38.5 gigabytes for hg38. For NA24385 assemblies, the peak memory usage of dnAQET stayed stable at around 30 gigabytes whereas QUAST-LG’s memory usage was stable at around 35 gigabytes. Our results demonstrated dnAQET outperformed QUAST-LG in terms of execution time; but both tools had comparable peak memory usage.",AssemblyEvaluation,"dnaqet  framework  compute  consolidate metric  benchmarking quality   novo assemblies
 dnaqet framework comprise  two main step  align assemble scaffold contigs   trust reference genome    calculate quality score   scaffold   whole assembly   alignment step dnaqet provide two separate alignment tool  users  choose  first one   nucmer pipeline   mummer4 package  whose predecessor  mummer3 package     widely use general purpose alignment tool  map long dna sequence   recently release mummer4 package contain  much faster  memory efficient nucmer version   handle large genomes   recent version   nucmer  use  dnaqet  second option  alignment  dnaqet offer   minimap2 aligner    also   fast pairwise aligner  nucleotide sequence  enhance  computational performance  alignment process  break   three substeps  partition   reference genome   assembly file  smaller chunk  align  partition   assembly   partition   reference genome  parallel  finally  filter  redundant  overlap alignments   scaffold  filter  remain alignments  use  compute  quality score scalable alignment  scaffold   reference genome  dnaqet handle alignment  scaffold   reference genome   parallelize manner  partition  assembly file  multiple approximatelyequal size file   reference genome  multiple reference file  contain  single chromosome additional file  figure  enable  method   scalable  handle assemblies  large genomes  total number  partition file   assembly   determine   user   default value  set  one  tool distribute  scaffold   user specify number  file    way   total number  base pair contain   file would  similar across  partition  partition  genome dnaqet distribute  chromosomes   reference genome  multiple file    assembly partition   align   single chromosome independently   highperformance compute hpc   multithreaded compute environment compute quality score  scaffold  dnaqet parse  alignment result   scaffold  filter   redundant  ambiguous alignments  obtain  longest consistent match   scaffold   reference   purpose  adapt  underlie algorithm  deltafilter utility   mummer package  implement   approach  dnaqet   alignment step  complete either use nucmer  minimap2  alignment result  scan use appropriate parsers specifically design  parse data   correspond alignment format    convert   internal alignment format   filter use  filter algorithm note   filter step  independent   choose alignment tool   apply   alignment result  matter  tool  use  generate    compute  quality score   individual contig use  set  best alignments base   total number  align base pair   scaffold reward  total number  misassembly determine   scaffold penalty   length   scaffold length scale coefficient  reward   scaffold  suppose   directly proportional   total number  base   align   reference genome   alignment tool base   assumption dnaqet assign  reward value denote     scaffold    equivalent   ratio   total number  align base   total number  base   without  scaffold gap 𝑅𝑠𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟  𝑎𝑙𝑖𝑔𝑛𝑒𝑑 𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠  𝑠𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟  𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠  𝑠𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟  𝑏𝑎𝑠𝑒 𝑝𝑎𝑖𝑟𝑠  𝑡ℎ𝑒 𝑠𝑐𝑎𝑓𝑓𝑜𝑙𝑑𝑖𝑛𝑔 𝑔𝑎𝑝𝑠    formulation  total number  ambiguous base pair   detect   scaffold gap  subtract   total size   scaffold see  denominator    formula  avoid improper reduction   reward   scaffold  penalty assign   scaffold  directly relate   misassembly   basically summarize   inconsistencies   flank alignments   scaffold   reference  dnaqet  consider three type  misassembly   also consistent   type  definitions   misassembly report  previous literature   misassembly type fig   describe  detail  fig  figure1 illustration  misassembly type  green  blue bar indicate two flank sequence   scaffold   white bar represent regions   reference genome  relocations   distance    overlap   mark  white  yellow bar   reference bar respectively translocation  denote  two chromosomes   arrows depict  directions  strand  red  show  inversion  full size image relocation  type  misassembly   scaffold  happen  two case two consecutive sequence segment   scaffold  align    chromosome   separation distance     base pair fig     align    chromosome   overlap     base pair fig   relocation threshold    base pair  default    adjust   user  distance  overlap   alignments   flank sequence smaller     consider   relocation translocation   type  misassembly   observe   two flank sequence   scaffold  align  two different chromosomes   reference genome fig  inversion   type  misassembly  occur  case   two flank sequence   scaffold  align   opposite strand    chromosome fig   theory one would expect  perfect alignment   reference chromosome scaffold back   however due  gap ambiguous sequence repetitive sequence   reference genome  limitations  alignment algorithms   still possible  observe misalignment  two identical long fragment sequence      consider  artifact  mummer  minimap2 thus dnaqet consider  fact   artifacts   expect even though  scaffold  assemble perfectly   artifacts   correlate   length  assemble scaffold   longer  scaffold    artifacts   expect  dnaqet process  scaffold  compute  penalty  need  decide   misassembly detect   scaffold compare   artifact expect   scaffold   size    give relocation threshold   dnaqet use  regression model  find  expect artifact give  scaffold  size    relocation threshold    use  decide  relocation type  misassembly   reference chromosome dnaqet first randomly create  set  artificial contigsscaffolds  cover  whole chromosome   depth  coverage   contigs along   original reference chromosome sequence  align back   whole reference genome  last  total number  misassembly  compute   contigscaffold    relocation threshold    range      base pair increments  compute artifact  fit   follow model 𝜀𝛼𝑐𝑙𝛽𝑐𝑡𝑘𝑐 via leastsquares regression     artifact     model parameters     intercept   model obtain  chromosome   determine  model parameters   intercept dnaqet fit  observe artifact     chromosomes scaffold   give reference genome    model additional file  table s1s5 provide  compute value   coefficients   chromosomes   latest build  human chimpanzee mouse rat  zebrafish genome assemblies  university  california santa cruz ucsc genome browser web site    model parameters   chromosome   reference  identify dnaqet compute  expect misassembly    scaffold   follow {𝛼𝑐𝑙𝑠𝛽𝑐𝑡𝑢𝑘𝑐𝑖𝑓 𝛼𝑐𝑙𝑠𝛽𝑐𝑡𝑢𝑘𝑐00𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒   denote  length   scaffold    relocation threshold set   user       model coefficients compute  chromosome   contain    alignments  scaffold  finally  penalty assign  scaffold   dnaqet denote    compute  follow {log100 𝑚𝑠𝜀𝑠𝑖𝑓 𝑚𝑠𝜀𝑠 0𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒   represent  total number  misassembly  dnaqet detect   scaffold    difference   observe misassembly    expect artifact    scaffold   specify relocation threshold  less   equal  one    penalty assign   scaffold otherwise  logarithmic value   difference   base    use   penalty  scaffold   length   scaffold   important indication  assembly quality  wellassembled  highquality scaffold   equal   longer   shortest chromosome   trust reference genome    quality  compute  length   shortest chromosome  reference genome   call length scale factor    denote     use   benchmarking value  assess  quality   scaffold    novo assembly thus dnaqet incorporate  coefficient call length scale coefficient   scaffold    quality score calculation formulation   denote    coefficient  compute  respect   shortest reference chromosome length  use  follow equation ⎧⎩⎨⎪⎪1𝑖𝑓 ≥θ𝐺1log10𝑙𝑠θ𝐺 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒   scaffold  larger   equal   shortest chromosome   reference  length scale coefficient  set    guarantee   contigs  scaffold longer   shortest chromosome   punish   scaffold shorter   shortest chromosome  coefficient value increase   scaffold get longer finally reach one    equal   instead  take  simple ratio   length   scaffold   length   shortest chromosome dnaqet use   function   penalize  small  medium size scaffold  harshly  reward penalty  length scale coefficient  calculate dnaqet integrate    overall quality score  scaffold   use  equation 𝑄𝑠𝐿𝑠𝑅𝑠𝑃𝑠  case dnaqet identify  misassembly   scaffold   misassembly  negligible  quality score  basically equal   alignment ratio   scaffold multiply   length scale coefficient    hand  quality   scaffold decrease   increase number  misassembly detect   scaffold compute quality score  assembly  quality score  individual scaffold  contigs  calculate dnaqet compute  quality score   whole  novo assembly use  individual quality score   redundancy observe   assembly   three factor  dnaqet consider  quality score computation  distribution   quality score  scaffold   assembly   reference genome coverage provide   scaffold  different quality thresholds    redundancy   scaffold   assembly  dnaqet framework use  quality score distribution   scaffold    novo assembly   component   final quality score  utilize  information  ratio   total number  base pair   scaffold  quality score higher  certain quality thresholds   whole assembly size  first plot   correspond quality thresholds       increments   curve see additional file  figure s2a   example  area   curve   calculate   denote     represent  assembly   ideal case   scaffold   assembly   perfect quality score    area   curve  equal  one         always possible  obtain perfectly score scaffold  practice  curve may reach  maximum ratio     lower quality threshold   case   case  would  much lower  one   respect  value directly reflect  quality distribution   scaffold    novo assembly like  cumulative scaffold quality distribution graph  plot  cumulative reference genome coverages   scaffold  meet certain quality thresholds  curve present  crucial information  two perspectives   much reference genome coverage   reach   scaffoldscontigs   assembly     reference coverage change  quality distribution   scaffold   assembly  example  cumulative reference genome coverage graph  give  additional file  figure s2b  dnaqet framework use  area   cumulative genome coverage ratio curve   denote     assembly   another component   overall quality score function  evaluate  assembly  case  whole reference genome  fully cover  perfect scaffold  scaffold   quality score      novo assembly    equal     coverage  less     high coverage  achieve  low quality scaffold dnaqet would reflect  less  ideal case   overall quality score   assembly  assign  much lower value    summary  incorporate    quality score scheme  distinguish two assemblies say      assemblies cover   amount   reference genome  different quality scaffold    case  one  high quality scaffold    larger area   cumulative genome coverage curve hence  higher  value thus  component  incorporate   final quality score formulation   use  distinguish     term   quality  last component  dnaqets assembly quality score scheme   invert redundancy identify   assembly  redundancy   assembly  define   unnecessarily repeatedly assemble sequence    novo assembly    compare   reference genome  example  give  additional file  figure   two scaffold     align   reference genome  alignments    respectively  total cover reference genome  denote    overlap   alignments  denote     share     redundantly   total invert redundancy ratio    novo assembly   set  scaffold   { … }  denote    compute  π𝐴φ𝐴∑∈𝐴𝜆𝑖   represent  total align base pair  scaffold    represent  total cover reference genome size   scaffold  assembly   invert redundancy ratio  always  value        novo assembly   optimal case     redundant alignments  invert redundancy would  equal     ratio get smaller  redundancy   assembly increase  make  assembly less desirable therefore   essential  include   component  measure redundancy   assembly   quality score computation   reason  consider  inverse   redundancy value   assembly  denote    incorporate    formula  dnaqet framework compute  final quality score   assembly  use  equation 𝑄𝐴δ𝐴ω𝐴π𝐴‾‾‾‾‾‾‾‾‾√  final quality score   geometric mean   area   cumulative distribution curve  quality score  scaffold  area   cumulative curve  genome coverage   invert redundancy    novo assembly   three components represent three aspects   desirable assembly  scaffold   high individual quality score  reference genome   mostly cover  high quality scaffold  iii redundancy   assembly   minimal note    three value  define    real value     guarantee   quality value obtain  take  geometric mean would always   real value    interval performance evaluation  dnaqet  evaluate  performance  dnaqets quality score  compare    currently wellestablished metrics three type  data  use  insilico scaffold data generate   latest human reference genome build  hg38  various genome coverages number  misassemblies per scaffold  mean scaffold lengths  various build  whole genome assemblies  five different organisms human mouse rat chimpanzee  zebrafish obtain  ucsc genome browser website   iii six  novo assemblies   sample  national institute  standards  technology nist  hg002  coriell  na24385   genome   bottle giab project   insilico performance analysis  generate four type  synthetic assembly datasets  human reference build hg38     datasets  design  analyze  different aspect   quality computation process  follow  mean scaffold length dataset  generate scaffold  randomly choose coordinate cover hg38   depth  coverage  lengths   scaffold  draw  normal distributions  mean lengths      base pair  standard deviations       respectively  misassembly dataset  randomly create assemblies  contain scaffold                   misassemblies per scaffold  hg38  lengths   scaffold  draw   normal distribution   mean    standard deviation    assembly   total genome coverage   iii coverage dataset assemblies  cover      increments     hg38 build  create randomly  didnt induce  artificial misassemblies   scaffold  lengths   scaffold  draw   normal distribution   mean  standard deviation    redundancy dataset  create assemblies  cover  hg38   twice  three time  four time   finally five time     artificially induce misassemblies   scaffold lengths  normally distribute   mean value    standard deviation   note      describe synthetic datasets  create five different assemblies   value   parameter     analyze  test dnaqet  human reference data  use fifteen build  reference genome assemblies  specifically hg4 hg5 hg6 hg7 hg8 hg10 hg11 hg12 hg13 hg15 hg16 hg17 hg18 hg19  hg38  dnaqet  use  evaluate    genome build   hg38 build since hg38    recent reference genome   expect    best among   build note     reference build   hg1 hg2 hg3 hg9 hg14  miss   study due   unavailability  download  ucsc genome browser data repository similarly   ten available build mm1  mm10  mouse reference genome six build rn1  rn6  rat reference genome six build pantro1  pantro6  chimpanzee reference genome  nine build danrer1 danrer2 danrer3 danrer4 danrer5 danrer6 danrer7 danrer10  danrer11  zebrafish genome  use  latest build mm10 rn6 pantro6  danrer11   reference  mouse rat chimpanzee  zebrafish respectively    rest   build  evaluate  dnaqet  calculate  quality score   chromosome   build    whole reference assemblies two restrictions  impose   datasets    primary chromosomes   build  include   analysis   sake  simplicity    fairly evaluate human mouse  rat genome build    exclude  chromosome   analysis due   lack  availability   chromosome     earlier build  use different build  reference genomes  evaluate dnaqets score  hypothesis    quality score   chromosomes    recent build   reference genome   organism   better     older build   reference genome    organism moreover  overall quality score  dnaqet   older build   lower     later build    organisms use   work  test dnaqet  real genome assemblies  use two  novo assemblies  giab data repository  sample na24385  first assembly  create use celera assembler    available  giab ftp site   second assembly  generate use falcon    available  giab ftp site    four assemblies  generate inhouse use  mecat assembler    pacbio data  four different depth  coverages namely      note   pacbio data  produce   giab consortium   freely available  giab ftp site  quality analysis  synthetic data  first analyze  dnaqets quality score change   mean scaffold length   assemblies increase use  first synthetic dataset  demonstrate  fig   quality score   assemblies increase   average size   scaffold get longer initially  quality score   assemblies  mean scaffold size   base pair concentrate around   quality value monotonically increase  assemblies  larger scaffold finally become slightly larger     assemblies   average   million base pair scaffold   sense  dnaqets quality score  concordant   general assumption   assembly  larger scaffold   better  quality   assembly  shorter scaffold give   assemblies   total length fig  figure2  dnaqet quality score  synthetic datasets  yaxis indicate  quality score  dnaqet compute   synthetic datasets  vary mean scaffold length  number  misassemblies per scaffold  genome coverage   redundancy  full size image use  second dataset  investigate  effect   increase number  misassemblies   dnaqets quality score formulation  show  fig   quality score   assemblies   artificially induce misassembly  slightly larger      introduction   misassemblies per scaffold   assembly  observe  quality score drop     trend  decrease  quality score continue   increase misassembly number  finally quality score settle      assemblies   misassemblies per scaffold  result  concordant   expectation   assembly  lower number  misassemblies    higher quality   one   misassemblies   respect  quality score formulation correctly characterize  effect  misassemblies   quality   assembly  fig   present  effect  increase genome coverage  dnaqets quality score  expect  quality   assembly  higher genome coverage  consistently higher     assembly  lower coverage  highest quality score  obtain  assemblies   coverage whereas  lowest quality score  around    coverage assemblies  result demonstrate  dnaqets quality score behave  expect  change genome coverage value finally  investigate  effect   redundancy   quality score formulation  present  fig   compute  quality score  datasets        genome coverages   coverages    correspond  redundant assemblies  present  fig   quality score   assemblies decrease   redundancy increase  result  concordant   hypothesis  assemblies  unnecessary repeat scaffold    lower quality  concise assemblies overall  result  obtain use synthetic data reflect  capability  dnaqets quality score  capture  combine different aspects  quality evaluation   single formula quality  reference genome build  five organisms  quality score distribution  chromosomes  different build  reference genomes   five organisms  give  fig   segregation  reference genome build  clear   five organisms  term   quality score  chromosomes calculate  dnaqet  example  quality score distributions  human reference genome build fig  reveal  approximately    chromosomes   earlier versions  human reference genome  build hg4  hg6  quality score   range      contrast  quality score   chromosomes   newer build hg7  hg12  improve   range      include      chromosomes   worst case  shift   quality score towards  higher quality bin continue  reach  point       chromosomes  hg13  quality score     majority   chromosomes  build hg15  hg16  quality score   range     whereas  majority   hg17 hg18  hg19 chromosomes  quality score     finally  percentage   highquality chromosomes  chromosomes  quality score higher   reach    build hg38 fig  figure3 heatmaps  quality score  chromosomes  scaffold   reference genome build  five organisms    novo assemblies  na24385  xaxis depict  build  assemblies  yaxis indicate  ten quality score bin      color give   legend show  percentage   chromosomes  scaffold  fall   quality bin indicate   yaxis  quality score heatmaps   human  mouse  rat  chimpanzee   zebrafish  reference chromosomes  specify build   scaffold   novo assemblies  sample na24385   six different assemblies note     quality interval bin divide    range  nine equal size leftclosed rightopen intervals   single close interval     full size image similarly  find     clear segregation   quality score  chromosomes   earlier    recent versions   mouse reference genomes  present  fig    earlier versions    build mm2  mm6  quality score  chromosomes  concentrate     interval however majority   chromosomes  build mm7  quality score     bin    hand  ratio  medium  high quality chromosomes  chromosomes   quality higher    less   gradually increase  build mm8  mm9 finally      chromosomes   latest build mm10  quality score higher   overall   five organisms  chromosome quality score  dnaqet  higher   recent build  result  concordant   hypothesis  evidently demonstrate  dnaqet could  use  fairly evaluate quality  scaffold  contigs additionally  present  individual quality score   chromosomes   genome build   consider organisms  additional file  figure   final quality score compute  dnaqet   reference genome build   five organisms    novo assemblies  na24385  plot  fig   quality score    recent versions   genome build  consistently higher   older build except one case  quality score   zebrafish reference genome build danrer6   slightly smaller quality score   older build danrer5 examine  quality score  human reference genome build fig   observe   quality score almost monotonically increase    hg4     hg13   quality score  hg15 jump      quality score stabilize  around     recent build   hg19  keep  slow increase rate finally  quality score reach   hg38  dramatic quality shift  hg13  hg15  concurrent   introduction   first finish human genome assembly date april    result clearly show   later versions   human reference genome build  higher quality score  dnaqet    consistent   expectation   build quality increase   newly introduce build fig  figure4 quality score  reference genome build   novo assemblies  na24385  xaxis depict  build  assemblies  yaxis indicate quality score  dnaqet compute   reference genome build  human  mouse  rat  chimpanzee  zebrafish     novo assemblies  na24385   yellow bar show  final quality score   line give  area   cumulative scaffold quality score distribution curve  blue  area   cumulative genome coverage curve  light brown   invert duplication ratio  grey full size image   interest  note  dnaqet give  quality score    hg38 instead   perfect score   since hg38  score      possible  compute  perfect score  hg38 even    compare back   due   ambiguous base pair  contain    genome   misassembly observe due   repetitive sequence  may  cause misalignment since dnaqet take  artifacts due  alignment tool  consideration  compute  nearly perfect score  hg38 assembly   also important  note   quality score   latest build  rat chimpanzee  zebrafish genomes  remarkably higher     predecessor build  strike quality shift   rat build rn5  rn6 could  attribute   introduction  additional pacbio data   coverage   assembly process  similarly  zebrafish reference build danrer11   significantly higher quality score  danrer10 due  additional assemblies wgs29 caak00000000  wgs32 czqb00000000 add  necessary  fill  gap  finally  quality score   recently introduce chimpanzee genome build pantro6  much higher    pantro5    high depth  coverage  data use   assembly   threestage progressive assembly methodology  incorporate data  multiple platforms  since     tool   literature  could report  single quality score  unify multiple measure    novo assembly  could  directly compare  quality score  dnaqet compute   assembly build   tool consequently  decide  compare  rank   dnaqet quality score   rank  quastlg metrics   build quastlg could provide  quality metrics  compare multiple assemblies  except benchmarking universal singlecopy orthologs busco completeness  kmerbased completeness  metrics require  read use  generate  assemblies  remain  quastlg quality metrics  calculate  almost   reference genome build except metrics   na75 ng75 nga50 nga75 etc  could   compute     earlier build  zebrafish  mouse   consistent   comparison across multiple organisms metrics    report   reference build   organisms  discard result  metrics   report   reference genome build  quastlg  provide  clear rank   build without  tie see additional file  table s6s10  detail  next compute pearson correlation coefficients   rank provide     quality metrics   rank   build  hypothesis  determine  rank   build   recent  build   better   quality  thus  higher rank    result give  table  show   rank   dnaqet quality score always   highest pearson correlation coefficients   consistent   inherent rank dictate   hypothesis  result demonstrate   dnaqet quality score reflect  quality   genome build better      metrics due   sophisticate design  combine different aspects   assembly   single value table   pearson correlation coefficients   rank  quality score yield  dnaqet  quastlg   rank   reference genome build  hypothesis full size table although  genome fraction   quastlg achieve   performance   dnaqet quality score  term   rank  genome build  human mouse rat  chimpanzee dnaqet quality score outperform  genome fraction  metric  rank   build  zebrafish genome like  genome fraction  metric  single metric  quastlg table    able  reliably rank  genome build compare  dnaqet quality score  thus   suitable  assess quality   novo assemblies   whole  metrics focus   one side   assemblies  evaluate   examine    single perspective  instance  genome fraction   report  much   reference genome  cover    novo assembly without consider  quality   individual scaffold   reference genome build  total genome fraction  increase almost every time    recent build   may    case    novo assemblies  example one assembly might   slightly higher genome fraction   lower n50  na50 value  higher number  misassemblies  another assembly  another case  genome fraction   multiple assemblies may indicate  reverse rank   assemblies    rank  respect  another metric another example  demonstrate  inconsistency   metrics   poor performance  metrics   number  indels per 100kbp number  mismatch per 100kbp na50 la50    use  rank  chimpanzee genome build although  metrics rank  high pearson correlation coefficients  use  human mouse rat  zebrafish data  considerably fail  correctly rank chimpanzee genome build therefore assess  quality   assembly use one metric  quastlg could  provide sufficient information   overall quality  see  clear need   quality metric  unify  multiple crucial metrics   single quality value    use  reliably assess  quality   assembly  need  meet   meticulously design quality score formulation  dnaqet  unite multiple metrics   single metric quality  six na24385 assemblies  quality score distributions   contigs   six assemblies  plot   fig  none   six assemblies   contigs   quality score higher    mecat assemblies generate     coverage data  contigs  quality score higher     percentage   contigs      perspective none   real data assemblies   outstanding performance  term   contig quality score   interest  note   best perform assemblies  generate  mecat   data      coverages      contigs   assemblies  quality score higher    contrast  mecat falcon  celera assemblies       contigs  quality score higher   respectively  overall quality score compute  dnaqet   six  novo assemblies  present  fig   assemblies generate  mecat assembler  high coverage data      best quality score   celera assembly also   quality score     slightly lower   score   lead assemblies  assembly generate  mecat use  coverage   quality score    follow   assembly  falcon  quality score    lowest quality score    obtain  mecat assembly create   coverage data    surprise    generate    low coverage data  also evaluate  assemblies use quastlg  compute quality metrics  present  additional file  table s11  mention     metrics produce multiple rank   assemblies  contradict     instance use na50 metric  assemblies  rank  mecat70x mecat50x falcon celera mecat25x  mecat5x   decrease order  higher  na50 better  assembly however  rank  completely change  celera falcon mecat50x mecat70x mecat25x  finally mecat5x  genome fraction   use   rank metric   decrease order  number  misassemblies metric  use  order   rank  mecat5x mecat25x mecat70x mecat50x celera  falcon   increase number  misassemblies  lower  misassembly better  assembly thus   evident   result     need  consolidate  metrics   single value    dnaqet  fairly evaluate  overall quality   assemblies since    inherent rank   real data assemblies available  compare  rank  quastlg metrics    dnaqet score  use  approach propose    infer  reliable rank  would  consider closest   grind truth   study  author calculate  zscores   metric   assemblies  consideration  sum  score  finally rank  assemblies base   sum zscores   call  zscorebased rank  obtain  grind truth rank   assemblies  apply  similar approach   metrics  give  clear rank   assemblies without  tie   commonly report   assemblies compute  quastlg  calculate  zscore   metric  sum  zscores   assembly  present  additional file  table s11 accord   zscorebased rank  rank   assemblies  mecat70x mecat50x mecat25x celera falcon  mecat5x  decrease order  higher  zscore better  assembly  pearson correlation coefficient  dnaqet quality score rank   assemblies   zscorebased rank   whereas  quastlg metrics namely largest contig n50 ng50 n75 l50 lg50 l75 largest alignment nga50  na75 provide  secondhigh correlation coefficient value     result indicate  dnaqets quality score outperform   metrics base   zscorebased rank runtime  peak memory usage  compare  runtime  memory performances  dnaqet    quastlg use  human reference genome build   six  novo assemblies  sample na24385   benchmarking test     server   gigabytes  memory run  core  intel xeon   50ghz cpus   able  fairly evaluate  performance  dnaqet  quastlg  partition neither  reference genome   input genome build   evaluate  dnaqet  quastlg  run use  thread   single server  minimap2  use   alignment tool  runtime  peak memory usage  dnaqet  quastlg   human reference genome build    novo assemblies  present  fig  clearly  runtime  dnaqet   stable across   human reference genome build   take     worst case  dnaqet  process  human reference genome build    hand quastlg perform poorly   earlier build fig   example quastlg take approximately    finish process build hg4 quastlg perform better    recent build   runtime  decrease  approximately      recent build   six na24385 assemblies dnaqet  faster  quastlg  process  assemblies fig   take    dnaqet  process  celera assembly    slowest performance  dnaqet   assemblies    hand quastlgs best runtime     mecat5x assembly fig  figure5 runtime  peak memory usage  dnaqet blue bar  quastlg orange bar runtimes  process  human reference genome build    six assemblies  na24385   plot   bar leave yaxis  peak memory usages  gigabytes  plot   line right yaxis  dnaqet gray line  quastlg yellow line full size image  term   peak memory usage  tool use    gigabytes  memory  analyze  human reference build  dnaqets memory usage increase  hg8  hg10 reach   gigabytes  average  stabilize   number   later build however quastlg   steady peak memory usage   gigabytes  build hg4  hg13  start  hg15  peak memory usage go  reach  gigabytes  hg38  na24385 assemblies  peak memory usage  dnaqet stay stable  around  gigabytes whereas quastlgs memory usage  stable  around  gigabytes  result demonstrate dnaqet outperform quastlg  term  execution time   tool  comparable peak memory usage",12
224,Laser,"LASER: Large genome ASsembly EvaluatoR
The most time consuming stage of QUAST is, by far, the maximal exact match (MEM) computation step of the alignment process, performed using the NUCmer aligner from MUMmer v3.23 [18]. Our recent E-MEM tool [19] clearly outperforms not only MUMmer but also the currently best tools for MEM computation in large genomes: [20–24]. It was therefore a natural choice for replacing MUMmer. Besides using E-MEM, we performed a number of other improvements as well. A large number of redundant string copy operations on large strings in the ‘show-snp’ utility program of the MUMmer toolkit have been avoided. The memory and performance of Python code was improved by replacing class objects with tuples. The rest of QUAST code has been reused in LASER. MUMmer and GlimmerHMM [25] are open source and the authors of GeneMarkS [26] have kindly allowed us to use their code in LASER. Go to: Results As mentioned before, all features of QUAST have been preserved and LASER has been designed to be used exactly the same way as QUAST. That is, LASER produces exactly the same output. The advantage of LASER consists of greatly increased speed and reduced memory usage. To prove these claims, we have compared LASER and QUAST on several datasets, presented in Table 1. As we are interested in improvement when it really matters, that is, for large genomes, all datasets are human. They were all produced by Illumina HiSeq2000 machines. All datasets were assembled using SOAPdenovo2 [6]. We used SOAPdenovo2 because of its good speed. The k-mer size producing the best assembly (as indicated by the aligned N50 size) was used. This was k = 65 for H1 and k = 71 for the other datasets. The assemblies are available for download from the website of LASER. Table 1 The datasets used for comparison; accession numbers are included for the datasets and for the corresponding reference genomes Dataset        Organism        Accession number        Read length        Number of reads        Total bp        Depth of coverage        Reference genome        Genome length H1        Homo sapiens        SRR1302280        101        1,287,175,558        130,004,731,358        41        Build 38        3,209,286,105 H2        Homo sapiens        ERR194146        101        1,626,361,156        164,262,476,756        51        Build 38        3,209,286,105 H3        Homo sapiens        ERR194147        101        1,574,530,218        159,027,552,018        50        Build 38        3,209,286,105 H4        Homo sapiens        ERR324433        101        1,614,713,636        163,086,077,236        51        Build 38        3,209,286,105 H5        Homo sapiens        ERX069505        101        1,708,169,546        172,525,124,146        54        Build 38        3,209,286,105 Open in a separate window All tests were performed on a DELL PowerEdge R620 computer with 12 cores Intel Xeon at 2.0'GHz and 256 GB of RAM, running Linux Red Hat, CentOS 6.3. Figure 1 gives the time and memory comparison between QUAST and LASER on the SOAPdenovo2 assemblies produced from the datasets in Table 1. LASER is 5.6 times faster than QUAST while using half the memory. An external file that holds a picture, illustration, etc. Object name is 13104_2015_1682_Fig1_HTML.jpg Fig. 1 Comparison. Visual comparison of the time (left plot) and memory (right plot) between QUAST and LASER Go to: Conclusions We hope that the improvement in genome assembly evaluation provided by LASER will further boost the use of thorough quality evaluation. N50 is still used as the most important parameter. (N50 is the length l such that the sum of the lengths of all contigs of length l or more is at least half of the total length of all contigs.) An aggressive assembler will produce a high N50 but at the cost of many misassemblies, thus lowering the overall quality. Therefore, a combination of parameters, as provided by QUAST or LASER, gives a much better evaluation of the actual assembly quality.",AssemblyEvaluation,"laser large genome assembly evaluator
  time consume stage  quast   far  maximal exact match mem computation step   alignment process perform use  nucmer aligner  mummer    recent emem tool  clearly outperform   mummer  also  currently best tool  mem computation  large genomes    therefore  natural choice  replace mummer besides use emem  perform  number   improvements  well  large number  redundant string copy operations  large string   showsnp utility program   mummer toolkit   avoid  memory  performance  python code  improve  replace class object  tuples  rest  quast code   reuse  laser mummer  glimmerhmm   open source   author  genemarks   kindly allow   use  code  laser   result  mention   feature  quast   preserve  laser   design   use exactly   way  quast   laser produce exactly   output  advantage  laser consist  greatly increase speed  reduce memory usage  prove  claim   compare laser  quast  several datasets present  table     interest  improvement   really matter    large genomes  datasets  human    produce  illumina hiseq2000 machine  datasets  assemble use soapdenovo2   use soapdenovo2    good speed  kmer size produce  best assembly  indicate   align n50 size  use               datasets  assemblies  available  download   website  laser table   datasets use  comparison accession number  include   datasets    correspond reference genomes dataset        organism        accession number        read length        number  read        total         depth  coverage        reference genome        genome length         homo sapiens        srr1302280                                        build                  homo sapiens        err194146                                        build                  homo sapiens        err194147                                        build                  homo sapiens        err324433                                        build                  homo sapiens        erx069505                                        build          open   separate window  test  perform   dell poweredge r620 computer   core intel xeon  'ghz     ram run linux red hat centos  figure  give  time  memory comparison  quast  laser   soapdenovo2 assemblies produce   datasets  table  laser   time faster  quast  use half  memory  external file  hold  picture illustration etc object name  13104_2015_1682_fig1_htmljpg fig  comparison visual comparison   time leave plot  memory right plot  quast  laser   conclusions  hope   improvement  genome assembly evaluation provide  laser   boost  use  thorough quality evaluation n50  still use    important parameter n50   length     sum   lengths   contigs  length      least half   total length   contigs  aggressive assembler  produce  high n50    cost  many misassemblies thus lower  overall quality therefore  combination  parameters  provide  quast  laser give  much better evaluation   actual assembly quality",12
